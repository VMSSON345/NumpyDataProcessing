number,state,contributor,content,created_at,closed_at,title,labels
27120,open,seberg,"In many places we still use `PyObject_GetAttrString` or `PyObject_GetAttr`.  It would be good to:
1. Ensure we use `PyObject_GetAttr` with an interned string wherever the lookup may be in a performance relevant code branch.  Exceptions are for example:
   - Anything that happens only once, e.g. import time.
   - It may not be worth making e.g. error paths more complicated just to use this. 
3. Use `PyObject_GetOptionalAttr` whenever a possible `AttributeError` is ignored (or all exceptoins, although it's best not to ignore all exceptions).

Creating the unicode objects isn't that fast, I think.  And `PyObject_GetOptionalAttr` can be massively faster if the attribute isn't found because it can avoid the error creation.",2024-08-06 14:11:17,,"PERF,TASK: Audit `PyObject_GetAttr` usages",['sprintable - C']
27109,open,HaoZeke,"Somehow I missed the work on `numpy-config` until I came across https://github.com/mesonbuild/meson/pull/12799 and https://github.com/mesonbuild/meson/pull/12891.

F2PY should also be accessible from `numpy-config` (and then by extension it simplifies the generated `meson.build` files).",2024-08-04 19:26:09,,ENH: Add support for F2PY headers in `numpy-config`,['component: numpy.f2py']
27086,open,jorenham,"### Proposed new feature or change:

Curently, it's allowed to pass some `typing.NamedTuple` instance to e.g. `numpy.empty` or `numpy.zeros`, but somewhere along the way, it gets converted to a vanilla `tuple`:

```pycon
>>> from typing import NamedTuple
>>> import numpy as np
>>> class Shape2D(NamedTuple):
...     height: int
...     width: int
... 
>>> x = np.zeros(Shape2D(width=22, height=7))
>>> type(x.shape)
<class 'tuple'>
```

I think it would be cool if you could have `ndarray.shape` be a named tuple. This way, you wouldn't have to remember which of the shape indices corresponds to the width or height, and use e.g. `x.shape.width` instead.

From a typing perspective, it also makes sense to have the `ShapeType` parameter of `ndarray[ShapeType: tuple[int, ...], DType]` match `ndarray.shape: ShapeType`, in case it isn't exactly a `tuple` but a subtype like `NamedTuple`.",2024-07-31 00:37:19,,ENH: Allow ``ndarray.shape`` to be a named tuple,['01 - Enhancement']
27082,open,seberg,"The power fast-paths use checks like `scalar == 2` to call `square`.  Which was more correct before NEP 50 (but I am sure there were bugs also before), but less so after, if you pass in something like `np.int64(2)` and the array is, say `int32`.

That logic is a bit confusing and diverges it from the ufunc.",2024-07-30 12:58:50,,BUG: Power fast-paths have wrong and confusing promotion logic,['00 - Bug']
27042,open,Zeutschler,"### Proposed new feature or change:

### Motivation
Modern analytical data processing often requires fast and efficient set operations (esp. intersection and merging) on unique and sorted sets/arrays, mainly being row-indexes/-masks of datasets or vectors. As of today, Numpy is not yet optimized for such increasingly relevant use cases. Better performance will lead to saved time, cost and energy - good for everyone.

### Benefits
**Performance:** Additional optimized implementations for set operations on sorted sets/arrays, indicated by an additional ""assume_sorted=True"" (default: False) attribute for the respective methods, will increase sorted-set operation performance by at least ±10x times up to ±1000x and more times for many scenarios, esp. those typical for data processing, where one set is rather small, and the other set is rather large and many consecutive set operations are performed, e.g. OLAP-style queries and point-queries.

**Simplicity:** This would make the use of common 3rd party acceleration methods. e.g. like using Numba, Cython, RoaringBitmaps, sortednp or DuckDB more or less redundant. In addition, subsequent libraries like Pandas could benefit from these improvements more or less 1:1, e.g. for filtering.

**Competitiveness:** Numpy and Pandas are currently loosing relative market share to new and much faster data procession libraries like DataTable, DuckDB, Polars, Vaex etc. The proposed extension could greatly contribute to improve Numpy's competitiveness by faster execution on typical data processing tasks. 

### Implementation
As a template for a reference implementation, the Pypi package [sortednp](https://pypi.org/project/sortednp/) could be used. This library by Frank Sauerburger was built to exactly solve the mentioned performance problems for sorted arrays/sets.

### Sample code to showcase potential performance gains
`
from timeit import timeit
import numpy as np
import sortednp as snp  # see https://pypi.org/project/sortednp/


print(""Numpy performance comparison for set operation 'intersection' ""
      ""actual vs. optimized for sorted sets:\n"")

sizes = [10**x for x in range(2, 8)]
for size in sizes:
    large = np.arange(size)
    small_size = size

    while small_size > 1:
        small = np.sort(np.random.choice(large, small_size, replace=False))
        t_numpy = timeit(lambda: np.intersect1d(small, large), number=10)
        t_optimized = timeit(lambda: snp.intersect(small, large), number=10)
        small_size //= 10

        print(f""Intersect(a[{small_size:,}], b[{size:,}]) actual in {t_numpy:.4f} seconds, ""
              f""optimized in {t_optimized:.4f} seconds.\n\t""
              f""...potential performance improvement {t_numpy / t_optimized:,.0f}x times."")
`




  ",2024-07-25 18:43:06,,"""assume_sorted"" attribute for efficient/fast Numpy set operations (intersect, merge etc.).",['unlabeled']
27038,open,e-pet,"### Issue with current documentation:

I was trying to convert a string '3' to `np.dtypes.Int64DType`. 

My first attempt was `np.dtypes.Int64DType('3')`, hoping that the dtype would be callable which it is not.

Then followed these attempts:
```python
dt = numpy.dtypes.Int64DType
dt('3')  # I was somewhat hoping this would work but the dtype is not callable
np.fromstring('3', dtype=dt)  # ValueError: string size must be a multiple of element size
np.fromstring('3', dtype=dt, count=1)  # ValueError: string is smaller than requested size
np.fromstring('[3]', dtype=dt, count=1)  # ValueError: Cannot create an object array from a string
```

For the latter three calls, I also got a `<string>:1: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead` - which I did not understand, because
- '3' is not a binary string
- I did not see a ""binary mode"" flag or anything of the sort

It took me a long time to realize that the deprecated *default optional argument* `sep=''` was causing this behavior. I personally find this highly unintuitive and surprising. (I did not look at this argument at all because I was interested in the scalar case.)

Indeed, `np.fromstring('3', dtype=int, sep=' ')`at least works.

However,  `np.fromstring('3', dtype=np.dtypes.Int64DType, sep=' ')`gives `ValueError: Cannot create an object array from a string` - which I again find surprising.

### Idea or request for content:

At the very least, I would suggest adding a note to the deprecation warning that the binary mode is triggered by the default optional argument `sep=''`, and that passing any other value to `sep` will fix the issue.

In addition, I would suggest adding some comment to the documentation on why passing things like `dtype=numpy.dtypes.Int64DType` does not work.

In addition, I would suggest adding a comment that `np.array(str, dtype=...)` might be what one is looking for.",2024-07-25 13:17:03,,DOC: Improve fromstring documentation & errors/warnings regarding binary mode,['04 - Documentation']
27035,open,champystile,"### Proposed new feature or change:

Today:
When using `numpy.linalg.inv` on a stack of matrix (shape=(m,n,n)), if one in the stack is singular, a `LinAlgError` is raised and all inverse matrices are discarded (even if they are all computed).
If you want to get the inverse matrix of invertible ones, you have to detect the good one first, then execute `inv` on them. As the algorithm used to determine the singular matrices is not clear, I don't know how to be 100% accurate.

Expected:
`numpy.linalg.inv` uses the `_umath_linalg.inv` C function that compute every inverse, with nan matrix for the singular ones. An error flag is raised, catch by a `with errstate...` block. This errstate cannot be override.
To correct this, there could be an additional argument (ie `noerr`) that doesn't ""activate"" the errstate.
The user will have to check after which matrices was not invertible, by checking the presence of nan: 
`np.isnan(ainv).any((1,2))`

One example of a calling to `_umath_linalg.inv` whithout errors can be found in the `cond` function.",2024-07-25 09:08:30,,ENH: Allow disabling error for `numpy.linalg.inv` when one matrix is singular in a stack,"['01 - Enhancement', 'component: numpy.linalg']"
27032,open,jorenham,"### Describe the issue:

The commonly used `@overload` pattern with `_ArrayLike{}_co` parameter types can lead to unexpected type inferences if the return types are incompatible with one another other.

An example of this is the `numpy.dtype` constructor:
It correctly infers `dtype(bool)` and `dtype(int)` as `dtype[np.bool]` and `dtype[signedinteger[Any]]`, respectively.
But in the case that either `bool` or `int` can be passed, e.g as `dtype(int_co)` with  `int_co: type[bool] | tuple[int]`, the resulting type is inferred as `dtype[signedinteger[Any]]`, which is incompatible with `dtype[bool]`.
The correct type in this case would be `dtype[bool | signedinteger[Any]]`.

### Reproduce the code example:

```python
# overlapping_overload_issue.pyi

import numpy as np

x: type[bool]
y: type[int]
z: type[bool] | type[int]

reveal_type(np.dtype(x))  # OK: dtype[bool]
reveal_type(np.dtype(y))  # OK: dtype[signedinteger[Any]]
reveal_type(np.dtype(z))  # WRONG: dtype[signedinteger[Any]]
```

But the last revealed type doesn't include `bool`, i.e. it should have been `dtype[bool | signedinteger[Any]]`.

This isn't limited to only `bool` and `int`; all possible non-trivial unions of `bool`, `int`, `float` or `complex` have this problem, see  https://typing.readthedocs.io/en/latest/spec/special-types.html#special-cases-for-float-and-complex

And this also isn't limited to `dtype.__new__`:
The same issue can be observed with the arithmetic operator methods (e.g. `__add__`) of `numpy.ndarray`, `numpy.bool`, and all subclasses of `numpy.number`, just to name a few.

### Error message:

With Pyright / Pylance, enabling the [`reportOverlappingOverload`](https://github.com/microsoft/pyright/blob/main/docs/configuration.md#reportOverlappingOverload) setting will in such cases result in an error message (unless there's a `# type: ignore` comment), e.g. for `numpy.dtype.__new__` Pylance reports:

```
Overload 1 for ""__new__"" overlaps overload 51 and returns an incompatible type
Overload 2 for ""__new__"" overlaps overload 3 and returns an incompatible type
Overload 2 for ""__new__"" overlaps overload 4 and returns an incompatible type
Overload 2 for ""__new__"" overlaps overload 5 and returns an incompatible type
```

### Python and NumPy Versions:

2.1.0.dev0+git20240724.3df4283
3.10.12 (main, Mar 22 2024, 16:50:05) [GCC 11.4.0]

### Type-checker version and settings:

basedpyright 1.15.0 (pyright 1.1.373)

### Additional typing packages.

typing-extensions 4.12.2",2024-07-24 17:58:48,,TYP: Overlapping overloads with incompatible return types,"['00 - Bug', 'Static typing']"
27029,open,gerritholl,"### Describe the issue:

Multiplying a masked array by 2 causes an unexpected type promotion from float32 to float64 or from int32 to int64.  The same does not happen for regular arrays.

### Reproduce the code example:

```python
import numpy as np

arr = np.array([0, 1], dtype=""float32"")
marr = np.ma.masked_array(arr)
print((arr * 2).dtype)
print((marr * 2).dtype)

arr = np.array([0, 1], dtype=""int32"")
marr = np.ma.masked_array(arr)
print((arr * 2).dtype)
print((marr * 2).dtype)
```


### Error message:

No traceback, but unexpected output:

```
float32
float64
int32
int64
```


### Python and NumPy Versions:

2.0.0
3.12.0 | packaged by conda-forge | (main, Oct  3 2023, 08:43:22) [GCC 12.3.0]

### Runtime Environment:

```
[{'numpy_version': '2.0.0',
  'python': '3.12.0 | packaged by conda-forge | (main, Oct  3 2023, 08:43:22) '
            '[GCC 12.3.0]',
  'uname': uname_result(system='Linux', node='oflws222', release='5.3.18-150300.59.106-default', version='#1 SMP Mon Dec 12 13:16:24 UTC 2022 (774239c)', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL',
                                    'AVX512_SPR']}},
 {'architecture': 'Haswell',
  'filepath': '/data/gholl/mambaforge/envs/py312/lib/libopenblasp-r0.3.27.so',
  'internal_api': 'openblas',
  'num_threads': 8,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.27'}]
```

### Context for the issue:

I'm not sure if I understand fully the new rules of type promotion, but I would expect consistency between regular arrays and masked arrays.",2024-07-24 14:15:24,,BUG: unexpected type promotion multiplying a masked array with an integer,"['00 - Bug', 'component: numpy.ma']"
27018,open,1234zou,"### Describe the issue:

I'm trying to compile my Fortran code using `f2py`. For python\>=3.12, the meson backend is now the default. But meson/f2py cannot automatically find the generated .mod files in the current directory. These .mod files could be automatically found for older versions of python and distutils backend. By the way, gcc/gfortran in this machine is 4.8.5.

Thanks for any kind help and suggestion!

Best,
Jingxiang

### Reproduce the code example:
A simple example is shown using two Fortran files and one Python script:

here is a.f90
```
module para
 implicit none
 integer, parameter :: n = 5
end module para
```

b.f90
```
subroutine prt_n
 use para, only: n
 implicit none
 write(6,'(A,I2)') 'n=', n
end subroutine prt_n
```

Compiling commands are
```
gfortran a.f90 -c
f2py -c a.o b.f90 -m test
```
The first command would generate `a.o` and `para.mod` files in the current directory. `para.mod` can be automatically detected for the distutils backend, but not for the meson backend.

test.py
```python
from test import prt_n
prt_n()
```
Assuming that the test.xxx.so is generated (which not True for the meson backend), the result would be `n= 5`.


### Error message:
After running `f2py -c a.o b.f90 -m test`, the error message is
```
Found ninja-1.10.2 at /public/home/jxzou/software/anaconda3/envs/py312/bin/ninja
INFO: autodetecting backend as ninja
INFO: calculating backend command to run: /public/home/jxzou/software/anaconda3/envs/py312/bin/ninja -C /public/home/jxzou/tmp/tmpgljbyueg/bbdir
ninja: Entering directory `/public/home/jxzou/tmp/tmpgljbyueg/bbdir'
[3/6] Compiling Fortran object test.cpython-312-x86_64-linux-gnu.so.p/b.f90.o
FAILED: test.cpython-312-x86_64-linux-gnu.so.p/b.f90.o
gfortran -Itest.cpython-312-x86_64-linux-gnu.so.p -I. -I.. -I/public/home/jxzou/software/anaconda3/envs/py312/lib/python3.12/site-packages/numpy/core/include -I/public/home/jxzou/software/anaconda3/envs/py312/lib/python3.12/site-packages/numpy/f2py/src -I/public/home/jxzou/software/anaconda3/envs/py312/include/python3.12 -fvisibility=hidden -D_FILE_OFFSET_BITS=64 -Wall -O3 -fPIC -Jtest.cpython-312-x86_64-linux-gnu.so.p -o test.cpython-312-x86_64-linux-gnu.so.p/b.f90.o -c ../b.f90
../b.f90:2.5:

 use para, only: n
     1
Fatal Error: Can't open module file 'para.mod' for reading at (1): No such file or directory
[5/6] Compiling C object test.cpython-312-x86_64-linux-gnu.so.p/3fa16a5cf62c3f6199e0fa73eb3f172c17c452f1_.._.._f2py_src_fortranobject.c.o
ninja: build stopped: subcommand failed.
Traceback (most recent call last):
  File ""/public/home/jxzou/software/anaconda3/envs/py312/bin/f2py"", line 11, in <module>
    sys.exit(main())
             ^^^^^^
  File ""/public/home/jxzou/software/anaconda3/envs/py312/lib/python3.12/site-packages/numpy/f2py/f2py2e.py"", line 766, in main
    run_compile()
  File ""/public/home/jxzou/software/anaconda3/envs/py312/lib/python3.12/site-packages/numpy/f2py/f2py2e.py"", line 738, in run_compile
    builder.compile()
  File ""/public/home/jxzou/software/anaconda3/envs/py312/lib/python3.12/site-packages/numpy/f2py/_backends/_meson.py"", line 178, in compile
    self.run_meson(self.build_dir)
  File ""/public/home/jxzou/software/anaconda3/envs/py312/lib/python3.12/site-packages/numpy/f2py/_backends/_meson.py"", line 173, in run_meson
    self._run_subprocess_command(compile_command, build_dir)
  File ""/public/home/jxzou/software/anaconda3/envs/py312/lib/python3.12/site-packages/numpy/f2py/_backends/_meson.py"", line 167, in _run_subprocess_command
    subprocess.run(command, cwd=cwd, check=True)
  File ""/public/home/jxzou/software/anaconda3/envs/py312/lib/python3.12/subprocess.py"", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['meson', 'compile', '-C', 'bbdir']' returned non-zero exit status 1.
```


### Python and NumPy Versions:

python\>=3.12 and numpy\>=1.26

### Runtime Environment:

```
[{'numpy_version': '1.26.4',
  'python': '3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, '
            '15:12:24) [GCC 11.2.0]',
  'uname': uname_result(system='Linux', node='mu003', release='3.10.0-862.el7.x86_64', version='#1 SMP Fri Apr 20 16:44:24 UTC 2018', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2',
                                'AVX512F',
                                'AVX512CD',
                                'AVX512_SKX',
                                'AVX512_CLX',
                                'AVX512_CNL',
                                'AVX512_ICL'],
                      'not_found': ['AVX512_KNL', 'AVX512_KNM']}},
 {'filepath': '/public/home/jxzou/software/anaconda3/envs/py312/lib/libmkl_rt.so.2',
  'internal_api': 'mkl',
  'num_threads': 64,
  'prefix': 'libmkl_rt',
  'threading_layer': 'intel',
  'user_api': 'blas',
  'version': '2023.1-Product'},
 {'filepath': '/public/home/jxzou/software/anaconda3/envs/py312/lib/libiomp5.so',
  'internal_api': 'openmp',
  'num_threads': 64,
  'prefix': 'libiomp',
  'user_api': 'openmp',
  'version': None}]
```

### Context for the issue:

I'm a developer of the open source package [MOKIT](https://gitlab.com/jxzou/mokit). By using `f2py`, we offer lots of convenient and efficient Python APIs to users in the computational chemistry field. We are grateful for your numpy f2py functionalities. MOKIT source code are mainly written using Fortran, so we heavily reply on the `f2py` to obtain corresponding .so files.

Currently my workaround is to change all `.o` filenames into `.f90` ones in f2py commands, e.g.
```
f2py -c a.f90 b.f90 -m test
```
This is a tiny change for this simple example. But a somewhat big work to do in MOKIT. And this change would make `test.so` contain more APIs than previous. For example, if there are many subroutines in `a.f90`, it will be all embraced into `test.so`. This problem does not exist when the distutils backend is used previously. I do not know whether this is a feature or bug for the new meson backend. I'm wondering if there is any elegant solution.",2024-07-23 17:26:06,,BUG: f2py cannot find .mod files automatically for the new meson backend,"['00 - Bug', '04 - Documentation', 'component: numpy.f2py']"
27007,open,me9hanics,"### Describe the issue:

I'm using arrays with specified datatypes. I noticed that sorting such an array with `np.sort(..., order='id')`seems to be very slow. This comes as a surprise, as if we just sort the ID column separately with `np.sort` it is handled fastly, and no other column is relevant for the sorting, therefore using `order` here should be comparably fast to just sorting a 1D array.

If I run `np.sort` with order by column as in the example, it takes around ~20 seconds on my machine. Compared to just sorting the column, which runs in 1/100th of the time, this seems unreasonably slow. One could argue that it is more complicated to sort multiple column and datatype arrays, but this way it'd be faster to just take the order column, sort it, get the permutation and order all columns with the same permutation.

I used numpy 2.0.1 to test it (Python 3.12.4), but it is an issue with older versions too, e.g. 3.12.4.

### Reproduce the code example:

```python
import numpy as np
import time
dtypes = [('x', 'u1'), ('id', '<u8')]
x_values = [n % 2 for n in range(5000000)]
ids = list(range(5000000, 0, -1))
x = np.array(list(zip(x_values, ids)), dtype=dtypes)

t1 = time.time()
multi_dtype_array_sort = np.sort(x, order='id')
t2 = time.time()
time_1 = t2 - t1

t1 = time.time()
sort_1D = np.sort(x['id'])
t2 = time.time()
time_2 = t2 - t1

print(time_1, time_2)
```


### Error message:

_No response_

### Python and NumPy Versions:

Python: 3.12.4
Numpy: 2.0.1


### Runtime Environment:

_No response_

### Context for the issue:

(I'm partially working with embedded code, hence the specified datatypes, these come from outputs of a sensor. I need to sort the array by the ID column.)",2024-07-22 10:35:21,,BUG: sort() with order slow on multi dtype arrays (sorting by unsigned integer ID),['00 - Bug']
27006,open,ndgrigorian,"With Numpy 2 I've observed that `where` seems to behave in a (now strange) way with out-of-bounds Python integers compared to other functions.

The first argument of `where` does not participate in type promotion, so if using a Python integer as `x1` or `x2` and an array as the other argument, the output should be an array with the data type of the Numpy array argument.

However, unlike every ufunc with binary type promotion, `where` permits an out-of-bounds Python integer.

For example,
```
In [24]: np.where(np.asarray([True, False], dtype=""?""), -1, np.asarray(0, dtype=""u1""))
Out[24]: array([255,   0], dtype=uint8)
```

where for any other function
```
In [28]: np.add(np.asarray(0, dtype=""u1""), -1)
---------------------------------------------------------------------------
OverflowError                             Traceback (most recent call last)
Cell In[28], line 1
----> 1 np.add(np.asarray(0, dtype=""u1""), -1)

OverflowError: Python integer -1 out of bounds for uint8

In [29]: np.clip(np.asarray(0, dtype=np.uint8), -1, np.asarray(0, dtype=np.uint8))
---------------------------------------------------------------------------
OverflowError                             Traceback (most recent call last)
...
OverflowError: Python integer -1 out of bounds for uint8
```

Given that not even assignment now permits out-of-bounds Python integers, it seems like an oversight.",2024-07-22 09:26:29,,`where` with out-of-bounds Python integers in Numpy 2,['01 - Enhancement']
26977,open,ganesh-k13,"### Describe the issue:

I'm trying to upgrade spin to 0.11 to pull in my changes and got the error pasted below


```python
~/os/numpy (main*) » spin test                                                                                                                                                                                  ganesh@ganesh-MS-7B86
Invoking `build` prior to running tests:
$ /home/ganesh/os/np-test/bin/python vendored-meson/meson/meson.py compile -C build
INFO: autodetecting backend as ninja
INFO: calculating backend command to run: /home/ganesh/os/np-test/bin/ninja -C /home/ganesh/os/numpy/build
ninja: Entering directory `/home/ganesh/os/numpy/build'
[1/1] Generating numpy/generate-version with a custom command
Saving version to numpy/version.py
$ /home/ganesh/os/np-test/bin/python vendored-meson/meson/meson.py install --only-changed -C build --destdir ../build-install
$ export PYTHONPATH=""/home/ganesh/os/numpy/build-install/usr/lib/python3/dist-packages""
$ export PYTHONPATH=""/home/ganesh/os/numpy/build-install/usr/lib/python3/dist-packages""
$ /home/ganesh/os/np-test/bin/python -c 'import sys; del sys.path[0]; import numpy'
$ cd /home/ganesh/os/numpy/build-install/usr/lib/python3/dist-packages
$ pytest --import-mode=importlib --pyargs numpy -m 'not slow'
ImportError while loading conftest '/home/ganesh/os/numpy/build-install/usr/lib/python3/dist-packages/numpy/conftest.py'.
numpy/__init__.py:121: in <module>
    from . import _core
numpy/_core/__init__.py:23: in <module>
    from . import multiarray
numpy/_core/multiarray.py:11: in <module>
    from . import _multiarray_umath
E   RuntimeError: CPU dispatcher tracer already initlized

```

`git clean -xdf` followed by spin test, does not solve the issue. I could not debug further than this.
The error is in this line: https://github.com/numpy/numpy/blob/b46446e58386beb3b55633421cea069fb1a555dd/numpy/_core/src/common/npy_cpu_dispatch.c#L12
The issue is not in 0.8:
- 0.10 gives circular import error:
```
~/os/numpy (main*) » spin test                                                                                                                                              ganesh@ganesh-MS-7B86
Invoking `build` prior to running tests:
$ /home/ganesh/os/np-test/bin/python vendored-meson/meson/meson.py compile -C build
INFO: autodetecting backend as ninja
INFO: calculating backend command to run: /home/ganesh/os/np-test/bin/ninja -C /home/ganesh/os/numpy/build
ninja: Entering directory `/home/ganesh/os/numpy/build'
[1/1] Generating numpy/generate-version with a custom command
Saving version to numpy/version.py
$ /home/ganesh/os/np-test/bin/python vendored-meson/meson/meson.py install --only-changed -C build --destdir ../build-install
$ export PYTHONPATH=""/home/ganesh/os/numpy/build-install/usr/lib/python3/dist-packages""
$ export PYTHONPATH=""/home/ganesh/os/numpy/build-install/usr/lib/python3/dist-packages""
$ /home/ganesh/os/np-test/bin/python -c 'import sys; del sys.path[0]; import numpy'
$ pytest --pyargs numpy -m 'not slow'
ImportError while loading conftest '/home/ganesh/os/numpy/numpy/conftest.py'.
numpy/__init__.py:97: in <module>
    from . import version
E   ImportError: cannot import name 'version' from partially initialized module 'numpy' (most likely due to a circular import) (/home/ganesh/os/numpy/numpy/__init__.py)
(np-test) ------------------------------------------------------------------------------------------------------------------------------------------------------------------------
~/os/numpy (main*) » spin --version                                                                                                                                     4 ↵ ganesh@ganesh-MS-7B86
spin 0.10

```

- 0.9 gives: 'NoneType' object has no attribute 'dir_info';



### Error message:

_No response_

### Python and NumPy Versions:

`Python 3.10.12`

No NumPy version, as it's a build issue. Built on `HEAD` of `main`

### Runtime Environment:

```
lsb_release -a                                                                                                                                                                         4 ↵ ganesh@ganesh-MS-7B86
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 22.04.4 LTS
Release:        22.04
Codename:       jammy
```

### Context for the issue:

_No response_

### Edits
1. Added verbose 0.11 error message
2. Added system information
3. Added 0.10 error message",2024-07-18 10:28:22,,BUG: Issue in upgrading Spin to 0.11,"['00 - Bug', '32 - Installation', 'Meson']"
26973,open,kconnour,"### Issue with current documentation:

I have a project that I can no longer compile with f2py, so I'm attempting to get a simple example working in hopes of getting my more complicated project to compile again. Specifically, I'm trying to follow the example provided here: https://numpy.org/doc/stable/f2py/buildtools/meson.html. However, line 2 of the code I should execute `meson setup builddir` throws an error, saying that I'm attempting to provide an absolute path to meson when it must only accept relative paths. 

The relevant part of the error in the meson-log.txt is as follows:

> Running command: /home/kconnour/repos/meson_numpy_test/.venv/bin/python -c 'import os; os.chdir(""..""); import numpy.f2py; print(numpy.f2py.get_include())'
> --- stdout ---
> /home/kconnour/repos/meson_numpy_test/.venv/lib/python3.12/site-packages/numpy/f2py/src
> 
> --- stderr ---
> 
> 
> 
> meson.build:24:9: ERROR: Tried to form an absolute path to a dir in the source tree.
> You should not do that but use relative paths instead, for
> directories that are part of your project.
> 
> To get include path to any directory relative to the current dir do
> 
> incdir = include_directories(dirname)
> 
> After this incdir will contain both the current source dir as well as the
> corresponding build dir. It can then be used in any subdirectory and
> Meson will take care of all the busywork to make paths work.
> 
> Dirname can even be '.' to mark the current directory. Though you should
> remember that the current source and build directories are always
> put in the include directories by default so you only need to do
> include_directories('.') if you intend to use the result in a
> different subdirectory.
> 
> Note that this error message can also be triggered by
> external dependencies being installed within your source
> tree - it's not recommended to do this.

I believe the error comes from this line of meson.build: `inc_np = include_directories(incdir_numpy, incdir_f2py)`, but I have a very minimal experience with meson so I'm not sure how to appease it. 

To elaborate on the environment I used to create this issue, I created a new project in Pycharm and installed the latest stable versions of numpy (v2.0.0) and meson-python (v0.16.0). I created the fib1.f file from the linked webpage, along with the meson.build file. When I'm in the top level directory of the project, this command works as expected in Terminal: `.venv/bin/python -m numpy.f2py fib1.f -m fib2` (i.e it creates a file called fib2-f2pywrappers.f). I then get the aforementioned error when running `.venv/bin/meson setup builddir`.

### Idea or request for content:

_No response_",2024-07-17 21:21:31,,DOC: The f2py meson documentation is seemingly not executable,"['04 - Documentation', 'component: numpy.f2py']"
26949,open,r-devulap,"Perhaps its time to come to a resolution on NEP 54? @Mousius and I agree that highway is the way forward. Additionally, we might also want to reopen the discussion on using Highway for dynamic dispatch. We are unfamiliar with all the SIMD related features in meson, perhaps we could still continue to use it but we would need some help maintaining it and also add some new features to it. ",2024-07-15 16:29:11,,NEP 54 resolution,"['54 - Needs decision', 'component: SIMD']"
26940,open,yairchu,"### Describe the issue:

A simple program which calls `np.square` twice on the exact same input vector produces different results for two consecutive calls.

IIUC (which I'm not sure about) this happens because:

* The first result vector allocated for `np.square` gets to sit close the input vector allocated briefly before it, which makes it fail the `is_mem_overlap` check and fall back to `CDOUBLE_square`'s `loop_scalar`
* `loop_scalar` is plain C code which may or may not use fused-multiply-add depending on compiler versions or options
* The simd code is used to produce the second result, and it does use fused-multiply-add regardless of compilers

### Reproduce the code example:

```python
import numpy as np

vec = np.array([-5.171866611150749e-07 + 2.5618634555957426e-07j, 0, 0])

def compute():
    return np.square(vec)

first_res = compute()
second_res = compute()

print(
    ""Results are consistent.""
    if (first_res == second_res).all()
    else ""INCONSISTENT!""
)
print(""Difference:"", second_res - first_res)
```


### Error message:

```shell
INCONSISTENT!
Difference: [2.5243549e-29+0.j 0.0000000e+00+0.j 0.0000000e+00+0.j]
```


### Python and NumPy Versions:

1.26.4
3.12.4 (main, Jun  6 2024, 18:26:44) [Clang 15.0.0 (clang-1500.3.9.4)]

### Runtime Environment:

[{'numpy_version': '1.26.4',
  'python': '3.12.4 (main, Jun  6 2024, 18:26:44) [Clang 15.0.0 '
            '(clang-1500.3.9.4)]',
  'uname': uname_result(system='Darwin', node='Sounds-MacBook-Pro.local', release='23.4.0', version='Darwin Kernel Version 23.4.0: Fri Mar 15 00:10:42 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T6000', machine='arm64')},
 {'simd_extensions': {'baseline': ['NEON', 'NEON_FP16', 'NEON_VFPV4', 'ASIMD'],
                      'found': ['ASIMDHP'],
                      'not_found': ['ASIMDFHM']}}]

### Context for the issue:

* While dormant in numpy-2.0.0, I suspect this bug may reappear
  * Perhaps my example or something similar could be added as a test to verify it stays fixed
* I suspect this issue affects more operations. IIRC I originally found it with complex vectors multiplication but minimized the example to this one.
* For info into my investigation see https://github.com/yairchu/numpy-floats-bug
  * I failed building numpy from source to match the pip install version, so I analyzed what's going on by having a look on the assembly (my first time reading arm assembly!)",2024-07-14 18:39:12,,BUG: Compiler-options-dependent bug in np.square for complex numbers affecting numpy-1.26.4 on macOS on ARM,['00 - Bug']
26932,open,paddyroddy,"### Issue with current documentation:

https://github.com/numpy/numpy/blob/c1d1ff4548209ad12e3bae794eeb4aadedaefc14/doc/source/dev/index.rst?plain=1#L210

When I run this command locally

> spin test --coverage

`Error: needs valid configuration in [.spin.toml], [spin.toml] or [pyproject.toml]`

`spin test` does work.

### Idea or request for content:

Either suggest a replacement for this command to `spin` or suggest the `pytest` command use in CI https://github.com/numpy/numpy/blob/c1d1ff4548209ad12e3bae794eeb4aadedaefc14/.github/workflows/linux.yml#L148",2024-07-13 17:30:48,,DOC: `spin` command for coverage doesn't work,"['05 - Testing', '04 - Documentation']"
26918,open,nicholasrossano,"### Steps to reproduce:

Hi, I’m developing an iOS app in SwiftUI via XCode on a MacBook Pro (M3 Max) and incorporating Firebase Functions into the project. I’ve been successfully running basic functions, but recently ran into a wall when I tried to add my functions that incorporate dependencies with numpy. The error is really consistent and I’ve tried a number of steps, listed below. The whole log of errors is listed below. It seems to be an architecture issue, but most solutions I find online seem address numpy + Apple Silicon, whereas **my issue seems specific to Numpy + Apple Silicon + Firebase Functions**. Any help would be greatly appreciated, have really hit a wall.

Steps I’ve tried:

1. Verified core Firebase Functions: I have shipped basic Cloud Functions and iterated on them consistently, so it’s not a core Firebase issue.
2. Verified Numpy on local device: I created a test script importing and using Numpy and it successfully ran.
3. Tried multiple virtual environments: I have tried the native venv environment initialized by Firebase and created my own virtual environment using Conda’s Miniforge as I’ve read it works better with the Apple Silicon architecture, but I still get the same error each time.
4. Tested different functions: The error seems to be consistent across all numpy-related dependencies. I have a few functions I was working on, and tried basic (numpy-related) test functions, and the error was consistent across, whether it used transformers or scikit or whatever else.
5. Installing and reinstalling dependencies: I have also reconfigured my requirements.txt file several times, from hard-coding versions to ranges to no versions specified and the error remains the same.
6. Other solutions I’ve tried to incorporate, though these seem tailored to getting numpy to work on the laptop itself which isn’t my issue directly:
    a. Running conda install cython pybind11 then pip install --no-binary :all: numpy as advised [here](https://github.com/numpy/numpy/issues/24961) and similarly [here](https://stackoverflow.com/questions/65336789/numpy-build-fail-in-m1-big-sur-11-1)
    b. I tried uninstalling numpy and running arch -arm64 pip install numpy as advised [here](https://stackoverflow.com/questions/71745890/incompatible-architecture-have-arm64-need-x86-64-error-while-installing)
    c. I tried to follow the [numpy.org instructions](http://numpy.org/) they linked to, but they seem fairly generic and I couldn’t resolve my issue with them.

### Error message:

```shell
[2024-07-11 18:58:08,231] ERROR in app: Exception on /__/functions.yaml [GET]
Traceback (most recent call last):
  File ""/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/numpy/_core/__init__.py"", line 23, in <module>
    from . import multiarray
  File ""/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/numpy/_core/multiarray.py"", line 10, in <module>
    from . import overrides
  File ""/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/numpy/_core/overrides.py"", line 8, in <module>
    from numpy._core._multiarray_umath import (
ImportError: dlopen(/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/numpy/_core/_multiarray_umath.cpython-312-darwin.so, 0x0002): tried: '/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/numpy/_core/_multiarray_umath.cpython-312-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/numpy/_core/_multiarray_umath.cpython-312-darwin.so' (no such file), '/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/numpy/_core/_multiarray_umath.cpython-312-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/numpy/__init__.py"", line 114, in <module>
    from numpy.__config__ import show as show_config
  File ""/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/numpy/__config__.py"", line 4, in <module>
    from numpy._core._multiarray_umath import (
  File ""/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/numpy/_core/__init__.py"", line 49, in <module>
    raise ImportError(msg)
ImportError: 
IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!
Importing the numpy C-extensions failed. This error can happen for
many reasons, often due to issues with your setup or how NumPy was
installed.
We have compiled some common reasons and troubleshooting tips at:
    https://numpy.org/devdocs/user/troubleshooting-importerror.html
Please note and check the following:
  * The Python version is: Python3.12 from ""/Users/nicholas/Development/project/functions/venv/bin/python3.12""
  * The NumPy version is: ""2.0.0""
and make sure that they are the versions you expect.
Please carefully study the documentation linked above for further help.
Original error was: dlopen(/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/numpy/_core/_multiarray_umath.cpython-312-darwin.so, 0x0002): tried: '/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/numpy/_core/_multiarray_umath.cpython-312-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/numpy/_core/_multiarray_umath.cpython-312-darwin.so' (no such file), '/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/numpy/_core/_multiarray_umath.cpython-312-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/flask/app.py"", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/flask/app.py"", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/flask/app.py"", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/flask/app.py"", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/firebase_functions/private/serving.py"", line 122, in get_functions_yaml
    functions = get_functions()
                ^^^^^^^^^^^^^^^
  File ""/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/firebase_functions/private/serving.py"", line 40, in get_functions
    spec.loader.exec_module(module)
  File ""<frozen importlib._bootstrap_external>"", line 994, in exec_module
  File ""<frozen importlib._bootstrap>"", line 488, in _call_with_frames_removed
  File ""/Users/nicholas/Development/project/functions/main.py"", line 4, in <module>
    from classify_by_topic import classify_by_topic
  File ""/Users/nicholas/Development/project/functions/classify_by_topic.py"", line 1, in <module>
    from transformers import pipeline
  File ""/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/transformers/__init__.py"", line 26, in <module>
    from . import dependency_versions_check
  File ""/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py"", line 16, in <module>
    from .utils.versions import require_version, require_version_core
  File ""/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/transformers/utils/__init__.py"", line 34, in <module>
    from .generic import (
  File ""/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/transformers/utils/generic.py"", line 29, in <module>
    import numpy as np
  File ""/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/numpy/__init__.py"", line 119, in <module>
    raise ImportError(msg) from e
ImportError: Error importing numpy: you should not try to import numpy from
        its source directory; please exit the numpy source tree, and relaunch
        your python interpreter from there.
127.0.0.1 - - [11/Jul/2024 18:58:08] ""GET /__/functions.yaml HTTP/1.1"" 500 -
127.0.0.1 - - [11/Jul/2024 18:58:08] ""GET /__/quitquitquit HTTP/1.1"" 200 -
/bin/sh: line 1: 26364 Terminated: 15          python3.12 ""/Users/nicholas/Development/project/functions/venv/lib/python3.12/site-packages/firebase_functions/private/serving.py""
```


### Additional information:

_No response_",2024-07-11 23:31:56,,Consistent Errors With Numpy Dependencies In Firebase Functions + M3 Mac,"['57 - Close?', '33 - Question', '32 - Installation']"
26916,open,jonas-eschle,"**Willing to contribue this; discussion here before opening a PR**

# TL;DR

The numpy array generating API is currently inconsistent, some functions take `shape`, some `size` as arguments. This requires an unnecessary pain point for any non-power users, as they need to look up the argument everytime.
Not excluding any other changes, the proposed change would add the `shape` argument wherever a `size` argument is expected (backwards compatible) and _may_ does it for the two breaking cases (`gamma`, `standard_gamma`, which take it as the first argument), i.e. adding an alternative name.

The result is a more consistent API inside, less pain for users to think about how to name `shape` in arguments. 

The array API adopted `shape` for some generating arrays and nothing definit yet for others, however, it can be safely said that this will be the standard for all generating methods. _Even if not, in the unlikely case_, numpy should still offer a more consistent API to improve numpy itself.

### Proposed new feature or change:

As briefly discussed at SciPy2024 (with @ngoldbaum), forward compatible version of functions that take a ""size"" argument vs a ""shape"" would be welcome. Opening this here to ask for opinions before starting a PR.

# Idea

Some functions take a `shape` argument (`np.zeros`, as in the array API), others take a `size` (as np.random.... does), describing in their docs as `shape`. Other libraries like TensorFlow ([random.normal](https://www.tensorflow.org/api_docs/python/tf/random/uniform) take consistently a `shape` argument and the array API (afaik) is also tending towards this, albeit the randomness sector is hard to agree on (given the different generator designs) and no such formal decision has been made.
IMHO, `shape` will come and be consistent throughout in the array API and would anyway be the better (more consistent) choice.

# Proposal


Adding an additional `shape` keyword to functions that take a `size` as argument for forward compatibility.
If both are specified, the code errors. No other changes are intended internally (but open to if needed), it's meant to be a zero-cost PR.

## Goal

Provide a consistent naming as an input name for users. Getting this in ASAP means that less legacy code is being written, future upgrades will be easier and it's simply less burden to remember where to use `size` vs `shape` for users.


## No-goals

No deprecation or breaking changes are planned, no behavioral changes either. Simply use a consistent naming.

If there are special cases where `size` maybe fits better, this can be left untouched.

## Future

In the future, and with the API standard adapting this formally, `size` could be depricated, internals could be changed. But that's explicitly not part of the PR.

# Compatibility

This is meant as a forward compatibility feature and should not pose any problems. The only way this could backfire seems to be if another name would be adopted in the array API standard, this seems to me to unlikely to rule out. Even if it were, I would still argue that numpy should allow `shape`. But it seems like an unrealistic scenario.


# Actions

If people agree on this, I'll gladly open a PR and we'll move it there for the more detailed discussion.
",2024-07-11 22:13:58,,"ENH: Forward compatible ""shape"" argument vs ""size""",['unlabeled']
26910,open,n-bes,"## Describe the issue:

Hi. I tried to build NumPy from source with clang. While this journey I found few problems:
- failed tests
- linking problems
- crashed compiler

Enjoy the report

## Reproduce the code example:

- Clone repo:
```shell
$ git clone --recursive --branch=v2.0.0 git@github.com:numpy/numpy.git
$ cd numpy
```

- Add Dockerfile:
```Dockerfile
# https://stackoverflow.com/a/54386573
# GCC_ASAN_PRELOAD_PATH=$(gcc -print-file-name=libasan.so)
# CLANG_ASAN_PRELOAD_PATH=$(clang -print-file-name=libclang_rt.asan-x86_64.so)


ARG ENV=""ubuntu_gcc_env""

FROM ubuntu:24.04 as ubuntu_env
RUN apt-get update -y && \
    apt-get install -y \
        gfortran \
        liblapack-dev \
        libopenblas-dev \
        pkg-config \
        python3-dbg \
        python3-dev \
        python3-dev \
        python3-pip \
        python3-venv



FROM ubuntu_env as ubuntu_gcc_env
RUN apt-get install -y \
        gcc \
        g++



FROM ubuntu_gcc_env as ubuntu_gcc_asan_env
ENV CFLAGS=""-g -O0 -fsanitize=address"" \
    CCFLAGS=""-g -O0 -fsanitize=address"" \
    CXXFLAGS=""-g -O0 -fsanitize=address"" \
    CPPFLAGS=""-g -O0 -fsanitize=address"" \
    LDFLAGS=""-fsanitize=address"" \
    LD_PRELOAD=""/usr/lib/gcc/aarch64-linux-gnu/13/libasan.so"" \
    ASAN_OPTIONS=""detect_leaks=0"" 



FROM ubuntu_gcc_env as ubuntu_gcc_hwasan_env
ENV CFLAGS=""-g -O0 -fsanitize=hwaddress"" \
    CCFLAGS=""-g -O0 -fsanitize=hwaddress"" \
    CXXFLAGS=""-g -O0 -fsanitize=hwaddress"" \
    CPPFLAGS=""-g -O0 -fsanitize=hwaddress"" \
    LDFLAGS=""-fsanitize=hwaddress"" \
    # LD_PRELOAD=""/usr/lib/gcc/aarch64-linux-gnu/13/libhwasan.so"" \
    ASAN_OPTIONS=""detect_leaks=0"" \
    HWASAN_OPTIONS=""detect_leaks=0""



FROM ubuntu_env as ubuntu_clang18_env
RUN apt-get install -y \
        clang \
        clang-tools \
        lld
RUN rm /usr/bin/ld && \
    ln -s /usr/bin/ld.lld /usr/bin/ld
ENV CC=clang \
    CXX=clang++



FROM ubuntu_clang18_env as ubuntu_clang18_asan_env
ENV CFLAGS=""-g -O0 -fsanitize=address -shared-libsan"" \
    CCFLAGS=""-g -O0 -fsanitize=address -shared-libsan"" \
    CXXFLAGS=""-g -O0 -fsanitize=address -shared-libsan"" \
    CPPFLAGS=""-g -O0 -fsanitize=address -shared-libsan"" \
    LDFLAGS=""-fsanitize=address -shared-libsan"" \
    LD_PRELOAD=""/usr/lib/llvm-18/lib/clang/18/lib/linux/libclang_rt.asan-aarch64.so"" \
    ASAN_OPTIONS=""detect_leaks=0""



FROM ubuntu_clang18_env as ubuntu_clang18_hwasan_env
ENV CFLAGS=""-g -O0 -fsanitize=hwaddress -shared-libsan"" \
    CCFLAGS=""-g -O0 -fsanitize=hwaddress -shared-libsan"" \
    CXXFLAGS=""-g -O0 -fsanitize=hwaddress -shared-libsan"" \
    CPPFLAGS=""-g -O0 -fsanitize=hwaddress -shared-libsan"" \
    LDFLAGS=""-fsanitize=hwaddress -shared-libsan"" \
    LD_PRELOAD=""/usr/lib/llvm-18/lib/clang/18/lib/linux/libclang_rt.hwasan-aarch64.so"" \
    ASAN_OPTIONS=""detect_leaks=0"" \
    HWASAN_OPTIONS=""detect_leaks=0""



FROM ubuntu_env as ubuntu_clang19_env
RUN apt-get install -y \
    lsb-release \
    wget \
    software-properties-common \
    gnupg
RUN wget https://apt.llvm.org/llvm.sh && \
    chmod +x llvm.sh && \
    ./llvm.sh 19 && \
    ln -s /usr/bin/lld-19 /usr/local/bin/lld && \
    ln -s /usr/bin/clang-19 /usr/local/bin/clang && \
    ln -s /usr/bin/clang++-19 /usr/local/bin/clang++ && \
    rm /usr/bin/ld && \
    ln -s /usr/lib/llvm-19/bin/ld.lld /usr/bin/ld
ENV CC=clang \
    CXX=clang++




FROM ubuntu_clang19_env as ubuntu_clang19_asan_env
ENV CFLAGS=""-g -O0 -fsanitize=address -shared-libsan"" \
    CCFLAGS=""-g -O0 -fsanitize=address -shared-libsan"" \
    CXXFLAGS=""-g -O0 -fsanitize=address -shared-libsan"" \
    CPPFLAGS=""-g -O0 -fsanitize=address -shared-libsan"" \
    LDFLAGS=""-fsanitize=address -shared-libsan"" \
    LD_PRELOAD=""/usr/lib/llvm-19/lib/clang/19/lib/linux/libclang_rt.asan-aarch64.so"" \
    ASAN_OPTIONS=""detect_leaks=0""




FROM ubuntu_clang19_env as ubuntu_clang19_hwasan_env
ENV CFLAGS=""-g -O0 -fsanitize=hwaddress -shared-libsan"" \
    CCFLAGS=""-g -O0 -fsanitize=hwaddress -shared-libsan"" \
    CXXFLAGS=""-g -O0 -fsanitize=hwaddress -shared-libsan"" \
    CPPFLAGS=""-g -O0 -fsanitize=hwaddress -shared-libsan"" \
    LDFLAGS=""-fsanitize=hwaddress -shared-libsan"" \
    LD_PRELOAD=""/usr/lib/llvm-19/lib/clang/19/lib/linux/libclang_rt.hwasan-aarch64.so"" \
    ASAN_OPTIONS=""detect_leaks=0"" \
    HWASAN_OPTIONS=""detect_leaks=0""



FROM ${ENV} as build_env
COPY . /src
WORKDIR /src
RUN python3 -m venv /venv && \
    . /venv/bin/activate && \
    pip3 install -r requirements/all_requirements.txt
```

- Add docker-compose.yaml:
```yaml
version: '3.8'

services:
  ubuntu_gcc_env:
    build:
      target: build_env
      args:
        ENV: ubuntu_gcc_env

  ubuntu_gcc_asan_env:
    build:
      target: build_env
      args:
        ENV: ubuntu_gcc_asan_env

  ubuntu_gcc_hwasan_env:
    build:
      target: build_env
      args:
        ENV: ubuntu_gcc_hwasan_env

  ubuntu_clang18_env:
    build:
      target: build_env
      args:
        ENV: ubuntu_clang18_env

  ubuntu_clang18_asan_env:
    build:
      target: build_env
      args:
        ENV: ubuntu_clang18_asan_env

  ubuntu_clang18_hwasan_env:
    build:
      target: build_env
      args:
        ENV: ubuntu_clang18_hwasan_env

  ubuntu_clang19_env:
    build:
      target: build_env
      args:
        ENV: ubuntu_clang19_env

  ubuntu_clang19_asan_env:
    build:
      target: build_env
      args:
        ENV: ubuntu_clang19_asan_env

  ubuntu_clang19_hwasan_env:
    build:
      target: build_env
      args:
        ENV: ubuntu_clang19_hwasan_env
```
- Build images
```
$ docker compose build
```
- Run commands in environment
```shell
$ source /venv/bin/activate
$ spin build
$ spin test
```

## Versions

```shell
$ gcc --version
gcc (Ubuntu 13.2.0-23ubuntu4) 13.2.0
Copyright (C) 2023 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```

```shell
$ python3 --version
Python 3.12.3
```

```shell
$ clang --version
Ubuntu clang version 18.1.3 (1ubuntu1)
Target: aarch64-unknown-linux-gnu
Thread model: posix
InstalledDir: /usr/bin
```

```shell
$ clang --version
Ubuntu clang version 19.0.0 (++20240709081800+2c42c2263dc6-1~exp1~20240709201923.273)
Target: aarch64-unknown-linux-gnu
Thread model: posix
InstalledDir: /usr/lib/llvm-19/bin
```

```shell
$ cat /etc/os-release
PRETTY_NAME=""Ubuntu 24.04 LTS""
NAME=""Ubuntu""
VERSION_ID=""24.04""
VERSION=""24.04 LTS (Noble Numbat)""
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo
```

```shell
$ docker version
Client:
 Cloud integration: v1.0.35+desktop.13
 Version:           26.1.1
 API version:       1.45
 Go version:        go1.21.9
 Git commit:        4cf5afa
 Built:             Tue Apr 30 11:44:56 2024
 OS/Arch:           darwin/arm64
 Context:           desktop-linux

Server: Docker Desktop 4.30.0 (149282)
 Engine:
  Version:          26.1.1
  API version:      1.45 (minimum version 1.24)
  Go version:       go1.21.9
  Git commit:       ac2de55
  Built:            Tue Apr 30 11:48:04 2024
  OS/Arch:          linux/arm64
  Experimental:     false
 containerd:
  Version:          1.6.31
  GitCommit:        e377cd56a71523140ca6ae87e30244719194a521
 runc:
  Version:          1.1.12
  GitCommit:        v1.1.12-0-g51d5e94
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0
```

## Runs
### ubuntu_gcc_env
OK
```shell
$ docker compose run ubuntu_gcc_env
$ source /venv/bin/activate
$ spin build 
OK
$ spin test
43233 passed, 890 skipped, 2785 deselected, 58 xfailed, 5 xpassed in 97.26s (0:01:37)
```

### ubuntu_gcc_asan_env
Stops with no reason
```shell
$ docker compose run ubuntu_gcc_asan_env
$ source /venv/bin/activate
$ spin build 
OK
$ spin test
Invoking `build` prior to running tests:
$ /venv/bin/python3 vendored-meson/meson/meson.py compile -C build
INFO: autodetecting backend as ninja
INFO: calculating backend command to run: /venv/bin/ninja -C /src/build
ninja: Entering directory `/src/build'
[1/1] Generating numpy/generate-version with a custom command
Saving version to numpy/version.py
$ /venv/bin/python3 vendored-meson/meson/meson.py install --only-changed -C build --destdir ../build-install
$ export PYTHONPATH=""/src/build-install/usr/lib/python3/dist-packages""
$ /venv/bin/python3 -P -c 'import numpy'
$ export PYTHONPATH=""/src/build-install/usr/lib/python3/dist-packages""
$ cd /src/build-install/usr/lib/python3/dist-packages
$ /venv/bin/python3 -m pytest --rootdir=/src/build-install/usr/lib/python3/dist-packages -m 'not slow' numpy
===================================================================================================== test session starts =====================================================================================================
platform linux -- Python 3.12.3, pytest-7.4.0, pluggy-1.5.0
rootdir: /src/build-install/usr/lib/python3/dist-packages
configfile: ../../../../../pytest.ini
plugins: cov-4.1.0, hypothesis-6.81.1, xdist-3.6.1
collected 46970 items / 2785 deselected / 1 skipped / 44185 selected

numpy/_core/tests/test__exceptions.py
numpy/_core/tests/test_abc.py
numpy/_core/tests/test_api.py
numpy/_core/tests/test_argparse.py
numpy/_core/tests/test_array_coercion.py .............................................................x................................................................................................................
```

### ubuntu_gcc_hwasan_env
Timeout (still waiting, ~12h)
```shell
$ docker compose run ubuntu_gcc_hwasan_env
$ source venv/bin/activate
$ spin build 
...
[319/323] Linking static target numpy/_core/libhighway_qsort_16bit.dispatch.h_ASIMDHP.a
```


### ubuntu_clang18_env
Tests failed:
```shell
$ docker compose run ubuntu_clang18_env
$ source venv/bin/activate
$ spin build 
OK
$ spin test
43233 passed, 890 skipped, 2785 deselected, 58 xfailed, 5 xpassed in 100.34s (0:01:40)

=============== FAILURES ===============
_______________ TestDivision.test_floor_division_errors[g] _______________

self = <numpy._core.tests.test_umath.TestDivision object at 0xffff74cf7800>, dtype = 'g'

    @pytest.mark.skipif(hasattr(np.__config__, ""blas_ssl2_info""),
            reason=""gh-22982"")
    @pytest.mark.skipif(IS_WASM, reason=""fp errors don't work in wasm"")
    @pytest.mark.parametrize('dtype', np.typecodes['Float'])
    def test_floor_division_errors(self, dtype):
        fnan = np.array(np.nan, dtype=dtype)
        fone = np.array(1.0, dtype=dtype)
        fzer = np.array(0.0, dtype=dtype)
        finf = np.array(np.inf, dtype=dtype)
        # divide by zero error check
        with np.errstate(divide='raise', invalid='ignore'):
            assert_raises(FloatingPointError, np.floor_divide, fone, fzer)
        with np.errstate(divide='ignore', invalid='raise'):
            np.floor_divide(fone, fzer)

        # The following already contain a NaN and should not warn
        with np.errstate(all='raise'):
>           np.floor_divide(fnan, fone)
E           FloatingPointError: invalid value encountered in floor_divide

dtype      = 'g'
finf       = array(inf, dtype=float128)
fnan       = array(nan, dtype=float128)
fone       = array(1., dtype=float128)
fzer       = array(0., dtype=float128)
self       = <numpy._core.tests.test_umath.TestDivision object at 0xffff74cf7800>

numpy/_core/tests/test_umath.py:680: FloatingPointError

_______________ TestRemainder.test_float_remainder_errors[remainder-g] _______________

self = <numpy._core.tests.test_umath.TestRemainder object at 0xffff746a5640>, dtype = 'g', fn = <ufunc 'remainder'>

    @pytest.mark.skipif(hasattr(np.__config__, ""blas_ssl2_info""),
            reason=""gh-22982"")
    @pytest.mark.skipif(IS_WASM, reason=""fp errors don't work in wasm"")
    @pytest.mark.xfail(sys.platform.startswith(""darwin""),
           reason=""MacOS seems to not give the correct 'invalid' warning for ""
                  ""`fmod`.  Hopefully, others always do."")
    @pytest.mark.parametrize('dtype', np.typecodes['Float'])
    @pytest.mark.parametrize('fn', [np.fmod, np.remainder])
    def test_float_remainder_errors(self, dtype, fn):
        fzero = np.array(0.0, dtype=dtype)
        fone = np.array(1.0, dtype=dtype)
        finf = np.array(np.inf, dtype=dtype)
        fnan = np.array(np.nan, dtype=dtype)

        # The following already contain a NaN and should not warn.
        with np.errstate(all='raise'):
            with pytest.raises(FloatingPointError,
                    match=""invalid value""):
                fn(fone, fzero)
            fn(fnan, fzero)
>           fn(fzero, fnan)
E           FloatingPointError: invalid value encountered in remainder

dtype      = 'g'
finf       = array(inf, dtype=float128)
fn         = <ufunc 'remainder'>
fnan       = array(nan, dtype=float128)
fone       = array(1., dtype=float128)
fzero      = array(0., dtype=float128)
self       = <numpy._core.tests.test_umath.TestRemainder object at 0xffff746a5640>

numpy/_core/tests/test_umath.py:827: FloatingPointError
=============== short test summary info ===============
FAILED numpy/_core/tests/test_umath.py::TestDivision::test_floor_division_errors[g] - FloatingPointError: invalid value encountered in floor_divide
FAILED numpy/_core/tests/test_umath.py::TestRemainder::test_float_remainder_errors[remainder-g] - FloatingPointError: invalid value encountered in remainder
=============== 2 failed, 43231 passed, 890 skipped, 2785 deselected, 58 xfailed, 5 xpassed in 193.05s (0:03:13) ===============
```

### ubuntu_clang18_hwasan_env
Compiler is going down:
```
$ docker compose run ubuntu_clang18_hwasan_env
$ source venv/bin/activate
$ spin build 
[208/323] Compiling C++ object numpy/_core/libhighway_qsort.dispatch.h_SVE.a.p/src_npysort_highway_qsort.dispatch.cpp.o
FAILED: numpy/_core/libhighway_qsort.dispatch.h_SVE.a.p/src_npysort_highway_qsort.dispatch.cpp.o
clang++ -Inumpy/_core/libhighway_qsort.dispatch.h_SVE.a.p -Inumpy/_core -I../numpy/_core -Inumpy/_core/include -I../numpy/_core/include -I../numpy/_core/src/common -I../numpy/_core/src/multiarray -I../numpy/_core/src/npymath -I../numpy/_core/src/umath -I../numpy/_core/src/highway -I/usr/include/python3.12 -I/usr/include/aarch64-linux-gnu/python3.12 -I/src/build/meson_cpu -fcolor-diagnostics -Wall -Winvalid-pch -std=c++17 -O2 -g -ftrapping-math -DNPY_HAVE_CLANG_FPSTRICT -g -O0 -fsanitize=hwaddress -shared-libsan -g -O0 -fsanitize=hwaddress -shared-libsan -fPIC -DNPY_INTERNAL_BUILD -DHAVE_NPY_CONFIG_H -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -D__STDC_VERSION__=0 -fno-exceptions -fno-rtti -O3 -DNPY_HAVE_NEON_VFPV4 -DNPY_HAVE_NEON_FP16 -DNPY_HAVE_NEON -DNPY_HAVE_ASIMD -DNPY_HAVE_ASIMDHP -DNPY_HAVE_SVE -march=armv8.2-a+sve+fp16 -DNPY_MTARGETS_CURRENT=SVE -MD -MQ numpy/_core/libhighway_qsort.dispatch.h_SVE.a.p/src_npysort_highway_qsort.dispatch.cpp.o -MF numpy/_core/libhighway_qsort.dispatch.h_SVE.a.p/src_npysort_highway_qsort.dispatch.cpp.o.d -o numpy/_core/libhighway_qsort.dispatch.h_SVE.a.p/src_npysort_highway_qsort.dispatch.cpp.o -c ../numpy/_core/src/npysort/highway_qsort.dispatch.cpp
fatal error: error in backend: Invalid size request on a scalable vector.
PLEASE submit a bug report to https://github.com/llvm/llvm-project/issues/ and include the crash backtrace, preprocessed source, and associated run script.
Stack dump:
0.	Program arguments: clang++ -Inumpy/_core/libhighway_qsort.dispatch.h_SVE.a.p -Inumpy/_core -I../numpy/_core -Inumpy/_core/include -I../numpy/_core/include -I../numpy/_core/src/common -I../numpy/_core/src/multiarray -I../numpy/_core/src/npymath -I../numpy/_core/src/umath -I../numpy/_core/src/highway -I/usr/include/python3.12 -I/usr/include/aarch64-linux-gnu/python3.12 -I/src/build/meson_cpu -fcolor-diagnostics -Wall -Winvalid-pch -std=c++17 -O2 -g -ftrapping-math -DNPY_HAVE_CLANG_FPSTRICT -g -O0 -fsanitize=hwaddress -shared-libsan -g -O0 -fsanitize=hwaddress -shared-libsan -fPIC -DNPY_INTERNAL_BUILD -DHAVE_NPY_CONFIG_H -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -D__STDC_VERSION__=0 -fno-exceptions -fno-rtti -O3 -DNPY_HAVE_NEON_VFPV4 -DNPY_HAVE_NEON_FP16 -DNPY_HAVE_NEON -DNPY_HAVE_ASIMD -DNPY_HAVE_ASIMDHP -DNPY_HAVE_SVE -march=armv8.2-a+sve+fp16 -DNPY_MTARGETS_CURRENT=SVE -MD -MQ numpy/_core/libhighway_qsort.dispatch.h_SVE.a.p/src_npysort_highway_qsort.dispatch.cpp.o -MF numpy/_core/libhighway_qsort.dispatch.h_SVE.a.p/src_npysort_highway_qsort.dispatch.cpp.o.d -o numpy/_core/libhighway_qsort.dispatch.h_SVE.a.p/src_npysort_highway_qsort.dispatch.cpp.o -c ../numpy/_core/src/npysort/highway_qsort.dispatch.cpp
1.	<eof> parser at end of file
2.	Optimizer
Stack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):
0  libLLVM.so.18.1      0x0000ffff7eead398 llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) + 84
1  libLLVM.so.18.1      0x0000ffff7eeab5a8 llvm::sys::RunSignalHandlers() + 116
2  libLLVM.so.18.1      0x0000ffff7edfb17c
3  libLLVM.so.18.1      0x0000ffff7edfb128
4  libLLVM.so.18.1      0x0000ffff7eea7fcc llvm::sys::Process::Exit(int, bool) + 52
5  clang++              0x0000aaaaaad32560
6  libLLVM.so.18.1      0x0000ffff7ee08a20 llvm::report_fatal_error(llvm::Twine const&, bool) + 248
7  libLLVM.so.18.1      0x0000ffff7ee08928 llvm::report_fatal_error(llvm::Twine const&, bool) + 0
8  libLLVM.so.18.1      0x0000ffff7ee62334 llvm::TypeSize::operator unsigned long() const + 0
9  libLLVM.so.18.1      0x0000ffff7ee6235c llvm::TypeSize::operator unsigned long() const + 40
10 libLLVM.so.18.1      0x0000ffff7fa12940 llvm::memtag::StackInfoBuilder::isInterestingAlloca(llvm::AllocaInst const&) + 276
11 libLLVM.so.18.1      0x0000ffff7fa124f0 llvm::memtag::StackInfoBuilder::visit(llvm::Instruction&) + 136
12 libLLVM.so.18.1      0x0000ffff7fb474e0
13 libLLVM.so.18.1      0x0000ffff7fb47210 llvm::HWAddressSanitizerPass::run(llvm::Module&, llvm::AnalysisManager<llvm::Module>&) + 6552
14 libLLVM.so.18.1      0x0000ffff7f03a09c llvm::PassManager<llvm::Module, llvm::AnalysisManager<llvm::Module>>::run(llvm::Module&, llvm::AnalysisManager<llvm::Module>&) + 332
15 libclang-cpp.so.18.1 0x0000ffff86f72fe0
16 libclang-cpp.so.18.1 0x0000ffff86f6b918 clang::EmitBackendOutput(clang::DiagnosticsEngine&, clang::HeaderSearchOptions const&, clang::CodeGenOptions const&, clang::TargetOptions const&, clang::LangOptions const&, llvm::StringRef, llvm::Module*, clang::BackendAction, llvm::IntrusiveRefCntPtr<llvm::vfs::FileSystem>, std::unique_ptr<llvm::raw_pwrite_stream, std::default_delete<llvm::raw_pwrite_stream>>, clang::BackendConsumer*) + 1844
17 libclang-cpp.so.18.1 0x0000ffff87285f58 clang::BackendConsumer::HandleTranslationUnit(clang::ASTContext&) + 1180
18 libclang-cpp.so.18.1 0x0000ffff85eea5b0 clang::ParseAST(clang::Sema&, bool, bool) + 572
19 libclang-cpp.so.18.1 0x0000ffff87c9fa34 clang::FrontendAction::Execute() + 112
20 libclang-cpp.so.18.1 0x0000ffff87c30590 clang::CompilerInstance::ExecuteAction(clang::FrontendAction&) + 744
21 libclang-cpp.so.18.1 0x0000ffff87d18e4c clang::ExecuteCompilerInvocation(clang::CompilerInstance*) + 616
22 clang++              0x0000aaaaaad321c4 cc1_main(llvm::ArrayRef<char const*>, char const*, void*) + 3360
23 clang++              0x0000aaaaaad2fbac
24 libclang-cpp.so.18.1 0x0000ffff8792a950
25 libLLVM.so.18.1      0x0000ffff7edfb0f8 llvm::CrashRecoveryContext::RunSafely(llvm::function_ref<void ()>) + 168
26 libclang-cpp.so.18.1 0x0000ffff8792a1bc clang::driver::CC1Command::Execute(llvm::ArrayRef<std::optional<llvm::StringRef>>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>*, bool*) const + 352
27 libclang-cpp.so.18.1 0x0000ffff878f8d3c clang::driver::Compilation::ExecuteCommand(clang::driver::Command const&, clang::driver::Command const*&, bool) const + 756
28 libclang-cpp.so.18.1 0x0000ffff878f8f4c clang::driver::Compilation::ExecuteJobs(clang::driver::JobList const&, llvm::SmallVectorImpl<std::pair<int, clang::driver::Command const*>>&, bool) const + 136
29 libclang-cpp.so.18.1 0x0000ffff879112f4 clang::driver::Driver::ExecuteCompilation(clang::driver::Compilation&, llvm::SmallVectorImpl<std::pair<int, clang::driver::Command const*>>&) + 324
30 clang++              0x0000aaaaaad2f344 clang_main(int, char**, llvm::ToolContext const&) + 9756
31 clang++              0x0000aaaaaad3ae40 main + 92
32 libc.so.6            0x0000ffff7dcf84c4
33 libc.so.6            0x0000ffff7dcf8598 __libc_start_main + 152
34 clang++              0x0000aaaaaad2c970 _start + 48
clang++: error: clang frontend command failed with exit code 70 (use -v to see invocation)
Ubuntu clang version 18.1.3 (1ubuntu1)
Target: aarch64-unknown-linux-gnu
Thread model: posix
InstalledDir: /usr/bin
clang++: note: diagnostic msg:
********************

PLEASE ATTACH THE FOLLOWING FILES TO THE BUG REPORT:
Preprocessed source(s) and associated run script(s) are located at:
clang++: note: diagnostic msg: /tmp/highway_qsort-02f1d0.cpp
clang++: note: diagnostic msg: /tmp/highway_qsort-02f1d0.sh
clang++: note: diagnostic msg:

********************
[223/323] Compiling C++ object numpy/_core/_multiarray_umath.cpython-312-aarch64-linux-gnu.so.p/src_npysort_timsort.cpp.o
ninja: build stopped: subcommand failed.
```

### ubuntu_clang19_env
Tests failed:
```shell
$ docker compose run ubuntu_clang19_env
$ spin build
OK
$ spin test
=============== FAILURES ===============
_______________ TestDivision.test_floor_division_errors[g] _______________

self = <numpy._core.tests.test_umath.TestDivision object at 0xffff6c7af5f0>, dtype = 'g'

    @pytest.mark.skipif(hasattr(np.__config__, ""blas_ssl2_info""),
            reason=""gh-22982"")
    @pytest.mark.skipif(IS_WASM, reason=""fp errors don't work in wasm"")
    @pytest.mark.parametrize('dtype', np.typecodes['Float'])
    def test_floor_division_errors(self, dtype):
        fnan = np.array(np.nan, dtype=dtype)
        fone = np.array(1.0, dtype=dtype)
        fzer = np.array(0.0, dtype=dtype)
        finf = np.array(np.inf, dtype=dtype)
        # divide by zero error check
        with np.errstate(divide='raise', invalid='ignore'):
            assert_raises(FloatingPointError, np.floor_divide, fone, fzer)
        with np.errstate(divide='ignore', invalid='raise'):
            np.floor_divide(fone, fzer)

        # The following already contain a NaN and should not warn
        with np.errstate(all='raise'):
>           np.floor_divide(fnan, fone)
E           FloatingPointError: invalid value encountered in floor_divide

dtype      = 'g'
finf       = array(inf, dtype=float128)
fnan       = array(nan, dtype=float128)
fone       = array(1., dtype=float128)
fzer       = array(0., dtype=float128)
self       = <numpy._core.tests.test_umath.TestDivision object at 0xffff6c7af5f0>

numpy/_core/tests/test_umath.py:680: FloatingPointError
_______________ TestRemainder.test_float_remainder_errors[remainder-g] _______________

self = <numpy._core.tests.test_umath.TestRemainder object at 0xffff6c165430>, dtype = 'g', fn = <ufunc 'remainder'>

    @pytest.mark.skipif(hasattr(np.__config__, ""blas_ssl2_info""),
            reason=""gh-22982"")
    @pytest.mark.skipif(IS_WASM, reason=""fp errors don't work in wasm"")
    @pytest.mark.xfail(sys.platform.startswith(""darwin""),
           reason=""MacOS seems to not give the correct 'invalid' warning for ""
                  ""`fmod`.  Hopefully, others always do."")
    @pytest.mark.parametrize('dtype', np.typecodes['Float'])
    @pytest.mark.parametrize('fn', [np.fmod, np.remainder])
    def test_float_remainder_errors(self, dtype, fn):
        fzero = np.array(0.0, dtype=dtype)
        fone = np.array(1.0, dtype=dtype)
        finf = np.array(np.inf, dtype=dtype)
        fnan = np.array(np.nan, dtype=dtype)

        # The following already contain a NaN and should not warn.
        with np.errstate(all='raise'):
            with pytest.raises(FloatingPointError,
                    match=""invalid value""):
                fn(fone, fzero)
            fn(fnan, fzero)
>           fn(fzero, fnan)
E           FloatingPointError: invalid value encountered in remainder

dtype      = 'g'
finf       = array(inf, dtype=float128)
fn         = <ufunc 'remainder'>
fnan       = array(nan, dtype=float128)
fone       = array(1., dtype=float128)
fzero      = array(0., dtype=float128)
self       = <numpy._core.tests.test_umath.TestRemainder object at 0xffff6c165430>

numpy/_core/tests/test_umath.py:827: FloatingPointError
=============== short test summary info ===============
FAILED numpy/_core/tests/test_umath.py::TestDivision::test_floor_division_errors[g] - FloatingPointError: invalid value encountered in floor_divide
FAILED numpy/_core/tests/test_umath.py::TestRemainder::test_float_remainder_errors[remainder-g] - FloatingPointError: invalid value encountered in remainder
=============== 2 failed, 43231 passed, 890 skipped, 2785 deselected, 58 xfailed, 5 xpassed in 98.56s (0:01:38) ===============
```

### ubuntu_clang19_asan_env
undefined symbol: _ZN3hwy11VectorBytesEv
```shell
$ docker compose run ubuntu_clang19_asan_env
$ spin build
OK
$ spin test
Invoking `build` prior to running tests:
$ /venv/bin/python3 vendored-meson/meson/meson.py compile -C build
INFO: autodetecting backend as ninja
INFO: calculating backend command to run: /venv/bin/ninja -C /src/build
ninja: Entering directory `/src/build'
[1/1] Generating numpy/generate-version with a custom command
Saving version to numpy/version.py
$ /venv/bin/python3 vendored-meson/meson/meson.py install --only-changed -C build --destdir ../build-install
$ export PYTHONPATH=""/src/build-install/usr/lib/python3/dist-packages""
$ /venv/bin/python3 -P -c 'import numpy'
Traceback (most recent call last):
  File ""/src/build-install/usr/lib/python3/dist-packages/numpy/_core/__init__.py"", line 23, in <module>
    from . import multiarray
  File ""/src/build-install/usr/lib/python3/dist-packages/numpy/_core/multiarray.py"", line 10, in <module>
    from . import overrides
  File ""/src/build-install/usr/lib/python3/dist-packages/numpy/_core/overrides.py"", line 8, in <module>
    from numpy._core._multiarray_umath import (
ImportError: /src/build-install/usr/lib/python3/dist-packages/numpy/_core/_multiarray_umath.cpython-312-aarch64-linux-gnu.so: undefined symbol: _ZN3hwy11VectorBytesEv

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/src/build-install/usr/lib/python3/dist-packages/numpy/__init__.py"", line 114, in <module>
    from numpy.__config__ import show as show_config
  File ""/src/build-install/usr/lib/python3/dist-packages/numpy/__config__.py"", line 4, in <module>
    from numpy._core._multiarray_umath import (
  File ""/src/build-install/usr/lib/python3/dist-packages/numpy/_core/__init__.py"", line 49, in <module>
    raise ImportError(msg)
ImportError:

IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!

Importing the numpy C-extensions failed. This error can happen for
many reasons, often due to issues with your setup or how NumPy was
installed.

We have compiled some common reasons and troubleshooting tips at:

    https://numpy.org/devdocs/user/troubleshooting-importerror.html

Please note and check the following:

  * The Python version is: Python3.12 from ""/venv/bin/python3""
  * The NumPy version is: ""2.0.0""

and make sure that they are the versions you expect.
Please carefully study the documentation linked above for further help.

Original error was: /src/build-install/usr/lib/python3/dist-packages/numpy/_core/_multiarray_umath.cpython-312-aarch64-linux-gnu.so: undefined symbol: _ZN3hwy11VectorBytesEv


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/src/build-install/usr/lib/python3/dist-packages/numpy/__init__.py"", line 119, in <module>
    raise ImportError(msg) from e
ImportError: Error importing numpy: you should not try to import numpy from
        its source directory; please exit the numpy source tree, and relaunch
        your python interpreter from there.
As a sanity check, we tried to import numpy.
Stopping. Please investigate the build error.
```

### ubuntu_clang19_hwasan_env
Build failed: HWAddressSanitizer: tag-mismatch
```shell
$ docker compose run ubuntu_clang19_hwasan_env
$ spin build
Invoking `build` prior to running tests:
$ /venv/bin/python3 vendored-meson/meson/meson.py compile -C build
INFO: autodetecting backend as ninja
INFO: calculating backend command to run: /venv/bin/ninja -C /src/build
ninja: Entering directory `/src/build'
[1/1] Generating numpy/generate-version with a custom command
Saving version to numpy/version.py
$ /venv/bin/python3 vendored-meson/meson/meson.py install --only-changed -C build --destdir ../build-install
$ export PYTHONPATH=""/src/build-install/usr/lib/python3/dist-packages""
$ /venv/bin/python3 -P -c 'import numpy'
==3669==ERROR: HWAddressSanitizer: tag-mismatch on address 0xffff923937a0 at pc 0xffff94427ec4
WRITE of size 353 at 0xffff923937a0 tags: ab/00 (ptr/mem) in thread T0
    #0 0xffff94427ec4 in __hwasan_memset (/usr/lib/llvm-19/lib/clang/19/lib/linux/libclang_rt.hwasan-aarch64.so+0x27ec4) (BuildId: 2f010f5286219c795eacb8b5bf9bef0e613730c3)
    #1 0xffff91e86c14 in npy__cpu_init_features /src/build/../numpy/_core/src/common/npy_cpu_features.c:780:5
    #2 0xffff91e86510 in npy_cpu_init /src/build/../numpy/_core/src/common/npy_cpu_features.c:44:5
    #3 0xffff917e62fc in PyInit__multiarray_umath /src/build/../numpy/_core/src/multiarray/multiarraymodule.c:4971:9
    #4 0x66def0 in _PyImport_LoadDynamicModuleWithSpec /usr/src/python3.12-3.12.3-1/build-static/../Python/importdl.c:169:9
    #5 0x66d338 in _imp_create_dynamic_impl /usr/src/python3.12-3.12.3-1/build-static/../Python/import.c:3775:11
    #6 0x66d338 in _imp_create_dynamic /usr/src/python3.12-3.12.3-1/build-static/../Python/clinic/import.c.h:506:20
    #7 0x502d4c in cfunction_vectorcall_FASTCALL /usr/src/python3.12-3.12.3-1/build-static/../Objects/methodobject.c:422:24
    #8 0x5652ac in _PyEval_EvalFrameDefault /usr/src/python3.12-3.12.3-1/build-static/Python/bytecodes.c:3254:26
    #9 0x4c2f00 in _PyObject_VectorcallTstate /usr/src/python3.12-3.12.3-1/build-static/../Include/internal/pycore_call.h:92:11
    #10 0x4c2f00 in object_vacall /usr/src/python3.12-3.12.3-1/build-static/../Objects/call.c:850:14
    #11 0x4c4ae4 in PyObject_CallMethodObjArgs /usr/src/python3.12-3.12.3-1/build-static/../Objects/call.c:911:24
    #12 0x58bee8 in import_find_and_load /usr/src/python3.12-3.12.3-1/build-static/../Python/import.c:2779:11
    #13 0x58bee8 in PyImport_ImportModuleLevelObject /usr/src/python3.12-3.12.3-1/build-static/../Python/import.c:2862:15
    #14 0x565f6c in import_name /usr/src/python3.12-3.12.3-1/build-static/../Python/ceval.c:2482:15
    #15 0x565f6c in _PyEval_EvalFrameDefault /usr/src/python3.12-3.12.3-1/build-static/Python/bytecodes.c:2135:19
    #16 0x560070 in _PyEval_EvalFrame /usr/src/python3.12-3.12.3-1/build-static/../Include/internal/pycore_ceval.h:89:16
    #17 0x560070 in _PyEval_Vector /usr/src/python3.12-3.12.3-1/build-static/../Python/ceval.c:1683:12
    #18 0x560070 in PyEval_EvalCode /usr/src/python3.12-3.12.3-1/build-static/../Python/ceval.c:578:21
    #19 0x55d264 in builtin_exec_impl /usr/src/python3.12-3.12.3-1/build-static/../Python/bltinmodule.c:1096:17
    #20 0x55d264 in builtin_exec /usr/src/python3.12-3.12.3-1/build-static/../Python/clinic/bltinmodule.c.h:586:20
    #21 0x502a38 in cfunction_vectorcall_FASTCALL_KEYWORDS /usr/src/python3.12-3.12.3-1/build-static/../Objects/methodobject.c:438:24
    #22 0x5652ac in _PyEval_EvalFrameDefault /usr/src/python3.12-3.12.3-1/build-static/Python/bytecodes.c:3254:26
    #23 0x4c2f00 in _PyObject_VectorcallTstate /usr/src/python3.12-3.12.3-1/build-static/../Include/internal/pycore_call.h:92:11
    #24 0x4c2f00 in object_vacall /usr/src/python3.12-3.12.3-1/build-static/../Objects/call.c:850:14
    #25 0x4c4ae4 in PyObject_CallMethodObjArgs /usr/src/python3.12-3.12.3-1/build-static/../Objects/call.c:911:24
    #26 0x58bee8 in import_find_and_load /usr/src/python3.12-3.12.3-1/build-static/../Python/import.c:2779:11
    #27 0x58bee8 in PyImport_ImportModuleLevelObject /usr/src/python3.12-3.12.3-1/build-static/../Python/import.c:2862:15
    #28 0x55d754 in builtin___import___impl /usr/src/python3.12-3.12.3-1/build-static/../Python/bltinmodule.c:275:12
    #29 0x55d754 in builtin___import__ /usr/src/python3.12-3.12.3-1/build-static/../Python/clinic/bltinmodule.c.h:107:20
    #30 0x502a38 in cfunction_vectorcall_FASTCALL_KEYWORDS /usr/src/python3.12-3.12.3-1/build-static/../Objects/methodobject.c:438:24
    #31 0x5652ac in _PyEval_EvalFrameDefault /usr/src/python3.12-3.12.3-1/build-static/Python/bytecodes.c:3254:26
    #32 0x4c2f00 in _PyObject_VectorcallTstate /usr/src/python3.12-3.12.3-1/build-static/../Include/internal/pycore_call.h:92:11
    #33 0x4c2f00 in object_vacall /usr/src/python3.12-3.12.3-1/build-static/../Objects/call.c:850:14
    #34 0x4c4ae4 in PyObject_CallMethodObjArgs /usr/src/python3.12-3.12.3-1/build-static/../Objects/call.c:911:24
    #35 0x58c120 in PyImport_ImportModuleLevelObject /usr/src/python3.12-3.12.3-1/build-static/../Python/import.c:2931:25
    #36 0x565f6c in import_name /usr/src/python3.12-3.12.3-1/build-static/../Python/ceval.c:2482:15
    #37 0x565f6c in _PyEval_EvalFrameDefault /usr/src/python3.12-3.12.3-1/build-static/Python/bytecodes.c:2135:19
    #38 0x560070 in _PyEval_EvalFrame /usr/src/python3.12-3.12.3-1/build-static/../Include/internal/pycore_ceval.h:89:16
    #39 0x560070 in _PyEval_Vector /usr/src/python3.12-3.12.3-1/build-static/../Python/ceval.c:1683:12
    #40 0x560070 in PyEval_EvalCode /usr/src/python3.12-3.12.3-1/build-static/../Python/ceval.c:578:21
    #41 0x55d264 in builtin_exec_impl /usr/src/python3.12-3.12.3-1/build-static/../Python/bltinmodule.c:1096:17
    #42 0x55d264 in builtin_exec /usr/src/python3.12-3.12.3-1/build-static/../Python/clinic/bltinmodule.c.h:586:20
    #43 0x502a38 in cfunction_vectorcall_FASTCALL_KEYWORDS /usr/src/python3.12-3.12.3-1/build-static/../Objects/methodobject.c:438:24
    #44 0x5652ac in _PyEval_EvalFrameDefault /usr/src/python3.12-3.12.3-1/build-static/Python/bytecodes.c:3254:26
    #45 0x4c2f00 in _PyObject_VectorcallTstate /usr/src/python3.12-3.12.3-1/build-static/../Include/internal/pycore_call.h:92:11
    #46 0x4c2f00 in object_vacall /usr/src/python3.12-3.12.3-1/build-static/../Objects/call.c:850:14
    #47 0x4c4ae4 in PyObject_CallMethodObjArgs /usr/src/python3.12-3.12.3-1/build-static/../Objects/call.c:911:24
    #48 0x58bee8 in import_find_and_load /usr/src/python3.12-3.12.3-1/build-static/../Python/import.c:2779:11
    #49 0x58bee8 in PyImport_ImportModuleLevelObject /usr/src/python3.12-3.12.3-1/build-static/../Python/import.c:2862:15
    #50 0x55d754 in builtin___import___impl /usr/src/python3.12-3.12.3-1/build-static/../Python/bltinmodule.c:275:12
    #51 0x55d754 in builtin___import__ /usr/src/python3.12-3.12.3-1/build-static/../Python/clinic/bltinmodule.c.h:107:20
    #52 0x502a38 in cfunction_vectorcall_FASTCALL_KEYWORDS /usr/src/python3.12-3.12.3-1/build-static/../Objects/methodobject.c:438:24
    #53 0x5652ac in _PyEval_EvalFrameDefault /usr/src/python3.12-3.12.3-1/build-static/Python/bytecodes.c:3254:26
    #54 0x4c2f00 in _PyObject_VectorcallTstate /usr/src/python3.12-3.12.3-1/build-static/../Include/internal/pycore_call.h:92:11
    #55 0x4c2f00 in object_vacall /usr/src/python3.12-3.12.3-1/build-static/../Objects/call.c:850:14
    #56 0x4c4ae4 in PyObject_CallMethodObjArgs /usr/src/python3.12-3.12.3-1/build-static/../Objects/call.c:911:24
    #57 0x58c120 in PyImport_ImportModuleLevelObject /usr/src/python3.12-3.12.3-1/build-static/../Python/import.c:2931:25
    #58 0x565f6c in import_name /usr/src/python3.12-3.12.3-1/build-static/../Python/ceval.c:2482:15
    #59 0x565f6c in _PyEval_EvalFrameDefault /usr/src/python3.12-3.12.3-1/build-static/Python/bytecodes.c:2135:19
    #60 0x560070 in _PyEval_EvalFrame /usr/src/python3.12-3.12.3-1/build-static/../Include/internal/pycore_ceval.h:89:16
    #61 0x560070 in _PyEval_Vector /usr/src/python3.12-3.12.3-1/build-static/../Python/ceval.c:1683:12
    #62 0x560070 in PyEval_EvalCode /usr/src/python3.12-3.12.3-1/build-static/../Python/ceval.c:578:21
    #63 0x55d264 in builtin_exec_impl /usr/src/python3.12-3.12.3-1/build-static/../Python/bltinmodule.c:1096:17
    #64 0x55d264 in builtin_exec /usr/src/python3.12-3.12.3-1/build-static/../Python/clinic/bltinmodule.c.h:586:20
    #65 0x502a38 in cfunction_vectorcall_FASTCALL_KEYWORDS /usr/src/python3.12-3.12.3-1/build-static/../Objects/methodobject.c:438:24
    #66 0x5652ac in _PyEval_EvalFrameDefault /usr/src/python3.12-3.12.3-1/build-static/Python/bytecodes.c:3254:26
    #67 0x4c2f00 in _PyObject_VectorcallTstate /usr/src/python3.12-3.12.3-1/build-static/../Include/internal/pycore_call.h:92:11
    #68 0x4c2f00 in object_vacall /usr/src/python3.12-3.12.3-1/build-static/../Objects/call.c:850:14
    #69 0x4c4ae4 in PyObject_CallMethodObjArgs /usr/src/python3.12-3.12.3-1/build-static/../Objects/call.c:911:24
    #70 0x58bee8 in import_find_and_load /usr/src/python3.12-3.12.3-1/build-static/../Python/import.c:2779:11
    #71 0x58bee8 in PyImport_ImportModuleLevelObject /usr/src/python3.12-3.12.3-1/build-static/../Python/import.c:2862:15
    #72 0x55d754 in builtin___import___impl /usr/src/python3.12-3.12.3-1/build-static/../Python/bltinmodule.c:275:12
    #73 0x55d754 in builtin___import__ /usr/src/python3.12-3.12.3-1/build-static/../Python/clinic/bltinmodule.c.h:107:20
    #74 0x502a38 in cfunction_vectorcall_FASTCALL_KEYWORDS /usr/src/python3.12-3.12.3-1/build-static/../Objects/methodobject.c:438:24
    #75 0x5652ac in _PyEval_EvalFrameDefault /usr/src/python3.12-3.12.3-1/build-static/Python/bytecodes.c:3254:26
    #76 0x4c2f00 in _PyObject_VectorcallTstate /usr/src/python3.12-3.12.3-1/build-static/../Include/internal/pycore_call.h:92:11
    #77 0x4c2f00 in object_vacall /usr/src/python3.12-3.12.3-1/build-static/../Objects/call.c:850:14
    #78 0x4c4ae4 in PyObject_CallMethodObjArgs /usr/src/python3.12-3.12.3-1/build-static/../Objects/call.c:911:24
    #79 0x58bee8 in import_find_and_load /usr/src/python3.12-3.12.3-1/build-static/../Python/import.c:2779:11
    #80 0x58bee8 in PyImport_ImportModuleLevelObject /usr/src/python3.12-3.12.3-1/build-static/../Python/import.c:2862:15
    #81 0x565f6c in import_name /usr/src/python3.12-3.12.3-1/build-static/../Python/ceval.c:2482:15
    #82 0x565f6c in _PyEval_EvalFrameDefault /usr/src/python3.12-3.12.3-1/build-static/Python/bytecodes.c:2135:19
    #83 0x560070 in _PyEval_EvalFrame /usr/src/python3.12-3.12.3-1/build-static/../Include/internal/pycore_ceval.h:89:16
    #84 0x560070 in _PyEval_Vector /usr/src/python3.12-3.12.3-1/build-static/../Python/ceval.c:1683:12
    #85 0x560070 in PyEval_EvalCode /usr/src/python3.12-3.12.3-1/build-static/../Python/ceval.c:578:21
    #86 0x55d264 in builtin_exec_impl /usr/src/python3.12-3.12.3-1/build-static/../Python/bltinmodule.c:1096:17
    #87 0x55d264 in builtin_exec /usr/src/python3.12-3.12.3-1/build-static/../Python/clinic/bltinmodule.c.h:586:20
    #88 0x502a38 in cfunction_vectorcall_FASTCALL_KEYWORDS /usr/src/python3.12-3.12.3-1/build-static/../Objects/methodobject.c:438:24
    #89 0x5652ac in _PyEval_EvalFrameDefault /usr/src/python3.12-3.12.3-1/build-static/Python/bytecodes.c:3254:26
    #90 0x4c2f00 in _PyObject_VectorcallTstate /usr/src/python3.12-3.12.3-1/build-static/../Include/internal/pycore_call.h:92:11
    #91 0x4c2f00 in object_vacall /usr/src/python3.12-3.12.3-1/build-static/../Objects/call.c:850:14
    #92 0x4c4ae4 in PyObject_CallMethodObjArgs /usr/src/python3.12-3.12.3-1/build-static/../Objects/call.c:911:24
    #93 0x58bee8 in import_find_and_load /usr/src/python3.12-3.12.3-1/build-static/../Python/import.c:2779:11
    #94 0x58bee8 in PyImport_ImportModuleLevelObject /usr/src/python3.12-3.12.3-1/build-static/../Python/import.c:2862:15
    #95 0x565f6c in import_name /usr/src/python3.12-3.12.3-1/build-static/../Python/ceval.c:2482:15
    #96 0x565f6c in _PyEval_EvalFrameDefault /usr/src/python3.12-3.12.3-1/build-static/Python/bytecodes.c:2135:19
    #97 0x560070 in _PyEval_EvalFrame /usr/src/python3.12-3.12.3-1/build-static/../Include/internal/pycore_ceval.h:89:16
    #98 0x560070 in _PyEval_Vector /usr/src/python3.12-3.12.3-1/build-static/../Python/ceval.c:1683:12
    #99 0x560070 in PyEval_EvalCode /usr/src/python3.12-3.12.3-1/build-static/../Python/ceval.c:578:21
    #100 0x55d264 in builtin_exec_impl /usr/src/python3.12-3.12.3-1/build-static/../Python/bltinmodule.c:1096:17
    #101 0x55d264 in builtin_exec /usr/src/python3.12-3.12.3-1/build-static/../Python/clinic/bltinmodule.c.h:586:20
    #102 0x502a38 in cfunction_vectorcall_FASTCALL_KEYWORDS /usr/src/python3.12-3.12.3-1/build-static/../Objects/methodobject.c:438:24
    #103 0x5652ac in _PyEval_EvalFrameDefault /usr/src/python3.12-3.12.3-1/build-static/Python/bytecodes.c:3254:26
    #104 0x4c2f00 in _PyObject_VectorcallTstate /usr/src/python3.12-3.12.3-1/build-static/../Include/internal/pycore_call.h:92:11
    #105 0x4c2f00 in object_vacall /usr/src/python3.12-3.12.3-1/build-static/../Objects/call.c:850:14
    #106 0x4c4ae4 in PyObject_CallMethodObjArgs /usr/src/python3.12-3.12.3-1/build-static/../Objects/call.c:911:24
    #107 0x58bee8 in import_find_and_load /usr/src/python3.12-3.12.3-1/build-static/../Python/import.c:2779:11
    #108 0x58bee8 in PyImport_ImportModuleLevelObject /usr/src/python3.12-3.12.3-1/build-static/../Python/import.c:2862:15
    #109 0x565f6c in import_name /usr/src/python3.12-3.12.3-1/build-static/../Python/ceval.c:2482:15
    #110 0x565f6c in _PyEval_EvalFrameDefault /usr/src/python3.12-3.12.3-1/build-static/Python/bytecodes.c:2135:19
    #111 0x560070 in _PyEval_EvalFrame /usr/src/python3.12-3.12.3-1/build-static/../Include/internal/pycore_ceval.h:89:16
    #112 0x560070 in _PyEval_Vector /usr/src/python3.12-3.12.3-1/build-static/../Python/ceval.c:1683:12
    #113 0x560070 in PyEval_EvalCode /usr/src/python3.12-3.12.3-1/build-static/../Python/ceval.c:578:21
    #114 0x598ddc in run_eval_code_obj /usr/src/python3.12-3.12.3-1/build-static/../Python/pythonrun.c:1722:9
    #115 0x598ddc in run_mod /usr/src/python3.12-3.12.3-1/build-static/../Python/pythonrun.c:1743:19
    #116 0x598ddc in PyRun_StringFlags /usr/src/python3.12-3.12.3-1/build-static/../Python/pythonrun.c:1618:15
    #117 0x67c990 in PyRun_SimpleStringFlags /usr/src/python3.12-3.12.3-1/build-static/../Python/pythonrun.c:480:9
    #118 0x68919c in pymain_run_command /usr/src/python3.12-3.12.3-1/build-static/../Modules/main.c:255:11
    #119 0x68919c in pymain_run_python /usr/src/python3.12-3.12.3-1/build-static/../Modules/main.c:620:21
    #120 0x68919c in Py_RunMain /usr/src/python3.12-3.12.3-1/build-static/../Modules/main.c:709:5
    #121 0x688ca4 in Py_BytesMain /usr/src/python3.12-3.12.3-1/build-static/../Modules/main.c:763:12
    #122 0xffff941284c0 in __libc_start_call_main csu/../sysdeps/nptl/libc_start_call_main.h:58:16
    #123 0xffff94128594 in __libc_start_main csu/../csu/libc-start.c:360:3
    #124 0x5f24ec in _start (/usr/bin/python3.12+0x5f24ec) (BuildId: 18160fe6beb052a7e6830ecc99e313a3498c377d)


Thread: T0 0xeffe00002000 stack: [0xfffffb0da000,0xfffffb8da000) sz: 8388608 tls: [0xffff94ed7460,0xffff94ed8320)
Thread: T1 0xeffe00012000 stack: [0xffff8eb00000,0xffff8f2ff140) sz: 8384832 tls: [0xffff8f2ff140,0xffff8f300000)
Thread: T2 0xeffe00022000 stack: [0xffff8e2f0000,0xffff8eaef140) sz: 8384832 tls: [0xffff8eaef140,0xffff8eaf0000)
Thread: T3 0xeffe00032000 stack: [0xffff8dae0000,0xffff8e2df140) sz: 8384832 tls: [0xffff8e2df140,0xffff8e2e0000)
Thread: T4 0xeffe00042000 stack: [0xffff8d2d0000,0xffff8dacf140) sz: 8384832 tls: [0xffff8dacf140,0xffff8dad0000)
Thread: T5 0xeffe00052000 stack: [0xffff8cac0000,0xffff8d2bf140) sz: 8384832 tls: [0xffff8d2bf140,0xffff8d2c0000)
Thread: T6 0xeffe00062000 stack: [0xffff8c2b0000,0xffff8caaf140) sz: 8384832 tls: [0xffff8caaf140,0xffff8cab0000)
Thread: T7 0xeffe00072000 stack: [0xffff8baa0000,0xffff8c29f140) sz: 8384832 tls: [0xffff8c29f140,0xffff8c2a0000)
Thread: T8 0xeffe00082000 stack: [0xffff8b290000,0xffff8ba8f140) sz: 8384832 tls: [0xffff8ba8f140,0xffff8ba90000)
Thread: T9 0xeffe00092000 stack: [0xffff8aa80000,0xffff8b27f140) sz: 8384832 tls: [0xffff8b27f140,0xffff8b280000)
Thread: T10 0xeffe000a2000 stack: [0xffff8a270000,0xffff8aa6f140) sz: 8384832 tls: [0xffff8aa6f140,0xffff8aa70000)
Thread: T11 0xeffe000b2000 stack: [0xffff89a60000,0xffff8a25f140) sz: 8384832 tls: [0xffff8a25f140,0xffff8a260000)
Thread: T12 0xeffe000c2000 stack: [0xffff89250000,0xffff89a4f140) sz: 8384832 tls: [0xffff89a4f140,0xffff89a50000)
Thread: T13 0xeffe000d2000 stack: [0xffff88a40000,0xffff8923f140) sz: 8384832 tls: [0xffff8923f140,0xffff89240000)

Memory tags around the buggy address (one tag corresponds to 16 bytes):
  0xffff92392f00: 00  00  00  00  00  00  00  00  00  00  00  00  00  00  00  00
  0xffff92393000: 00  00  00  00  00  00  00  00  00  00  00  00  00  00  00  00
  0xffff92393100: 00  00  00  00  00  00  00  00  00  00  00  00  00  00  00  00
  0xffff92393200: 00  00  00  00  00  00  00  00  00  00  00  00  00  00  00  00
  0xffff92393300: 00  00  00  00  00  00  00  00  00  00  00  00  00  00  00  00
  0xffff92393400: 00  00  00  00  00  00  00  00  00  00  00  00  00  00  00  00
  0xffff92393500: 00  00  00  00  00  00  00  00  00  00  00  00  00  00  00  00
  0xffff92393600: 00  00  00  00  00  00  00  00  00  00  00  00  00  00  00  00
=>0xffff92393700: 00  00  00  00  00  00  00  00  00  00 [00] 00  00  00  00  00
  0xffff92393800: 00  00  00  00  00  00  00  00  00  00  00  00  00  00  00  00
  0xffff92393900: 00  00  00  00  00  00  00  00  00  00  00  00  00  00  00  00
  0xffff92393a00: 00  00  00  00  00  00  00  00  00  00  00  00  00  00  00  00
  0xffff92393b00: 00  00  00  00  00  00  00  00  00  00  00  00  00  00  00  00
  0xffff92393c00: 00  00  00  00  00  00  00  00  00  00  00  00  00  00  00  00
  0xffff92393d00: 00  00  00  00  00  00  00  00  00  00  00  00  00  00  00  00
  0xffff92393e00: 00  00  00  00  00  00  00  00  00  00  00  00  00  00  00  00
  0xffff92393f00: 00  00  00  00  00  00  00  00  00  00  00  00  00  00  00  00
Tags for short granules around the buggy address (one tag corresponds to 16 bytes):
  0xffff92393600: ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..
=>0xffff92393700: ..  ..  ..  ..  ..  ..  ..  ..  ..  .. [..] ..  ..  ..  ..  ..
  0xffff92393800: ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..
See https://clang.llvm.org/docs/HardwareAssistedAddressSanitizerDesign.html#short-granules for a description of short granule tags
SUMMARY: HWAddressSanitizer: tag-mismatch /src/build/../numpy/_core/src/common/npy_cpu_features.c:780:5 in npy__cpu_init_features
As a sanity check, we tried to import numpy.
Stopping. Please investigate the build error.
```

### Notes
Compiler warnings:
```
[142/323] Compiling C object numpy/_core/_multiarray_umath.cpython-312-aarch64-linux-gnu.so.p/src_multiarray_alloc.c.o
../numpy/_core/src/multiarray/alloc.c: In function ‘PyDataMem_RENEW’:
../numpy/_core/src/multiarray/alloc.c:274:9: warning: pointer ‘ptr’ may be used after ‘realloc’ [-Wuse-after-free]
  274 |         PyTraceMalloc_Untrack(NPY_TRACE_DOMAIN, (npy_uintp)ptr);
      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../numpy/_core/src/multiarray/alloc.c:272:14: note: call to ‘realloc’ here
  272 |     result = realloc(ptr, size);
      |              ^~~~~~~~~~~~~~~~~~
```
```
[8/323] Generating numpy/generate-version with a custom command
Saving version to numpy/version.py
[141/323] Compiling C object numpy/_core/_multiarray_umath.cpython-312-aarch64-linux-gnu.so.p/src_multiarray_alloc.c.o
../numpy/_core/src/multiarray/alloc.c: In function ‘PyDataMem_RENEW’:
../numpy/_core/src/multiarray/alloc.c:274:9: warning: pointer ‘ptr’ may be used after ‘realloc’ [-Wuse-after-free]
  274 |         PyTraceMalloc_Untrack(NPY_TRACE_DOMAIN, (npy_uintp)ptr);
      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../numpy/_core/src/multiarray/alloc.c:272:14: note: call to ‘realloc’ here
  272 |     result = realloc(ptr, size);
      |              ^~~~~~~~~~~~~~~~~~
[267/323] Compiling C++ object numpy/_core/_multiarray_umath.cpython-312-aarch64-linux-gnu.so.p/src_umath_stringdtype_ufuncs.cpp.o
In file included from ../numpy/_core/src/umath/stringdtype_ufuncs.cpp:21:
../numpy/_core/src/umath/string_buffer.h:265:23: warning: template-id not allowed for constructor in C++20 [-Wtemplate-id-cdtor]
  265 |     inline Buffer<enc>()
      |                       ^
../numpy/_core/src/umath/string_buffer.h:265:23: note: remove the ‘< >’
../numpy/_core/src/umath/string_buffer.h:270:23: warning: template-id not allowed for constructor in C++20 [-Wtemplate-id-cdtor]
  270 |     inline Buffer<enc>(char *buf_, npy_int64 elsize_)
      |                       ^
../numpy/_core/src/umath/string_buffer.h:270:23: note: remove the ‘< >’
[284/323] Compiling C++ object numpy/_core/_multiarray_umath.cpython-312-aarch64-linux-gnu.so.p/src_umath_string_ufuncs.cpp.o
In file included from ../numpy/_core/src/umath/string_ufuncs.cpp:21:
../numpy/_core/src/umath/string_buffer.h:265:23: warning: template-id not allowed for constructor in C++20 [-Wtemplate-id-cdtor]
  265 |     inline Buffer<enc>()
      |                       ^
../numpy/_core/src/umath/string_buffer.h:265:23: note: remove the ‘< >’
../numpy/_core/src/umath/string_buffer.h:270:23: warning: template-id not allowed for constructor in C++20 [-Wtemplate-id-cdtor]
  270 |     inline Buffer<enc>(char *buf_, npy_int64 elsize_)
      |                       ^
../numpy/_core/src/umath/string_buffer.h:270:23: note: remove the ‘< >’ 
```
",2024-07-11 16:51:43,,BUG: Failed to build from source,['00 - Bug']
26899,open,ArthurGW,"### Describe the issue:

When creating a masked array with object dtype, with the init argument `fill_value=datetime.datetime.max` (for example), then later changing the fill value to a different datetime value, calling `broken.astype('datetime64[...]')` results in an error converting the fill value to the new dtype. (This is also the case if you re-set exactly the same fill value.)

Internally, the fill value you pass in ends up wrapped in two layers of numpy.array, i.e. `broken._fill_value`  == `array(array(datetime.datetime(1, 1, 1, 0, 0), dtype=object), dtype=object)`. This object cannot be converted.

I suspect this would happen for other object arrays, since the code that wraps the fill value in an array to make it a numpy scalar doesn't do anything specific with datetimes.

The error message you get is a bit misleading, ""TypeError: Cannot convert fill_value 0001-01-01 00:00:00 to dtype datetime64[s]"", because it doesn't show that it's actually wrapped in arrays. Took me a while to puzzle over that one!

There is a workaround: instead of _changing_ the fill value on the MA, create a new MA: `numpy.ma.array(broken, fill_value=datetime.datetime.min).astype('datetime64[...])` works. I don't know if this results in any inefficient copying though.

### Reproduce the code example:

```python
import numpy as np

# This array converts fine
works = numpy.ma.array([datetime.datetime.now(), None],
                       mask=[False, True], 
                       fill_value=datetime.datetime.min)

works.fill_value  # output: datetime.datetime(1, 1, 1, 0, 0)
works._fill_value  # output: array(datetime.datetime(1, 1, 1, 0, 0), dtype=object)
works.astype('datetime64[s]')  # works

# This doesn't after setting `broken.fill_value`
broken = numpy.ma.array([datetime.datetime.now(), None],
                        mask=[False, True],
                        fill_value=datetime.datetime.max)

broken.fill_value = datetime.datetime.min  # This breaks things
broken.fill_value  # output: array(datetime.datetime(1, 1, 1, 0, 0), dtype=object) - note the extra array/scalar wrapper around the object
broken._fill_value  # output: array(array(datetime.datetime(1, 1, 1, 0, 0), dtype=object), dtype=object) - note the double array wrapper
broken.astype('datetime64[s]')  # fails
```


### Error message:

```shell
Traceback (most recent call last):
  File ""...\lib\site-packages\numpy\ma\core.py"", line 462, in _check_fill_value
    fill_value = np.array(fill_value, copy=False, dtype=ndtype)
ValueError: Could not convert object to NumPy datetime

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""...\lib\site-packages\numpy\ma\core.py"", line 3062, in __array_finalize__
    self._fill_value = _check_fill_value(self._fill_value, self.dtype)
  File ""...\lib\site-packages\numpy\ma\core.py"", line 468, in _check_fill_value
    raise TypeError(err_msg % (fill_value, ndtype)) from e
TypeError: Cannot convert fill_value 0001-01-01 00:00:00 to dtype datetime64[s]
```


### Python and NumPy Versions:

1.21.6
3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]

### Runtime Environment:

<numpy version 1.21>

### Context for the issue:

We often use object arrays of python datetime.date(/time) instances",2024-07-10 07:17:38,,BUG: Cannot convert masked array of python datetimes to datetime64 after changing fill value,"['00 - Bug', '57 - Close?']"
26893,open,flying-sheep,"### Describe the issue:

See title: serializing a string array to a buffer and trying to deserialize it fails.

No matter if if the problem is generic or about using `None` as `na_object`, the error message is wrong:
I’m not trying to create an object array.

### Reproduce the code example:

```python
import numpy as np

string_dtype = np.dtypes.StringDType(na_object=None)

buf = np.array([[None], [""bcdef""]], dtype=dtype).tobytes()
np.frombuffer(buf, dtype=string_dtype)
```


### Error message:

```shell
ValueError
----> 6 np.frombuffer(buf, dtype=string_dtype)
ValueError: cannot create an OBJECT array from memory buffer
```


### Python and NumPy Versions:

2.0.0
3.12.4 (main, Jun  7 2024, 06:33:07) [GCC 14.1.1 20240522]

### Runtime Environment:

```python
[{'numpy_version': '2.0.0',
  'python': '3.12.4 (main, Jun  7 2024, 06:33:07) [GCC 14.1.1 20240522]',
  'uname': uname_result(system='Linux', node='gnosis', release='6.9.8-zen1-1-zen', version='#1 ZEN SMP PREEMPT_DYNAMIC Fri, 05 Jul 2024 22:11:01 +0000', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL',
                                    'AVX512_SPR']}},
 {'architecture': 'Zen',
  'filepath': '/usr/lib/libopenblas.so.0.3',
  'internal_api': 'openblas',
  'num_threads': 16,
  'prefix': 'libopenblas',
  'threading_layer': 'openmp',
  'user_api': 'blas',
  'version': '0.3.27'},
 {'filepath': '/usr/lib/libgomp.so.1.0.0',
  'internal_api': 'openmp',
  'num_threads': 16,
  'prefix': 'libgomp',
  'user_api': 'openmp',
  'version': None}]
```

### Context for the issue:

_No response_",2024-07-09 12:57:03,,ENH: improve StringDType error in from buffer results,['01 - Enhancement']
26886,open,chjz1024,"### Describe the issue:

Found searchsorted is extremely slow on large structured arrays (around 10 billilion elements) which are sorted on some given fields. Further investigation shows the main culprits seem to be the different memory layouts and datatypes. Should this be a feature or just a bug? I suppose this is not the expected behaviour of this function.

### Reproduce the code example:

```python
import numpy as np
N = 100_000_000
test_array = np.zeros(N, dtype=[('x',np.int64),('y',np.int32),('z',np.float64)])
test_array['x']+=np.arange(N)
test_array['z']+=np.arange(N)
x_copy = test_array['x'].copy()
z_copy = test_array['z'].copy()
%timeit np.searchsorted(test_array['x'], np.random.randint(N)) # 165ms
%timeit np.searchsorted(test_array['x'], float(np.random.randint(N))+0.5) # 163ms
%timeit np.searchsorted(x_copy, np.random.randint(N)) # 3.93us
%timeit np.searchsorted(x_copy, float(np.random.randint(N))) # 131ms
%timeit np.searchsorted(test_array['z'], np.random.randint(N)) # 160ms
%timeit np.searchsorted(test_array['z'], float(np.random.randint(N))+0.5) # 164ms
%timeit np.searchsorted(z_copy, np.random.randint(N)) # 3.71us
%timeit np.searchsorted(z_copy, float(np.random.randint(N))) # 3.59us
```


### Error message:

_No response_

### Python and NumPy Versions:

1.26.4
3.9.19 (main, May  6 2024, 20:12:36) [MSC v.1916 64 bit (AMD64)]

### Runtime Environment:

_No response_

### Context for the issue:

My current workaround is to manually breackdown the array into smaller ones and apply the searchsorted function, which shows a decent speedup for structured array (still 10x slower than the original searchsorted on plain arrays), to avoid copying very-large arrays. However, hope this performance issue to be fixed in the following numpy releases.
```py
def searchsortedNTree(array, xs, N=1000):
    def searchSortedNTree(array, x, N):
        if array.size > N:
            stride = array.shape[0]//N+1
            idx = np.searchsorted(array[::stride], x)
            if idx*stride < array.shape[0] and array[idx*stride] == x:
                return idx*stride
            else:
                return (idx-1)*stride+searchSortedNTree(array[(idx-1)*stride:idx*stride], x, N)
        else:
            return np.searchsorted(array, x)
        
    return np.array([searchSortedNTree(array, x, N) for x in xs])
```",2024-07-07 03:43:04,,ENH: Support strided layout in numpy.searchsorted,['00 - Bug']
26878,open,yangdong02,"### Proposed new feature or change:

Following https://github.com/numpy/numpy/issues/11161 and related PR, we can support no-copy pickling for C-contiguous and Fortran-contiguous numpy arrays. However, if we have a 3D C-contiguous array, then transpose its first two axis (e.g. `np.random.rand(2,3,4).transpose((1,0,2))`), it will be neither C-contiguous nor Fortran-contiguous, therefore won't enjoy no-copy pickling. To address this, can we generalize no-copy pickling further to any numpy arrays that can be transposed to a C-contiguous arrays?

Implementing this should be simple. In fact, Fortran-contiguous numpy arrays is supported via transposing to C-contiguous array during serialization, then transpose back during deserialization (see below code). We can generalize this idea:

- First, look at strides and size, and figure out whether the array can be transposed to a C-contiguous array
- Then, we do the transpose and record the original dimension order during serialization.
- Finally, during deserialization, we transpose the array back based on the recorded original dimension order.

I can implement this feature and create a PR if you think this enhancement makes sense :)

https://github.com/numpy/numpy/blob/b77d2c6cc214cdcde567f356688ebddb2a5e7c8c/numpy/_core/src/multiarray/methods.c#L1877-L1885

https://github.com/numpy/numpy/blob/b77d2c6cc214cdcde567f356688ebddb2a5e7c8c/numpy/_core/numeric.py#L1914-L1915",2024-07-06 09:19:16,,ENH: support no-copy pickling for any array that can be transposed to a C-contiguous array,['unlabeled']
26861,open,gerritholl,"### Describe the issue:

According to `np.issubdtype(""m8"", np.integer)`, timedelta should be considered an integer dtype.  I would expect that any integer dtype is accepted by `np.iinfo`, but not so: `np.iinfo(""m8"")` fails with `ValueError`.

### Reproduce the code example:

```python
import numpy as np
print(np.issubdtype(""m8"", np.integer))
print(np.iinfo(""m8""))
```


### Error message:

```shell
True
Traceback (most recent call last):
  File ""/data/gholl/checkouts/protocode/mwe/np-timedelta-int.py"", line 3, in <module>
    print(np.iinfo(""m8""))
          ^^^^^^^^^^^^^^
  File ""/data/gholl/mambaforge/envs/npy2/lib/python3.12/site-packages/numpy/_core/getlimits.py"", line 697, in __init__
    raise ValueError(""Invalid integer data type %r."" % (self.kind,))
ValueError: Invalid integer data type 'm'.
```


### Python and NumPy Versions:

* Numpy 2.0.0
* Python 3.12.4 | packaged by conda-forge | (main, Jun 17 2024, 10:23:07) [GCC 12.3.0]



### Runtime Environment:

```
[{'numpy_version': '2.0.0',
  'python': '3.12.4 | packaged by conda-forge | (main, Jun 17 2024, 10:23:07) '
            '[GCC 12.3.0]',
  'uname': uname_result(system='Linux', node='oflws222', release='5.3.18-150300.59.106-default', version='#1 SMP Mon Dec 12 13:16:24 UTC 2022 (774239c)', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL',
                                    'AVX512_SPR']}},
 {'architecture': 'Haswell',
  'filepath': '/data/gholl/mambaforge/envs/npy2/lib/libopenblasp-r0.3.27.so',
  'internal_api': 'openblas',
  'num_threads': 8,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.27'}]
```

### Context for the issue:

I can deal with timedelta being integers or not.  But I think users should be able to expect that any dtype x for which `npissubdtype(x, np.integer)` holds, should also be a valid argument to `np.iinfo`.  Either the former is False and the latter a ValueError, or the former is True and the latter works, but it is unexpected for the former to be True and the latter to fail with ValueError.",2024-07-05 11:59:54,,BUG:  issubdtype and iinfo disagree on whether timedelta is an integer,['00 - Bug']
26844,open,tylerjereddy,"Sebastian asked me to open this as a follow-up to the bug fix work in gh-26558. Here's a small reproducer he provided:

```python
a = np.arange(10)
a[2:][a[:-2]] = 3
```

To expand on that a bit, even in the cross-linked branch `a` works out to be `[0 1 3 3 4 3 3 7 8 3]`, while if we use `a.copy()` for the sub-indexing we get `[0 1 3 3 3 3 3 3 3 3]` for `a`. I suspect we want the latter behavior by default, without requiring the user to explicitly copy, based on Sebastian's comments.",2024-07-03 15:43:06,,BUG: memory overlap in some tricky indexing scenarios,"['00 - Bug', 'component: numpy._core']"
26819,open,agriyakhetarpal,"### Steps to reproduce:

I am trying to run the doctests for NumPy with `spin check-docs` on a clean build (I have performed `git clean -xdf` beforehand), and I am facing issues with a few of the doctests locally, which do not fail in CI runs on PRs I can notice at the time of writing.

The steps to reproduce are as follows:

1. `mamba env create --file environment.yml` (`mamba` has been installed via `brew install miniforge`)
2. `mamba activate numpy-dev`
3. `spin check-docs`


### Error message:

```shell
❯ spin check-docs
Invoking `build` prior to running tests:
$ /opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/bin/python3.11 vendored-meson/meson/meson.py compile -C build
INFO: autodetecting backend as ninja
INFO: calculating backend command to run: /opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/bin/ninja -C /Users/agriyakhetarpal/Desktop/numpy/build
ninja: Entering directory `/Users/agriyakhetarpal/Desktop/numpy/build'
[1/1] Generating numpy/generate-version with a custom command
Saving version to numpy/version.py
$ /opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/bin/python3.11 vendored-meson/meson/meson.py install --only-changed -C build --destdir ../build-install
$ export PYTHONPATH=""/Users/agriyakhetarpal/Desktop/numpy/build-install/usr/lib/python3.11/site-packages""
$ /opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/bin/python3.11 -P -c 'import numpy'
$ export PYTHONPATH=""/Users/agriyakhetarpal/Desktop/numpy/build-install/usr/lib/python3.11/site-packages""
$ cd /Users/agriyakhetarpal/Desktop/numpy/build-install/usr/lib/python3.11/site-packages
$ /opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/bin/python3.11 -m pytest --rootdir=/Users/agriyakhetarpal/Desktop/numpy/build-install/usr/lib/python3.11/site-packages numpy --doctest-modules --doctest-collect=api
================================================================== test session starts ===================================================================
platform darwin -- Python 3.11.9, pytest-8.2.2, pluggy-1.5.0
rootdir: /Users/agriyakhetarpal/Desktop/numpy/build-install/usr/lib/python3.11/site-packages
configfile: ../../../../../pytest.ini
plugins: cov-5.0.0, hypothesis-6.104.1, scipy_doctest-1.3, xdist-3.6.1
collected 3892 items  

numpy/linalg/__init__.py .......F...............                                                                                                   [ 46%]

... (truncated output)

/opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/lib/python3.11/site-packages/scipy_doctest/impl.py:305: SyntaxError

During handling of the above exception, another exception occurred:

self = <scipy_doctest.impl.DTChecker object at 0x10a6ea6d0>, want = 'array([1., 6.])\narray([6.+0.j, 1.+0.j])'
got = 'array([1., 6.])\narray([6.-7.39557099e-32j, 1.-6.16297582e-33j])', optionflags = 12

    def check_output(self, want, got, optionflags):
    
        # cut it short if they are equal
        if want == got:
            return True
    
        # skip random stuff
        if any(word in want for word in self.rndm_markers):
            return True
    
        # skip function/object addresses
        if self.obj_pattern.search(got):
            return True
    
        # ignore comments (e.g. signal.freqresp)
        if want.lstrip().startswith(""#""):
            return True
    
        # try the standard doctest
        try:
            if self.vanilla.check_output(want, got, optionflags):
                return True
        except Exception:
            pass
    
        # OK then, convert strings to objects
        ns = dict(self.config.check_namespace)
        try:
            with warnings.catch_warnings():
                # NumPy's ragged array deprecation of np.array([1, (2, 3)]);
                # also array abbreviations: try `np.diag(np.arange(1000))`
                warnings.simplefilter('ignore', VisibleDeprecationWarning)
    
>               a_want = eval(want, dict(ns))
E                 File ""<string>"", line 2
E                   array([6.+0.j, 1.+0.j])
E                   ^^^^^
E               SyntaxError: invalid syntax

cond       = False
got        = 'array([1., 6.])\narray([6.-7.39557099e-32j, 1.-6.16297582e-33j])'
ndim_array = True
ns         = {'Inf': inf, 'NaN': nan, 'StringDType': <class 'numpy.dtypes.StringDType'>, 'array': <built-in function array>, ...}
optionflags = 12
s_got      = 'array([1., 6.])\narray([6.-7.39557099e-32j, 1.-6.16297582e-33j])'
s_want     = 'array([1., 6.])\narray([6.+0.j, 1.+0.j])'
self       = <scipy_doctest.impl.DTChecker object at 0x10a6ea6d0>
want       = 'array([1., 6.])\narray([6.+0.j, 1.+0.j])'

/opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/lib/python3.11/site-packages/scipy_doctest/impl.py:305: SyntaxError

During handling of the above exception, another exception occurred:

self = <scipy_doctest.impl.DTChecker object at 0x10a6ea6d0>, want = 'array([1., 6.])\narray([6.+0.j, 1.+0.j])'
got = 'array([1., 6.])\narray([6.-7.39557099e-32j, 1.-6.16297582e-33j])', optionflags = 12

    def check_output(self, want, got, optionflags):
    
        # cut it short if they are equal
        if want == got:
            return True
    
        # skip random stuff
        if any(word in want for word in self.rndm_markers):
            return True
    
        # skip function/object addresses
        if self.obj_pattern.search(got):
            return True
    
        # ignore comments (e.g. signal.freqresp)
        if want.lstrip().startswith(""#""):
            return True
    
        # try the standard doctest
        try:
            if self.vanilla.check_output(want, got, optionflags):
                return True
        except Exception:
            pass
    
        # OK then, convert strings to objects
        ns = dict(self.config.check_namespace)
        try:
            with warnings.catch_warnings():
                # NumPy's ragged array deprecation of np.array([1, (2, 3)]);
                # also array abbreviations: try `np.diag(np.arange(1000))`
                warnings.simplefilter('ignore', VisibleDeprecationWarning)
    
>               a_want = eval(want, dict(ns))
E                 File ""<string>"", line 2
E                   array([6.+0.j, 1.+0.j])
E                   ^^^^^
E               SyntaxError: invalid syntax

cond       = False
got        = 'array([1., 6.])\narray([6.-7.39557099e-32j, 1.-6.16297582e-33j])'
ndim_array = True
ns         = {'Inf': inf, 'NaN': nan, 'StringDType': <class 'numpy.dtypes.StringDType'>, 'array': <built-in function array>, ...}
optionflags = 12
s_got      = 'array([1., 6.])\narray([6.-7.39557099e-32j, 1.-6.16297582e-33j])'
s_want     = 'array([1., 6.])\narray([6.+0.j, 1.+0.j])'
self       = <scipy_doctest.impl.DTChecker object at 0x10a6ea6d0>
want       = 'array([1., 6.])\narray([6.+0.j, 1.+0.j])'

/opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/lib/python3.11/site-packages/scipy_doctest/impl.py:305: SyntaxError

During handling of the above exception, another exception occurred:

self = <scipy_doctest.impl.DTChecker object at 0x10a6ea6d0>, want = 'array([1., 6.])\narray([6.+0.j, 1.+0.j])'
got = 'array([1., 6.])\narray([6.-7.39557099e-32j, 1.-6.16297582e-33j])', optionflags = 12

... (truncated output, several lines long)

/opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/lib/python3.11/site-packages/pluggy/_callers.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @pytest.hookimpl(wrapper=True, tryfirst=True)
    def pytest_runtest_call() -> Generator[None, None, None]:
>       yield from thread_exception_runtest_hook()


/opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/lib/python3.11/site-packages/_pytest/threadexception.py:87: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def thread_exception_runtest_hook() -> Generator[None, None, None]:
        with catch_threading_exception() as cm:
            try:
>               yield

cm         = <_pytest.threadexception.catch_threading_exception object at 0x140148990>

/opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/lib/python3.11/site-packages/_pytest/threadexception.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_name = 'pytest_runtest_call'
hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/opt/homebrew/Caskroom/miniforge/base/envs/nump...pt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/lib/python3.11/site-packages/_pytest/unraisableexception.py'>>, ...]
caller_kwargs = {'item': <DoctestItem numpy.linalg.linalg.eigvalsh>}, firstresult = False

    def _multicall(
        hook_name: str,
        hook_impls: Sequence[HookImpl],
        caller_kwargs: Mapping[str, object],
        firstresult: bool,
    ) -> object | list[object]:
        """"""Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from HookCaller.__call__().
        """"""
        __tracebackhide__ = True
        results: list[object] = []
        exception = None
        only_new_style_wrappers = True
        try:  # run impl and wrapper setup functions in a loop
            teardowns: list[Teardown] = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    f""hook call must provide argument {argname!r}""
                                )
    
                    if hook_impl.hookwrapper:
                        only_new_style_wrappers = False
                        try:
                            # If this cast is not valid, a type error is raised below,
                            # which is the desired response.
                            res = hook_impl.function(*args)
                            wrapper_gen = cast(Generator[None, Result[object], None], res)
                            next(wrapper_gen)  # first yield
                            teardowns.append((wrapper_gen, hook_impl))
                        except StopIteration:
                            _raise_wrapfail(wrapper_gen, ""did not yield"")
                    elif hook_impl.wrapper:
                        try:
                            # If this cast is not valid, a type error is raised below,
                            # which is the desired response.
                            res = hook_impl.function(*args)
                            function_gen = cast(Generator[None, object, object], res)
                            next(function_gen)  # first yield
                            teardowns.append(function_gen)
                        except StopIteration:
                            _raise_wrapfail(function_gen, ""did not yield"")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException as exc:
                exception = exc
        finally:
            # Fast path - only new-style wrappers, no Result.
            if only_new_style_wrappers:
                if firstresult:  # first result hooks return a single value
                    result = results[0] if results else None
                else:
                    result = results
    
                # run all wrapper post-yield blocks
                for teardown in reversed(teardowns):
                    try:
                        if exception is not None:
                            teardown.throw(exception)  # type: ignore[union-attr]
                        else:
                            teardown.send(result)  # type: ignore[union-attr]
                        # Following is unreachable for a well behaved hook wrapper.
                        # Try to force finalizers otherwise postponed till GC action.
                        # Note: close() may raise if generator handles GeneratorExit.
                        teardown.close()  # type: ignore[union-attr]
                    except StopIteration as si:
                        result = si.value
                        exception = None
                        continue
                    except BaseException as e:
                        exception = e
                        continue
                    _raise_wrapfail(teardown, ""has second yield"")  # type: ignore[arg-type]
    
                if exception is not None:
                    raise exception.with_traceback(exception.__traceback__)
                else:
                    return result
    
            # Slow path - need to support old-style wrappers.
            else:
                if firstresult:  # first result hooks return a single value
                    outcome: Result[object | list[object]] = Result(
                        results[0] if results else None, exception
                    )
                else:
                    outcome = Result(results, exception)
    
                # run all wrapper post-yield blocks
                for teardown in reversed(teardowns):
                    if isinstance(teardown, tuple):
                        try:
                            teardown[0].send(outcome)
                        except StopIteration:
                            pass
                        except BaseException as e:
                            _warn_teardown_exception(hook_name, teardown[1], e)
                            raise
                        else:
                            _raise_wrapfail(teardown[0], ""has second yield"")
                    else:
                        try:
                            if outcome._exception is not None:
>                               teardown.throw(outcome._exception)
E                               RecursionError: maximum recursion depth exceeded

__tracebackhide__ = True
args       = [<DoctestItem numpy.linalg.linalg.eigvalsh>]
caller_kwargs = {'item': <DoctestItem numpy.linalg.linalg.eigvalsh>}
exception  = RecursionError('maximum recursion depth exceeded')
firstresult = False
function_gen = <generator object pytest_runtest_call at 0x137c93e20>
hook_impl  = <HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/lib/python3.11/site-packages/_pytest/runner.py'>>
hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/opt/homebrew/Caskroom/miniforge/base/envs/nump...pt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/lib/python3.11/site-packages/_pytest/unraisableexception.py'>>, ...]
hook_name  = 'pytest_runtest_call'
only_new_style_wrappers = False
outcome    = <pluggy._result.Result object at 0x14032fb20>
res        = <generator object pytest_runtest_call at 0x137c93e20>
results    = []
teardown   = <generator object pytest_runtest_call at 0x104ce7f40>
teardowns  = [<generator object pytest_runtest_call at 0x104ce7f40>, <generator object pytest_runtest_call at 0x15072b640>, <genera...ev/lib/python3.11/site-packages/_hypothesis_pytestplugin.py'>>), <generator object pytest_runtest_call at 0x137c93e20>]
wrapper_gen = <generator object pytest_runtest_call at 0x137789000>

/opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/lib/python3.11/site-packages/pluggy/_callers.py:167: RecursionError
!!! Recursion detected (same locals & position)
================================================================ short test summary info =================================================================
FAILED numpy/linalg/__init__.py::numpy.linalg.eigvalsh - RecursionError: maximum recursion depth exceeded
FAILED numpy/linalg/linalg.py::numpy.linalg.linalg.eigvalsh - RecursionError: maximum recursion depth exceeded
================================================== 2 failed, 2659 passed, 7 skipped in 67.41s (0:01:07) ==================================================
```


### Additional information:

The error trace is excessively long and is essentially the same lines repeated over and over, and therefore a part of it has been provided above.

Platform specification: Apple Macbook Pro machine with M3 Pro chip, arm64/AArch64 architecture aka Apple Silicon. I am running Mac OS X Sonoma, version 14.5.

Importing NumPy gives me a circular import error, which is the same as the one noticed in gh-26816 – however, the suggested solution of installing an older version of `spin`, i.e., version 0.8 does not work for me – I already have the same version. Importing NumPy from outside the root directory works of course, so this might be unrelated.

<details>
<summary>Output from np.show_config()</summary>

```
Build Dependencies:
  blas:
    detection method: pkgconfig
    found: true
    include directory: /opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/include
    lib directory: /opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/lib
    name: blas
    openblas configuration: unknown
    pc file directory: /opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/lib/pkgconfig
    version: 3.9.0
  lapack:
    detection method: pkgconfig
    found: true
    include directory: /opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/include
    lib directory: /opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/lib
    name: lapack
    openblas configuration: unknown
    pc file directory: /opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/lib/pkgconfig
    version: 3.9.0
Compilers:
  c:
    args: -ftree-vectorize, -fPIC, -fstack-protector-strong, -O2, -pipe, -isystem,
      /opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/include, -fdebug-prefix-map=/Users/runner/miniforge3/conda-bld/numpy_1718615086963/work=/usr/local/src/conda/numpy-2.0.0,
      -fdebug-prefix-map=/opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev=/usr/local/src/conda-prefix,
      -D_FORTIFY_SOURCE=2, -isystem, /opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/include,
      -mmacosx-version-min=11.0, -mmacosx-version-min=11.0
    commands: arm64-apple-darwin20.0.0-clang
    linker: ld64
    linker args: -Wl,-headerpad_max_install_names, -Wl,-dead_strip_dylibs, -Wl,-rpath,/opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/lib,
      -L/opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/lib, -ftree-vectorize,
      -fPIC, -fstack-protector-strong, -O2, -pipe, -isystem, /opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/include,
      -fdebug-prefix-map=/Users/runner/miniforge3/conda-bld/numpy_1718615086963/work=/usr/local/src/conda/numpy-2.0.0,
      -fdebug-prefix-map=/opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev=/usr/local/src/conda-prefix,
      -D_FORTIFY_SOURCE=2, -isystem, /opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/include,
      -mmacosx-version-min=11.0, -mmacosx-version-min=11.0
    name: clang
    version: 16.0.6
  c++:
    args: -ftree-vectorize, -fPIC, -fstack-protector-strong, -O2, -pipe, -stdlib=libc++,
      -fvisibility-inlines-hidden, -fmessage-length=0, -isystem, /opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/include,
      -fdebug-prefix-map=/Users/runner/miniforge3/conda-bld/numpy_1718615086963/work=/usr/local/src/conda/numpy-2.0.0,
      -fdebug-prefix-map=/opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev=/usr/local/src/conda-prefix,
      -D_FORTIFY_SOURCE=2, -isystem, /opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/include,
      -mmacosx-version-min=11.0, -mmacosx-version-min=11.0
    commands: arm64-apple-darwin20.0.0-clang++
    linker: ld64
    linker args: -Wl,-headerpad_max_install_names, -Wl,-dead_strip_dylibs, -Wl,-rpath,/opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/lib,
      -L/opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/lib, -ftree-vectorize,
      -fPIC, -fstack-protector-strong, -O2, -pipe, -stdlib=libc++, -fvisibility-inlines-hidden,
      -fmessage-length=0, -isystem, /opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/include,
      -fdebug-prefix-map=/Users/runner/miniforge3/conda-bld/numpy_1718615086963/work=/usr/local/src/conda/numpy-2.0.0,
      -fdebug-prefix-map=/opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev=/usr/local/src/conda-prefix,
      -D_FORTIFY_SOURCE=2, -isystem, /opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/include,
      -mmacosx-version-min=11.0, -mmacosx-version-min=11.0
    name: clang
    version: 16.0.6
  cython:
    commands: cython
    linker: cython
    name: cython
    version: 3.0.10
Machine Information:
  build:
    cpu: aarch64
    endian: little
    family: aarch64
    system: darwin
  cross-compiled: true
  host:
    cpu: arm64
    endian: little
    family: aarch64
    system: darwin
Python Information:
  path: /opt/homebrew/Caskroom/miniforge/base/envs/numpy-dev/bin/python
  version: '3.11'
SIMD Extensions:
  baseline:
  - NEON
  - NEON_FP16
  - NEON_VFPV4
  - ASIMD
  found:
  - ASIMDHP
  not found:
  - ASIMDFHM
```

</details>

I would be happy to provide additional logs as necessary.",2024-07-01 13:44:11,,"BUG: recursive import errors encountered with `numpy.linalg`, syntax errors with arrays for complex dtypes (macOS arm64)",['32 - Installation']
26818,open,bobbyocean,"### Describe the issue:

It appears during casting operations, numpy will unexpectedly convert large object ints to floats. Why was ONLY array `B` below converted to a float? 

### Reproduce the code example:

```python
import numpy as np
A = np.array([1,1],dtype=object) + [2**62,0]
B = np.array([1,1],dtype=object) + [2**63,0]
C = np.array([1,1],dtype=object) + [2**64,0]
D = np.array([1,1],dtype=object) + [2**63]
E = np.array([1,1],dtype=object) + [2**63,2**63]
print(type(A[0]),A[0]) # <class 'int'> 4611686018427387905
print(type(B[0]),B[0]) # <class 'float'> 9.223372036854776e+18
print(type(C[0]),C[0]) # <class 'int'> 18446744073709551617
print(type(D[0]),D[0]) # <class 'int'> 9223372036854775809
print(type(E[0]),E[0]) # <class 'int'> 9223372036854775809
```


### Error message:

```shell
None.
```


### Python and NumPy Versions:

> import sys, numpy; print(numpy.__version__); print(sys.version)
1.24.3
3.11.4 (main, Jul  5 2023, 14:15:25) [GCC 11.2.0]

### Runtime Environment:

> import numpy; numpy.show_runtime()
[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2',
                                'AVX512F',
                                'AVX512CD',
                                'AVX512_SKX',
                                'AVX512_CLX',
                                'AVX512_CNL',
                                'AVX512_ICL'],
                      'not_found': ['AVX512_KNL', 'AVX512_KNM']}},
 {'filepath': '/tools/anaconda/Anaconda_2023.07/lib/libmkl_rt.so.2',
  'internal_api': 'mkl',
  'num_threads': 80,
  'prefix': 'libmkl_rt',
  'threading_layer': 'intel',
  'user_api': 'blas',
  'version': '2023.1-Product'},
 {'filepath': '/tools/anaconda/Anaconda_2023.07/lib/libiomp5.so',
  'internal_api': 'openmp',
  'num_threads': 160,
  'prefix': 'libiomp',
  'user_api': 'openmp',
  'version': None}]

### Context for the issue:

Was working on several large combinatorial problems. At some threshold all my arrays converted to floats. Had to dig through many many loops to isolate a specific place where it was happening. I now need to go replace all my numpy casting operations with python for-loops to prevent ending up with floats (as the example code shows, adding scalers appears to not cause this problem). ",2024-06-30 22:42:03,,BUG: Numpy object arrays improperly convert python-int's to python-float's. ,['00 - Bug']
26817,open,BartSchuurmans,"### Describe the issue:

When I import `scipy.interpolate`, initializing code triggers a RecursionError in numpy (only on my production system, not locally). The error is triggered when a dtype that is defined within a f2py Fortran module is used.

### Reproduce the code example:

```python
# Initial trigger
from sklearn.neighbors import KernelDensity

# Smaller example
import scipy.interpolate._dfitpack as dfitpack
print(type(dfitpack.types.intvar.dtype)) # prints ""<class 'numpy.dtypes.Int32DType'>""
print(dfitpack.types.intvar.dtype) # prints ""int32"" locally, crashes on my production system
```


### Error message:

```shell
RecursionError: maximum recursion depth exceeded
Traceback:
<snipped>
File ""/usr/local/lib/python3.12/site-packages/redacted.py"", line 9, in <module>
    from sklearn.neighbors import KernelDensity
File ""/usr/local/lib/python3.12/site-packages/sklearn/__init__.py"", line 96, in <module>
    from .base import clone
File ""/usr/local/lib/python3.12/site-packages/sklearn/base.py"", line 19, in <module>
    from .utils._estimator_html_repr import _HTMLDocumentationLinkMixin, estimator_html_repr
File ""/usr/local/lib/python3.12/site-packages/sklearn/utils/__init__.py"", line 13, in <module>
    from ._chunking import gen_batches, gen_even_slices
File ""/usr/local/lib/python3.12/site-packages/sklearn/utils/_chunking.py"", line 8, in <module>
    from ._param_validation import Interval, validate_params
File ""/usr/local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py"", line 14, in <module>
    from .validation import _is_arraylike_not_scalar
File ""/usr/local/lib/python3.12/site-packages/sklearn/utils/validation.py"", line 29, in <module>
    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace
File ""/usr/local/lib/python3.12/site-packages/sklearn/utils/_array_api.py"", line 11, in <module>
    from .fixes import parse_version
File ""/usr/local/lib/python3.12/site-packages/sklearn/utils/fixes.py"", line 20, in <module>
    import scipy.stats
File ""/usr/local/lib/python3.12/site-packages/scipy/stats/__init__.py"", line 610, in <module>
    from ._stats_py import *
File ""/usr/local/lib/python3.12/site-packages/scipy/stats/_stats_py.py"", line 49, in <module>
    from . import distributions
File ""/usr/local/lib/python3.12/site-packages/scipy/stats/distributions.py"", line 10, in <module>
    from . import _continuous_distns
File ""/usr/local/lib/python3.12/site-packages/scipy/stats/_continuous_distns.py"", line 12, in <module>
    from scipy.interpolate import BSpline
File ""/usr/local/lib/python3.12/site-packages/scipy/interpolate/__init__.py"", line 167, in <module>
    from ._interpolate import *
File ""/usr/local/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py"", line 12, in <module>
    from . import _fitpack_py
File ""/usr/local/lib/python3.12/site-packages/scipy/interpolate/_fitpack_py.py"", line 8, in <module>
    from ._fitpack_impl import bisplrep, bisplev, dblint  # noqa: F401
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/scipy/interpolate/_fitpack_impl.py"", line 103, in <module>
    'iwrk': array([], dfitpack_int), 'u': array([], float),
            ^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/numpy/core/_dtype.py"", line 46, in __repr__
    arg_str = _construction_repr(dtype, include_align=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/numpy/core/_dtype.py"", line 100, in _construction_repr
    return _scalar_str(dtype, short=short)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/numpy/core/_dtype.py"", line 143, in _scalar_str
    elif np.issubdtype(dtype, np.number):
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/numpy/core/numerictypes.py"", line 417, in issubdtype
    arg1 = dtype(arg1).type
           ^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/numpy/core/_dtype.py"", line 46, in __repr__
    arg_str = _construction_repr(dtype, include_align=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/numpy/core/_dtype.py"", line 100, in _construction_repr
    return _scalar_str(dtype, short=short)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/numpy/core/_dtype.py"", line 143, in _scalar_str
    elif np.issubdtype(dtype, np.number):
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/numpy/core/numerictypes.py"", line 417, in issubdtype
    arg1 = dtype(arg1).type
           ^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/numpy/core/_dtype.py"", line 46, in __repr__
    arg_str = _construction_repr(dtype, include_align=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/numpy/core/_dtype.py"", line 100, in _construction_repr
    return _scalar_str(dtype, short=short)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/numpy/core/_dtype.py"", line 143, in _scalar_str
    elif np.issubdtype(dtype, np.number):
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/numpy/core/numerictypes.py"", line 417, in issubdtype
    arg1 = dtype(arg1).type
           ^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/numpy/core/_dtype.py"", line 46, in __repr__
    arg_str = _construction_repr(dtype, include_align=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/numpy/core/_dtype.py"", line 100, in _construction_repr
    return _scalar_str(dtype, short=short)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/numpy/core/_dtype.py"", line 143, in _scalar_str
    elif np.issubdtype(dtype, np.number):
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/numpy/core/numerictypes.py"", line 417, in issubdtype
    arg1 = dtype(arg1).type
           ^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/numpy/core/_dtype.py"", line 46, in __repr__
    arg_str = _construction_repr(dtype, include_align=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/numpy/core/_dtype.py"", line 100, in _construction_repr
    return _scalar_str(dtype, short=short)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/numpy/core/_dtype.py"", line 143, in _scalar_str
    elif np.issubdtype(dtype, np.number):
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/numpy/core/numerictypes.py"", line 417, in issubdtype
    arg1 = dtype(arg1).type
           ^^^^^^^^^^^
<snipped>
```


### Python and NumPy Versions:

numpy version: 1.26.4
python version: sys.version_info(major=3, minor=12, micro=4, releaselevel='final', serial=0)

### Runtime Environment:

_No response_

### Context for the issue:

I can't use scikit-learn due to this issue. NumPy 2.0 does not show this issue, but I can't upgrade yet because Pyomo is incompatible

First reported at https://github.com/scipy/scipy/issues/21072",2024-06-29 17:51:32,,BUG: Infinite recursion when trying to use a f2py dtype,"['00 - Bug', 'component: numpy.f2py']"
26807,open,WarrenWeckesser,"### Describe the issue:

On a Linux machine where I build with gcc 11.4.0, I noticed that `np.log(z)` with a large `complex64` array is significantly slower than with a `complex128` array of the same size.  This does not happen with numpy 1.26.4.

Here's numpy 1.26.4 (installed with conda):

```
In [41]: import numpy as np

In [42]: n = 1000000

In [43]: rng = np.random.default_rng()

In [44]: z = rng.normal(loc=[0, 0], scale=2, size=(n, 2)).view(np.complex128)[:, 0]

In [45]: zs = z.astype(np.complex64)

In [46]: %timeit np.log(z)
46.4 ms ± 65.8 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

In [47]: %timeit np.log(zs)
39.6 ms ± 41.6 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
```
`complex128` takes 46.4 ms, and `complex64` takes 39.6 ms.

Here's `'2.1.0.dev0+git20240627.2f3da0d'`, built with gcc 11.4.0 on a Linux machine:

```
In [13]: import numpy as np

In [14]: np.__version__
Out[14]: '2.1.0.dev0+git20240627.2f3da0d'

In [15]: n = 1000000

In [16]: rng = np.random.default_rng()

In [17]: z = rng.normal(loc=[0, 0], scale=2, size=(n, 2)).view(np.complex128)[:, 0]

In [18]: zs = z.astype(np.complex64)

In [19]: %timeit np.log(z)
46.4 ms ± 25 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

In [20]: %timeit np.log(zs)
65.2 ms ± 88 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
```
`complex128` still takes 46.4 ms, but `complex64` takes 65.2 ms.

I don't see this performance regression on a Macbook (m2, macos 13.6).




### Python and NumPy Versions:

```
2.1.0.dev0+git20240627.2f3da0d
3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:35:02) [GCC 11.2.0]
```

### Runtime Environment:

```
[{'numpy_version': '2.1.0.dev0+git20240627.2f3da0d',
  'python': '3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, '
            '17:35:02) [GCC 11.2.0]',
  'uname': uname_result(system='Linux', node='pop-os', release='6.9.3-76060903-generic', version='#202405300957~1718348209~22.04~7817b67 SMP PREEMPT_DYNAMIC Mon J', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Zen',
  'filepath': '/usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so',
  'internal_api': 'openblas',
  'num_threads': 24,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.20'}]
```
",2024-06-27 15:19:36,,BUG: Performance regression of `np.log` with `complex64` input on Linux,['00 - Bug']
26800,open,ppashakhanloo,"### Issue with current documentation:

Hello, Numpy folks!
There is a ruff rule (`NPY201`) that is recommended for migration from numpy to numpy2. I did a thorough comparison between Python API removals and deprecations with what the `NPY201` rule provides. There are many removals and deprecations that are in the documentation but the ruff rule does not consider.
In many of the cases, a fix could be performed automatically.

Here is a summary of what is missing from the rule `NPY201`:

## Changes that are not detected at all
- `np.geterrobj`, `np.seterrobj` and the related ufunc keyword argument `extobj=` have been removed. The preferred replacement for all of these is using the context manager `with np.errstate():`.
- removed `ERR_*`, `SHIFT_*`, `np.kernel_version`, `np.numarray`, `np.oldnumeric` and `np.set_numeric_ops`.
- removed namespaces: `np.FLOATING_POINT_SUPPORT`, `np.FPE_*`, `np.CLIP`, `np.WRAP`, `np.RAISE`, `np.BUFSIZE`, `np.UFUNC_BUFSIZE_DEFAULT`, `np.UFUNC_PYVALS_NAME`, `np.ALLOW_THREADS`, `np.MAXDIMS`, `np.MAY_SHARE_EXACT`, `np.MAY_SHARE_BOUNDS`.
- `np.issctype`, `np.maximum_sctype`, `np.obj2sctype`, `np.sctype2char`, `np.sctypes` were all removed from the main namespace without replacement, as they where niche members.
- `np.compare_chararrays` has been removed from the main namespace. Use `np.char.compare_chararrays` instead.
- The `charrarray` in the main namespace has been deprecated. It can be imported without a deprecation warning from `np.char.chararray` for now, but we are planning to fully deprecate and remove `chararray` in the future.
- `np.format_parser` has been removed from the main namespace. Use `np.rec.format_parser `instead.
- The experimental `numpy.array_api` submodule has been removed. Use the main numpy namespace for regular usage instead, or the separate `array-api-strict` package for the compliance testing use case for which `numpy.array_api` was mostly used.
- Support for seven data type string aliases has been removed from `np.dtype`: `int0`, `uint0`, `void0`, `object0`, `str0`, `bytes0` and `bool8`.
- `np.trapz` has been deprecated. Use `np.trapezoid` or a `scipy.integrate` function instead.
- `np.in1d` has been deprecated. Use `np.isin` instead.
- Arrays of 2-dimensional vectors for `np.cross` have been deprecated. Use arrays of 3-dimensional vectors instead.
- `np.dtype(""a"")` alias for `np.dtype(np.bytes_)` was deprecated. Use `np.dtype(""S"")` alias instead.
- Use of keyword arguments `x` and `y` with functions `assert_array_equal` and `assert_array_almost_equal` has been deprecated. Pass the first two arguments as positional arguments instead.

## Changes that are detected but no automatic fix is provided
- `np.cast` has been removed. The literal replacement for `np.cast[dtype](arg)` is `np.asarray(arg, dtype=dtype)`.
- `np.set_string_function` has been removed. Use `np.set_printoptions` instead with a formatter for custom printing of NumPy objects.
- `np.recfromcsv` and `recfromtxt` are now only available from `np.lib.npyio`.
- `np.asfarray` has been removed. Use `np.asarray` with a proper `dtype` instead.
- `np.find_common_type` has been removed. Use `numpy.promote_types` or `numpy.result_type` instead. To achieve semantics for the `scalar_types` argument, use `numpy.result_type` and pass `0`, `0.0`, or `0j` as a Python scalar instead.
- `np.nbytes` has been removed. Use `np.dtype(<dtype>).itemsize` instead.

### Idea or request for content:

My suggestion is to add these changes to the ruff rule. Otherwise, it would be really helpful if the provided and not-provided changes were documented (either in the corresponding ruff rule page or the migration guide) so the users would know what they were getting (and not getting) by applying the rule.",2024-06-26 14:03:05,,DOC: the recommended ruff rule for migration to numpy2 is missing many detections and fixes,['04 - Documentation']
26791,open,jorisvandenbossche,"### Describe the issue:

I don't know if this is a bug (and if it would be, also probably in Accelerate and not NumPy) or rather a wrong expectation about floating point issues in matrix multiplication. 
But, for the downstream issue in shapely (https://github.com/shapely/shapely/issues/2066) this behaviour change _is_ a problem (the array represents coordinates of a ring of a Polygon, and the first and last coordinate pair (first and last row in the array) always have to be exactly equal).

The issue is that we are transformating an array of coordinates using `np.matmul`, where the first and last coordinates are equal. After transforming them, the new first and last coordinates are still equal in general, except that this started to no longer be the case with NumPy 2.0 on MacOS 14 (so probably related to those wheels using Accelerate instead of OpenBLAS).

Should we generally not rely on the `np.matmul` returning identical values for identical values in the input array?

### Reproduce the code example:

```python
# using frombuffer to get the exact numbers at full precision
>>> coords = np.frombuffer(b'\xcb\x92\x87\x89\xfe\xde4@\x92\xce\xf0W\\L,\xc0\xb3\x03b\xffN!3\xc0\xe1y\x97b\x82\xb6+\xc0&ww\x99E\xe52\xc0\xfa\x8e\xbaY\xf7*0@\x04d\x85\xa0|\x1a5@\x14\x14h\xaap\xac/@\xcb\x92\x87\x89\xfe\xde4@\x92\xce\xf0W\\L,\xc0').reshape(5, 2)
>>> coords
array([[ 20.87107143, -14.14914203],
       [-19.13011166, -13.85646351],
       [-18.89559325,  16.16783677],
       [ 21.10346416,  15.83679707],
       [ 20.87107143, -14.14914203]])

>>> A = np.frombuffer(b'7\xb3\xf1\xdd\xc7\xff\xef\xbf\xeb\xe4b\x9b\xf5\xf7}?\xeb\xe4b\x9b\xf5\xf7}\xbf7\xb3\xf1\xdd\xc7\xff\xef\xbf').reshape(2, 2)
>>> A
array([[-0.99997323,  0.00731655],
       [-0.00731655, -0.99997323]])

>>> result = np.matmul(A, coords.T).T
>>> result
array([[-20.9740357 ,  13.99605905],
       [ 19.0282181 ,  13.99605905],
       [ 19.01338028, -16.02915345],
       [-20.98702857, -15.99077774],
       [-20.9740357 ,  13.99605905]])

# first and last row have *exactly* equal values in the input
>>> coords[0, :] == coords[-1, :]
array([ True,  True])

# for me (on linux, with OpenBLAS) this is still the case in the output
>>> result[0, :] == result[-1, :]
array([ True,  True])
```

But from other people that have tested this on MacOS 14, the last line here returns `False`.

### Python and NumPy Versions:

2.0.0
3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]

(I haven't included my Runtime Environment information, because I cannot actually reproduce it on my system (Linux))",2024-06-24 14:39:58,,BUG?: precision issue using np.matmul on macOS 14 with Accelerate,['00 - Bug']
26783,open,Andrej730,"### Proposed new feature or change:

Consider the snippet below.
```python
from typing_extensions import Buffer

class ClassA:
    def accept_buffer2(self, buffer: Buffer):
        pass

a = ClassA()

from array import array
import numpy as np
array_buffer = array(""l"", [1, 2, 3, 4, 5])
np_buffer = np.array([1, 2, 3, 4, 5], dtype=np.int32)
print(memoryview(array_buffer))
# typing issue
print(memoryview(np_buffer))

a.accept_buffer2(array_buffer)
# Argument of type ""NDArray[signedinteger[_32Bit]]"" cannot be assigned to parameter ""buffer"" of type ""Buffer"" in function ""accept_buffer2""
#   ""ndarray[Any, dtype[signedinteger[_32Bit]]]"" is incompatible with protocol ""Buffer""
#     ""__buffer__"" is not present
a.accept_buffer2(np_buffer)
```

In Python 3.11 it will fail as `__buffer__` in a stub file is only implemented for Python 3.12:
https://github.com/numpy/numpy/blob/768724556fd9a60556fea5203b1489bb51c507a5/numpy/__init__.pyi#L1462-L1463

But technically it buffer protocol is implemented before Python 3.11 but it's just done internally, not using `__buffer__`.

`__buffer__` dunder seems to be the way `Buffer` checks whether some class has implemented buffer protocol and `Buffer` is available not only in Python 3.12 with `collections.abc.Buffer` but for older python versions too using `typing_extensions.Buffer`.

In 3.12 `np.ndarray.__buffer__` is indeed accessable and in <3.11 it isn't but standard library's `array.array` has a similar issue and they implement `__buffer__` for convenience.
```python
# Python 3.12+
> print(np_buffer.__buffer__)
<method-wrapper '__buffer__' of numpy.ndarray object at 0x7f152ba13f90>
> print(array_buffer.__buffer__)
<method-wrapper '__buffer__' of array.array object at 0x7fdba349f100>

# Python <3.12
> print(np_buffer.__buffer__)
AttributeError: 'numpy.ndarray' object has no attribute '__buffer__'
> print(array_buffer.__buffer__)
AttributeError: 'array.array' object has no attribute '__buffer__'
```

See `array.array` stub file and related PR https://github.com/python/typeshed/pull/10225
https://github.com/python/typeshed/blob/434f6528b7607d9ae6c3d34329840a18922df925/stdlib/array.pyi#L87

I've submitted a small PR #26784 that resolves this.",2024-06-22 05:56:26,,ENH: virtually support __buffer__ method for python <3.12,['unlabeled']
26782,open,tankbattle,"### Proposed new feature or change:

In assert_allclose (https://github.com/numpy/numpy/blob/main/numpy/testing/_private/utils.py#L1581), it requires all elements to match within tolerance. However, there's always outliers especially for very large arrays. If we provide a small tolerance, unit tests can only work for small arrays. If we provide large tolerance, the test may be too relaxed for majority of the elements. 

I'm proposing to introduce an additional parameter to assert_allclose() to indicate the number of elements that can go beyond tolerance. In the test, the above issue can be addressed by having multiple assert_allclose() with different tolerance and #elements within tolerance. For example, we can assert 99.9% of elements are within rtol=0.01 while for all elements the rtol may be 100, by
    # assume a & b are all very large arrays
    assert_allclose(a, b, rtol=0.01, outlier_elements=10)
    assert_allclose(a, b, rtol=100, outlier_elements=0)",2024-06-22 04:02:46,,ENH: assert_allclose to allow element number tolerance,['component: numpy.testing']
26779,open,chimaerase,"### Proposed new feature or change:

Many `np.random.Generator` [distribution methods](https://numpy.org/doc/stable/reference/random/generator.html#distributions) generate floating point arrays, but don't accept a `dtype` parameter to control the desired precision.  As a result, the generated arrays have limited utility in some situations, or for downcasting, have to be reconstructed afterward with the desired precision.  For example, `standard_exponential()` has a `dtype` parameter, but `uniform()` doesn't.  Providing intentionally-typed float parameters to `uniform()` still doesn't affect the output `dtype` (always `np.float64`)

```
>>> rng = np.random.default_rng(42)
>>> result = rng.uniform(np.float32(0.0), np.float32(10), 20)
>>> result.dtype
dtype('float64')
```",2024-06-21 19:46:38,,ENH: Add optional `dtype` parameter to `Generator's` distribution methods,['01 - Enhancement']
26772,open,Rheoscope,"### Proposed new feature or change:

In eigenvalue solvers in LAPACK, e.g., https://netlib.org/lapack/explore-html-3.4.2/d2/d8a/group__double_s_yeigen.html#ga442c43fca5493590f8f26cf42fed4044
There is some option to print out in ascending order
```
  W is DOUBLE PRECISION array, dimension (N)
          If INFO = 0, the eigenvalues in ascending order.
```
Would it be possible to let `eigh` in `numpy` have a similar option? Thanks",2024-06-20 19:19:03,,ENH: Sort eigenvalues for eigh,"['01 - Enhancement', '57 - Close?']"
26761,open,LevN0,"### Describe the issue:

In NumPy < 2.0,

```python
>>> arr = np.asarray([-10, -5], 'i1')
>>> arr += 128
>>> arr
array([118, 123], dtype=int8)
```

In NumPy 2.0,

```python
>>> arr = np.asarray([-10, -5], 'i1')
>>> arr += 128
OverflowError: Python integer 128 out of bounds for int8
```

This may have been an intended change as part of https://numpy.org/devdocs/release/1.24.0-notes.html#conversion-of-out-of-bound-python-integers, but it's not clearly stated there or in #26635.

The release notes in 1.24.0 say ""Attempting a conversion from a Python integer to a NumPy value will now always check whether the result can be represented by NumPy"". By that wording, the above operation should still work as the result is still an int8, although it is perhaps ambiguous in that it could be implied there is a conversion of `128` to a NumPy value prior to the operation. The release notes for 2.0 don't directly touch on this case, only talking about ""conversion of out of bounds python integers to integer arrays"". 

The more significant issue IMO is that the new behavior cannot be trivially worked around as far as I can tell, see discussion below.

### Reproduce the code example:

```python
>>> arr = np.asarray([-10, -5], 'i1')
>>> arr += 128
OverflowError: Python integer 128 out of bounds for int8
```


### Error message:

```shell
OverflowError: Python integer 128 out of bounds for int8
```


### Python and NumPy Versions:

2.0.0
3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]


### Runtime Environment:

_No response_

### Context for the issue:

My use-case was applying scaling and offset values to a numeric starting value while maintaining the minimum possible dtype. I had a separate code to check that the result of the operation fits in the final dtype (otherwise the dtype was adjusted), so the operation was safe. Now NumPy refuses to do the operation directly.

Recovering the old behavior is not trivial from what I can see. Although the documentation suggests `arr += np.asarray(128)` could be used instead, this doesn't always quite work. For example,

```python
>>> arr = np.asarray([5, 10], 'uint16')
>>> arr += np.asarray(30000) # default dtype for this is int64
numpy._core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'add' output from dtype('int64') to dtype('uint16') with casting rule 'same_kind'
```

Another approach that also doesn't completely work is using e.g. `np.add(..., casting='unsafe')`. It solves the problem just above, but it behaves differently with MaskedArrays than ""+="" approach. Specifically it modifies masked values, whereas the ""+="" approach did not.

The fastest way I see to recover the old behavior in the general case (i.e. where starting dtype can be anything) and where the operation is known apriori to produce a safe result is a multi-step process,

1) Check if `N` in `arr += np.asarray(N)` is an integer value
2) If so, determine and pass minimum necessary dtype for the intenger value, i.e. `arr += np.asarray(N, dtype=min_dtype)`
3) Otherwise, use `arr += np.asarray(N)`

This is quite a bit more complicated than `arr += N`. I would have expected the new behavior to throw an exception when the final result of the operation overflows, but not when `N` is technically out of bounds of `arr`'s dtype but the operation still produces a non-overflow result.",2024-06-19 20:16:53,,BUG: type cast behaviour change in NumPy 2.0,['00 - Bug']
26758,open,tpwrules,"### Issue with current documentation:

* The transition guide [here](https://numpy.org/doc/stable/reference/routines.polynomials.html#transitioning-from-numpy-poly1d-to-numpy-polynomial) does not explain how to transition uses of `np.polyval`.
* The legacy class documentation [here](https://numpy.org/doc/stable/reference/generated/numpy.poly1d.html#numpy.poly1d) does have an evaluation example, but the actual method that does the work (`__call__`) has nonsense documentation.
* The class documentation page [here](https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.Polynomial.html#numpy.polynomial.polynomial.Polynomial) doesn't explain how to use the class to evaluate the polynomial, and again `__call__` has nonsense documentation.

### Idea or request for content:

I would appreciate some more clarity on how to use the newer classes. I was able to figure it out in a REPL but it was not obvious and directly answerable from the docs.",2024-06-19 16:57:17,,DOC: Unclear how to evaluate using polynomial package,"['04 - Documentation', 'component: numpy.polynomial']"
26746,open,fa0311,"### Describe the issue:

Windows and Ubuntu have different execution results.
I would expect the behavior of np.fmin to be consistent across different operating systems.

### Windows (AMD64) / Mac (AppleSilicon)

-0.0 is always considered smaller.

The logs are from Windows.

```py
>>> import numpy as np
>>> np.fmin(np.float64(-0.0), np.float64(0.0))
-0.0
>>> np.fmin(np.float64(0.0), np.float64(-0.0))  
-0.0
```

### Ubuntu (AMD64)

The second argument is always considered smaller.

The same results were confirmed on the actual machine and in CodeSpace.
The logs are from CodeSpace.

```py
>>> import numpy as np
>>> np.fmin(np.float64(-0.0), np.float64(0.0))
0.0
>>> np.fmin(np.float64(0.0), np.float64(-0.0))
-0.0
```

---

https://numpy.org/doc/stable/reference/generated/numpy.fmin.html

> The fmin is equivalent to np.where(x1 <= x2, x1, x2) when neither x1 nor x2 are NaNs, but it is faster and does proper broadcasting.

If this issue is by design, the following documentation is incorrect.
Strictly speaking, it is not equivalent to `np.where(x1 <= x2, x1, x2)`.

`np.float64(-0.0) == np.float64(0.0)` is evaluated as True on any OS.

### Reproduce the code example:

```python
import numpy as np
np.fmin(np.float64(-0.0), np.float64(0.0))
```


### Error message:

_No response_

### Python and NumPy Versions:

I have tried the following versions
I confirmed the same result with numpy 2
```txt
Python: 3.10.11
Numpy: 1.26.4
```
```txt
Python: 3.10.11
Numpy: 2.0.0
```

### Runtime Environment:

Windows + numpy 1.26

```py
[{'numpy_version': '1.26.4',
  'python': '3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC '
            'v.1929 64 bit (AMD64)]',
  'uname': uname_result(system='Windows', node='DESKTOP-TP9ENTI', release='10', version='10.0.22621', machine='AMD64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Haswell',
  'filepath': 'Z:\\Project\\py\\wasm-runtime-py\\.venv\\Lib\\site-packages\\numpy.libs\\libopenblas64__v0.3.23-293-gc2f4bdbb-gcc_10_3_0-2bde3a66a51006b2b53eb373ff767a3f.dll',
  'internal_api': 'openblas',
  'num_threads': 16,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.23.dev'}]
  ```


Windows + numpy 2.0

```py
>>> import numpy; numpy.show_runtime()
[{'numpy_version': '2.0.0',
  'python': '3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC '
            'v.1929 64 bit (AMD64)]',
  'uname': uname_result(system='Windows', node='DESKTOP-TP9ENTI', release='10', version='10.0.22621', machine='AMD64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Haswell',
  'filepath': 'Z:\\Project\\py\\wasm-runtime-py\\.venv\\Lib\\site-packages\\numpy.libs\\libscipy_openblas64_-fb1711452d4d8cee9f276fd1449ee5c7.dll',
  'internal_api': 'openblas',
  'num_threads': 16,
  'prefix': 'libscipy_openblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.27'}]
  ```



Ubuntu + numpy 1.26

The logs are from CodeSpace.

```py
>>> import numpy; numpy.show_runtime()
[{'numpy_version': '1.26.4',
  'python': '3.10.13 (main, May 30 2024, 20:38:07) [GCC 9.4.0]',
  'uname': uname_result(system='Linux', node='codespaces-60eb98', release='6.5.0-1021-azure', version='#22~22.04.1-Ubuntu SMP Tue Apr 30 16:08:18 UTC 2024', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Zen',
  'filepath': '/usr/local/python/3.10.13/lib/python3.10/site-packages/numpy.libs/libopenblas64_p-r0-0cf96a72.3.23.dev.so',
  'internal_api': 'openblas',
  'num_threads': 2,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.23.dev'}]

```

Ubuntu + numpy 2.0

The logs are from CodeSpace.

```py
>>> import numpy; numpy.show_runtime()
[{'numpy_version': '2.0.0',
  'python': '3.10.13 (main, May 30 2024, 20:38:07) [GCC 9.4.0]',
  'uname': uname_result(system='Linux', node='codespaces-60eb98', release='6.5.0-1021-azure', version='#22~22.04.1-Ubuntu SMP Tue Apr 30 16:08:18 UTC 2024', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Zen',
  'filepath': '/usr/local/python/3.10.13/lib/python3.10/site-packages/numpy.libs/libscipy_openblas64_-99b71e71.so',
  'internal_api': 'openblas',
  'num_threads': 2,
  'prefix': 'libscipy_openblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.27'}]
  ```

### Context for the issue:

I am facing an issue where tests that pass on Windows/Mac do not pass on GitHub Actions.",2024-06-18 16:42:23,,np.fmin returns inconsistent results for -0.0 and 0.0 across different OS,['component: numpy._core']
26740,open,Svalorzen,"### Describe the issue:

I'm trying to restrict the problem, but for now it seems that with newer numpy versions on x64 certain complex products return different results depending on whether the operands are wrapped in a numpy array or not. Example code and some results I got below:

My machine:
```
1.18920906599179332e+00 3.77425653250409567e+01
1.18920906599179510e+00 3.77425653250409567e+01
False
SYS VERSION 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]
PLATFORM x86_64
NP VERSION 2.0.0
```

Google Colab:
```
1.18920906599179332e+00 3.77425653250409567e+01
1.18920906599179510e+00 3.77425653250409567e+01
False
SYS VERSION 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]
PLATFORM x86_64
NP VERSION 1.25.2
```

----------------------------------

Some passing results:
```
Python 3.9.6 / NumPy 2.0 / arm64 (Apple Silicon):
1.18920906599179510e+00 3.77425653250409567e+01
1.18920906599179510e+00 3.77425653250409567e+01
True
```

```
Python 3.9.6 / NumPy 2.0.0 / x86_64 (Rosetta emulation under Apple Silicon):
1.18920906599179332e+00 3.77425653250409567e+01
1.18920906599179332e+00 3.77425653250409567e+01
True
```

Some online REPL:
```
1.18920906599179332e+00 3.77425653250409567e+01
1.18920906599179332e+00 3.77425653250409567e+01
True
SYS VERSION 3.8.2 (default, Mar 13 2020, 10:14:16)  [GCC 9.3.0]
PLATFORM x86_64
NP VERSION 1.18.2
```

### Reproduce the code example:

```python
import platform
import numpy as np
import sys
def ff(x):
    return f""{x:.17e}""

x = (-0.7173752049805099+0.6966870282122177j)
y = (25.441646575927734-27.90408706665039j)
xy = x * y
print(ff(xy.real), ff(xy.imag))
xxyy = (np.array([x]) * np.array([y]))[0]
print(ff(xxyy.real), ff(xxyy.imag))
print(xy == xxyy)
print(""SYS VERSION"", sys.version)
print(""PLATFORM"", platform.processor())
print(""NP VERSION"", np.version.version)
```


### Error message:

_No response_

### Python and NumPy Versions:

2.0.0
3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]

### Runtime Environment:

[{'numpy_version': '2.0.0',
  'python': '3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]',
  'uname': uname_result(system='Linux', node='Luchino', release='5.15.0-112-generic', version='#122-Ubuntu SMP Thu May 23 07:48:21 UTC 2024', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Zen',
  'filepath': '/home/svalorzen/Tests/python/venv/lib/python3.10/site-packages/numpy.libs/libscipy_openblas64_-99b71e71.so',
  'internal_api': 'openblas',
  'num_threads': 32,
  'prefix': 'libscipy_openblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.27'}]


### Context for the issue:

I am porting some Python code to C++, and it is quite important that the output stays consistent. In my case, my C++ code produces the result of the ""normal"" Python multiplication, while the Python code uses numpy arrays and so generates the other result. This breaks my tests, alongside my sanity.",2024-06-18 11:02:11,,Complex multiplication returns different results if wrapped in np array,['33 - Question']
26729,open,WarrenWeckesser,"### Proposed new feature or change:

I'm working on improving an existing ufunc (complex `log1p`) in numpy, and I'm implementing the updated version in C++.  The function will use several functions from `npy_math` (`npy_log`, `npy_log1p`, `npy_fabs`, `npy_atan2`, `npy_hypot`), and these all follow the C convention of having three versions, one for each of the types `npy_float`, `npy_double` and `npy_longdouble`, e.g. `npy_fabsf`, `npy_fabs` and `npy_fabsl`.

It would really nice if we had a C++ namespace (e.g. `npy` to match the ""poor man's namespace"" implemented with the prefix `npy_` in C) that provides a single name with overloaded implementations for each type, e.g. `npy::fabs`.  This would make writing templated code in C++ *much* easier.

As an experiment, I added this code to the end of `npy_math.h`:

```
#ifdef __cplusplus

//
// Trivial C++ wrappers for several npy_* functions.
//
#define CPP_WRAP1(name) \
inline npy_float name(const npy_float x)                \
{                                                       \
    return ::npy_ ## name ## f(x);                      \
}                                                       \
inline npy_double name(const npy_double x)              \
{                                                       \
    return ::npy_ ## name(x);                           \
}                                                       \
inline npy_longdouble name(const npy_longdouble x)      \
{                                                       \
    return ::npy_ ## name ## l(x);                      \
}                                                       \


#define CPP_WRAP2(name) \
inline npy_float name(const npy_float x,                \
                      const npy_float y)                \
{                                                       \
    return ::npy_ ## name ## f(x, y);                   \
}                                                       \
inline npy_double name(const npy_double x,              \
                       const npy_double y)              \
{                                                       \
    return ::npy_ ## name(x, y);                        \
}                                                       \
inline npy_longdouble name(const npy_longdouble x,      \
                           const npy_longdouble y)      \
{                                                       \
    return ::npy_ ## name ## l(x, y);                   \
}                                                       \

namespace npy {

CPP_WRAP1(fabs)
CPP_WRAP1(log)
CPP_WRAP1(log1p)
CPP_WRAP2(atan2)
CPP_WRAP2(hypot)

}

#endif // __cplusplus
```

Then in a templated C++ function that includes `npy_math.h`, I use `npy::fabs`, `npy::log`, etc.  This works fine.

Is there interest in such an update to `npy_math.h` (or some other header) to facilitate writing code in C++?  Of course, giving all the functions in `npy_math.h` similar wrappers will be more work, and there might be some gotchas that show up in the process, but with more and more code being implemented in C++ in NumPy, I think this is something we need.",2024-06-17 18:35:19,,ENH: C++ namespace and function overrides for npy_math,"['01 - Enhancement', 'component: npy_math', '63 - C API', 'C++']"
26727,open,NeilGirdhar,"In [the API documentation page](https://numpy.org/doc/stable/reference/index.html#reference), there are some packages shown in parentheses (`numpy.exceptions`, `numpy.linalg`, etc.)  It might be nice to fill in the missing ones:
* [`numpy.strings`](https://numpy.org/doc/stable/reference/routines.strings.html)
* [`numpy.emath`](https://numpy.org/doc/stable/reference/routines.emath.html#module-numpy.emath)
* [`numpy.ma`](https://numpy.org/doc/stable/reference/routines.ma.html)
* [`numpy.polynomial`](https://numpy.org/doc/stable/reference/routines.polynomials.html)",2024-06-17 17:04:01,,DOC: Add missing package names in API documentation page,['04 - Documentation']
26718,open,OH-AU,"### Describe the issue:

When running numpy.test() - all tests pass as expected on the first run. If you attempt to run the command a second time in the same session, 4 tests fail (presumably because of pre-existing data?). Quit python and run tests for the first time, everything again works the first time and fails again on the second attempt.
Test was done with local build as well as pip installed wheel with the same results.

### Reproduce the code example:

```python
import numpy
numpy.test()
numpy.test()
```


### Error message:

```shell
FAILED _core/tests/test_dtype.py::TestPickling::test_pickle_dtype_class[dtype[rational]] - _pickle.PicklingError: Can't pickle <function _DType_reconstruct at 0x7fffb67dd4e0>: it's not the same object as numpy._core._DType_reconstruct
FAILED f2py/tests/test_f2py2e.py::test_gen_pyf_no_overwrite - ValueError: I/O operation on closed file.
FAILED lib/tests/test_io.py::TestLoadTxt::test_max_rows_empty_lines[1-data2] - Failed: DID NOT WARN. No warnings of type (<class 'UserWarning'>,) matching the regex were emitted.
FAILED lib/tests/test_io.py::TestLoadTxt::test_max_rows_empty_lines[0-data5] - Failed: DID NOT WARN. No warnings of type (<class 'UserWarning'>,) matching the regex were emitted.
```


### Python and NumPy Versions:

python 3.12.3
numpy 2.0.0

### Runtime Environment:

[{'numpy_version': '2.0.0',
  'python': '3.12.3 (main, Apr 29 2024, 13:56:22) [GCC 12.3.0]',
  'uname': uname_result(system='Linux', node='c002', release='5.14.21-150400.24.28-default', version='#1 SMP PREEMPT_DYNAMIC Mon Oct 10 15:21:12 UTC 2022 (f82da2c)', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE',
                                   'SSE2',
                                   'SSE3',
                                   'SSSE3',
                                   'SSE41',
                                   'POPCNT',
                                   'SSE42',
                                   'AVX',
                                   'F16C',
                                   'FMA3',
                                   'AVX2'],
                      'found': [],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL',
                                    'AVX512_SPR']}},
 {'architecture': 'ZEN',
  'filepath': '/opt/openblas/0.3.24/lib/libopenblas_zenp-r0.3.24.so',
  'internal_api': 'openblas',
  'num_threads': 8,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.24'}]

### Context for the issue:

low priority I suspect? Who runs tests twice in the same session? But perhaps tests could be improved to handle this scenario (if it's just a test case issue)?",2024-06-17 10:42:11,,BUG: test fail when run a second time,"['00 - Bug', 'sprintable']"
26704,open,HaoZeke,"The test  for #26703 exposed a new regression instance.
- The testcase was specially crafted to test passing flags
  - ""specially crafted"" in this case means it will build a module, though it cannot be imported, since the parser doesn't pick up the subroutine name
    - Not sure if this is actually going to be a problem for anyone, nor is it clear that this is a regression, earlier versions (1.26) with `distutils` also seem to generate the same broken module
    - Actually this does work with `1.19.5` so there's another regression lurking with the parser for F77....
      - Basically `strictf77` should be set if `-ffixed-form` is passed, irrespective of the file ending...
      - Since the same code works if its in a `.f` file

The issue is that F2PY while generating wrappers needs to know if the files are F77 or not, and the compiler arguments are not transparently available to it.

However, this is really a bit of a weird state, it isn't a great idea to have the compile time arguments ""bleed"" into the meson setup.

Here are the concrete questions:
- Since F2PY is meant to generate a meson.build (sometimes) and therefore should take fortran_args shouldn't the `crackfortran` and other parts also be aware of these arguments?

On whole I'm veering towards a wont-fix since:
- F2PY isn't really a `meson.build` generator, it is primarily to generate the corresponding wrapper code
- In doing so, it wouldn't make sense to expect `--fflags` to provide information on weather the file should be parsed as a Fortran 77 file or not
  - The current (not so robust) way is to check the filename, and also the arguments passed (e.g. `-fix` `-f77` etc)

This will be a non-issue (trivially fixed) if / when the various different interfaces to calling F2PY are unified (noted elsewhere, but we currently have one path without `-c` one with, and additional argument handling in `crackfortran.py`, all of which basically do not talk to each other and bleed state).",2024-06-16 04:45:40,,"ENH, BUG?: Set F77 mode based on compiler arguments",['component: numpy.f2py']
26701,open,WarrenWeckesser,"### Describe the issue:

When given a string, it looks like `np.clongdouble()` is parsing the numeric values as `float64` and losing precision in the conversion.  For example, in the following, I expect `np.longdouble(s)` and `np.clongdouble(s).real` to give the same result:

```
In [73]: s = ""0.333333333333333333333333""

In [74]: np.longdouble(s)
Out[74]: np.longdouble('0.33333333333333333334')

In [75]: np.clongdouble(s).real
Out[75]: np.longdouble('0.33333333333333331483')

``` 
Instead, `clongdouble()` appears to be parsing to `float64`, c.f.

```
In [78]: np.longdouble(np.float64(s))
Out[78]: np.longdouble('0.33333333333333331483')
```

This is on a Linux machine where the `long double` type is 80 bit extended precision.


### Python and NumPy Versions:

2.1.0.dev0+git20240613.787ff0f
3.12.3 (main, Apr 25 2024, 11:11:06) [GCC 11.4.0]


### Runtime Environment:

[{'numpy_version': '2.1.0.dev0+git20240613.787ff0f',
  'python': '3.12.3 (main, Apr 25 2024, 11:11:06) [GCC 11.4.0]',
  'uname': uname_result(system='Linux', node='pop-os', release='6.8.0-76060800daily20240311-generic', version='#202403110203~1715181801~22.04~aba43ee SMP PREEMPT_DYNAMIC Wed M', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Zen',
  'filepath': '/usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so',
  'internal_api': 'openblas',
  'num_threads': 24,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.20'}]

",2024-06-15 20:26:13,,BUG: `clongdouble(str)` parses string to `float64` and loses precision in string conversion.,"['00 - Bug', 'component: numpy._core', 'sprintable - C']"
26700,open,Aloqeely,"### Issue with current documentation:

In https://numpy.org/neps/nep-0029-deprecation_policy.html the status is set as 'final' but a note says that it's been superseded by SPEC 0

### Idea or request for content:

Set the status to superseded",2024-06-15 15:28:28,,DOC: Set status of NEP 29 to 'superseded',['04 - Documentation']
26699,open,mglisse,"### Describe the issue:

I would expect `linspace(X, X, N)` to produce an array with N copies of X. This works for finite values, but not for ±inf. This is an edge case that probably doesn't affect many people, but I don't think there is any ambiguity about the expected output.

### Reproduce the code example:

```python
import numpy as np
np.linspace(np.inf, np.inf, 4)
```


### Error message:

```shell
/tmp/np/lib/python3.12/site-packages/numpy/_core/function_base.py:145: RuntimeWarning: invalid value encountered in subtract
  delta = np.subtract(stop, start, dtype=type(dt))
array([nan, nan, nan, inf])
```


### Python and NumPy Versions:

2.0.0rc2
3.12.3 (main, Apr 10 2024, 05:33:47) [GCC 13.2.0]

### Runtime Environment:

[{'numpy_version': '2.0.0rc2',
  'python': '3.12.3 (main, Apr 10 2024, 05:33:47) [GCC 13.2.0]',
  'uname': uname_result(system='Linux', node='hippo', release='6.7.12-amd64', version='#1 SMP PREEMPT_DYNAMIC Debian 6.7.12-1 (2024-04-24)', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Haswell',
  'filepath': '/tmp/np/lib/python3.12/site-packages/numpy.libs/libscipy_openblas64_-99b71e71.so',
  'internal_api': 'openblas',
  'num_threads': 16,
  'prefix': 'libscipy_openblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.27'}]

### Context for the issue:

_No response_",2024-06-15 12:06:34,,"linspace(inf, inf, N) outputs NaN",['00 - Bug']
26681,open,bilderbuchi,"### Describe the issue:

When using `callstatement` to overrride a subroutine with different code as per [the docs ](https://numpy.org/doc/stable/f2py/signature-file.html#f2py-statements), this works when said subroutine is directly invoked by Python. However, when invoking another subroutine that calls the first subroutine, this override is not in effect.

MWE:
test.f90
```fortran
module utils
    implicit none
  contains
    subroutine my_abort(message)
      implicit none
      character(len=*), intent(in) :: message
      !f2py callstatement PyErr_SetString(PyExc_ValueError, message);f2py_success = 0;
      !f2py callprotoargument char*
      write(0,*) ""THIS SHOULD NOT APPEAR""
      stop 1
    end subroutine my_abort

    subroutine do_something()
        call my_abort(""aborting inside"")
    end subroutine do_something
end module utils
```

To avoid #2547, you have to do the compilation in 2 steps:
```
python3 -m numpy.f2py -h test.pyf --no-lower -m test test.f90
python3 -m numpy.f2py -c --f90exec=gfortran --fcompiler=gnu95 test.pyf test.f90
```

Invoking `my_abort` directly works as expected, raising the ValueError I have defined:
```
$ python3 -c ""import test; test.utils.my_abort('aborting directly')""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ValueError: aborting directly
```

However, when calling `do_something`, which in turn calls `my_abort`, the override seems to do nothing, and we run into the `stop`:
```
$ python3 -c ""import test; test.utils.do_something()""
 THIS SHOULD NOT APPEAR
STOP 1
```
Expected result: Python raises `ValueError: aborting inside`


### Reproduce the code example:

```python
python3 -c ""import test; test.utils.do_something()""
 THIS SHOULD NOT APPEAR
STOP 1

# Expected result: Python raises `ValueError: aborting inside`
```


### Error message:

_No response_

### Python and NumPy Versions:

Numpy: 2.0.0rc2 via conda-forge
python: 3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:36:13) [GCC 12.3.0]
gfortran: GNU Fortran (conda-forge gcc 11.2.0-16) 11.2.0

Using 2.0.0rc2 to avoid issue #26156

### Runtime Environment:

[{'numpy_version': '2.0.0rc2',
  'python': '3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:36:13) '
            '[GCC 12.3.0]',
  'uname': uname_result(system='Linux', node='b5dc6932b437', release='5.15.146.1-microsoft-standard-WSL2', version='#1 SMP Thu Jan 11 04:09:03 UTC 2024', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2',
                                'AVX512F',
                                'AVX512CD',
                                'AVX512_SKX',
                                'AVX512_CLX',
                                'AVX512_CNL',
                                'AVX512_ICL'],
                      'not_found': ['AVX512_KNL', 'AVX512_KNM', 'AVX512_SPR']}},
 {'architecture': 'SkylakeX',
  'filepath': '/opt/conda/envs/xplengine/lib/libopenblasp-r0.3.27.so',
  'internal_api': 'openblas',
  'num_threads': 4,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.27'}]

### Context for the issue:

What I'm intending here is to avoid executing the Fortran `stop` statement when running wrapped code in Python, because that also kills the Python *interpreter* and thus, in my case, the `pytest` run of a whole test suite that this code is part of.

This `my_abort` cleanup subroutine is potentially called from many places throughout the actual codebase.

A similar-feeling issue involving callbacks: #7855
Feels related: #26156
ping @HaoZeke",2024-06-13 11:09:35,,f2py callstatement is ineffective in subroutines called from other subroutines,"['00 - Bug', 'component: numpy.f2py']"
26669,open,rgommers,"There are a lot of issues, both in numpy itself and in downstream packages, where masked arrays silently do the wrong thing when they get passed to a function that isn't aware of masked arrays and starts off by calling `np.asarray` on its inputs. gh-26530 is a recent example. Across the main `numpy` namespace, almost all functions are not mask-aware, and for those that do the right thing it's mostly by accident. Masked arrays should only be passed to functions in the `numpy.ma` namespace. In SciPy it's the same: `scipy.stats.mstats` supports masked arrays, nothing else in SciPy does.

Masked arrays should be treated like sparse arrays or other array types with different semantics: conversion should not be done implicitly, because dropping a mask implicitly is almost never the correct thing to do.

gh-18675 basically came to this conclusion. Other related issues:
- gh-4356
- gh-15200
- gh-8516
- I'm sure there are many more that touch on this, since it's very confusing.

We should aim to add a `FutureWarning` first I think, since raising immediately may be disruptive.

EDIT: code example:
```python
>>> import numpy as np
>>> x = np.arange(12).reshape((3, 4))
>>> x = np.ma.masked_less(x, 6)
>>> x
masked_array(
  data=[[--, --, --, --],
        [--, --, 6, 7],
        [8, 9, 10, 11]],
  mask=[[ True,  True,  True,  True],
        [ True,  True, False, False],
        [False, False, False, False]],
  fill_value=999999)
>>> np.asarray(x)  # mask gets dropped
array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11]])
```
So if dropping the mask is wrong and `np.asarray(masked_array)` starts raising, what if you do want the underlying `ndarray`/values? https://numpy.org/devdocs/reference/maskedarray.generic.html#accessing-the-data suggests that using either `x.data` or np.ma.getdata(x)` is the way to do that.",2024-06-12 14:23:01,,`np.asarray(masked_array)` should raise rather than silently dropping the mask,"['17 - Task', 'component: numpy.ma']"
26621,open,JophiArcana,"### Describe the issue:

At the end of np.record's __getattr__ method, it uses a try-except to get the simultaneously check if the attribute is a standard object (e.g. any non NumPy component) (records.py lines 255-259), and also set up the check for whether that attribute otherwise is a recarray/record, or an array with a standard dtype (records.py lines 260-261). This creates a weird interaction with other libraries with data structures that possess a 'dtype' attribute, but whose dtype attribute does not contain the attribute 'names'. The current workaround for this is to store the tensor in an additional container such as a tuple, and dereference it later, but this is inconvenient as it adds an additional layer of logic.

### Reproduce the code example:

```python
import numpy as np
import torch

recarr = np.recarray((), dtype=[(""x"", int), (""y"", object)])
recarr[()] = (1212, torch.randn((5, 5)))
print(recarr.y)        # This line does not break
print(recarr[()].y)    # This line breaks
```


### Error message:

```shell
Traceback (most recent call last):
  File ""/Users/wentinnliao/Desktop/College/KF_RNN/sandbox/sandbox.py"", line 695, in <module>
    print(recarr[()].y)
          ^^^^^^^^^^^^
  File ""/Users/wentinnliao/anaconda3/lib/python3.11/site-packages/numpy/core/records.py"", line 260, in __getattribute__
    if dt.names is not None:
       ^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'names'
```


### Python and NumPy Versions:

1.23.5
3.11.4 (main, Jul  5 2023, 08:54:11) [Clang 14.0.6 ]

### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2024-06-05 11:21:15,,"BUG: np.record check for dtype yields AttributeError when interacting with other datatype libraries such as Pandas, PyTorch, etc.",['00 - Bug']
26620,open,megies,"### Describe the issue:

`np.cross()` warns about arrays of vectors when used with simply two 2-d vectors (this use case is even in the [""Examples"" section of `np.cross()` docstring](https://github.com/numpy/numpy/blob/42e9aa2c8ce6791cf175bd1eeec46d533c7c41d1/numpy/_core/numeric.py#L1591-L1596)). I believe, that check whether a warning should be shown is missing a check for number of dimensions of the input.

https://github.com/numpy/numpy/blob/42e9aa2c8ce6791cf175bd1eeec46d533c7c41d1/numpy/_core/numeric.py#L1645-L1653

I feel like there should be a change, either
- change the warning message to something like `'2-dimensional vectors are deprecated. Use 3-dimensional vectors instead.'` without the `'Arrays of..'` parts, or..
- the check whether the warning should be shown or not should also check `if a.ndim > 1 or b.ndim > 1`

Tbh, I'm a bit confused what that warning is implying as recommended usage anyway. Is the user supposed to add an all-zero third dimension for 2-d vectors and then calculate cross product in 3-d and then extract the valid part from it? That seems kinda odd-looking, I guess if that really is the case it looks like this prompts to look at #13718 again potentially?

So instead of..

```python
x = [[1, 2], [1, 2]]
y = [[4, 5], [4, 5]]
result = np.cross(x, y)
```

user should do something like..?

```python
x = [[1, 2, 0], [1, 2, 0]]
y = [[4, 5, 0], [4, 5, 0]]
result = np.cross(x, y)[:, -1]
```

### Reproduce the code example:

```python
import numpy as np

x = [1, 2]
y = [4, 5]
# the next line shows a warning, even though it's even from the docstring examples section
np.cross(x, y)
```


### Error message:

```shell
DeprecationWarning: Arrays of 2-dimensional vectors are deprecated. Use arrays of 3-dimensional vectors instead. (deprecated in NumPy 2.0)
```


### Python and NumPy Versions:

2.0.0rc2
3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]


### Runtime Environment:

```
[{'numpy_version': '2.0.0rc2',
  'python': '3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) '
            '[GCC 12.3.0]',
  'uname': uname_result(system='Linux', node='mother', release='5.10.0-25-amd64', version='#1 SMP Debian 5.10.191-1 (2023-08-16)', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2',
                                'AVX512F',
                                'AVX512CD',
                                'AVX512_SKX',
                                'AVX512_CLX',
                                'AVX512_CNL',
                                'AVX512_ICL'],
                      'not_found': ['AVX512_KNL', 'AVX512_KNM', 'AVX512_SPR']}},
 {'architecture': 'SkylakeX',
  'filepath': '/home/megies/miniconda3/envs/np2/lib/libopenblasp-r0.3.27.so',
  'internal_api': 'openblas',
  'num_threads': 8,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.27'}]
```

### Context for the issue:

In [our](https://github.com/obspy/obspy/) testsuite, we also have a test runner with `pytest -W error` to make sure our code is not causing deprecation warnings in order to keep our code base work with upcoming releases of dependencies. And that's where this deprecation warning came up.",2024-06-05 10:47:06,,BUG: np.cross() warns about arrays of vectors when used with two simple 2-d vectors,"['00 - Bug', '04 - Documentation', 'Numpy 2.0 API Changes']"
26617,open,greplazy,"### Describe the issue:

Converting a polynomial to natural window yields in unexpected behaviour: the degree of the polynomial might change.
E.g. when fitting data with a polynomial of degree 1, where data ist just constant, convert() return a degree 0 polynomial.

When implementing a fit to data, you don't know in advance, how data looks like. So fitting data with a polynomial of degree 1 you expect to get a polynomial of degree 1 in all cases. If the data is just constant, the first order coefficient is then 0 but it's not ok to remove that coefficient from output. 

### Reproduce the code example:

```python
import numpy as np
x = np.arange(0, 10, 1)
y = 0*x + 5.32
param = np.polynomial.Polynomial.fit(x, y, 1)
param.convert().coef
```


### Error message:

_No response_

### Python and NumPy Versions:

1.26.4
3.12.0 

### Runtime Environment:

_No response_

### Context for the issue:

Fitting a polynomial is a standard task. If the result has a different output format depending on data you put in, makes the method almost unusable (or at least unreliable).",2024-06-05 07:31:39,,BUG: polynomial.Polynomial.convert() returns wrong size of polynomial,['00 - Bug']
26615,open,duanev,"### Proposed new feature or change:

Some (arguably broken) compilers round 2.0 ** -1022 to 0.0.  Lines 206 to 223 of numpy/core/getlimits.py (copied below) will then cause a divide by zero instead of disabling the f64 type.  Maybe a try/except around the divide on line 222 would be prudent?

numpy/_core/getlimits.py

    206     # Known parameters for float64
    207     f64 = ntypes.float64
    208     epsneg_f64 = 2.0 ** -53.0
    209     tiny_f64 = 2.0 ** -1022.0
    210     float64_ma = MachArLike(f64,
    211                             machep=-52,
    212                             negep=-53,
    213                             minexp=-1022,
    214                             maxexp=1024,
    215                             it=52,
    216                             iexp=11,
    217                             ibeta=2,
    218                             irnd=5,
    219                             ngrd=0,
    220                             eps=2.0 ** -52.0,
    221                             epsneg=epsneg_f64,
    222                             huge=(1.0 - epsneg_f64) / tiny_f64 * f64(4),
    223                             tiny=tiny_f64)
",2024-06-04 22:26:45,,ENH: f64 type check can raise divide by zero,['unlabeled']
26613,open,asishm,"### Issue with current documentation:

When I go to release notes, they stop at 1.26.0 (and doesn't show 1.26.1 through 1.26.4) at https://numpy.org/doc/stable/release.html However switching to the dev dropdown does show the other 1.26.x releases as well as the 2.x one.

Apologies if I'm misinterpreting what the UI should be showing or maybe this is how the numpy docs always behaved and I never noticed. 

![image](https://github.com/numpy/numpy/assets/1978117/7563a9c4-083c-4bcf-a57f-bb17b05380f8)


### Idea or request for content:

_No response_",2024-06-04 15:35:31,,DOC: Stable release notes stop at 1.26.0,['04 - Documentation']
26612,open,lesteve,"### Describe the issue:

`numpy.result.dtype` on non-array objects that have a `.dtype` attribute will unexpectedly use the `.dtype` attribute instead of returning dtype object.

Creating an array with the same objects will as expected return an array of dtype object, but sometimes it is convenient to know the dtype without creating the array.

### Reproduce the code example:

```python
import numpy as np

class MyClass:
    def __init__(self):
        self.dtype = np.dtype('float64')


print(np.result_type(MyClass(), np.float64))

print(np.array([MyClass(), np.float64]).dtype)
```


### Error message:

```shell
float64
object
```


### Python and NumPy Versions:

Tried with 1.26.3 and development Numpy wheel

### Runtime Environment:

_No response_

### Context for the issue:

This was noticed in scikit-learn where some estimators like `OrdinalEncoder` have a `dtype` parameter and hence a `.dtype` atribute: https://github.com/scikit-learn/scikit-learn/issues/29157#issuecomment-2147435461",2024-06-04 15:28:59,,BUG: numpy.result_type on non-array objects that have a dtype attribute should always return dtype object,['00 - Bug']
26593,open,rootsmusic,"### Idea or request for content:
Adding a cheat sheet would be helpful, especially [for beginners](https://numpy.org/doc/stable/user/absolute_beginners.html).",2024-06-01 21:43:31,,DOC: add cheat sheet,['04 - Documentation']
26577,open,bmwoodruff,"### Issue with current documentation:

When you run `help(np.ma.alltrue)` or `help(np.ma.sometrue)`, you get
```
Help on method reduce in module numpy.ma.core:

reduce(target, axis=0, dtype=None) method of numpy.ma.core._MaskedBinaryOperation instance
    Reduce `target` along the given `axis`.
```
When you run help on any of the other 50+ ufuncs in the `ma` module, you get an appropriate doc message. For example `help(np.ma.logical_and)` yields (the first few lines):
```
Help on _MaskedBinaryOperation in module numpy.ma.core:

logical_and = <numpy.ma.core._MaskedBinaryOperation object>
    logical_and(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature])
    
    Compute the truth value of x1 AND x2 element-wise.
... it continues....
```
The issue seems to be the `.reduce` that appears on just these two functions (see below). 
https://github.com/numpy/numpy/blob/98e86d52ed79eb8810960bdcfac11271fc6c5434/numpy/ma/core.py#L1282-L1286

One consequence is that the `attributes` are different, with `ma.alltrue` and `ma.sometrue` being recognized as routines (while none of the others are). 
```
>>>[getattr(np.ma,'logical_and'),getattr(np.ma,'alltrue')]
[<numpy.ma.core._MaskedBinaryOperation at 0x7e0545aa66d0>,
 <bound method _MaskedBinaryOperation.reduce of <numpy.ma.core._MaskedBinaryOperation object at 0x7e0545aa6750>>]
>>>[inspect.isroutine(np.ma.logical_and),inspect.isroutine(np.ma.alltrue)]
[False, True]
```

I'm hoping to get a [script](https://github.com/numpy/numpy/issues/21351#issuecomment-2138016499) put into the CI build (thanks to @ngoldbaum 's suggestion)  that checks to make sure new routines are always appropriately tagged in a `.rst` file so they get documented on the web.  

### Idea or request for content:

I have a couple questions:

- How can we handle the ending `.reduce` at the of the definitions of `ma.alltrue` and `ma.sometrue` in a way that properly tags both the same as the rest of the ufuncs?  Is that possible (or even wanted).
- Do we want any of these ufuncs showing up in the web documentation as routines of the `ma` module?  They all appear already with `help()`, but don't appear online.",2024-05-30 18:28:32,,DOC: Docs point to incorrect reduce for ma.alltrue and ma.sometrue,['04 - Documentation']
26560,open,skirpichev,"### Describe the issue:

It seems, that numpy uses Smith's algorithm to do complex true division.  In some cases (see below) it produces ``(nan+nanj)``, but meaningful components could be recovered.  See e.g. C11 Annex G.5.2, ``_Cdivd()``'s example.

FYI: see also CPython's issue https://github.com/python/cpython/issues/119372

### Reproduce the code example:

```python
>>> 1/np.complex128('inf')
0j
>>> 1/np.complex128('infj')
-0j
>>> 1/np.complex128('(inf+infj)')  # oops, should be 0j
<stdin>:1: RuntimeWarning: invalid value encountered in scalar divide
(nan+nanj)
```


### Error message:

_No response_

### Python and NumPy Versions:

```pycon
>>> np.__version__
'1.26.4'
```

### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2024-05-28 14:03:29,,BUG: Invalid corner cases (resulting in nan+nanj) in complex division,['00 - Bug']
26522,open,vfdev-5,"Hello, 
I wonder whether the following behaviour is expected:
```python
import numpy as np
print(
    np.var(np.array([]), ddof=1),
    np.var(np.array([1.0]), ddof=1), np.var(np.array([1.0]), ddof=2),
    np.var(np.array([1.0, 2.0]), ddof=2), np.var(np.array([1.0, 2.0]), ddof=3),
    np.var(np.array([1.0, 2.0, 3.0]), ddof=3), np.var(np.array([1.0, 2.0, 3.0]), ddof=4),
)
> nan nan nan inf inf inf inf
```
or numpy 2.0 will be array-API compatible which states for cases N-correction <=0  we should get NaN: https://data-apis.org/array-api/latest/API_specification/generated/array_api.var.html#var

Thanks!",2024-05-24 15:34:12,,"Should np.var, np.std be array-API compatible for `N-correction <=0` ?",['unlabeled']
26510,open,ngoldbaum,"I also did some basic performance tests in `v2.0.0rc2` for the experimental `StringDType` but it seems to be much slower than `""O""` or `""U""` dtypes in certain cases, are you aware of these issues:

```python
import numpy
import random
import timeit

print(numpy.__version__)  # '2.0.0rc2'

options = [""a"", ""bb"", ""ccc"", ""dddd""]
lst = random.choices(options, k=1000)
arr_s = numpy.fromiter(lst, dtype=""T"", count=len(lst))
arr_o = numpy.fromiter(lst, dtype=""O"", count=len(lst))
arr_u = numpy.fromiter(lst, dtype=""U5"", count=len(lst))

print(timeit.timeit(lambda: numpy.unique(arr_s), number=10000))  # 4.270 <- why so much slower?
print(timeit.timeit(lambda: numpy.unique(arr_o), number=10000))  # 2.357
print(timeit.timeit(lambda: numpy.unique(arr_u), number=10000))  # 0.502
print(timeit.timeit(lambda: sorted(set(lst)), number=10000))  #  0.078
```

Profiling (will include in a separate comment below) indicates that the overhead is from acquiring and releasing the allocator lock (it does it for every entry in the array because sorting is implemented using getitem).

We could improve performance for sorting specifically by adding a fast getitem path that assumes the allocator lock is already acquired and using it in the sorting implementation after acquiring the lock once.

Optimizing getitem and setitem would also help (maybe by adding a stringdtype scalar that interns the packed string).

_Originally posted by @MarkusSintonen in https://github.com/numpy/numpy/issues/25693#issuecomment-2126856099_
            ",2024-05-23 16:01:30,,ENH: np.unique and sorting is slow for StringDType,['01 - Enhancement']
26498,open,AnnaTrainingG,"### Describe the issue:

np.load(""a.npz"") very slow when a.npz file very large

### Reproduce the code example:

```python
`import numpy as np
import time
x1 = np.arange(250000000, dtype=np.uint8)
x2 = np.arange(250000000, dtype=np.uint8)
x={""x1"":x1,""x2"":x2}
np.savez('/tmp/test.npz', row=x)
time0=time.time()
xx = np.load(""/tmp/test.npz"", allow_pickle=True)['row']
print(time.time()-time0)`

```
```
faster may be can use:
def load_from_npz(path, name):
    zf = zipfile.ZipFile(path)
    info = zf.NameToInfo[name + '.npy']
    assert info.compress_type == 0
    zf.fp.seek(info.header_offset + len(info.FileHeader()) + 20)

    encoding='ASCII'
    fix_imports=True
    pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)
    array = np.lib.format.read_array(zf.fp, allow_pickle=True, pickle_kwargs=pickle_kwargs)
    return array
```
the different is the input of read_array, if use zf.open(name + '.npy'), read_array will be very slow

all test code :
```
import numpy as np
import zipfile
from numpy.compat import (
    isfileobj, os_fspath, pickle
    )
def load_from_npz(path, name):
    zf = zipfile.ZipFile(path)
    info = zf.NameToInfo[name + '.npy']
    assert info.compress_type == 0
    zf.fp.seek(info.header_offset + len(info.FileHeader()) + 20)

    encoding='ASCII'
    fix_imports=True
    pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)
    array = np.lib.format.read_array(zf.fp, allow_pickle=True, pickle_kwargs=pickle_kwargs)
    return array

# create .npz file
import numpy as np
import time
x1 = np.arange(250000000, dtype=np.uint8)
x2 = np.arange(250000000, dtype=np.uint8)
x={""x1"":x1,""x2"":x2}
np.savez('/tmp/test.npz', row=x)
time0=time.time()
xx = np.load(""/tmp/test.npz"", allow_pickle=True)['row']
print(""np base:"", time.time()-time0)

time0=time.time()
xx = load_from_npz(""/tmp/test.npz"", 'row')
print(""faster:"", time.time()-time0)

```


### Error message:

```shell
no
```


### Python and NumPy Versions:

python3.9
numpy1.24.4

### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2024-05-22 08:30:56,,"np.load(""a.npz"") very slow when a.npz file very large ",['01 - Enhancement']
26491,open,olcc,"### Describe the issue:

Consider the code below (in the ""code example"" section). The `domain` is changed between `P` and `Q`.
However, I feel that `Q` should keep the same `domain` as `P` when `kind` is prescribed. 
I think this would minimize numerical error for the resulting `Q`.
I think that `ABCPolyBase.convert()` should be changed to include:
```
if kind is None:
  kind = inherit = self.__class__
else:
  inherit = self
if domain is None:
  domain = inherit.domain
if window is None:
  window = kind.window
```

### Reproduce the code example:

```python
from numpy.polynomial import Chebyshev
P = Chebyshev.fit([0, 10, 20, 30, 40], [3, 7, 7, 3, 6], 3)
Q = P.convert(kind=Polynomial)
print(f""Domain for P: {P.domain} and Q: {Q.domain}"")
# Domain for P: [ 0. 40.] and Q: [-1.  1.]
```


### Error message:

_No response_

### Python and NumPy Versions:

Numpy 1.24.2
Python 3.11.9

### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2024-05-20 21:37:53,,BUG: Polynomial / ABCPolyBase.convert() should not reset domain if kind is given,['00 - Bug']
26484,open,ev-br,"### Describe the issue:

Under NEP 50, I'd expect that `np.polyval` with single precision coefficients and a scalar evaluation value preserves single precision, but it converts to float64 instead.

### Reproduce the code example:

```python
In [6]: a1 = np.array([1., 2., 3., 4., 5.], dtype=np.float32)
   ...: a2 = 5.0
   ...: np.polyval(a1, a2).dtype
Out[6]: dtype('float64')

In [7]: np.__version__
Out[7]: '2.0.0rc2'
```


### Error message:

_No response_

### Python and NumPy Versions:

```
2.0.0rc2
3.11.0 | packaged by conda-forge | (main, Jan 14 2023, 12:27:40) [GCC 11.3.0]
```

### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2024-05-20 06:54:41,,BUG: ``np.polyval`` returns float64 for float32 coefficients,['00 - Bug']
26454,open,disa-star,"### Describe the issue:

When define a simple function like `foo = lambda i,j : 3`
`numpy.fromfunction` will only return a value instead of an array.
And this is going to be my very first time to write an issue report, but I will try my best.

### Reproduce the code example:

```python
import numpy as np
def foo(i,j):
  return 3+i*0

array = np.fromfunction(lambda i, j: foo(i,j), (2, 2), dtype=float)
print(array) 
bar = lambda i,j : 3
print(np.fromfunction(bar, (2, 2), dtype=float))
```


### Error message:

```shell
[[3. 3.]
 [3. 3.]]
3
```


### Python and NumPy Versions:

Name: numpy
Version: 1.25.2

Python 3.10.12 (google colab)

### Runtime Environment:

[{'numpy_version': '1.25.2',
  'python': '3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]',
  'uname': uname_result(system='Linux', node='696c451497cf', release='6.1.58+', version='#1 SMP PREEMPT_DYNAMIC Sat Nov 18 15:31:17 UTC 2023', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Haswell',
  'filepath': '/usr/local/lib/python3.10/dist-packages/numpy.libs/libopenblas64_p-r0-5007b62f.3.23.dev.so',
  'internal_api': 'openblas',
  'num_threads': 2,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.23.dev'}]
None

### Context for the issue:

This is counterintuitive, and I think it violates the integrity.",2024-05-16 11:15:57,,DOC: numpy.fromfunction docs should be clearer about how the wrapped function is called,['04 - Documentation']
26448,open,FreddieWitherden,"### Proposed new feature or change:

It would be nice if `np.searchsorted` could have an option to return *both* the left and right insertion indices.  This change would be backwards compatible and roughly functionally equivalent to:

```python
def searchsorted(a, v, side='left', sorter=None):
    if side == 'both':
        l = np.searchsorted(a, v, side='left', sorter=sorter)
        r = np.searchsorted(a, v, side='right', sorter=sorter)
        return l, r
    else:
        return np.searchsorted(a, v, side=side, sorter=sorter)
```
But, naturally, implemented so as to take advantage of the fact that the left and right indices are likely to be close, thus enabling some redundant searching to be avoided.

A use case for this is when you want to replace ranges of values in an array.  From a real piece of code:
```python
ls = np.searchsorted(con, mfrom, side='left')
le = np.searchsorted(con, mfrom, side='right')
for s, e, t in zip(ls, le, mto):
    con[s:e] = t
```
where there are a pair of calls to `np.searchsorted` which is then iterated over to yield ranges.  This would be simplified under the new proposal to a single such call.",2024-05-15 23:16:38,,ENH: Support side='both' in np.searchsorted to return a pair of arrays,['unlabeled']
26433,open,tim994018,"### Proposed new feature or change:

Consider multiple 1D signals with identical x-coordinate sample points. It is sometimes desired to resample all of these signals to the same interpolated x-coordinates via linear interpolation. Currently, this requires multiple calls to numpy.interp, one for each signal. This is wasteful, because it requires re-calculation of interpolation weights.

For example, consider original time sample coordinates of t, and updated time sample coordinates of t_new. Also, consider three signals sampled in this way: s0, s1, and s2. To resample all three with current methods:

s0_resamp = np.interp(t_new, t, s0)
s1_resamp = np.interp(t_new, t, s1)
s2_resamp = np.interp(t_new, t, s2)

It would be helpful to do this in a single call, like this:

s = np.row_stack((s0, s1, s2))
s_new = np.interp(t_new, t, s)

In this case, the number of rows in s_new would be 3 (the number of 1D signals). The number of columns in s_new would be len(t_new).

In general, fp could be any number of dimensions. Interpolation could be performed along the final dimension by default, with an additional optional input to specify a different dimension for interpolation.

Note: Matlab's interp1 function offers this functionality, as noted in the function description: ""If you have multiple sets of data that are sampled at the same point coordinates, then you can pass v as an array.""",2024-05-14 10:55:43,,ENH: Allow interp to process multiple signals by passing multidimensional fp,['unlabeled']
26404,open,johnthagen,"### Describe the issue:

Running Mypy on some functions within `numpy.ma` lead to type checking errors due to missing annotations.

### Reproduce the code example:

```python
import numpy

a = numpy.ma.array([1, 2, 3, 4], mask=[0, 0, 1, 0])
b = numpy.ma.masked_array([1, 2, 3, 4], mask=[0, 0, 1, 0])
```


### Error message:

```shell
$ mypy test.py 
test.py:3: error: Call to untyped function ""array"" in typed context  [no-untyped-call]
test.py:4: error: Call to untyped function ""MaskedArray"" in typed context  [no-untyped-call]
```


### Python and NumPy Versions:

```
1.26.4
3.12.2 (main, Feb  6 2024, 20:19:44) [Clang 15.0.0 (clang-1500.1.0.2.5)]
```

### Runtime Environment:

```
[{'numpy_version': '1.26.4',
  'python': '3.12.2 (main, Feb  6 2024, 20:19:44) [Clang 15.0.0 '
            '(clang-1500.1.0.2.5)]',
  'uname': uname_result(system='Darwin', node='M2DF94775W-ML', release='23.4.0', version='Darwin Kernel Version 23.4.0: Fri Mar 15 00:12:37 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T6031', machine='arm64')},
 {'simd_extensions': {'baseline': ['NEON', 'NEON_FP16', 'NEON_VFPV4', 'ASIMD'],
                      'found': ['ASIMDHP'],
                      'not_found': ['ASIMDFHM']}},
 {'architecture': 'armv8',
  'filepath': '/Users/hagenjt1/Library/Caches/pypoetry/virtualenvs/non-package-mode-eSjUyYda-py3.12/lib/python3.12/site-packages/numpy/.dylibs/libopenblas64_.0.dylib',
  'internal_api': 'openblas',
  'num_threads': 16,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.23.dev'}]
None
```

### Context for the issue:

We rely on type checking to find bugs in our software that uses Numpy before it is executed. This enables higher code quality and reliability. False positive type errors cost time to track down and can distract from real issues.

Type annotations also help IDEs such as PyCharm and VS Code perform better intellisense during development.",2024-05-08 18:48:01,,ENH: add type annotations to `numpy.ma`,"['01 - Enhancement', 'component: numpy.ma', 'Static typing']"
26401,open,Lolcroc,"Currently polynomial fits work as follows
```python
x = [0, 1]
y = [0, 1]
fit = Polynomial.fit(x, y, deg=1)

print(f""Coefficients for 1-order fit on {x=}, {y=}:"")
print(f""Raw: {fit.coef}"")
print(f""Converted: {fit.convert().coef}"")
```
which prints
```
Coefficients for 1-order fit on x=[0, 1], y=[0, 1]:
Raw: [0.5 0.5]
Converted: [-1.66533454e-16  1.00000000e+00]
```

The result for the raw `fit.coef` is unexpected. The fit is scaled with respect to its current domain, which is by default the endpoints of `x`. The `fit.convert().coef` gives the expected result, but this extra call is somewhat awkward and can easily be overlooked.

### Proposed new feature or change:

I would prefer to set `domain=[-1, 1]` as a default argument of `numpy.polynomial.polynomial.Polynomial.fit`, just as it is for the `numpy.polynomial.polynomial.Polynomial` class. This creates commonality, but also avoids the unexpected outcome as in the above example.",2024-05-08 13:22:12,,ENH: Change behavior for `numpy.polynomial.polynomial.Polynomial.fit` coefficients,['unlabeled']
26380,open,paugier,"### Proposed new feature or change:

Type annotations in code using Numpy (and other Python array libraries) can be used for 3 purposes:

- Python-Numpy compilers (for example Cython or Pythran)
- documentation
- type checking

Currently numpy.typing is more oriented towards type checking. It would be nice if numpy.typing could also be used to easily add information useful for documentation and Python-Numpy compilers. It seems to me that the needs are a bit different from what currently supports Mypy.

For some projects (namely fluidsim, fluidfft, fluidimage), we already use type annotations so that Transonic can automatically produce Pythran, Numba and Cython code. For these projects, I now often feel the need to add type annotations only for documentation even for functions/classes that are not compiled. Unfortunately, numpy.typing is really not yet suitable for theses needs.

Moreover, I see some discussions about enhancing numpy.typing (for example https://github.com/numpy/numpy/issues/16544, see also https://github.com/ramonhagenaars/nptyping) and the solutions proposed seem quite complicated and not very suitable for documentation and Python-Numpy compilers. I mean I see nothing simple and short for something like `Array[""2d"", Type(np.float32, np.float64), ""C""]` (I guess one can guess what it means).

For these purposes (doc and compilers), some very common type information that can be given are about 

1. the number of dimensions of an array (sometimes fused, i.e. `ndim` 2 or 3), 
2. dtypes (sometimes fused, i.e. float64 or complex128) and 
3. memory contiguity/strides. 

It is also very useful and common to specify that a function is limited to some particular arrays (only C contiguous for example, or only ndim equal to 2 or 3).

Specifying the number of elements in one dimension (https://github.com/numpy/numpy/issues/16544) can also be useful but it is less common that specifying the number of dimensions of an array.

Numpy compilers have their own way to describe arrays, often inspired by C notations:

- https://pythran.readthedocs.io/en/latest/MANUAL.html#concerning-pythran-specifications
- https://cython.readthedocs.io/en/latest/src/userguide/memoryviews.html

With Transonic, one can use [annotations with C or Python styles](https://transonic.readthedocs.io/en/latest/examples/type_hints.html), 

```python
from transonic import Array, Type, NDim

A2D = ""float32[:,:]""
# equivalent
A2Dbis = Array[""2d"", np.float32]

Afused = Array[NDim(2, 3), Type(np.float32, np.float64)]
``` 

I'm not saying that numpy.typing should support such things but it seems to me that it is important when designing numpy.typing to consider the different purposes of type annotations in code using Numpy and not to be mostly focus on what is currently supported by Mypy.

Simple things like specifying that an array is a one or two-dimensional array of float64 should be simple and short with numpy.typing.

I add a short real life example about only documenting code. In Fluidimage, I recently wrote when I rediscovered and refactored code written by other developers:

```python
class ThinPlateSplineSubdom:

    num_centers: int
    tps_matrices: List[""float[:,:]""]
    norm_coefs: ""float[:]""
    norm_coefs_domains: List[""float[:]""]

    num_new_positions: int
    ind_new_positions_domains: List[""np.int64[:]""]
    norm_coefs_new_pos: ""float[:]""
    norm_coefs_new_pos_domains: List[""float[:]""]
```

It would be nice if I could replace that by elegant annotations using `numpy.typing`.",2024-05-03 09:49:17,,"ENH: numpy.typing for type checking, documentation and Numpy compilers","['01 - Enhancement', 'Static typing']"
26366,open,inducer,"### Describe the issue:

I create an object exhibiting the array interface for a size-zero array with a NULL pointer as storage. Numpy mumbles something seemingly spurious  about `float` when trying to call `asarray` on that object.

### Reproduce the code example:

```python
import numpy as np

class EmptyArray:
    def __init__(self):
        self.__array_interface__ = {'version': 3, 'shape': (0,), 'strides': (8,), 'typestr': '<f8', 'data': (0, False)} 

empt = EmptyArray()
np.asarray(empt)
```


### Error message:

```shell
Traceback (most recent call last):
  File ""/home/andreas/tmp/numpy-empty-ary-interface.py"", line 8, in <module>
    np.asarray(empt)
TypeError: float() argument must be a string or a real number, not 'EmptyArray'
```


### Python and NumPy Versions:

1.26.4
3.12.3 (main, Apr 10 2024, 05:33:47) [GCC 13.2.0]

### Runtime Environment:
```
[{'numpy_version': '1.26.4',
  'python': '3.12.3 (main, Apr 10 2024, 05:33:47) [GCC 13.2.0]',
  'uname': uname_result(system='Linux', node='arc', release='6.7.9-amd64', version='#1 SMP PREEMPT_DYNAMIC Debian 6.7.9-2 (2024-03-13)', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Prescott',
  'filepath': '/home/andreas/src/env-3.12/lib/python3.12/site-packages/numpy.libs/libopenblas64_p-r0-0cf96a72.3.23.dev.so',
  'internal_api': 'openblas',
  'num_threads': 12,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.23.dev'}]
```

### Context for the issue:

I maintain PyOpenCL. The issue arose in the handling of empty shared-virtual-memory (""SVM"") allocations, which PyOpenCL needs to support. PyOpenCL tightly integrates with numpy, and particularly SVM allocations (i.e. memory that is accessible both from the host and the compute device/GPU) are exposed as numpy arrays. Numpy seems to support empty arrays just fine, just creating them through the array interface appears to lead to this, seemingly, spurious error.",2024-05-01 15:04:03,,BUG: Spurious `TypeError` exception upon calling `asarray` on empty array buffer,['00 - Bug']
26352,open,keithini,"### Describe the issue:

I have an old fortran program that I access from python using f2py. I have used this successfully on both linux and windows. I have moved successfully to 1.26 on linux but get a link error (1120 unresolved externals) on windows 11. I replicated the problem using a basic fibonacci fortran program (below)

### Reproduce the code example:

```python
subroutine fib(a, n)
  use iso_c_binding
   integer(c_int), intent(in) :: n
   integer(c_int), intent(out) :: a(n)
   do i = 1, n
      if (i .eq. 1) then
         a(i) = 0.0d0
      elseif (i .eq. 2) then
         a(i) = 1.0d0
      else
         a(i) = a(i - 1) + a(i - 2)
      end if
   end do
end
```


### Error message:

```shell
(aov) C:\fib_test>f2py -c fib1.f90 -m fib1 --backend meson
Using meson backend
Will pass --lower to f2py
See https://numpy.org/doc/stable/f2py/buildtools/meson.htmlReading fortran codes...
        Reading file 'fib1.f90' (format:free)
Post-processing...
        Block: fib1
                        Block: fib
In: :fib1:fib1.f90:fib
get_useparameters: no module iso_c_binding info used by fib
Applying post-processing hooks...
  character_backward_compatibility_hook
Post-processing (stage 2)...
Building modules...
    Building module ""fib1""...
    Generating possibly empty wrappers""
    Maybe empty ""fib1-f2pywrappers.f""
        Constructing wrapper function ""fib""...
          a = fib(n)
    Wrote C/API module ""fib1"" to file "".\fib1module.c""
The Meson build system
Version: 1.2.1
Source dir: C:\Users\keith\AppData\Local\Temp\tmp679okcip
Build dir: C:\Users\keith\AppData\Local\Temp\tmp679okcip\bbdir
Build type: native build
Project name: fib1
Project version: 0.1
Fortran compiler for the host machine: ifx (intel-llvm-cl 2022.1.0)
Fortran linker for the host machine: xilink.exe xilink 2022.1.0
C compiler for the host machine: cl (msvc 19.29.30146 ""Microsoft (R) C/C++ Optimizing Compiler Version 19.29.30146 for x64"")
C linker for the host machine: link link 14.29.30146.0
Host machine cpu family: x86_64
Host machine cpu: x86_64
Program python3 found: YES (C:\Users\keith\anaconda3\envs\aov\python.exe)
Run-time dependency python found: YES 3.12
Build targets in project: 1

Found ninja-1.10.2 at C:\Users\keith\anaconda3\envs\aov\Library\bin\ninja.EXE
INFO: autodetecting backend as ninja
INFO: calculating backend command to run: C:\Users\keith\anaconda3\envs\aov\Library\bin\ninja.EXE -C C:/Users/keith/AppData/Local/Temp/tmp679okcip/bbdir
ninja: Entering directory `C:/Users/keith/AppData/Local/Temp/tmp679okcip/bbdir'
[4/6] Compiling Fortran object fib1.cp312-win_amd64.pyd.p/fib1-f2pywrappers.f.obj
../fib1-f2pywrappers.f: remark #5133: The input stream is empty
[6/6] Linking target fib1.cp312-win_amd64.pyd
FAILED: fib1.cp312-win_amd64.pyd
""link""  /MACHINE:x64 /OUT:fib1.cp312-win_amd64.pyd fib1.cp312-win_amd64.pyd.p/fib1.f90.obj fib1.cp312-win_amd64.pyd.p/fib1module.c.obj fib1.cp312-win_amd64.pyd.p/fib1-f2pywrappers.f.obj fib1.cp312-win_amd64.pyd.p/42bc432c09fd0a50dc493f9b8a0dabb66e5dad74_.._.._f2py_src_fortranobject.c.obj ""/nologo"" ""/release"" ""/nologo"" ""/OPT:REF"" ""/DLL"" ""/IMPLIB:fib1.cp312-win_amd64.lib"" ""-shared"" ""C:\Users\keith\anaconda3\envs\aov\libs\python312.lib"" ""kernel32.lib"" ""user32.lib"" ""gdi32.lib"" ""winspool.lib"" ""shell32.lib"" ""ole32.lib"" ""oleaut32.lib"" ""uuid.lib"" ""comdlg32.lib"" ""advapi32.lib""
LINK : warning LNK4044: unrecognized option '/shared'; ignored
   Creating library fib1.cp312-win_amd64.lib and object fib1.cp312-win_amd64.exp
fib1module.c.obj : error LNK2001: unresolved external symbol fib_
fib1.cp312-win_amd64.pyd : fatal error LNK1120: 1 unresolved externals
ninja: build stopped: subcommand failed.
Traceback (most recent call last):
  File ""C:\Users\keith\anaconda3\envs\aov\Scripts\f2py-script.py"", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File ""C:\Users\keith\anaconda3\envs\aov\Lib\site-packages\numpy\f2py\f2py2e.py"", line 732, in main
    run_compile()
  File ""C:\Users\keith\anaconda3\envs\aov\Lib\site-packages\numpy\f2py\f2py2e.py"", line 705, in run_compile
    builder.compile()
  File ""C:\Users\keith\anaconda3\envs\aov\Lib\site-packages\numpy\f2py\_backends\_meson.py"", line 131, in compile
    self.run_meson(self.build_dir)
  File ""C:\Users\keith\anaconda3\envs\aov\Lib\site-packages\numpy\f2py\_backends\_meson.py"", line 124, in run_meson
    raise subprocess.CalledProcessError(
subprocess.CalledProcessError: Command '['meson', 'compile', '-C', 'bbdir']' returned non-zero exit status 1.
```


### Python and NumPy Versions:

python 3.12.3
Numpy 1.26.0


### Runtime Environment:

(aov) C:\Users\keith\temp\fib_test>python -c ""import numpy; print(numpy.show_runtime())""
[{'numpy_version': '1.26.0',
  'python': '3.12.3 | packaged by Anaconda, Inc. | (main, Apr 19 2024, '
            '16:41:55) [MSC v.1916 64 bit (AMD64)]',
  'uname': uname_result(system='Windows', node='LENNY2', release='11', version='10.0.22631', machine='AMD64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'filepath': 'C:\\Users\\keith\\anaconda3\\envs\\aov\\Library\\bin\\mkl_rt.2.dll',
  'internal_api': 'mkl',
  'num_threads': 8,
  'prefix': 'mkl_rt',
  'threading_layer': 'intel',
  'user_api': 'blas',
  'version': '2023.1-Product'}]
None

### Context for the issue:

A user has upgraded to python 3.12 and cannot use our product",2024-04-26 17:54:46,,f2py fails to link program on windows using v1.26.0 - works on earlier versions of numpy ,"['00 - Bug', 'component: numpy.f2py']"
26336,open,carlosgmartin,"### Proposed new feature or change:

_[Mailing list post](https://mail.python.org/archives/list/numpy-discussion@python.org/thread/MBUWZRBWN5RQR7OCPQR5JHRKCJEETT3M/)_

The following reduction functions (and their nan* counterparts) receive a `where` argument:

- [max](https://numpy.org/doc/stable/reference/generated/numpy.max.html)
- [min](https://numpy.org/doc/stable/reference/generated/numpy.min.html)
- [sum](https://numpy.org/doc/stable/reference/generated/numpy.sum.html)
- [prod](https://numpy.org/doc/stable/reference/generated/numpy.prod.html)
- [mean](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)
- [var](https://numpy.org/doc/stable/reference/generated/numpy.var.html)
- [std](https://numpy.org/doc/stable/reference/generated/numpy.std.html)

**Feature request:** Add a `where` argument to the following reduction functions (and their nan* counterparts):

- [ ] [ptp](https://numpy.org/doc/stable/reference/generated/numpy.ptp.html)
- [ ] [average](https://numpy.org/doc/stable/reference/generated/numpy.average.html)
- [ ] [median](https://numpy.org/doc/stable/reference/generated/numpy.median.html)
- [ ] [quantile](https://numpy.org/doc/stable/reference/generated/numpy.quantile.html)
- [ ] [percentile](https://numpy.org/doc/stable/reference/generated/numpy.percentile.html)
- [ ] [argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)
- [ ] [argmin](https://numpy.org/doc/stable/reference/generated/numpy.argmin.html)
    - https://github.com/numpy/numpy/issues/14371
    - https://github.com/numpy/numpy/pull/21625",2024-04-23 05:03:27,,ENH: Add ``where`` argument to reduction functions that are missing it,['unlabeled']
26289,open,rgommers,"No longer shipping the tests with the numpy wheels uploaded to PyPI was discussed in gh-25737. This is now possible after gh-26274. Here are some questions about how we want to go about this. My assumption is that we want to do this in a fairly non-disruptive way, meaning no moving of all tests files (that would yield a ton of merge conflicts on all open PRs).

I think these will be the next steps in a follow-up PR to gh-26274:

- Drop the `tests` label from the default set of install tags in `pyproject.toml` **or** modify our cibuildwheel config to do that (this needs deciding)
- Start producing an installable test suite (`numpy-tests` wheels).
    - Decision 2: do we want that to land in-tree inside a `numpy` install, or out-of-tree (e.g. in a separate `numpy-tests` directory)? The latter seems cleaner, but may require some more tweaks to test structure (e.g., `import numpy._core._multiarray_tests` needs modifying).
- Tweak our CI for wheel building to make everything work again.
- Ensure that `python -c ""import numpy; numpy.test()""` gives a clear error message when the tests are not installed
- Add docs for distro packagers and others who want to run the tests.

Thoughts?",2024-04-16 12:13:41,,Making the numpy test suite separately installable,"['01 - Enhancement', '17 - Task', '14 - Release']"
26278,open,HenryAsa,"### Describe the issue:

There are a number of areas throughout the `numpy` module that have Pylint linting issues, specifically in regards to using f-strings as opposed to the `.format()` method.  I noticed that [Pylint `C0209`](https://pylint.readthedocs.io/en/latest/user_guide/messages/convention/consider-using-f-string.html) was particularly prevalent throughout the numpy repository, so I updated these linting errors.

See [Pylint `C0209`](https://pylint.readthedocs.io/en/latest/user_guide/messages/convention/consider-using-f-string.html) for more details.

### Reproduce the code example:

```python
import numpy as np
```


### Error message:

```shell
Formatting a regular string which could be an f-string: Pylint C0209: consider-using-f-string
```


### Python and NumPy Versions:

Python version >= 3.6

### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2024-04-15 03:54:07,,STY: Pylint f-string Errors `C0209`,['03 - Maintenance']
26257,open,r-devulap,"### Describe the issue:

While I can build NumPy with icx and icpx compilers, it fails at import time. 

### Reproduce the code example:

```python
import numpy as np
```


### Error message:

```shell
Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/dist-packages/numpy/_core/__init__.py"", line 23, in <module>
    from . import multiarray
  File ""/usr/local/lib/python3.10/dist-packages/numpy/_core/multiarray.py"", line 10, in <module>
    from . import overrides
  File ""/usr/local/lib/python3.10/dist-packages/numpy/_core/overrides.py"", line 8, in <module>
    from numpy._core._multiarray_umath import (
ImportError: /usr/local/lib/python3.10/dist-packages/numpy/_core/_multiarray_umath.cpython-310-x86_64-linux-gnu.so: undefined symbol: tan8_h

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/dist-packages/numpy/__init__.py"", line 114, in <module>
    from numpy.__config__ import show as show_config
  File ""/usr/local/lib/python3.10/dist-packages/numpy/__config__.py"", line 4, in <module>
    from numpy._core._multiarray_umath import (
  File ""/usr/local/lib/python3.10/dist-packages/numpy/_core/__init__.py"", line 49, in <module>
    raise ImportError(msg)
ImportError:

IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!

Importing the numpy C-extensions failed. This error can happen for
many reasons, often due to issues with your setup or how NumPy was
installed.

We have compiled some common reasons and troubleshooting tips at:

    https://numpy.org/devdocs/user/troubleshooting-importerror.html

Please note and check the following:

  * The Python version is: Python3.10 from ""/usr/bin/python3""
  * The NumPy version is: ""2.1.0.dev0+git20240402.4517378""

and make sure that they are the versions you expect.
Please carefully study the documentation linked above for further help.

Original error was: /usr/local/lib/python3.10/dist-packages/numpy/_core/_multiarray_umath.cpython-310-x86_64-linux-gnu.so: undefined symbol: tan8_h
```


### Python and NumPy Versions:

'3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]'


### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2024-04-10 17:10:26,,BUG: Cannot import numpy built with Intel oneAPI compiler icx and icpx,['00 - Bug']
26247,open,charris,"Cygwin currently supports Python 3.9, which we are dropping in Numpy 2.1, so the test has been disabled. The next Python Cygwin plans to support is 3.12, so wait until that release to reenable.

The test can be enabled by going to `actions`, selecting the cygwin action, and hitting the `...` button to get the options.",2024-04-09 18:35:15,,Reenable the Cygwin build test when Cygwin support Python 3.12,"['17 - Task', 'component: CI']"
26233,open,lpsinger,"### Proposed new feature or change:

I maintain several Python packages with C extensions that define ufuncs and generalized ufuncs. I have been relying upon the `add_newdoc_ufunc` function to add docstrings to those functions because it is such a pain to write legible multiline strings in C. According to the [Numpy 2.0.0 migration guide](https://numpy.org/devdocs/numpy_2_0_migration_guide.html#main-namespace), `add_newdoc_ufunc` is ""an internal function and doesn’t have a replacement."" Is it possible to add it back to the public namespace as `numpy.lib.add_newdoc_ufunc`?",2024-04-08 21:09:21,,ENH: add __dict__ to the ufunc object,"['01 - Enhancement', 'Project', 'Numpy 2.0 API Changes']"
26226,open,y-koj,"### Issue with current documentation:

`order` kwargs documentation in the ufunc API reference is confusing, especially around ""if the inputs are F-contiguous and not also not C-contiguous, C-contiguous otherwise"".

Current `order` kwargs description is:
> Specifies the calculation iteration order/memory layout of the output array. Defaults to 'K'. 'C' means the output should be C-contiguous, 'F' means F-contiguous, 'A' means F-contiguous if the inputs are F-contiguous and not also not C-contiguous, C-contiguous otherwise, and 'K' means to match the element ordering of the inputs as closely as possible.

#### Related links
https://github.com/numpy/numpy/blob/main/doc/source/reference/ufuncs.rst
https://numpy.org/doc/stable/reference/ufuncs.html
PR #3221

### Idea or request for content:

The description will be clear by rewriting like below. It replaces ""F-contiguous and not also not C-contiguous"" with ""F-contiguous and not C-contiguous"" and divides the long sentence into short.
> Specifies the calculation iteration order/memory layout of the output array. Defaults to 'K'. 'C' means the output should be C-contiguous. 'F' means F-contiguous. 'A' means F-contiguous if the inputs are F-contiguous and not C-contiguous, C-contiguous otherwise. 'K' means to match the element ordering of the inputs as closely as possible.

My concerns about this documentation change idea are:
1. Whether ""F-contiguous and not C-contiguous"" reflects the actual API semantics or not
2. A clearer rephrase of ""if the inputs are F-contiguous and not C-contiguous, C-contiguous otherwise"". The last ""C-contiguous, C-contiguous otherwise"" looks a bit weird.

The concern (1) is important because it relates to the API semantics. I need a comment from those who understand how ufunc-compliant functions are implemented or know where/how the ufunc `order` kwargs behavior is decided.",2024-04-08 12:14:24,,DOC: ufunc API documentation needs clarification for `order` kwargs description,['04 - Documentation']
26224,open,khood5,"still getting 
<pre>pickle.dump(array, fp, protocol=3, **pickle_kwargs)                                                             
OverflowError: serializing a bytes object larger than 4 GiB requires pickle protocol 4 or higher
</pre>
with 
<pre>Python 3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:35:02) [GCC 11.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.                                              
>>> import pickle                                                                                                   
>>> print(pickle.format_version)                                                                                    
4.0                                                                                                                
 >>> exit 
</pre>
and 
<pre>Python 3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:35:02) [GCC 11.2.0] on linux                    
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.                                              
>>> import numpy                                                                                                    
>>> numpy.version.version                                                                                           
'1.26.4'                                                                                                            
>>> exit                                                                                                            
Use exit() or Ctrl-D (i.e. EOF) to exit 
</pre>

I think this is incorrect still set to 3. Or am I missing something?
https://github.com/numpy/numpy/blob/e59c074842e3f73483afa5ddef031e856b9fd313/numpy/lib/format.py#L744

_Originally posted by @khood5 in https://github.com/numpy/numpy/issues/18784#issuecomment-2041631588_
            ",2024-04-07 22:52:55,,BUG: cannot use `np.save` and `allow_pickle=True` with data larger than 4 GB,['unlabeled']
26220,open,josephernest,"### Proposed new feature or change:

In many situations, it is required to shift an array (like `np.roll`) and fill the values with zeros, instead of making the array ""loop"". 

Is there a way to implement this:

    x = np.array([1, 2, 3, 4, 5]
    np.roll(x, 2, method=""constant"")   #  [ 0 0 1 2 3 ]   instead of  [ 4 5 1 2 3 ]
    np.roll(x, -2, method=""constant"")  #  [ 3 4 5 0 0 ]  instead of   [ 3 4 5 1 2 ]

?

Example : [python numpy roll with padding](https://stackoverflow.com/questions/2777907/python-numpy-roll-with-padding)

Important notes:

* solutions with `np.pad` exist but are slower, see https://stackoverflow.com/questions/2777907/python-numpy-roll-with-padding#comment3270288_3153267

* other people have the needs for this: https://stackoverflow.com/questions/2777907/python-numpy-roll-with-padding#comment120688507_2778068: *I think np.roll should do it natively with a keyword argument.* (quite upvoted)

* this solution works: https://stackoverflow.com/a/2778068 but it would be interesting for the Numpy API to have this useful option (in the same way `np.pad` has `constant` mode, etc.)",2024-04-07 10:23:24,,ENH: np.roll: extra argument to pad with zeros or constant,['unlabeled']
26216,open,guilhermeleobas,"### Describe the issue:

Not sure if this is by design or actually a bug but one cannot call `np.median` on datetime arrays.

This was also reported on [StackOverflow](https://stackoverflow.com/questions/43889611/median-of-panda-datetime64-column) some years ago.

### Reproduce the code example:

```python
import numpy as np
a = np.array([np.datetime64('2005-02-25'), np.datetime64(1, 'Y')])
np.median(a)
```


### Error message:

```shell
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<__array_function__ internals>"", line 180, in median
  File ""/Users/guilhermeleobas/micromamba/envs/numba/lib/python3.9/site-packages/numpy/lib/function_base.py"", line 3793, in median
    r, k = _ureduce(a, func=_median, axis=axis, out=out,
  File ""/Users/guilhermeleobas/micromamba/envs/numba/lib/python3.9/site-packages/numpy/lib/function_base.py"", line 3702, in _ureduce
    r = func(a, **kwargs)
  File ""/Users/guilhermeleobas/micromamba/envs/numba/lib/python3.9/site-packages/numpy/lib/function_base.py"", line 3847, in _median
    rout = mean(part[indexer], axis=axis, out=out)
  File ""<__array_function__ internals>"", line 180, in mean
  File ""/Users/guilhermeleobas/micromamba/envs/numba/lib/python3.9/site-packages/numpy/core/fromnumeric.py"", line 3474, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File ""/Users/guilhermeleobas/micromamba/envs/numba/lib/python3.9/site-packages/numpy/core/_methods.py"", line 179, in _mean
    ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)
numpy.core._exceptions._UFuncBinaryResolutionError: ufunc 'add' cannot use operands with types dtype('<M8[D]') and dtype('<M8[D]')
```


### Python and NumPy Versions:

1.22.4
3.9.18 | packaged by conda-forge | (main, Dec 23 2023, 16:35:41)
[Clang 16.0.6 ]

Also tried on Numpy 1.24, 1.25 and 1.26.

### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2024-04-05 12:21:34,,BUG: Cannot call `np.median` on datetime array,['00 - Bug']
26211,open,charris,"The numpy repository has gathered a lot of maintenance branches that are no longer used. This is beginning to make such simple tasks as basing backports on the current release branch cumbersome due to the large number of branches listed in the base, and this also makes the choice subject to errors. I propose that we figure out how we want to save the history in an accessible way, yet unclutter the main repo. Searching on that topic shows several ways to do this, none of which look very appealing at first glance. Does anyone have experience with this or suggestions? ",2024-04-04 15:30:55,,Archive and prune the github repo.,"['17 - Task', '23 - Wish List']"
26206,open,timhoffm,"### Proposed new feature or change:

I would like to be able to generate all permutations of an array; e.g.

```
>>> np.permutations([0, 1, 2])
array([[0, 1, 2],
       [0, 2, 1],
       [1, 0, 2],
       [1, 2, 0],
       [2, 0, 1],
       [2, 1, 0]])
````
AFAICS, there's neither a dedicated function for that in numpy already nor is it easily expressable through a combination of existing functions. I've only found the `choice` and `permutation` functions from the random generators, but these only create samples not a  complete set of permutations.

Would such a function be of interest for numpy? If so, I could start with a PR proposal.

Note: The standard library provides `itertools.permutations(...)` but that's significantly slower that a pure numpy implementation.

",2024-04-03 15:58:09,,ENH: Function to generate all permutations of an array,['unlabeled']
26200,open,luyahan,"I'm currently planing in add RISC-V Vector support for NumPy. 

Originally, I want to port rvv into _simd. But I found a great solution NEP-054.
I read NEP-054 (https://numpy.org/neps/nep-0054-simd-cpp-highway.html#discussion) and would like to get more information about its current status.
I noticed some PR had been opened ( https://github.com/numpy/numpy/pull/25781  and  https://github.com/numpy/numpy/pull/25934 ) for the NEP-054 but still under review and discussion. 
I wonder  if there are any specific tasks or contributions I can undertake to assist with its progress. 

And whether I should plan RVV support following the current NPY_SIMD implementation style or in the NEP-054 way?
Any suggestion or reference for this topic?",2024-04-03 04:26:48,,ENH: Add RISC-V Vector V1.0 Support,"['01 - Enhancement', 'component: SIMD']"
26191,open,rgommers,"This list tracks the compatibility status of packages that depend on or support NumPy. If ""compatible release on PyPI"" does not say ""yes"" but a version number is listed: this is based on plans announced in a tracking issue or other communication by the authors of the package.

_Maintainers: please feel free to edit directly (please refresh the page first to avoid overwriting edits from others!). Others who want to update things: please do comment, or feel free to ping me elsewhere._


 
| Package name  | Compatible release on PyPI? | Min compatible version | Notes                                                                                                   |
|---------------|-----------------------------|------------------------|---------------------------------------------------------------------------------------------------------|
| Adaptive                 |  yes         |  1.3.0    | https://github.com/python-adaptive/adaptive/pull/458   |
| arch    | yes    | 7.0.0    | https://github.com/bashtage/arch/pull/720    |
| AstroPy       |  yes          | 6.1.0                  | https://github.com/astropy/astropy/issues/16200                                 |
| astropy-healpix  | yes    | 1.0.3    | https://github.com/astropy/astropy-healpix/pull/214  |
| autograd    | yes   | 1.7.0    | https://github.com/HIPS/autograd/pull/618    | 
| AwkwardArray  | yes               |    2.6.3               |  https://github.com/scikit-hep/awkward/pull/3064                                                |
| BioPython     | yes     |  1.84          | https://github.com/biopython/biopython/issues/4676          |
| Bokeh    |  yes    | 3.4.1    | https://github.com/bokeh/bokeh/issues/13835    |
| boost-histogram | yes    | 1.4.1   | https://github.com/numpy/numpy/issues/26191#issuecomment-2179127999     |
| Boost.Python |     |  1.86.0      | https://github.com/boostorg/python/issues/431   |
| bottleneck   | yes   | 1.4.0    | https://github.com/pydata/bottleneck/issues/453   |
| CARMA | n/a   | 0.8.0     | https://github.com/RUrlus/carma/issues/129   | 
| Cartopy    |yes      | 0.23              | https://github.com/SciTools/cartopy/issues/2339     |
| Catboost  |      |      | https://github.com/catboost/catboost/issues/2671   |
| clawpack  |    |    | https://github.com/clawpack/clawpack/issues/252   |
| cmocean  | yes | 3.1.0    | https://github.com/matplotlib/cmocean/pull/99   |
| ContourPy  | yes      | 1.2.1           | https://github.com/contourpy/contourpy/pull/371      |
| CPNest  | yes    | 0.11.6    | https://github.com/johnveitch/cpnest/issues/92    |
| CuPy          | (13.2.0 only) | 14.0.0 (13.2.0 partial support)      | https://github.com/cupy/cupy/issues/8306  |
| Cython        | yes         | 3.0.4                  | Version is an estimate, it's worked fine for quite a while            |
| Cython-BLIS    | yes    | 1.0.0    | https://github.com/explosion/cython-blis/issues/106       |
| Dask          | yes       | 2024.5.1            | https://github.com/dask/dask/issues/11066     |
| Datashader | yes    | 0.16.2    | https://github.com/holoviz/datashader/issues/1324  |
| EigenPy  |    | 3.5.0 (possibly more work still needed)           | https://github.com/stack-of-tasks/eigenpy/pull/448   |
| fastparquet         | yes  | 2024.5.0               | https://github.com/dask/fastparquet/pull/922        |
| fitsio  | yes    | 1.2.3    | https://github.com/esheldon/fitsio/issues/393  |
| GDAL        | yes      | 3.9.0   | https://github.com/OSGeo/gdal/issues/9751   |
| GeoPandas     | yes              | 0.14.4                | https://github.com/geopandas/geopandas/issues/3258            |
| GNU Radio  | n/a    | 3.10.10.1    | https://github.com/gnuradio/gnuradio/issues/7378  |
| h5py         | yes                |         3.11.0         |    https://github.com/h5py/h5py/issues/2353                                              |
| holoviews   | yes   | 1.19.0   | https://github.com/holoviz/holoviews/pull/6238   |
| hypothesis | yes   | 6.100.2  | https://github.com/HypothesisWorks/hypothesis/issues/3950    |
| imagecodecs  | yes    | 2024.6.1         | https://github.com/cgohlke/imagecodecs/issues/100    |
| Imageio | yes     | 2.34.2        | https://github.com/imageio/imageio/issues/1090    |
| iminuit | yes   | 2.26.0   | https://github.com/scikit-hep/iminuit/pull/977       |
| ITK    | yes    | 5.4.0    | https://github.com/InsightSoftwareConsortium/ITK/issues/4700    |
| JAX           | yes                         |       0.4.26           | https://github.com/google/jax/issues/19246                                                              |
| Keras         | yes                         | 3.5.0                  | https://github.com/keras-team/keras/issues/19691                         |
| LightGBM    | yes   | 4.4.0            | https://github.com/microsoft/LightGBM/issues/6454 & https://github.com/microsoft/LightGBM/pull/6439            |
| lightly | yes      | 1.5.11    | https://github.com/lightly-ai/lightly/issues/1558 |
| linearmodels   | yes    | 6.0.0    | https://github.com/bashtage/linearmodels/pull/593   |
| Matplotlib    | yes                         | 3.8.4                 | https://github.com/matplotlib/matplotlib/issues/26778                                                   |
| MDAnalysis |                            | 2.8.0   |         https://github.com/MDAnalysis/mdanalysis/pull/4482     |                                                                            
| ml_dtypes        | yes                        | 0.4.0                  | https://github.com/jax-ml/ml_dtypes/pull/143                |
| mlpack                                 | yes | 4.5.0                           | https://github.com/mlpack/mlpack/issues/3723        |
| MNE-Python  | yes   | 1.7.0   | https://github.com/mne-tools/mne-python/issues/12672   |
| Modin   | yes   | 0.31.0    | https://github.com/modin-project/modin/issues/7310   |
| netCDF4    | yes      | 1.7.0      | https://github.com/Unidata/netcdf4-python/pull/1317         |    
| NetworkX      |  yes           | 3.3                       |  https://github.com/networkx/networkx/pull/7390                   |
| Nipy       | yes                   | 0.6.1                |  https://github.com/nipy/nipy/pull/565 |
| Numba         |  yes        | 0.60                   | https://github.com/numba/numba/issues/9544, [Discourse post with context](https://numba.discourse.group/t/communicating-numpy-2-0-changes-to-numba-users/2457/1)                 |
| numcodecs     | yes    | 0.12.1   | Likely older versions too; has been stable for a while. https://github.com/zarr-developers/numcodecs/issues/521  |
| numexpr       | yes                   | 2.10.0               |  https://github.com/pydata/numexpr/pull/478                  |
| numpngw       | yes                   | 0.1.4                | |
| OpenCV      |  yes     | 4.10.0.84     | https://github.com/opencv/opencv-python/issues/943              |
| Pandas        | yes                  | 2.2.2                | https://github.com/pandas-dev/pandas/issues/55519                  |
| pint    | yes   | 0.24   | https://github.com/hgrecco/pint/issues/1974    |
| Polars  | yes    | 1.1.0     | https://github.com/pola-rs/polars/issues/16998    |
| polyagamma   | yes   | 2.0.0    | https://github.com/zoj613/polyagamma/pull/127   |
| pvlib           | yes  | 0.10.5                         | https://github.com/pvlib/pvlib-python/issues/2026    |
| PyArrow       | yes                      |16.0                  | https://github.com/apache/arrow/issues/39532                                                            |
| Py-ART / arm-pyart        | yes  | 1.18.2                         | https://github.com/ARM-DOE/pyart/issues/1550   |
| Pybind11      | yes                         | 2.12.0                 | https://github.com/pybind/pybind11/issues/5009                  |
| PyData Sparse | yes               | 0.15.2       | https://github.com/pydata/sparse/issues/680  |
| pyEDFlib   | yes | 0.1.38 | https://github.com/holgern/pyedflib/issues/259    |
| PyGSL      | no  |        |                                                   |
| PyKrige    | yes   | 1.7.2     | https://github.com/GeoStat-Framework/PyKrige/pull/290    |
| PyMC         |                             |                        | depends on PyTensor                                          |
| Pyomo   | yes   | 6.8.0   | https://github.com/Pyomo/pyomo/pull/3292   | 
| PySide6    | yes    | 6.7.0    | from a comment on this issue: https://github.com/numpy/numpy/issues/26191#issuecomment-2106770424   |
| PySpark    |    | 4.0.0   | https://github.com/apache/spark/pull/47083    |
| PyTables         | yes                        | 3.10                  |      https://github.com/PyTables/PyTables/issues/1083                      |
| PyTensor    |          |              | https://github.com/pymc-devs/pytensor/pull/689   |
| Pythran       | yes              | 0.16.0                 | (0.15.0 works mostly, SciPy builds with it) https://github.com/serge-sans-paille/pythran/issues/2189          |
| PyTorch       | yes                | 2.3.0 (*)                 | https://github.com/pytorch/pytorch/issues/107302 (Windows binaries issue with 2.3.x-2.4.0: [pytorch#131668](https://github.com/pytorch/pytorch/issues/131668)           |
| PyVista    | yes    | 0.44.0    | [pyvista/pyvista/releases/tag/v0.44.0](https://github.com/pyvista/pyvista/releases/tag/v0.44.0)    |
| PyWavelets    | yes                         | 1.6.0                  | https://github.com/PyWavelets/pywt/pull/731                       |
| QuTiP       | yes    | 5.0.3        | https://github.com/qutip/qutip/pull/2421, https://github.com/qutip/qutip/pull/2457    |
| randomgen   | yes    | 2.0.0    | https://github.com/bashtage/randomgen/pull/375   |
| RAPIDS    |     | 24.10    | https://github.com/rapidsai/build-planning/issues/38   |
| Rasterio     | yes                 | 1.3.10                  | https://github.com/rasterio/rasterio/issues/3024            |
| RDKit    |    | 2024.3.4    | https://github.com/rdkit/rdkit/issues/7477    |
| reikna   | yes  | 0.9.0    | https://github.com/fjarri/reikna/pull/69   |
| rust-numpy  | yes  | 0.22     | https://github.com/PyO3/rust-numpy/issues/409   | 
| sagemaker   |   |   | https://github.com/aws/sagemaker-python-sdk/issues/4882 |
| scikit-bio    | yes     | 0.6.1  | https://github.com/scikit-bio/scikit-bio/issues/1964                                                    |
| scikit-image  |yes             | 0.23.1             | https://github.com/scikit-image/scikit-image/issues/7282                                                |
| scikit-learn  | yes           | 1.4.2                 | https://github.com/scikit-learn/scikit-learn/issues/27075              |
| scikit-sparse    |    |     | https://github.com/scikit-sparse/scikit-sparse/issues/120    |
| scipp    | yes    | 24.06.0     | https://github.com/scipp/scipp/pull/3427    |
| SciPy         | yes                        | 1.13.0                 |  https://github.com/scipy/scipy/pull/20375                    |
| Shapely       | yes             |  2.0.4       | https://github.com/shapely/shapely/issues/1972                                |
| SpaCy    |    |    | https://github.com/explosion/spaCy/issues/13528   |
| spglib  | yes    | 2.5.0    | https://github.com/spglib/spglib/issues/407    |
| SymPy         | yes        | 1.12.1                 |                                                                                                         |
| Seaborn       | yes      |  0.13.2       |  https://github.com/mwaskom/seaborn/pull/3683                    |
| statsmodels   | yes         | 0.14.2         | https://github.com/statsmodels/statsmodels/issues/9194          |
| TensorBoard    |    | 1.17.1 or 1.18.0    | https://github.com/tensorflow/tensorboard/issues/6869    |
| TensorFlow    |        | 1.18.0       | https://github.com/tensorflow/tensorflow/issues/67291,   has `<2` upper bound for 2.16.1, [requirements](https://github.com/tensorflow/tensorflow/blob/master/ci/official/requirements_updater/requirements.in), [lock file](https://github.com/tensorflow/tensorflow/blob/master/requirements_lock_3_12.txt)            |
| Thinc    |    |    | https://github.com/explosion/thinc/issues/939    |
| threadpoolctl | yes | 3.5.0       | https://github.com/joblib/threadpoolctl/pull/175     |
| tifffile      | yes    | 2024.4.24  | https://github.com/cgohlke/tifffile/issues/252  |
| TorchGeo   | yes    | 0.6.0    | https://github.com/microsoft/torchgeo/pull/2151    |
| torchvision   |    | 0.19.1 (0.18.0 is >98% compatible)   | https://github.com/pytorch/vision/issues/8460 (0.19.0 wheels have a Windows issue)   |
| treelite    | yes       | 4.2.1       | https://github.com/dmlc/treelite/issues/560    |
| unyt          | yes                         | 3.0.2                 | https://github.com/yt-project/unyt/pull/493                |
| VisPy   | yes    | 0.14.3    | https://github.com/vispy/vispy/pull/2599    |
| wavio   | yes    | 0.0.9     |                                             |
| Xarray        | yes                        | 2024.06.0         |  https://github.com/pydata/xarray/issues/8844               |
| XGBoost   | yes      | 2.1.0  | https://github.com/dmlc/xgboost/issues/10221  |
| yt            | yes                         | 4.3.1                 | https://github.com/yt-project/yt/pull/4859                   |
| Zarr          | yes      | 2.18.0    | https://github.com/zarr-developers/zarr-python/issues/1818  |
| zfpy          |           |        | https://github.com/LLNL/zfp/issues/210    |





",2024-04-01 21:34:12,,Ecosystem compatibility with numpy 2.0,['Tracking / planning']
26177,open,Wainberg,"### Describe the issue:

When `np.datetime64('NaT')` and `np.timedelta64('NaT')` appear inside arrays with `dtype=object`, they're incorrectly cast to `-9.22337204e+18` when the array is cast to floating-point types (including complex), rather than giving an error.

### Reproduce the code example:

```python
>>> import numpy as np
>>> np.array([np.datetime64('NaT'), 1], dtype=object).astype(float)
array([-9.22337204e+18,  1.00000000e+00])
>>> np.array([np.timedelta64('NaT'), 1+2j], dtype=object).astype(complex)
array([-9.22337204e+18+0.j,  1.00000000e+00+2.j])
```


### Error message:

_No response_

### Python and NumPy Versions:

1.26.4
3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]

### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2024-03-29 23:08:22,,BUG: NaT is incorrectly cast inside object arrays,['00 - Bug']
26176,open,ngoldbaum,"We talked a little bit about this in #25993 but didn't resolve things.

Currently, the following functions don't have implementations in `np.strings`:

https://github.com/numpy/numpy/blob/ef6299c7c2bd309646b6cc64f302de93de5e99b8/numpy/_core/strings.py#L66-L67

We should figure out the appropriate API for adding fast ufuncs for these. I'm actually not sure if it's possible to write ufuncs for these given that they have an unknown number of outputs for each array element, you really need ragged arrays for this to make sense. Although that said, we could also return object arrays of python lists or just leave them with the `_vec_string` implementation.",2024-03-29 21:28:54,,MAINT: what should we do about string ufuncs that return lists?,['component: numpy.strings']
26159,open,ngoldbaum,"As noted in [PEP 703](https://peps.python.org/pep-0703/#borrowed-references) C API functions that return borrowed references are problematic in the nogil build.

We need to audit our usages and replace any that upgrade a borrowed reference to an owned reference with alternate C API functions that return new references.

Problematic Functions:

* [x] `PyList_GetItem` (#26246)
* [x] `PyList_GET_ITEM` (all existing usages are in argument parsing)
* [x] `PyDict_GetItem`
* [x] `PyDict_GetItemWithError`
* [x] `PyDict_Next`
* [x] `PyDict_GetItemString` (#27145 for the PyDict items).
* [x] `_PyDict_GetItemStringWithError` (https://github.com/numpy/numpy/pull/26282)
* [ ] Some sort of linter to force us to audit new usages of problematic functions.",2024-03-28 19:43:38,,MNT: Add linter for thread-unsafe C API uses,"['03 - Maintenance', 'sprintable', '39 - free-threading']"
26153,open,lovetheguitar,"### Describe the issue:

Hello lovely people :) 

please check the title for a description of the issue at hand.

Thanks for your great work!

### Reproduce the code example:

```python
import numpy as np

np.polynomial.Polynomial.fit([1, 2, 3], [1, 2, 3], deg=1)
```


### Error message:

```shell
numpy_demo.py:2: error: Call to untyped function ""fit"" of ""ABCPolyBase"" in typed context  [no-untyped-call]
```


### Python and NumPy Versions:

1.24.4
3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]

### Runtime Environment:

Mypy via pre-commit:  6e63c9e9c65e1df04465cdcda0f2490e89291f58 -> v1.9.0 (frozen)

args:
  - --show-error-codes
  - --strict
  - --warn-unused-ignores
  - --warn-redundant-cast
  - --warn-unreachable
  
 Numpy mypy plugin enabled (but does influence behavior here)
```toml
[tool.mypy]
mypy_path = ""src""
plugins = ""numpy.typing.mypy_plugin""
```

### Context for the issue:

We use strict typing with mypy on all our code and this necessitates lots of ` # type: ignore[no-untyped-call]` exceptions.",2024-03-28 14:50:14,,BUG: `np.polynomial.Polynomial.fit` lacks type annotations,['00 - Bug']
26145,open,mhvk,"### Describe the issue:

Currently, `np.abs` for the most negative integer of a given types cannot return the absolute value in the same type, so instead wraps and gives a negative value. This is surprising, and could be avoided by promoting to the unsigned type (e.g.,  `i1`->`u1`). However, this may have its own issues if further calculations are done, so really not sure what is the best option. (Hence, not sure whether this is a bug or a question...)

Note that passing in `dtype='u1'` works, though one then has to also give `casting='unsafe'` - even though in this case it is of course not unsafe at all...



### Reproduce the code example:

```python
import numpy as np
np.abs(np.array([-128, -127], 'i1'))
# array([-128,  127], dtype=int8)
np.abs(np.array([-128, -127], 'i1'), dtype='u1', casting='unsafe')
# array([128, 129], dtype=uint8)
```

EDIT: I didn't even notice that using `u1` actually produces nonsense for `-127`. From gh-5657, the ""correct"" way to avoid the overflow (or, rather, correct for the overflow) is,
```
np.abs(np.array([-128, -127], 'i1')).view('u1')
# array([128, 127], dtype=uint8)
```

### Error message:

_No response_

### Python and NumPy Versions:

2.1.0.dev0+git20240326.9992c3a
3.11.2 (main, Mar 13 2023, 12:18:29) [GCC 12.2.0]


### Runtime Environment:

N/A

### Context for the issue:

Found indirectly in #26143",2024-03-27 14:51:54,,BUG/question: Should np.abs always return a positive number?,"['00 - Bug', 'component: numpy._core', '33 - Question']"
26141,open,lschoe,"Numpy 2.0.0b1 comes with [pypy39 wheels](https://pypi.org/project/numpy/2.0.0b1/#files) but support for Python 3.9 has been (recommended to be) dropped since [2023 - Q4](https://scientific-python.org/specs/spec-0000/#2023---quarter-4).

Will you have pypy310 wheels for the upcoming Numpy 2.0.0 release candidates?",2024-03-27 08:19:53,,PyPy 3.10 support for Numpy 2.0.0rc1 or later?,['24 - PyPy']
26115,open,robb-brown,"### Describe the issue:

When slicing an array with an invalid object, like a list, the exception text is:

""IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices""

which is generated on line 606 of _core/src/multiarray/mapping.c.

This was appropriate when lists of indices were accepted or detected as deprecated, but is misleading now. Would it be better to report something like the following:

IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`), **tuples of these objects**, and integer or boolean arrays are valid indices

### Reproduce the code example:

```python
import numpy

myarray = numpy.zeros((5,5))

# generates misleading exception
slice = myarray[[slice(None),slice(None)]]

# correct
slice = myarray[(slice(None),slice(None))]
```


### Error message:

```shell
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices
```


### Python and NumPy Versions:

Python version: all

Numpy version: all up to current main branch (commit 20185fd).



### Runtime Environment:

_No response_

### Context for the issue:

Seeing the exception, especially debugging legacy code that uses lists of slices, it's easy to waste time checking the individual indices, rather than the acutal indexing object.",2024-03-22 18:29:06,,BUG: Exception text when slicing with an invalid object does not report tuple as an option,['00 - Bug']
26104,open,ngoldbaum,"As noted by @mhvk [here](https://github.com/numpy/numpy/pull/26102#discussion_r1534636116), the order that promoters get added matters, and it shouldn't.

Taking a look at the promoter resolution algorithm:

https://github.com/numpy/numpy/blob/6bac54efcb5a4b6b099607f6b4372b5f20e3e715/numpy/_core/src/umath/dispatching.c#L244-L471

Indeed, it does exit if it finds two possible ambiguous promoters and doesn't check any subsequent ones for better matches.",2024-03-21 20:53:47,,BUG: order of promoters matters,['unlabeled']
26096,open,Crejan,"### Describe the issue:

High CPU load when using simple numpy matrix operation periodically
In the example i am only calculating one matrix inverse every 20ms but I have 3/8 CPU cores under full load at all times

### Reproduce the code example:

```python
import time
import numpy
import psutil

def test_numpy():
    N = 100
    data = [numpy.random.rand(4, 4) for _ in range(N)]
    cpu = 0
    for m in data:
        numpy.linalg.inv(m)
        time.sleep(0.05)
        cpu += psutil.cpu_percent()
    print(f""\ncpu core usage: {round(cpu / N * psutil.cpu_count())}%"")
```


### Error message:

```shell
No error. But very high CPU load on the overall system even for small calculations
```


### Python and NumPy Versions:

1.24.4
3.11.8

### Runtime Environment:

[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Haswell',
  'filepath': '/home/jan/dev/pyurctrl/build/venv/lib/python3.11/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so',
  'internal_api': 'openblas',
  'num_threads': 8,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',

### Context for the issue:

periodic calculations on robot telemetry are slowing down the overall system performance dramatically. ",2024-03-21 09:20:20,,BUG:High CPU load with periodical matrix calculations,['00 - Bug']
26091,open,duburcqa,"### Describe the issue:

I am getting some undefined symbol error when compiling a shared library (C Python Module) that is linking against yet another shared library (C Python Module), both compiled against Numpy 2.0. Everything was fine before the update of Numpy to 2.0.0b1.

Does anyone have any idea regarding what is causing this issue?

### Reproduce the code example:

I don't have a MRE. I'm running the following [Github Action pipeline](https://github.com/duburcqa/jiminy/blob/dev/.github/workflows/manylinux.yml)

I could write one if really necessary. Basically I compile the library Eigenpy and then I linked my own module on it. It is the latter that contains undefined symbols.

### Error message:

```shell
ImportError: /home/runner/.local/lib/python3.10/site-packages/jiminy_py/core/core.cpython-310-x86_64-linux-gnu.so: undefined symbol: EIGENPY_ARRAY_APIPyArray_RUNTIME_VERSION
```


### Python and NumPy Versions:

Python 3.10, Numpy 2.0.0b1

### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2024-03-20 14:39:47,,BUG: Undefined symbol when compiling shared library for numpy 2.0,"['00 - Bug', '57 - Close?']"
26078,open,h-vetinari,"While [building](https://github.com/conda-forge/numpy-feedstock/pull/312) 2.0.0b1 for conda-forge, the builds for linux-aarch64 all fail with:
```
=========================== short test summary info ============================
FAILED linalg/tests/test_linalg.py::TestCholesky::test_basic_property[False-float32-shape3] - AssertionError: 
FAILED linalg/tests/test_linalg.py::TestCholesky::test_basic_property[False-float32-shape4] - AssertionError: 
FAILED linalg/tests/test_linalg.py::TestCholesky::test_basic_property[True-float32-shape3] - AssertionError: 
FAILED linalg/tests/test_linalg.py::TestCholesky::test_basic_property[True-float32-shape4] - AssertionError: 
4 failed, 45545 passed, 1071 skipped, 82 xfailed, 7 xpassed in 4036.05s (1:07:16)
```

It's _possible_ that this is a problem with the emulation in QEMU, c.f. this scipy [issue](https://github.com/scipy/scipy/issues/19210). But I'd thought I'd flag it at least, as it's not a minor tolerance violation, but rather complete garbage:
```
Mismatched elements: 1617 / 2500 (64.7%)
Max absolute difference among violations: 89.25262
Max relative difference among violations: 2200.9631
```

<details>
<summary>More detailed error logs</summary>

```
=========================== short test summary info ============================
FAILED linalg/tests/test_linalg.py::TestCholesky::test_basic_property[False-float32-shape3] - AssertionError: 
Not equal to tolerance rtol=1e-07, atol=0.00298023
(50, 50) <class 'numpy.float32'>
[[ 44.06959     -1.2917366   -0.61489445 ...  -1.7078063   -0.06026965
    3.5968263 ]
 [ -1.2917366   50.031647    -0.93625164 ...  -9.246351   -12.977635
   -0.22433573]
 [ -0.61489445  -0.93625164  56.479916   ...   3.1758978    6.96665
   -2.8806841 ]
 ...
 [ -1.7078063   -9.246351     3.1758978  ...  53.048283     6.9221125
    6.9285684 ]
 [ -0.06026965 -12.977635     6.96665    ...   6.9221125   49.258278
  -11.654396  ]
 [  3.5968263   -0.22433573  -2.8806841  ...   6.9285684  -11.654396
   47.500088  ]]
[[ 6.638493    0.          0.         ...  0.          0.
   0.        ]
 [-0.1945828   7.070628    0.         ...  0.          0.
   0.        ]
 [-0.0926256  -0.13496326  7.5135293  ...  0.          0.
   0.        ]
 ...
 [-0.25725812 -1.3147925   0.3959019  ...  0.7488741   0.
   0.        ]
 [-0.00907881 -1.8356787   0.8941284  ... -0.11000964  0.70787036
   0.        ]
 [ 0.5418137  -0.01681719 -0.3770223  ... -0.01939702  0.88379747
   1.6186445 ]]
Mismatched elements: 1617 / 2500 (64.7%)
Max absolute difference among violations: 89.25262
Max relative difference among violations: 2200.9631
 ACTUAL: array([[ 4.406959e+01, -1.291737e+00, -6.148944e-01, ..., -9.883090e+00,
        -2.005572e+00,  7.965497e+00],
       [-1.291737e+00, -7.090253e+00,  1.104174e+00, ..., -9.404308e-03,...
 DESIRED: array([[ 44.06959 ,  -1.291737,  -0.614894, ...,  -1.707806,  -0.06027 ,
          3.596826],
       [ -1.291737,  50.031647,  -0.936252, ...,  -9.246351, -12.977635,...
FAILED linalg/tests/test_linalg.py::TestCholesky::test_basic_property[False-float32-shape4] - AssertionError: 
Not equal to tolerance rtol=1e-07, atol=0.000178814
(3, 10, 10) <class 'numpy.float32'>
[[[10.734333   -4.8768277  -3.1554568  -4.5997534   2.1030364
   -1.2435193   3.0306861  -4.149522   -2.9620435   0.03625464]
  [-4.8768277   8.896948    3.812659   -0.03711339 -1.0476841
    4.170913    0.7686802   0.3364729   1.0283238  -3.826606  ]
  [-3.1554568   3.812659    3.9959366   1.1625074   0.8306916
    0.39898828 -0.77481    -1.9865611  -0.5670059  -2.2059839 ]
  [-4.5997534  -0.03711339  1.1625074   6.6847258  -0.24326262
   -0.47384936 -1.5542715   4.114234   -1.7963581   1.5196537 ]
  [ 2.1030364  -1.0476841   0.8306916  -0.24326262  3.89964
   -4.6199813   1.9657652  -1.6450504  -0.18314792  0.349001  ]
  [-1.2435193   4.170913    0.39898828 -0.47384936 -4.6199813
   11.839529   -2.9300923   0.8218101  -1.1914967  -1.5600258 ]
  [ 3.0306861   0.7686802  -0.77481    -1.5542715   1.9657652
   -2.9300923   7.020295    1.0565685   1.5246283  -3.3805852 ]
  [-4.149522    0.3364729  -1.9865611   4.114234   -1.6450504
    0.8218101   1.0565685  10.698549    4.4092803   0.4416819 ]
  [-2.9620435   1.0283238  -0.5670059  -1.7963581  -0.18314792
   -1.1914967   1.5246283   4.4092803   8.2551985  -1.8104365 ]
  [ 0.03625464 -3.826606   -2.2059839   1.5196537   0.349001
   -1.5600258  -3.3805852   0.4416819  -1.8104365   6.6920257 ]]

 [[ 5.9013734  -0.11575395 -1.8299209  -0.09370685  0.29582188
    0.94792557 -1.7730689   3.0220997  -2.5740616   2.1073136 ]
  [-0.11575395 10.544705    0.79119295 -5.9150043  -1.6428163
   -1.3827156   0.25873902 -1.2621315  -0.7767093   4.0993524 ]
  [-1.8299209   0.79119295 16.183662    6.329022   -1.1351309
   -2.710929    6.666974    0.74811184  2.4979637   1.5011857 ]
  [-0.09370685 -5.9150043   6.329022   13.666377   -2.3753624
    0.70267767  4.4945393   1.8476337  -2.3819492   1.1581259 ]
  [ 0.29582188 -1.6428163  -1.1351309  -2.3753624   6.571176
   -4.508656   -2.573465   -0.43898493  5.2973313  -1.8500149 ]
  [ 0.94792557 -1.3827156  -2.710929    0.70267767 -4.508656
    6.8080935   0.086738    2.382368   -5.498594   -0.53745675]
  [-1.7730689   0.25873902  6.666974    4.4945393  -2.573465
    0.086738    6.919965   -0.40823123  1.9532615  -0.06429972]
  [ 3.0220997  -1.2621315   0.74811184  1.8476337  -0.43898493
    2.382368   -0.40823123  5.7033362  -2.0585406   0.21069823]
  [-2.5740616  -0.7767093   2.4979637  -2.3819492   5.2973313
   -5.498594    1.9532615  -2.0585406   9.462108   -3.9543934 ]
  [ 2.1073136   4.0993524   1.5011857   1.1581259  -1.8500149
   -0.53745675 -0.06429972  0.21069823 -3.9543934   7.435262  ]]

 [[ 4.6643376   2.6854048   4.0593524   0.1535468  -1.5118827
    3.397957   -2.9920995   0.80777925 -1.7318133   0.82475716]
  [ 2.6854048   7.829547   -0.5087376   4.3746657  -1.4713306
   -4.861691   -6.0652037   0.99772763 -4.4356337   5.25942   ]
  [ 4.0593524  -0.5087376  13.68554    -5.680109   -4.2268295
    8.056314   -6.3996224  -0.8219479  -1.7537458   1.7455587 ]
  [ 0.1535468   4.3746657  -5.680109   15.329586   -1.0279889
   -9.684909   -4.311326   -0.32577878 -8.493642    7.0410876 ]
  [-1.5118827  -1.4713306  -4.2268295  -1.0279889   6.4806786
   -1.5806129   3.722421    1.541739   -0.32345185 -3.0850477 ]
  [ 3.397957   -4.861691    8.056314   -9.684909   -1.5806129
   15.264503    3.3202677  -0.3396074   4.926388   -7.346789  ]
  [-2.9920995  -6.0652037  -6.3996224  -4.311326    3.722421
    3.3202677  11.730729    1.6513809   7.1151867  -8.045567  ]
  [ 0.80777925  0.99772763 -0.8219479  -0.32577878  1.541739
   -0.3396074   1.6513809   4.239041   -0.10079275 -0.07716311]
  [-1.7318133  -4.4356337  -1.7537458  -8.493642   -0.32345185
    4.926388    7.1151867  -0.10079275 12.2900095  -7.07966   ]
  [ 0.82475716  5.25942     1.7455587   7.0410876  -3.0850477
   -7.346789   -8.045567   -0.07716311 -7.07966     9.485415  ]]]
[[[ 3.2763293   0.          0.          0.          0.
    0.          0.          0.          0.          0.        ]
  [-1.4885036   2.584822    0.          0.          0.
    0.          0.          0.          0.          0.        ]
  [-0.96310735  0.9204      1.4903773   0.          0.
    0.          0.          0.          0.          0.        ]
  [-1.4039351  -0.8228325   0.38091183  1.9726999   0.
    0.          0.          0.          0.          0.        ]
  [ 0.641888   -0.03568195  0.9942048   0.12664968  1.5753931
    0.          0.          0.          0.          0.        ]
  [-0.3795465   1.3950504  -0.83908963  0.23359051 -2.2355907
    1.9982008   0.          0.          0.          0.        ]
  [ 0.9250249   0.83006996 -0.43472758  0.3006057   1.1398793
   -0.8125721   1.7990714   0.          0.          0.        ]
  [-1.2665155  -0.599167   -1.7813463   1.2782736   0.4796653
    0.22821037  0.67006654  1.7880605   0.          0.        ]
  [-0.90407383 -0.12279119 -0.8888414  -1.4336116   0.9255097
    0.14752597  0.87395006  1.3292007   1.0810157   0.        ]
  [ 0.01106563 -1.4740415  -0.5626888   0.27203056  0.5168721
    0.5606847  -1.4603251  -0.657102   -0.46530387  0.87528366]]

 [[ 2.4292743   0.          0.          0.          0.
    0.          0.          0.          0.          0.        ]
  [-0.0476496   3.2469115   0.          0.          0.
    0.          0.          0.          0.          0.        ]
  [-0.75327885  0.23262091  3.9448855   0.          0.
    0.          0.          0.          0.          0.        ]
  [-0.03857401 -1.8222985   1.7044525   2.7274456   0.
    0.          0.          0.          0.          0.        ]
  [ 0.12177376 -0.50417566 -0.2347646  -1.059335    2.263813
    0.          0.          0.          0.          0.        ]
  [ 0.39020938 -0.42012918 -0.5879161   0.3498526  -2.0034354
    1.4133387   0.          0.          0.          0.        ]
  [-0.729876    0.06897654  1.5465921   0.7171502  -0.58618873
   -0.08172157  1.768033    0.          0.          0.        ]
  [ 1.2440339  -0.37046087  0.4490354   0.16688555 -0.2186787
    1.067539   -0.18652947  1.6021042   0.          0.        ]
  [-1.0596011  -0.25476483  0.44590706 -1.3371879   1.7607771
   -0.6612598   1.3828325   0.3352417   0.8516632   0.        ]
  [ 0.86746633  1.2752694   0.47098336  0.9946084  -0.06559554
   -0.3839536  -0.58293706 -0.30378163 -0.96378136  1.5273129 ]]

 [[ 2.1597078   0.          0.          0.          0.
    0.          0.          0.          0.          0.        ]
  [ 1.2434112   2.5066862   0.          0.          0.
    0.          0.          0.          0.          0.        ]
  [ 1.8795841  -1.1352971   2.977214    0.          0.
    0.          0.          0.          0.          0.        ]
  [ 0.0710961   1.7099324  -1.3006988   3.272437    0.
    0.          0.          0.          0.          0.        ]
  [-0.7000404  -0.23971593 -1.0691853  -0.5986391   2.1051443
    0.          0.          0.          0.          0.        ]
  [ 1.5733411  -2.719926    0.6755185  -1.3039918  -0.5650853
    1.707355    0.          0.          0.          0.        ]
  [-1.3854187  -1.7323902  -1.9354969  -1.1514527  -0.20018531
    0.2816568   1.2723097   0.          0.          0.        ]
  [ 0.37402248  0.21249725 -0.43117726 -0.39009395  0.5510192
   -0.1500179   1.1054928   1.4722989   0.          0.        ]
  [-0.8018739  -1.3717611  -0.60590625 -2.102138   -1.4820251
   -0.41726547 -0.11362489  0.19627525  1.5991338   0.        ]
  [ 0.38188368  1.9087278   1.0730667   1.572491   -0.12896962
   -0.8804634  -0.07867548  0.32361656 -0.51923645  0.9485597 ]]]
Mismatched elements: 253 / 300 (84.3%)
Max absolute difference among violations: 20.29966
Max relative difference among violations: 237.33156
 ACTUAL: array([[[10.734334, -4.876828, -3.155457, -4.599753,  2.103036,
         -1.243519,  3.030686, -4.149522,  0.      ,  0.      ],
        [-4.876828,  2.244246,  1.433589,  8.771067,  1.423618,...
 DESIRED: array([[[10.734333, -4.876828, -3.155457, -4.599753,  2.103036,
         -1.243519,  3.030686, -4.149522, -2.962044,  0.036255],
        [-4.876828,  8.896948,  3.812659, -0.037113, -1.047684,...
FAILED linalg/tests/test_linalg.py::TestCholesky::test_basic_property[True-float32-shape3] - AssertionError: 
Not equal to tolerance rtol=1e-07, atol=0.00298023
(50, 50) <class 'numpy.float32'>
[[ 44.06959     -1.2917366   -0.61489445 ...  -1.7078063   -0.06026965
    3.5968263 ]
 [ -1.2917366   50.031647    -0.93625164 ...  -9.246351   -12.977635
   -0.22433573]
 [ -0.61489445  -0.93625164  56.479916   ...   3.1758978    6.96665
   -2.8806841 ]
 ...
 [ -1.7078063   -9.246351     3.1758978  ...  53.048283     6.9221125
    6.9285684 ]
 [ -0.06026965 -12.977635     6.96665    ...   6.9221125   49.258278
  -11.654396  ]
 [  3.5968263   -0.22433573  -2.8806841  ...   6.9285684  -11.654396
   47.500088  ]]
[[ 6.638493   -0.1945828  -0.0926256  ... -0.25725812 -0.00907881
   0.5418137 ]
 [ 0.          7.070628   -0.13496326 ... -1.3147925  -1.8356787
  -0.01681719]
 [ 0.          0.          7.5135293  ...  0.3959019   0.8941284
  -0.3770223 ]
 ...
 [ 0.          0.          0.         ...  0.7488741  -0.11000964
  -0.01939702]
 [ 0.          0.          0.         ...  0.          0.70787036
   0.88379747]
 [ 0.          0.          0.         ...  0.          0.
   1.6186445 ]]
Mismatched elements: 1617 / 2500 (64.7%)
Max absolute difference among violations: 89.25262
Max relative difference among violations: 2200.9631
 ACTUAL: array([[ 4.406959e+01, -1.291737e+00, -6.148944e-01, ..., -9.883090e+00,
        -2.005572e+00,  7.965497e+00],
       [-1.291737e+00, -7.090253e+00,  1.104174e+00, ..., -9.404308e-03,...
 DESIRED: array([[ 44.06959 ,  -1.291737,  -0.614894, ...,  -1.707806,  -0.06027 ,
          3.596826],
       [ -1.291737,  50.031647,  -0.936252, ...,  -9.246351, -12.977635,...
FAILED linalg/tests/test_linalg.py::TestCholesky::test_basic_property[True-float32-shape4] - AssertionError: 
Not equal to tolerance rtol=1e-07, atol=0.000178814
(3, 10, 10) <class 'numpy.float32'>
[[[10.734333   -4.8768277  -3.1554568  -4.5997534   2.1030364
   -1.2435193   3.0306861  -4.149522   -2.9620435   0.03625464]
  [-4.8768277   8.896948    3.812659   -0.03711339 -1.0476841
    4.170913    0.7686802   0.3364729   1.0283238  -3.826606  ]
  [-3.1554568   3.812659    3.9959366   1.1625074   0.8306916
    0.39898828 -0.77481    -1.9865611  -0.5670059  -2.2059839 ]
  [-4.5997534  -0.03711339  1.1625074   6.6847258  -0.24326262
   -0.47384936 -1.5542715   4.114234   -1.7963581   1.5196537 ]
  [ 2.1030364  -1.0476841   0.8306916  -0.24326262  3.89964
   -4.6199813   1.9657652  -1.6450504  -0.18314792  0.349001  ]
  [-1.2435193   4.170913    0.39898828 -0.47384936 -4.6199813
   11.839529   -2.9300923   0.8218101  -1.1914967  -1.5600258 ]
  [ 3.0306861   0.7686802  -0.77481    -1.5542715   1.9657652
   -2.9300923   7.020295    1.0565685   1.5246283  -3.3805852 ]
  [-4.149522    0.3364729  -1.9865611   4.114234   -1.6450504
    0.8218101   1.0565685  10.698549    4.4092803   0.4416819 ]
  [-2.9620435   1.0283238  -0.5670059  -1.7963581  -0.18314792
   -1.1914967   1.5246283   4.4092803   8.2551985  -1.8104365 ]
  [ 0.03625464 -3.826606   -2.2059839   1.5196537   0.349001
   -1.5600258  -3.3805852   0.4416819  -1.8104365   6.6920257 ]]

 [[ 5.9013734  -0.11575395 -1.8299209  -0.09370685  0.29582188
    0.94792557 -1.7730689   3.0220997  -2.5740616   2.1073136 ]
  [-0.11575395 10.544705    0.79119295 -5.9150043  -1.6428163
   -1.3827156   0.25873902 -1.2621315  -0.7767093   4.0993524 ]
  [-1.8299209   0.79119295 16.183662    6.329022   -1.1351309
   -2.710929    6.666974    0.74811184  2.4979637   1.5011857 ]
  [-0.09370685 -5.9150043   6.329022   13.666377   -2.3753624
    0.70267767  4.4945393   1.8476337  -2.3819492   1.1581259 ]
  [ 0.29582188 -1.6428163  -1.1351309  -2.3753624   6.571176
   -4.508656   -2.573465   -0.43898493  5.2973313  -1.8500149 ]
  [ 0.94792557 -1.3827156  -2.710929    0.70267767 -4.508656
    6.8080935   0.086738    2.382368   -5.498594   -0.53745675]
  [-1.7730689   0.25873902  6.666974    4.4945393  -2.573465
    0.086738    6.919965   -0.40823123  1.9532615  -0.06429972]
  [ 3.0220997  -1.2621315   0.74811184  1.8476337  -0.43898493
    2.382368   -0.40823123  5.7033362  -2.0585406   0.21069823]
  [-2.5740616  -0.7767093   2.4979637  -2.3819492   5.2973313
   -5.498594    1.9532615  -2.0585406   9.462108   -3.9543934 ]
  [ 2.1073136   4.0993524   1.5011857   1.1581259  -1.8500149
   -0.53745675 -0.06429972  0.21069823 -3.9543934   7.435262  ]]

 [[ 4.6643376   2.6854048   4.0593524   0.1535468  -1.5118827
    3.397957   -2.9920995   0.80777925 -1.7318133   0.82475716]
  [ 2.6854048   7.829547   -0.5087376   4.3746657  -1.4713306
   -4.861691   -6.0652037   0.99772763 -4.4356337   5.25942   ]
  [ 4.0593524  -0.5087376  13.68554    -5.680109   -4.2268295
    8.056314   -6.3996224  -0.8219479  -1.7537458   1.7455587 ]
  [ 0.1535468   4.3746657  -5.680109   15.329586   -1.0279889
   -9.684909   -4.311326   -0.32577878 -8.493642    7.0410876 ]
  [-1.5118827  -1.4713306  -4.2268295  -1.0279889   6.4806786
   -1.5806129   3.722421    1.541739   -0.32345185 -3.0850477 ]
  [ 3.397957   -4.861691    8.056314   -9.684909   -1.5806129
   15.264503    3.3202677  -0.3396074   4.926388   -7.346789  ]
  [-2.9920995  -6.0652037  -6.3996224  -4.311326    3.722421
    3.3202677  11.730729    1.6513809   7.1151867  -8.045567  ]
  [ 0.80777925  0.99772763 -0.8219479  -0.32577878  1.541739
   -0.3396074   1.6513809   4.239041   -0.10079275 -0.07716311]
  [-1.7318133  -4.4356337  -1.7537458  -8.493642   -0.32345185
    4.926388    7.1151867  -0.10079275 12.2900095  -7.07966   ]
  [ 0.82475716  5.25942     1.7455587   7.0410876  -3.0850477
   -7.346789   -8.045567   -0.07716311 -7.07966     9.485415  ]]]
[[[ 3.2763293  -1.4885036  -0.96310735 -1.4039351   0.641888
   -0.3795465   0.9250249  -1.2665155  -0.90407383  0.01106563]
  [ 0.          2.584822    0.9204     -0.8228325  -0.03568195
    1.3950504   0.83006996 -0.599167   -0.12279119 -1.4740415 ]
  [ 0.          0.          1.4903773   0.38091183  0.9942048
   -0.83908963 -0.43472758 -1.7813463  -0.8888414  -0.5626888 ]
  [ 0.          0.          0.          1.9726999   0.12664968
    0.23359051  0.3006057   1.2782736  -1.4336116   0.27203056]
  [ 0.          0.          0.          0.          1.5753931
   -2.2355907   1.1398793   0.4796653   0.9255097   0.5168721 ]
  [ 0.          0.          0.          0.          0.
    1.9982008  -0.8125721   0.22821037  0.14752597  0.5606847 ]
  [ 0.          0.          0.          0.          0.
    0.          1.7990714   0.67006654  0.87395006 -1.4603251 ]
  [ 0.          0.          0.          0.          0.
    0.          0.          1.7880605   1.3292007  -0.657102  ]
  [ 0.          0.          0.          0.          0.
    0.          0.          0.          1.0810157  -0.46530387]
  [ 0.          0.          0.          0.          0.
    0.          0.          0.          0.          0.87528366]]

 [[ 2.4292743  -0.0476496  -0.75327885 -0.03857401  0.12177376
    0.39020938 -0.729876    1.2440339  -1.0596011   0.86746633]
  [ 0.          3.2469115   0.23262091 -1.8222985  -0.50417566
   -0.42012918  0.06897654 -0.37046087 -0.25476483  1.2752694 ]
  [ 0.          0.          3.9448855   1.7044525  -0.2347646
   -0.5879161   1.5465921   0.4490354   0.44590706  0.47098336]
  [ 0.          0.          0.          2.7274456  -1.059335
    0.3498526   0.7171502   0.16688555 -1.3371879   0.9946084 ]
  [ 0.          0.          0.          0.          2.263813
   -2.0034354  -0.58618873 -0.2186787   1.7607771  -0.06559554]
  [ 0.          0.          0.          0.          0.
    1.4133387  -0.08172157  1.067539   -0.6612598  -0.3839536 ]
  [ 0.          0.          0.          0.          0.
    0.          1.768033   -0.18652947  1.3828325  -0.58293706]
  [ 0.          0.          0.          0.          0.
    0.          0.          1.6021042   0.3352417  -0.30378163]
  [ 0.          0.          0.          0.          0.
    0.          0.          0.          0.8516632  -0.96378136]
  [ 0.          0.          0.          0.          0.
    0.          0.          0.          0.          1.5273129 ]]

 [[ 2.1597078   1.2434112   1.8795841   0.0710961  -0.7000404
    1.5733411  -1.3854187   0.37402248 -0.8018739   0.38188368]
  [ 0.          2.5066862  -1.1352971   1.7099324  -0.23971593
   -2.719926   -1.7323902   0.21249725 -1.3717611   1.9087278 ]
  [ 0.          0.          2.977214   -1.3006988  -1.0691853
    0.6755185  -1.9354969  -0.43117726 -0.60590625  1.0730667 ]
  [ 0.          0.          0.          3.272437   -0.5986391
   -1.3039918  -1.1514527  -0.39009395 -2.102138    1.572491  ]
  [ 0.          0.          0.          0.          2.1051443
   -0.5650853  -0.20018531  0.5510192  -1.4820251  -0.12896962]
  [ 0.          0.          0.          0.          0.
    1.707355    0.2816568  -0.1500179  -0.41726547 -0.8804634 ]
  [ 0.          0.          0.          0.          0.
    0.          1.2723097   1.1054928  -0.11362489 -0.07867548]
  [ 0.          0.          0.          0.          0.
    0.          0.          1.4722989   0.19627525  0.32361656]
  [ 0.          0.          0.          0.          0.
    0.          0.          0.          1.5991338  -0.51923645]
  [ 0.          0.          0.          0.          0.
    0.          0.          0.          0.          0.9485597 ]]]
Mismatched elements: 253 / 300 (84.3%)
Max absolute difference among violations: 20.29966
Max relative difference among violations: 237.33156
 ACTUAL: array([[[10.734334, -4.876828, -3.155457, -4.599753,  2.103036,
         -1.243519,  3.030686, -4.149522,  0.      ,  0.      ],
        [-4.876828,  2.244246,  1.433589,  8.771067,  1.423618,...
 DESIRED: array([[[10.734333, -4.876828, -3.155457, -4.599753,  2.103036,
         -1.243519,  3.030686, -4.149522, -2.962044,  0.036255],
        [-4.876828,  8.896948,  3.812659, -0.037113, -1.047684,...
4 failed, 45545 passed, 1071 skipped, 82 xfailed, 7 xpassed in 4036.05s (1:07:16)
```

</details>",2024-03-19 02:30:30,,TST: failures in `TestCholesky::test_basic_property` on aarch64,['component: numpy.linalg']
26059,open,mhvk,"### Proposed new feature or change:

(Mostly for @ngoldbaum probably)

The new `StrngDTyype` is quite beautiful but also fairly inefficient for arrays that contain mostly smaller strings, since it uses at least the 16 bytes per array element that store its information. In principle, an 8-byte version of `StringDType` exists, as that is used on 32 bit systems, but it cannot directly be used on 64 bit systems, since it cannot store the pointer to a long string stored outside the arena. However, this could be remedied if, instead, this pointer itself is stored inside the arena (and, thus, the actual array entry continues to contain the offset in the arena).

Overall, one could envision a string dtype with a slightly changed logic also for short string replacements, which would ensure that offsets to the arena are always kept:
1. Initialization is exactly the same as is: short strings are stored in the array, everything else in the arena. The minimum length of an arena entry is thus 8 bytes.
2. If a short string is replaced by a longer one (at least 8 bytes), a new entry in the arena is added.
3. If an arena string is replaced by a shorter one, it is kept in the arena, even if short enough to be stored in the array, so that the arena offset is not lost (and repeated string replacements can thus not lead the arena to continue to grow; it can never have more entries than the size of the array).
4. If an arena string is replaced by string too long to be stored in the existing entry, it is stored on the heap, with the corresponding pointer stored in the arena entry (which is at least 8 bytes in size, so this fits). The offset into the arena is thus now used to find the pointer to the heap.
5. If an entry on the heap is replaced by one that fits in the corresponding arena entry, it will be stored there.

Compared to the existing `StringDType`, this shorter type would be equally efficient for first-time entries, but slightly less efficient for short or long string replacements, as those both require an extra lookup. It also has the limitation that the arena cannot be larger than 4 GB. It would generally use quite a bit less memory if the array is mostly short strings except if they are mostly 8-15 bytes long.

Would this be worth implementing, perhaps with a different initialization option for `StringDType`?",2024-03-17 13:23:47,,ENH: Half-sized StringDType?,"['01 - Enhancement', '15 - Discussion', 'component: numpy.strings']"
26037,open,ischurov,"### Describe the issue:

I am trying to create a custom array-like object that stores zero-length one-dimensional array using `__array_interface__`. As the array is zero-length and therefore does not contain any data, I provide a null pointer as the `data` field of `__array_interface__` (i.e. `""data"": (0, False)`). Then my object is seemingly treated as a scalar by `np.array`, despite the fact that `shape` is `(0,)`. If I provide anything different from 0 as the data, everything works.

### Reproduce the code example:

```python
import numpy as np

class TestArray:
    def __init__(self):
        self.__array_interface__ = {
            ""data"": (0, False),
            ""strides"": None,
            ""descr"": [("""", ""<f8"")],
            ""typestr"": ""<f8"",
            ""shape"": (0,),
            ""version"": 3,
        }

np.array(TestArray())
```


### Error message:

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[24], line 1
----> 1 np.array(TestArray())

TypeError: float() argument must be a string or a real number, not 'TestArray'
```


### Python and NumPy Versions:

1.26.1
3.11.6 (main, Oct  2 2023, 13:45:54) [GCC 12.3.0]

### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2024-03-15 16:37:57,,BUG: Cannot create zero-length array-like object via __array_interface__ providing null pointer as data,['00 - Bug']
26032,open,anntzer,"### Proposed new feature or change:

I understand that row_stack is being deprecated in numpy 2.0 because it is directly an alias for vstack, but sometimes the spelling row_stack is much more consistent when used next to column_stack.
For example I have the following snippet, which computes, for a binary mask, for each pixel, the number of neighboring pixels that are set:
```python
neighbor_count = (
    np.row_stack([mask[1:, :], np.zeros(mask.shape[1])])
    + np.row_stack([np.zeros(mask.shape[1]), mask[:-1, :]])
    + np.column_stack([mask[:, 1:], np.zeros(mask.shape[0])])
    + np.column_stack([np.zeros(mask.shape[0]), mask[:, :-1]])
)
```
Obviously there are other ways to compute this, e.g. by padding first, or with a scipy 2D convolution, or using vstack and hstack and explicitly passing (1, N) column arrays to hstack, but at least in the way it's written above, it would seem less readable to use vstack for the first two terms and column_stack for the last two.

I would therefore like to suggest reverting this deprecation, even if this function is perhaps marked as discouraged or explicitly points to vstack as an alternative.  (As an aside, this deprecation does not seem to be caught by ruff's NPY201 rule).",2024-03-15 09:59:53,,ENH: Should row_stack really be deprecated?,['unlabeled']
26028,open,miladm,"### Proposed new feature or change:

### 🚀 The feature, motivation and pitch

Please provide `total_repeat_length` field to the pytorch `repeat` / `repeat_interleave` API. This feature is available on [`jnp.repeat`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.repeat.html) and has valuable use cases for sparse LLMs applications.",2024-03-14 21:05:10,,Please provide `total_repeat_length` field to `numpy.repeat` / `repeat_interleave` API,['unlabeled']
26026,open,SimpleConjugate,"### Proposed new feature or change:

This is a feature request to support specifying that the dtype of a numpy array must be hashable for type checking purposes.

The example below gives two functions that will hash a numpy array. The first function, `hash_with_poor_specification` is a syntactically correct implementation that does not raise type checking errors, but will fail runtime due to `NonHashableData` not being hashable. The second function, `hash_with_invalid_specialization`, uses `Hashable` as the `dtype` which is not valid specialization of `NDArray`. 

The desired outcome of this feature request is that the second function become valid syntax which would lead to errors catchable by type checking.

```python
from dataclasses import dataclass, field
from typing import Hashable, TypeVar

import numpy.typing as npt
import numpy as np

_DType = TypeVar(""_DType"", bound=np.generic)

#  Function used by all functions in examples
def _prepare_for_hashing(array: npt.NDArray[_DType]) -> tuple[_DType, ...]:
    """"""
    Prepare array to be hashed.

    Notes
    -----
    Flattens array to remove nesting of unhashable lists, then
    convert flattened array into tuple of objects of type `_DType`.
    This function will not consider the shape of the data when creating
    the hash. For demonstration purposes this is sufficient.
    """"""
    reshaped: npt.NDArray[_DType] = array.reshape(-1)
    as_list: list[_DType] = reshaped.tolist()
    data = tuple(as_list)
    return data

# A function with syntactically valid type hints
# but does not reflect a key property of the data type (can be hashed)
def hash_with_poor_specification(array: npt.NDArray[_DType]) -> int:
    """"""
    Create hash of the array.

    The data type of the array is not restricted to hashable
    data, so type checking does not catch preventable errors
    before runtime.
    """"""
    data = _prepare_for_hashing(array)
    return hash(data)

# A function that captures the spirit of the feature request
# but raises flags and errors w.r.t to type checking.
#
# error: Type argument ""Hashable"" of ""NDArray"" must be a subtype of ""generic""  [type-var]
def hash_with_invalid_specialization(array: npt.NDArray[Hashable]) -> int:
    """"""
    Create hash of array.

    This function has the desirable signature but is flagged as
    incorrect specialization by type checkers as `Hashable` is not
    subclass of `numpy.generic`.
    """"""
    # error: Value of type variable ""_DType"" of ""_prepare_for_hashing"" cannot be ""Hashable""  [type-var]
    data = _prepare_for_hashing(array)
    return hash(data)



@dataclass
class NonHashableData(object):
    """"""
    Object that is not hashable.
    """"""

    data: list[int] = field(default_factory=lambda: list(range(0)))


d1 = NonHashableData()
d2 = NonHashableData()
object_array = np.array([d1, d2])
# Type checking is not helpful here as
# it is not aware `NonHashableData` should be hashable
hash_with_poor_specification(object_array)
# Type checking is useless because `Hashable` already wasn't
# a valid specialization of `NDArray`
hash_with_invalid_specialization(object_array)


# Type checking is not helpful here as
# it is not aware `NonHashableData` should be hashable
hash_with_poor_specification(hashable_object_array)
# Type checking is useless because `Hashable` already wasn't
# a valid specialization of `NDArray`
hash_with_invalid_specialization(hashable_object_array)
hash_with_custom_type(hashable_object_array)

```",2024-03-14 19:03:52,,ENH: Add support for typing.Hashable as valid specialization for numpy.ndarray and numpy.typing.NDArray,['unlabeled']
26019,open,adrinjalali,"Do C-API docs need an update? For instance, `dev/internals.code-explanations.rst`
mentions `PyArray_Descr->f`, but looking at the implementation in `ndarraytypes.h`
I don't see an `f` attribute.

Also, the first ""Iteration example"" under `c-api/iterator.rst` uses code which
to me it seems not valid anymore:

```c
    PyArray_NonzeroFunc* nonzero = PyArray_DESCR(self)->f->nonzero;
```

and if I'm not mistaken, the modern way to do this is:

```c
    PyArray_NonzeroFunc* nonzero = PyDataType_GetArrFuncs(PyArray_DESCR(self))->nonzero;
```

But I also didn't manage to make g++ happy even after a `#include
""numpy/npy_2_compat.h""`, so I'm probably missing something.


I'm not sure where else these instances occur, still trying to wrap my head around
the C-API.

Also, it would be really nice if we could have info on which includes are necessary
for which part of the docs, and maybe some basic minimal examples which can be
compiled on their own. Another nice thing to have would be to have examples to deal
with the actual values stored in the arrays instead of always using functions
attached to `PyArray_Descr`.

This is all in the context of https://github.com/numpy/numpy/pull/26018",2024-03-14 15:25:35,,DOC update C-API docs,['unlabeled']
26004,open,bernt-matthias,"### Issue with current documentation:

See the docs here: https://numpy.org/devdocs/reference/generated/numpy.lib.npyio.NpzFile.html#numpy.lib.npyio.NpzFile

But int he code, Npy files are silently loaded https://github.com/numpy/numpy/blob/82eca7bae8249f2576d28dae0800147546d0604d/numpy/lib/_npyio_impl.py#L199C1-L200C37


### Idea or request for content:

My suggestion would be to ignore them or even better raise an exception.",2024-03-12 17:43:08,,DOC:  `NpzFile` is documented to ignore files without a `.npy` extension - but it doesn't,['04 - Documentation']
26002,open,pryvkin10x,"### Describe the issue:

`numpy.power` result is nondeterministic when the exponent is a view with certain strides and AVX512 is available. Problem goes away with `NPY_DISABLE_CPU_FEATURES=""AVX512F""`

### Reproduce the code example:

```python
import numpy as np

for r in range(1000):
    for c in range(1, 10):
        src = np.linspace(0, 1, num=r * c).reshape(r, c)
        for j in range(0, c):
            x = src[:, j]
            if not np.array_equal(np.power(1.2, x), np.power(1.2, x)):
                print(f""diff! {(r,c,j)=}; {x.__array_interface__=}"")
                break
```


### Error message:

```shell
Snippet of results:
diff! (r,c,j)=(3, 3, 1); x.__array_interface__={'data': (94083650133912, False), 'strides': (24,), 'descr': [('', '<f8')], 'typestr': '<f8', 'shape': (3,), 'version': 3}
diff! (r,c,j)=(134, 5, 4); x.__array_interface__={'data': (94083654872944, False), 'strides': (40,), 'descr': [('', '<f8')], 'typestr': '<f8', 'shape': (134,), 'version': 3}
diff! (r,c,j)=(134, 8, 2); x.__array_interface__={'data': (94083654872928, False), 'strides': (64,), 'descr': [('', '<f8')], 'typestr': '<f8', 'shape': (134,), 'version': 3}
```


### Python and NumPy Versions:

1.26.4
3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0]

### Runtime Environment:

[{'numpy_version': '1.26.4',
  'python': '3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) '
            '[GCC 12.3.0]',
  'uname': uname_result(system='Linux', node='<redacted>, release='6.1.0-18-cloud-amd64', version='#1 SMP PREEMPT_DYNAMIC Debian 6.1.76-1 (2024-02-01)', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2',
                                'AVX512F',
                                'AVX512CD',
                                'AVX512_SKX',
                                'AVX512_CLX',
                                'AVX512_CNL',
                                'AVX512_ICL'],
                      'not_found': ['AVX512_KNL', 'AVX512_KNM', 'AVX512_SPR']}},
 {'filepath': '<redacted>/conda_package_mkl/lib/libmkl_rt.so.2',
  'internal_api': 'mkl',
  'num_threads': 48,
  'prefix': 'libmkl_rt',
  'threading_layer': 'intel',
  'user_api': 'blas',
  'version': '2023.2-Product'},
 {'filepath': '<redacted>/conda_package_llvm_openmp/lib/libomp.so',
  'internal_api': 'openmp',
  'num_threads': 96,
  'prefix': 'libomp',
  'user_api': 'openmp',
  'version': None}]

### Context for the issue:

The result is unexpectedly nondeterministic and causes flaky tests.",2024-03-12 17:17:08,,BUG: numpy.power result is nondeterministic when AVX512 enabled,"['00 - Bug', 'component: SIMD']"
26001,open,adamjstewart,"### Proposed new feature or change:

Given a dtype, if I want to know the max values of that dtype, I currently need an if-statement of the form:
```python
if np.isdtype(dtype, 'integral'):
    max = np.iinfo(dtype).max
else:
    max = np.finfo(dtype).max
```
I propose to have a single `np.info` method that dispatches to the submethod based on type:
```python
max = np.info(dtype).max
```",2024-03-12 13:53:06,,ENH: np.info = np.iinfo + np.finfo,['unlabeled']
25998,open,mattip,"Now that regular doc builds are using nit-picky mode, we could get NEPs to also build cleanly. Curently there are [39 warnings](https://app.circleci.com/pipelines/github/numpy/numpy/25555/workflows/8ccf567d-fe17-4b11-9114-cad4faba20b2/jobs/38840), mostly in NEP 13.

Once the warnings are gone, we should add a `-n` option to make sure they don't come back.

For some examples, see [all the great work @F3eQnxN3RriK](https://github.com/numpy/numpy/pulls?q=is%3Apr+author%3AF3eQnxN3RriK) (and others) did to get the regular docs build clean.",2024-03-11 21:55:04,,DOC: add nit-picky mode to NEP building,['component: documentation']
25969,open,steppi,"I'd like to assess interest in adding interactive examples to NumPy's docs. @mattip mentioned in https://github.com/scipy/scipy/issues/19729#issuecomment-1969621588, that this was briefly discussed in one of the NumPy community meetings.

For background info. The [jupyterlite-sphinx](https://jupyterlite-sphinx.readthedocs.io/en/latest/) extension offers the [try_examples](https://jupyterlite-sphinx.readthedocs.io/en/latest/directives/try_examples.html) directive, which adds a button which can be used to swap the rendered examples sections for a docstring in-place with an embedded Jupyterlite notebook. The notebooks run in-browser using a wasm based kernel like [pyodide](https://github.com/jupyterlite/pyodide-kernel) .

 I've deployed SciPy's docs with this extension in action here: https://steppi.github.io/scipy/reference/index.html

Here's a direct link to a page with an example: https://steppi.github.io/scipy/reference/generated/scipy.integrate.quad.html#scipy.integrate.quad.

https://github.com/scipy/scipy/pull/20019 was recently merged, adding jupyterlite-sphinx to SciPy's docs. Examples are currently disabled (it's possible to enable or disable them for deployed docs without rebuilding), but we plan to roll them out within the next two weeks.

I'd be interested in getting feedback on the extension, and if there are any changes or new features you'd like to see before it could be used for NumPy. NumPy being able to ship interactive examples would need to wait for Pyodide support for NumPy 2.0, which I suspect would not be too difficult. My understanding is the primary blocker making updates of SciPy in Pyodide difficult is the dependence on Fortran.",2024-03-08 20:33:39,,DOC: Add interactive examples with jupyterlite-sphinx,"['01 - Enhancement', 'component: documentation']"
25967,open,bzoracler,"### Proposed new feature or change:

The mypy plugin currently imports the same modules as the numpy runtime, the vast majority of which aren't needed for static typing:

```
{'numpy.fft._pocketfft', 'numpy.typing', '_compat_pickle', 'numpy.lib.type_check', 'numpy.core._dtype_ctypes', 'numpy._typing._add_docstring', 'numpy.core._multiarray_tests', 'numpy.polynomial.hermite', 'numpy.core.multiarray', 'base64', 'numpy.random._sfc64', 'numpy.dtypes', 'numpy._typing._char_codes', 'numpy._typing._nbit', 'numpy.core.numerictypes', 'numpy.lib.index_tricks', 'numpy.core._add_newdocs_scalars', 'cython_runtime', 'numpy.random._philox', 'numpy.lib.arrayterator', 'numpy.lib.shape_base', 'numpy.core.einsumfunc', 'numpy._utils', 'numpy.polynomial', 'numpy.lib.mixins', 'numpy.core.numeric', 'numpy.fft', 'numpy.compat.py3k', 'numbers', 'numpy.linalg', 'numpy.core._exceptions', 'numpy.lib._iotools', 'numpy._typing._shape', 'numpy.lib.ufunclike', 'numpy._typing._dtype_like', 'numpy.core._ufunc_config', 'numpy.random._common', 'numpy.fft.helper', 'numpy.random._mt19937', '_cython_3_0_8', 'numpy.core.fromnumeric', 'numpy.lib.histograms', 'numpy.core.getlimits', 'numpy.lib.polynomial', 'numpy.polynomial.polyutils', 'numpy.typing.mypy_plugin', 'ctypes', 'numpy.ma.core', 'numpy.exceptions', 'numpy.polynomial._polybase', 'numpy.core._asarray', 'numpy.core.umath', 'numpy.__config__', 'numpy.lib.function_base', 'numpy.compat', 'numpy.polynomial.chebyshev', 'numpy.lib.arraypad', 'numpy.core._dtype', 'numpy._globals', 'numpy.core.records', 'numpy._typing', 'numpy.lib.format', 'numpy._utils._convertions', 'numpy.core._internal', 'numpy.random.mtrand', 'numpy.random._pcg64', 'numpy.random._pickle', 'numpy.core._multiarray_umath', 'numpy.core.shape_base', 'numpy.lib._datasource', 'numpy.lib.nanfunctions', 'numpy.lib._version', 'numpy.core._add_newdocs', 'numpy.fft._pocketfft_internal', 'numpy.lib.utils', 'numpy.core.defchararray', 'numpy.random', 'contextvars', 'numpy.matrixlib.defmatrix', 'numpy.polynomial.legendre', 'numpy.core', 'numpy.random._bounded_integers', 'numpy.ma.extras', 'numpy.core._methods', '_ctypes', '_contextvars', 'pickle', 'numpy._distributor_init', 'numpy.core._type_aliases', 'numpy._typing._array_like', 'numpy.matrixlib', 'hmac', 'numpy.polynomial.laguerre', 'numpy.core.function_base', 'numpy.polynomial.hermite_e', 'numpy.lib.npyio', 'numpy.core._machar', '_pickle', 'secrets', 'numpy.core.arrayprint', 'ctypes._endian', 'numpy._typing._scalars', 'numpy.lib.arraysetops', 'numpy.core._string_helpers', 'numpy._pytesttester', 'numpy.version', 'numpy', 'numpy.random._generator', 'numpy.lib.scimath', 'numpy.linalg._umath_linalg', 'numpy.lib', 'numpy.core.memmap', 'numpy._utils._inspect', 'numpy._typing._nested_sequence', 'numpy.lib.stride_tricks', 'numpy.lib.twodim_base', 'numpy.polynomial.polynomial', 'numpy.ctypeslib', 'numpy.ma', 'numpy.core.overrides', 'numpy.linalg.linalg', 'numpy.random.bit_generator'}
```

I understand that, currently, the mypy plugin [uses objects available in `numpy.__init__`](https://github.com/numpy/numpy/blob/0c54c6e54f2f3f76fcaac6f9009f8f7ef5c5ade9/numpy/typing/mypy_plugin.py#L56-L93) for determining the precision of dtype aliases. However, the mypy configuration with the path as `plugins = numpy.typing.mypy_plugin` also means that any code in `numpy.__init__` and `numpy.typing.__init__` will be executed regardless of whether `numpy` is actually used by the mypy plugin, due to how Python's import mechanism works; all runtime effects which occur in the `numpy[.typing]` namespaces will propagate to static analysis.

Is it possible to restrict the runtime in the mypy plugin, by
 - Employing a way of determining precision without importing `numpy`?
 - Prevent loading `numpy` and `numpy.typing` when specifying the mypy plugin? (This is probably more of a question to mypy than here).",2024-03-08 17:49:54,,ENH: Reduce the scope of imports when running the mypy plugin,['unlabeled']
25953,open,CeadeS,"### Describe the issue:

I do not know what happens, but I see different behavior when I repeat the same operation. I don't know exactly if this is a bug, but if it is, it could be serious because the behavior is undefined for very basic functionality in my eyes. Please close this if I am just wrong or if my usage is wrong. In this case, an assertion could solve my issue.


### Reproduce the code example:

```python
#https://www.python.org/
#Python 3.10.5 (main, Jul 22 2022, 17:09:35) [GCC 9.4.0] on linux
#Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
import numpy as np
a = np.array([0.,0.,0.])
b = np.divide(a,a.max(), where=~np.all(np.equal(a, 0.)))

print(b)
#[6.9424691e-310 6.9424691e-310 6.9424600e-310]
print(b>0)
#[ True  True  True]
print(b)
#[6.9424691e-310 6.9424691e-310 6.9424600e-310]

a = np.array([0.,0.,0.])
b = np.divide(a,a.max(), where=~np.all(np.equal(a, 0.)))

print(b)
#[0. 0. 0.]
print(b>0)
#[False False False]
print(b)
#[0. 0. 0.]

np.__version__
'1.21.6'

import sys, numpy; print(numpy.__version__); print(sys.version)
1.21.6
3.10.5 (main, Jul 22 2022, 17:09:35) [GCC 9.4.0]
```


### Error message:

```shell
Lines 12 and 22 yield different results for the exact same operation.
```


### Python and NumPy Versions:

1.21.6
3.10.5 (main, Jul 22 2022, 17:09:35) [GCC 9.4.0]

### Runtime Environment:

I used the www.phython.org online python 3.10 interpreter for verification.

### Context for the issue:

I found undefined behavior in a larger project and it took me a week to trace the bug because, in my case, this is a very basic functionality. I faced the wrong results in testing a functionality unrelated to this behavior; I accidentally created that case and had no idea that such behavior could possibly occur.",2024-03-07 08:42:22,,BUG: seeing uninitialized memory in ufuncs,['33 - Question']
25936,open,gcaria,"### Describe the issue:

This came out from @mathause  's investigation in https://github.com/pydata/xarray/issues/8802#issuecomment-1975370245

### Reproduce the code example:

```python
import numpy as np
otype = ""datetime64[ns]""
arr = np.array(['2024-01-01', '2024-01-02', '2024-01-03'], dtype='datetime64[ns]')
np.vectorize(lambda x: x, signature=""(i)->(j)"", otypes=[otype])(arr)
```


### Error message:

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[1], line 4
      2 otype = ""datetime64[ns]""
      3 arr = np.array(['2024-01-01', '2024-01-02', '2024-01-03'], dtype='datetime64[ns]')
----> 4 np.vectorize(lambda x: x, signature=""(i)->(j)"", otypes=[otype])(arr)

File ~/envs/test-xarray/lib/python3.10/site-packages/numpy/lib/function_base.py:2372, in vectorize.__call__(self, *args, **kwargs)
   2369     self._init_stage_2(*args, **kwargs)
   2370     return self
-> 2372 return self._call_as_normal(*args, **kwargs)

File ~/envs/test-xarray/lib/python3.10/site-packages/numpy/lib/function_base.py:2365, in vectorize._call_as_normal(self, *args, **kwargs)
   2362     vargs = [args[_i] for _i in inds]
   2363     vargs.extend([kwargs[_n] for _n in names])
-> 2365 return self._vectorize_call(func=func, args=vargs)

File ~/envs/test-xarray/lib/python3.10/site-packages/numpy/lib/function_base.py:2446, in vectorize._vectorize_call(self, func, args)
   2444 """"""Vectorized call to `func` over positional `args`.""""""
   2445 if self.signature is not None:
-> 2446     res = self._vectorize_call_with_signature(func, args)
   2447 elif not args:
   2448     res = func()

File ~/envs/test-xarray/lib/python3.10/site-packages/numpy/lib/function_base.py:2506, in vectorize._vectorize_call_with_signature(self, func, args)
   2502         outputs = _create_arrays(broadcast_shape, dim_sizes,
   2503                                  output_core_dims, otypes, results)
   2505     for output, result in zip(outputs, results):
-> 2506         output[index] = result
   2508 if outputs is None:
   2509     # did not call the function even once
   2510     if otypes is None:

ValueError: Cannot convert from specific units to generic units in NumPy datetimes or timedeltas
```


### Python and NumPy Versions:
```shell
1.26.4
3.10.1 (main, Dec 15 2021, 17:45:54) [GCC 9.3.0]
```
### Runtime Environment:
```shell
[{'numpy_version': '1.26.4',
  'python': '3.10.1 (main, Dec 15 2021, 17:45:54) [GCC 9.3.0]',
  'uname': uname_result(system='Linux', node='ip-172-34-24-37', release='5.15.0-1051-aws', version='#56~20.04.1-Ubuntu SMP Tue Nov 28 15:43:31 UTC 2023', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2',
                                'AVX512F',
                                'AVX512CD',
                                'AVX512_SKX'],
                      'not_found': ['AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'SkylakeX',
  'filepath': '/home/ubuntu/envs/test-xarray/lib/python3.10/site-packages/numpy.libs/libopenblas64_p-r0-0cf96a72.3.23.dev.so',
  'internal_api': 'openblas',
  'num_threads': 4,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.23.dev'}]
None
```
### Context for the issue:

_No response_",2024-03-05 09:28:50,,`np.vectorize` fails with output dtype `datetime64[ns]`,['00 - Bug']
25910,open,lysnikolaou,"As noted in https://github.com/numpy/numpy/pull/25891#discussion_r1504990588, we can optimize `expandtabs` to avoid unnecessary `memcpy`s.",2024-03-01 12:09:11,,ENH: Optimize np.strings.expandtabs to avoid unnecessary copies,"['01 - Enhancement', 'component: numpy._core', 'sprintable - C']"
25909,open,radomirgr,"### Describe the issue:

When you compare results of operations after copy with order of 'C' and 'F' then results will be different when you have multiple columns

### Reproduce the code example:

```python
import numpy as np

def fn(x):
    return (x).sum(axis=0, keepdims=True)

def fun(data, n = 100_000):
    x = np.tile(data, (n,1))
    rf = fn(x.copy(order='F'))
    rc = fn(x.copy(order='C'))
    print(f""F order: {rf}, C order {rc}"")

data =  np.array([[-0.16385849, -0.06833044],[ 0.14264219, -0.06838101]], dtype=np.float32)
fun(data) # F order: [[ -2121.6326 -13671.142 ]], C order [[ -2117.505 -13671.854]]
fun(data[:, 0:1]) # F order: [[-2121.6326]], C order [[-2121.6326]]
fun(data[:, 1:2]) # F order: [[-13671.142]], C order [[-13671.142]]
```


```python
n = 100_000_000
a = np.array((np.full(n, 1, dtype=np.float32), np.full(n, 0.01, dtype=np.float32), np.full(n, -1, dtype=np.float32))).T
print(a.shape) # (100000000, 3)
print(a.sum(axis=0)) # [ 1.0000000e+08  1.0000628e+06 -1.0000000e+08]
print(a.copy().sum(axis=0)) # [ 16777216.    262144. -16777216.]
print(a.copy(order=""C"").sum(axis=0)) # [ 16777216.    262144. -16777216.]
print(a.copy(order=""F"").sum(axis=0)) # [ 1.0000000e+08  1.0000628e+06 -1.0000000e+08]
```

### Error message:

_No response_

### Python and NumPy Versions:

1.26.4
3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]

### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2024-03-01 11:54:58,,BUG: Sum per axis is wrong after copy of data is C (default),"['00 - Bug', '57 - Close?']"
25872,open,gresavage,"### Proposed new feature or change:

There have been many advancements over the past decade in the algorithms for calculating the Moore-Penrose Generalized Inverse. These advanced algorithms have considerable speed improvements over the SVD method of calculation especially for large matrices, and some of which are remarkably simple.

Three of the algorithms are given below (in decreasing order of execution time):

1. `geninv` - P. Courrieu, “Fast computation of moore-penrose inverse matrices,” Nueural Information Processing-Letters and Review, vol. 8, pp. 25–29, 2005
2. `qrginv` - V. N. Katsikis, D. Pappas, and A. Petralias, “An improved method for the computation of the Moore-Penrose inverse matrix,” Applied Mathematics and Computation,vol.217,no.23, pp. 9828–9834, 2011
3. `imqrginv` - Ataei, Alireza. “Improved Qrginv Algorithm for Computing Moore-Penrose Inverse Matrices.” International Scholarly Research Notices 2014 (March 12, 2014): e641706. https://doi.org/10.1155/2014/641706.

The fastest, `imqrginv`, is also the simplest since it consists of simply calculating the reduced QR-factorization of the input matrix A and returning:

$$A^{\dagger}=R^{T}(RR^{T})^{-1}Q^{T}$$

I have implemented (1) and (3) in python and confirmed the speed and error-magnitude results of the papers. I found a consistent and considerable improvement over `np.linalg.pinv` for large matrices which matched the paper results. For smaller matrices (<100K entries) the improvement is diminished. However I suspect this is due to the speed of the Python language, especially loops, becoming non-negligible compared to NumPy's C implementation.

Hence, the speed of `np.linalg.pinv` could be considerably improved by using `imqrginv` in pure C as the base algorithm instead of SVD.

Below are the python implementations of the two algorithms:

## `geninv`

```python
from cmath import sqrt
import numpy as np

def fast_pinv(a: np.ndarray, rcond: float = 1e-9) -> np.ndarray:
    m, n = a.shape
    if m < n:
        transpose = True
        g = a @ a.transpose(1, 0).conj()
        n = m
    else:
        transpose = False
        g = a.transpose(1, 0).conj() @ a

    diag_g = np.diag(g)
    tol = diag_g[diag_g > 0].min() * rcond
    l = np.zeros_like(g)

    # full rank Cholesky factorization
    r = -1
    for k in range(n):
        r += 1
        l[k:n, [r]] = g[k:n, [k]] - l[k:n, :r] @ l[[k], :r].transpose(1, 0).conj() if r != 0 else g[k:n, [k]]
        if l[k, r] > tol:
            l[k, r] = sqrt(l[k, r])
            if k < n:
                l[k + 1 : n, [r]] = l[k + 1 : n, [r]] / l[[k], [r]]
        else:
            r -= 1
    l = l[:, : r + 1]

    m = np.linalg.inv(l.transpose(1, 0).conj() @ l)
    return (
        a.transpose(1, 0).conj() @ l @ m @ m @ l.transpose(1, 0).conj()
        if transpose
        else l @ m @ m @ l.transpose(1, 0).conj() @ a.transpose(1, 0).conj()
    )
```

## `imqrginv`

```python
import numpy as np

def imqrginv(a: np.ndarray) -> np.ndarray:
    q, r = np.linalg.qr(a, mode=""reduced"")
    return r.transpose(1, 0).conj() @ np.linalg.inv(r @ r.transpose(1, 0).conj()) @ q.transpose(1, 0).conj()
```

> The authors of `imqrginv` don't explicitly handle the complex case where $A\in\mathbb{C}^{n\times m}$ and I haven't proven mathematically whether a) their method holds or whether b) we should be using the conjugate transposes. My intuition is that the affirmative is true for both questions.

## Excerpts from texts

The image excerpts below from the research texts demonstrate the ladder of improvement from `pinv` &rarr; `geninv`  &rarr; `qrginv` &rarr; `imqrginv`

### `geninv` vs `pinv` (SVD) in MATLAB

![image](https://github.com/numpy/numpy/assets/7985947/99119f6e-8327-4448-8ef0-300c629d4ede)

### `ginv` vs `geninv` vs `pinv` (SVD) in MATLAB

`ginv` is the pre-cursor to `qrginv`, which is a further improvement

![image](https://github.com/numpy/numpy/assets/7985947/6fb86e28-ed13-4abe-8b56-b21da051f1e7)

### `qrginv` vs `pinv` (SVD) in MATLAB

![image](https://github.com/numpy/numpy/assets/7985947/f7a6ee21-7471-4377-bb8b-63e35a9b64c1)

### `imqrginv` vs `qrginv` in MATLAB

![image](https://github.com/numpy/numpy/assets/7985947/ab008848-7aff-40a5-8475-0a0a07fc3b6a)

",2024-02-22 17:23:27,,ENH: Faster Generalized Inverse Algorithm,['unlabeled']
25869,open,opoplawski,"### Describe the issue:

On i686, numpy defines:
* `npy_int32` type as `long int`.
*  `npy_uint32` type as `long unsigned int`.
These are incompatible with the stdint definitions of:
* `int32_t` as `int`
* `uint32_t` as `unsigned int`

This is causing h5py to fail to build on Fedora 40 due to the use of `-Werror=incompatible-pointer-types`.  I think it's fairly reasonable to expect that the `npy_*int*` types to be completely interchangeable with the `stdint` types.

### Reproduce the code example:

```python
from numpy cimport int8_t, uint8_t, int16_t, uint16_t, int32_t, uint32_t, int64_t, uint64_t
...
    def read_direct_chunk(self, offsets, PropID dxpl=None, unsigned char[::1] out=None):
...
        cdef uint32_t filters
...
            H5Dread_chunk(dset_id, dxpl_id, offset, &filters, chunk_buffer)
```


### Error message:

```shell
/builddir/build/BUILD/h5py-3.10.0/serial/h5py/defs.c: In function ‘__pyx_f_4h5py_4defs_H5Dread_chunk’:
/builddir/build/BUILD/h5py-3.10.0/serial/h5py/defs.c:14922:85: error: passing argument 4 of ‘H5Dread_chunk’ from incompatible pointer type [-Wincompatible-pointer-types]
14922 |         __pyx_v_r = H5Dread_chunk(__pyx_v_dset_id, __pyx_v_dxpl_id, __pyx_v_offset, __pyx_v_filters, __pyx_v_buf);
      |                     ^~~~~~~~~~~~~~~
      |                     |
      |                     __pyx_t_5numpy_uint32_t * {aka long unsigned int *}
In file included from /usr/include/hdf5.h:25,
                 from /builddir/build/BUILD/h5py-3.10.0/serial/h5py/api_compat.h:27,
                 from /builddir/build/BUILD/h5py-3.10.0/serial/h5py/defs.c:1246:
/usr/include/H5Dpublic.h:1003:92: note: expected ‘uint32_t *’ {aka ‘unsigned int *’} but argument is of type ‘__pyx_t_5numpy_uint32_t *’ {aka ‘long unsigned int *’}
 1003 | H5_DLL herr_t H5Dread_chunk(hid_t dset_id, hid_t dxpl_id, const hsize_t *offset, uint32_t *filters,
      |                  ~~~~~~~~~~^~~~~~~
```


### Python and NumPy Versions:

1.26.2
3.12.2 (main, Feb  7 2024, 00:00:00) [GCC 14.0.1 20240127 (Red Hat 14.0.1-0)]

### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2024-02-22 05:16:45,,BUG: incompatible pointer type for `npy_int32t` and `npy_uint32` on i686,"['00 - Bug', 'component: numpy._core']"
25859,open,XavierGeerinck,"### Proposed new feature or change:

Seeing that WASI just reached [Tier 2](https://peps.python.org/pep-0011/#tier-2) status, I was wondering if there were plans for NumPy to build for the WASI target as well? 
",2024-02-20 13:21:58,,ENH: WASI Build,['unlabeled']
25857,open,hroncok,"### Describe the issue:

Hello, I have noticed that the Test_X86_Features.test_features repeatedly (but not always) fails in Fedora Linux Copr builders. I've recently reproduced that with 1.26.2, but can try with 1.26.4 as well, if there is an indication that it should matter.

I've isolated the failure to one particular builder configuration.

```
CPU info:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      46 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             4
On-line CPU(s) list:                0-3
Vendor ID:                          GenuineIntel
Model name:                         Intel(R) Xeon(R) Platinum 8488C
CPU family:                         6
Model:                              143
Thread(s) per core:                 2
Core(s) per socket:                 2
Socket(s):                          1
Stepping:                           8
BogoMIPS:                           4800.00
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd ida arat avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid cldemote movdiri movdir64b md_clear serialize amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities
Hypervisor vendor:                  KVM
Virtualization type:                full
L1d cache:                          96 KiB (2 instances)
L1i cache:                          64 KiB (2 instances)
L2 cache:                           4 MiB (2 instances)
L3 cache:                           105 MiB (1 instance)
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-3
```

### Error message:

```
=================================== FAILURES ===================================
_______________________ Test_X86_Features.test_features ________________________

self = <numpy.core.tests.test_cpu_features.Test_X86_Features object at 0x7f498e01c3e0>

    def test_features(self):
        self.load_flags()
        for gname, features in self.features_groups.items():
            test_features = [self.cpu_have(f) for f in features]
>           assert_features_equal(__cpu_features__.get(gname), all(test_features), gname)
E           AssertionError: Failure Detection
E            NAME: 'AVX512_SPR'
E            ACTUAL: True
E            DESIRED: False
E           
###########################################
### Extra debugging information
###########################################
-------------------------------------------
--- NumPy Detections
-------------------------------------------
{MMX: True, SSE: True, SSE2: True, SSE3: True, SSSE3: True, SSE41: True, POPCNT: True, SSE42: True, AVX: True, F16C: True, XOP: False, FMA4: False, FMA3: True, AVX2: True, AVX512F: True, AVX512CD: True, AVX512ER: False, AVX512PF: False, AVX5124FMAPS: False, AVX5124VNNIW: False, AVX512VPOPCNTDQ: True, AVX512VL: True, AVX512BW: True, AVX512DQ: True, AVX512VNNI: True, AVX512IFMA: True, AVX512VBMI: True, AVX512VBMI2: True, AVX512BITALG: True, AVX512FP16: True, AVX512_KNL: False, AVX512_KNM: False, AVX512_SKX: True, AVX512_CLX: True, AVX512_CNL: True, AVX512_ICL: True, AVX512_SPR: True, VSX: False, VSX2: False, VSX3: False, VSX4: False, VX: False, VXE: False, VXE2: False, NEON: False, NEON_FP16: False, NEON_VFPV4: False, ASIMD: False, FPHP: False, ASIMDHP: False, ASIMDDP: False, ASIMDFHM: False}
-------------------------------------------
--- SYS / CPUINFO
-------------------------------------------
processor	: 0
vendor_id	: GenuineIntel
cpu family	: 6
model		: 143
model name	: Intel(R) Xeon(R) Platinum 8488C
stepping	: 8
microcode	: 0x2b000571
cpu MHz		: 3792.916
cache size	: 107520 KB
physical id	: 0
siblings	: 4
core id		: 0
cpu cores	: 2
apicid		: 0
initial apicid	: 0
fpu		: yes
fpu_exception	: yes
cpuid level	: 31
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd ida arat avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid cldemote movdiri movdir64b md_clear serialize amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities
bugs		: spectre_v1 spectre_v2 spec_store_bypass swapgs eibrs_pbrsb
bogomips	: 4800.00
clflush size	: 64
cache_alignment	: 64
address sizes	: 46 bits physical, 48 bits virtual
power management:
E           
processor	: 1
vendor_id	: GenuineIntel
cpu family	: 6
model		: 143
model name	: Intel(R) Xeon(R) Platinum 8488C
stepping	: 8
microcode	: 0x2b000571
cpu MHz		: 3759.239
cache size	: 107520 KB
physical id	: 0
siblings	: 4
core id		: 1
cpu cores	: 2
apicid		: 2
initial apicid	: 2
fpu		: yes
fpu_exception	: yes
cpuid level	: 31
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pdcm pcid....
-------------------------------------------
--- SYS / AUXV
-------------------------------------------
AT_SYSINFO_EHDR:      0x7ffdbc3d4000
AT_MINSIGSTKSZ:       11952
AT_HWCAP:             1f8bfbff
AT_PAGESZ:            4096
AT_CLKTCK:            100
AT_PHDR:              0x561a72b70040
AT_PHENT:             56
AT_PHNUM:             13
AT_BASE:              0x7fa02ec94000
AT_FLAGS:             0x0
AT_ENTRY:             0x561a72b72970
AT_UID:               1001
AT_EUID:              1001
AT_GID:               135
AT_EGID:              135
AT_SECURE:            0
AT_RANDOM:            0x7ffdbc228689
AT_HWCAP2:            0x2
AT_EXECFN:            /bin/true
AT_PLATFORM:          x86_64
AT_??? (0x1b): 0x1c
AT_??? (0x1c): 0x20

features   = ['AVX512F', 'AVX512CD', 'AVX512BW', 'AVX512DQ', 'AVX512VL', 'AVX512IFMA', ...]
gname      = 'AVX512_SPR'
self       = <numpy.core.tests.test_cpu_features.Test_X86_Features object at 0x7f498e01c3e0>
test_features = [True, True, True, True, True, True, ...]

../../../../BUILDROOT/numpy-1.26.2-4.fc41.x86_64/usr/lib64/python3.12/site-packages/numpy/core/tests/test_cpu_features.py:77: AssertionError
```

### Python and NumPy Versions:

1.26.2
3.12.2, 3.12.1, 3.13.0a4

### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2024-02-20 10:56:48,,"BUG: Test_X86_Features.test_features fails on Fedora Copr builders: AVX512_SPR actual-True, desired-False","['00 - Bug', 'component: SIMD']"
25848,open,bc118,"### Proposed new feature or change:

Code Suggestion:

```python
def round_to_sig_figs(value, sig_figs=3):
   """"""Round a number to the selected number of significant figures.

   Round a number to the selected number of significant figures.

   Parameters
   ----------
   value: int or float
       The number that will be rounded to the selected number of
       significant figures.
   sig_figs: int, default=3
       The number significant figures that the 'value' variable
       will be rounded too. If sig_fig=0, it will return 0.0.

   Returns
   -------
   value_rounded_to_sig_figs: float
       The input 'value' variable rounded to the selected number
       significant figures. If sig_fig=0, it will return 0.0.
   """"""
   if value == 0:
      value_rounded_to_sig_figs = 0

   else:
      value_rounded_to_sig_figs = float(
         round(value, sig_figs - int(math.floor(math.log10(abs(value)))) - 1)
      )

   return value_rounded_to_sig_figs
```",2024-02-18 17:05:35,,ENH: Add round to significant figures in package ,['01 - Enhancement']
25846,open,penzaijun,"### Describe the issue:

Both np.amax and np.argmax are expected to have a time complexity of O(n)，so they should have similar computational times.
However, they only exhibit comparable performance on 1D arrays. 
For 2D or higher dimensional arrays, np.amax consistently outperforms np.argmax by a factor of 8x or more. It's strange.



### Reproduce the code example:

```python
import timeit

stmt1 = ""np.argmax(a, axis=0)""
stmt2 = ""np.amax(a, axis=0)""
setup_1d = ""import numpy as np; a = np.random.rand(3*768*768)""
setup_2d = ""import numpy as np; a = np.random.rand(3,768*768)""

execution_time1 = timeit.timeit(stmt1, setup=setup_2d, number=1000)
print(f""Execution time for np.argmax on 2d array: {execution_time1} seconds"")

execution_time2 = timeit.timeit(stmt2, setup=setup_2d, number=1000)
print(f""Execution time for np.amax on 2d array: {execution_time2} seconds"")

execution_time1 = timeit.timeit(stmt1, setup=setup_1d, number=1000)
print(f""Execution time for np.argmax on 1d array: {execution_time1} seconds"")

execution_time2 = timeit.timeit(stmt2, setup=setup_1d, number=1000)
print(f""Execution time for np.amax on 1d array: {execution_time2} seconds"")
```


### Error message:

```shell
Execution time for np.argmax on 2d array: 16.13085489999503 seconds
Execution time for np.amax on 2d array: 2.400201399810612 seconds
Execution time for np.argmax on 1d array: 0.6763406000100076 seconds
Execution time for np.amax on 1d array: 0.4886799002997577 seconds
```


### Python and NumPy Versions:

Python: 3.10.13
Numpy: 1.26.4

### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2024-02-18 09:35:25,,ENH : np.argmax is unusually time consuming for multidimensional array,['00 - Bug']
25835,open,dg-pb,"### Proposed new feature or change:

This is to follow up the discussion in the mailing list.

From my POV, sum for medium sized arrays (50k+) often becomes a bottleneck in greedy algorithms that need to compute distance of partial space over and over.

In short, sum does not seem to be parallelized and sum operation via dot product becomes faster from a certain threshold.

* This is using OpenBLAS

![](https://gcdnb.pbrd.co/images/j8n3EsRz5g5v.png?o=1)

npy.sum being:
```python
def dotsum(arr):
    a = arr.reshape(1000, 100)
    return a.dot(np.ones(100)).sum()
```

So one might need to resort to constructing `sum` operation via other functions (when program design is single process, but taking advantage of multiprocessing of individual components. Which IMO is still fairly common approach, where cost of implementing multi-process design doesn't justify the benefits)

Numpy being fairly low level library in the whole python ecosystem, I think user should be able to rely that something as basic as `sum` is going to be near optimal solution for the operation it is designed to do.

Also note, there seems to exist a library for what I am proposing https://github.com/quansight/pnumpy.

The sum implemented in it converges with speed of `dotsum` function above:

![](https://gcdnb.pbrd.co/images/NWfFXvAVAQ4p.png?o=1)

However, even with `pnumpy` enabled there is a significant size range, where `npy.sum aka dotsum` is more performant.

Also, shouldn't sum generally be faster than dot product? After all, dot product needs to do one additional operation (multiplication) compared to vanilla sum? Or is there some clever trick of dot product, which results in the same number of operations as vanilla sum?",2024-02-16 15:34:54,,ENH: array summation (np.add.reduce) parallelisation,['unlabeled']
25823,open,Chuck321123,"### Proposed new feature or change:

After some searching, I found out there is no easy way of doing backward filling/forward filling with numpy. However, this is super easy to do in pandas. Would be nice to have these functions added in numpy as well.",2024-02-14 13:21:58,,ENH: Add bfill() and ffill() function to numpy,['unlabeled']
25805,open,ricardoV94,"### Describe the issue:

Adding axis to the right is perplexing, specially when `atleast_2d`  adds to the left and numpy broadcasting behavior always grows to the left.

Is there a good reason for this? I see there's a PR to customize behavior via kwarg but the default behavior is kept.

I suspect this is more of a bug/ historical accident than a feature but happy to be corrected. This is relevant for libraries that try to emulate numpy API but understandably don't want to frustrate users with obscure behavior.

### Reproduce the code example:

```python
Not needed
```


### Error message:

_No response_

### Python and NumPy Versions:

Latest and main, it's documented behavior 

### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2024-02-11 21:39:56,,BUG: Behavior of `atleast_3d` is surprising ,['00 - Bug']
25799,open,KybernetikJo,"### Describe the issue:

# F2PY - Fortranname


As I understand it, the statement `fortranname` should work for both
subroutines and functions. However, The F2PY statement `fortranname`
seems only to work for subroutines and not for functions.

### Reproduce the code example:

## Subroutine

The F2PY statement `fortranname` works in subroutines.

`subfortranname.f`

```fortran
      SUBROUTINE SUBFORTRANNAME(A,B,C)
      REAL*8 A, B, C
      C = A + B
      END SUBROUTINE
```

`subfortranname.pyf`

``` python
python module subfortranname ! in 
    interface  ! in :subfortranname
        subroutine subfortranname_default(a,b,c) ! in :subfortranname:subfortranname.f
            fortranname subfortranname
            real*8 :: a
            real*8 :: b
            real*8, intent(out) :: c
        end subroutine subfortranname_default
    end interface 
end python module subfortranname
```

`test_subfortranname.py`

``` python
import subfortranname

print(subfortranname.__doc__)
```

``` bash
This module 'subfortranname' is auto-generated with f2py (version:1.26.4).
Functions:
    c = subfortranname_default(a,b)
.
```

## Function

The F2PY statement `fortranname` does not work in functions.

`funcfortranname.f`

``` fortran
      REAL*8 FUNCTION FUNCFORTRANNAME(A,B)
      REAL*8 A, B
      FUNCFORTRANNAME = A + B
      RETURN
      END FUNCTION
```

`funcfortranname.pyf`

``` python
python module funcfortranname ! in 
    interface  ! in :funcfortranname
        function funcfortranname_default(a,b) ! in :funcfortranname:funcfortranname.f
            fortranname funcfortranname
            real*8 :: a
            real*8 :: b
            real*8, intent(out) :: funcfortranname
        end function funcfortranname_default
    end interface 
end python module funcfortranname
```

`test_funcfortranname.py`

``` python
import funcfortranname

print(funcfortranname.__doc__)
```

``` bash
Traceback (most recent call last):
  File ""/home/johannes/Sandbox/f2py/fortranname/test_f.py"", line 1, in <module>
    import funcfortranname
ImportError: /home/johannes/Sandbox/f2py/fortranname/funcfortranname.cpython-310-x86_64-linux-gnu.so: undefined symbol: funcfortranname_default_
```


### Error message:

_No response_

### Python and NumPy Versions:

Python 3.10.12 and 3.12.1
Numpy 1.26.4

```bash
1.26.4
3.12.1 (main, Dec 10 2023, 15:07:36) [GCC 11.4.0]
```

### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2024-02-09 21:09:05,,BUG: The F2PY statement `fortranname` does not work for functions. ,"['00 - Bug', 'component: numpy.f2py']"
25787,open,etienneschalk,"### Proposed new feature or change:

#### Motive 

The default dtype of array is platform-dependant. ( #9464 )

When running tests in a continuous integration context, that are ran on multiple platforms (Windows, macOS, Linux), the fact that the default dtypes of arrays can vary must be taken into account.

The issue appears for tests relying on numpy's arrays representations. Indeed, the default dtype of the array is not displayed in the array representation. This means that an expected output representation is now dependant on the platform. Writing OS-specific tests is now unavoidable.

What I would like is being able to write platform independent repeatable outputs that can be used for automated testing.

#### Example

##### Actual

On my machine, the default dtype for integer arrays is `int64`. Here are some examples of array creations and their representations:
```python
In [3]: import numpy as np

In [4]: np.array([1, 2, 3])
Out[4]: array([1, 2, 3])

In [5]: np.array([1, 2, 3], dtype=np.int64)
Out[5]: array([1, 2, 3])

In [6]: np.array([1, 2, 3], dtype=np.int32)
Out[6]: array([1, 2, 3], dtype=int32)
```

We can see that:
- When creating an array with no dtype kwarg, the default dtype is used. The array representation solely is not enough to know the actual dtype.
- When creating an array with a dtype kwarg matching the default integer dtype of the platform, the resulting array representation is the same, and dtype is also implicit.
- The last case is the most explicit: the user provides the expected dtype, and the representation reflects that. This only works for non-default dtypes.

##### Desired 

```python
In [3]: import numpy as np

In [4]: np.array([1, 2, 3])
Out[4]: array([1, 2, 3], dtype=int64)

In [5]: np.array([1, 2, 3], dtype=np.int64)
Out[5]: array([1, 2, 3], dtype=int64)

In [6]: np.array([1, 2, 3], dtype=np.int32)
Out[6]: array([1, 2, 3], dtype=int32)
```

The dtype is always printed out, and the default dtype does not influence the representation. So, since the default dtype depends on the platform, and the representation depends on the dtype, the chain is broken and the representation does not depend anymore on the platform. Writing platform independant tests relying on representation is now easier.

from
`platform <- default dtype <- repr` => `platform <- repr`
to
`platform <- default dtype </- repr` => `platform </- repr`


#### Existing solutions I looked for 

##### `np.set_printoptions`

I first looked into https://numpy.org/doc/stable/reference/generated/numpy.set_printoptions.html 
I experimented with kwarg `legacy='1.13'` and `legacy='1.21`, without success. Also, even if I were successful, I would have dislike relying on a kwarg named `legacy`, strongly implying it should not be used anymore in new code.

#### Proposed solution

Adding a new `dtype` printing option

```python
import numpy


np.set_printoptions(dtype=""default"") # current behaviour
np.set_printoptions(dtype=""always"") # always print dtype
np.set_printoptions(dtype=""never"") # never print dtype
```

#### Technical Analysis 

The function [`_array_repr_implementation`](https://github.com/numpy/numpy/blob/d35cd07ea997f033b2d89d349734c61f5de54b0d/numpy/core/arrayprint.py#L1487) implements the array representation logic. We can see the logic where it adds the suffix, and there is no way to force print the dtype, or force not printing it.

Allow to override this param could be helpful: 
```python
def _array_repr_implementation(
        arr, max_line_width=None, precision=None, suppress_small=None,
+       skipdtype: bool | None = None,       
        array2string=array2string):
        ...
- skipdtype = dtype_is_implied(arr.dtype) and arr.size > 0
+ if skipdtype is None:
+     skipdtype = dtype_is_implied(arr.dtype) and arr.size > 0
```

Role of the proposed new `skipdtype` three-valued kwarg:
- `None`: current behaviour, platform-dependant
- `False`: always print the `, dtype=...` suffix
- `True`: never print the `, dtype=...` suffix

#### Additional links

- https://github.com/pydata/xarray/pull/8702
",2024-02-07 22:59:28,,ENH: Print Option: Always show an array's dtype,['01 - Enhancement']
25777,open,broukema,"### Describe the issue:

The example for 'basic support' for f90 module allocatable arrays, as quoted in the `f2py` documentation, gives two compile warnings. Use of the compiled .so file as described further in the same documentation works fine to me, without errors.

online version of f2py doc: https://numpy.org/doc/stable/f2py/python-usage.html#allocatable-arrays
Related issue: https://github.com/numpy/numpy/issues/19157

### Reproduce the code example:

```python
! fortran module code as given at https://numpy.org/doc/stable/f2py/python-usage.html#allocatable-arrays
module mod
  real, allocatable, dimension(:,:) :: b 
contains
  subroutine foo
    integer k
    if (allocated(b)) then
       print*, ""b=[""
       do k = 1,size(b,1)
          print*, b(k,1:size(b,2))
       enddo
       print*, ""]""
    else
       print*, ""b is not allocated""
    endif
  end subroutine foo
end module mod
```


### Error message:

```shell
$ f2py -c -m allocarr allocarr.f90
running build
running config_cc
INFO: unifing config_cc, config, build_clib, build_ext, build commands --compiler options
running config_fc
INFO: unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options
running build_src
INFO: build_src
INFO: building extension ""allocarr"" sources
INFO: f2py options: []
INFO: f2py:> /tmp/tmp2q0rmg2g/src.linux-x86_64-3.11/allocarrmodule.c
creating /tmp/tmp2q0rmg2g/src.linux-x86_64-3.11
Reading fortran codes...
        Reading file 'allocarr.f90' (format:free)
Post-processing...
        Block: allocarr
                        Block: mod
                                Block: foo
Applying post-processing hooks...
  character_backward_compatibility_hook
Post-processing (stage 2)...
        Block: allocarr
                Block: unknown_interface
                        Block: mod
                                Block: foo
Building modules...
    Building module ""allocarr""...
                Constructing F90 module support for ""mod""...
                  Variables: b
getarrdims:warning: assumed shape array, using 0 instead of ':'
getarrdims:warning: assumed shape array, using 0 instead of ':'
            Constructing wrapper function ""mod.foo""...
              foo()
    Wrote C/API module ""allocarr"" to file ""/tmp/tmp2q0rmg2g/src.linux-x86_64-3.11/allocarrmodule.c""
    Fortran 90 wrappers are saved to ""/tmp/tmp2q0rmg2g/src.linux-x86_64-3.11/allocarr-f2pywrappers2.f90""
INFO:   adding '/tmp/tmp2q0rmg2g/src.linux-x86_64-3.11/fortranobject.c' to sources.
INFO:   adding '/tmp/tmp2q0rmg2g/src.linux-x86_64-3.11' to include_dirs.
copying /usr/lib/python3/dist-packages/numpy/f2py/src/fortranobject.c -> /tmp/tmp2q0rmg2g/src.linux-x86_64-3.11
copying /usr/lib/python3/dist-packages/numpy/f2py/src/fortranobject.h -> /tmp/tmp2q0rmg2g/src.linux-x86_64-3.11
INFO:   adding '/tmp/tmp2q0rmg2g/src.linux-x86_64-3.11/allocarr-f2pywrappers2.f90' to sources.
INFO: build_src: building npy-pkg config files
/usr/lib/python3/dist-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.
  warnings.warn(
running build_ext
INFO: customize UnixCCompiler
INFO: customize UnixCCompiler using build_ext
INFO: get_default_fcompiler: matching types: '['arm', 'gnu95', 'intel', 'lahey', 'pg', 'nv', 'absoft', 'nag', 'vast', 'compaq', 'intele', 'intelem', 'gnu', 'g95', 'pathf95', 'nagfor', 'fujitsu']'
INFO: customize ArmFlangCompiler
WARN: Could not locate executable armflang
INFO: customize Gnu95FCompiler
INFO: Found executable /usr/bin/gfortran
INFO: customize Gnu95FCompiler
INFO: customize Gnu95FCompiler using build_ext
INFO: building 'allocarr' extension
INFO: compiling C sources
INFO: C compiler: x86_64-linux-gnu-gcc -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC

creating /tmp/tmp2q0rmg2g/tmp
creating /tmp/tmp2q0rmg2g/tmp/tmp2q0rmg2g
creating /tmp/tmp2q0rmg2g/tmp/tmp2q0rmg2g/src.linux-x86_64-3.11
INFO: compile options: '-DNPY_DISABLE_OPTIMIZATION=1 -I/tmp/tmp2q0rmg2g/src.linux-x86_64-3.11 -I/usr/lib/python3/dist-packages/numpy/core/include -I/usr/include/python3.11 -c'
INFO: x86_64-linux-gnu-gcc: /tmp/tmp2q0rmg2g/src.linux-x86_64-3.11/allocarrmodule.c
INFO: x86_64-linux-gnu-gcc: /tmp/tmp2q0rmg2g/src.linux-x86_64-3.11/fortranobject.c
/tmp/tmp2q0rmg2g/src.linux-x86_64-3.11/allocarrmodule.c: In function ‘f2py_setup_mod’:
/tmp/tmp2q0rmg2g/src.linux-x86_64-3.11/allocarrmodule.c:186:31: warning: assignment to ‘f2py_init_func’ {aka ‘void (*)(int *, long int *, void (*)(char *, long int *), int *)’} from incompatible pointer type ‘void (*)(int *, int *, void (*)(char *, int *), int *)’ [-Wincompatible-pointer-types]
  186 |   f2py_mod_def[i_f2py++].func = b;
      |                               ^
INFO: compiling Fortran 90 module sources
INFO: Fortran f77 compiler: /usr/bin/gfortran -Wall -g -ffixed-form -fno-second-underscore -fPIC -O3 -funroll-loops
Fortran f90 compiler: /usr/bin/gfortran -Wall -g -fno-second-underscore -fPIC -O3 -funroll-loops
Fortran fix compiler: /usr/bin/gfortran -Wall -g -ffixed-form -fno-second-underscore -Wall -g -fno-second-underscore -fPIC -O3 -funroll-loops
INFO: compile options: '-I/tmp/tmp2q0rmg2g/src.linux-x86_64-3.11 -I/usr/lib/python3/dist-packages/numpy/core/include -I/usr/include/python3.11 -c'
extra options: '-J/tmp/tmp2q0rmg2g/ -I/tmp/tmp2q0rmg2g/'
INFO: gfortran:f90: allocarr.f90
INFO: compiling Fortran sources
INFO: Fortran f77 compiler: /usr/bin/gfortran -Wall -g -ffixed-form -fno-second-underscore -fPIC -O3 -funroll-loops
Fortran f90 compiler: /usr/bin/gfortran -Wall -g -fno-second-underscore -fPIC -O3 -funroll-loops
Fortran fix compiler: /usr/bin/gfortran -Wall -g -ffixed-form -fno-second-underscore -Wall -g -fno-second-underscore -fPIC -O3 -funroll-loops
INFO: compile options: '-I/tmp/tmp2q0rmg2g/src.linux-x86_64-3.11 -I/usr/lib/python3/dist-packages/numpy/core/include -I/usr/include/python3.11 -c'
extra options: '-J/tmp/tmp2q0rmg2g/ -I/tmp/tmp2q0rmg2g/'
INFO: gfortran:f90: /tmp/tmp2q0rmg2g/src.linux-x86_64-3.11/allocarr-f2pywrappers2.f90
/tmp/tmp2q0rmg2g/src.linux-x86_64-3.11/allocarr-f2pywrappers2.f90:37:10:

   37 |       use mod, only : b
      |          1
Warning: Unused module variable ‘b’ which has been explicitly imported at (1) [-Wunused-variable]
INFO: /usr/bin/gfortran -Wall -g -Wall -g -shared /tmp/tmp2q0rmg2g/tmp/tmp2q0rmg2g/src.linux-x86_64-3.11/allocarrmodule.o /tmp/tmp2q0rmg2g/tmp/tmp2q0rmg2g/src.linux-x86_64-3.11/fortranobject.o /tmp/tmp2q0rmg2g/allocarr.o /tmp/tmp2q0rmg2g/tmp/tmp2q0rmg2g/src.linux-x86_64-3.11/allocarr-f2pywrappers2.o -L/usr/lib/gcc/x86_64-linux-gnu/12 -L/usr/lib/gcc/x86_64-linux-gnu/12 -L/usr/lib/x86_64-linux-gnu -lgfortran -o ./allocarr.cpython-311-x86_64-linux-gnu.so
Removing build directory /tmp/tmp2q0rmg2g
```


### Python and NumPy Versions:
Versions including Debian/bookworm patch IDs:
````
python3-numpy                          1:1.24.2-1                             amd64
python3                                3.11.2-1+b1                            amd64
gfortran-12                            12.2.0-14                              amd64
gcc-12                                 12.2.0-14                              amd64
````

### Speculation:

It looks like the first warning is an issue of a mismatch between `long int` on the python side and `int` on the fortran side.

The second warning is probably normally a feature, but in this situation invalid.",2024-02-06 14:59:25,,BUG: f2py module allocatable array example gives two compile warnings,"['00 - Bug', 'component: numpy.f2py']"
25766,open,creiter64,"### Proposed new feature or change:

In https://github.com/numpy/numpy/issues/13375 `is_integer()` got added to numpy float types, but it can not be applied on ndarrays of dtype=floating.

I'd expect that the function is also available on ndarrays and will return an ndarray of same dimensions with the boolean result.",2024-02-05 13:29:15,,ENH: implement is_integer() for np.ndarray,['unlabeled']
25749,open,stinodego,"### Describe the issue:

I am trying to implement the array interface protocol for Polars Series. Polars represents its boolean arrays (and validity buffers) as bit-packed booleans, e.g. 8 booleans per byte. I figured I would need to specify a bit field (e.g. `t` type) to do this, however, it seems numpy does not recognize any way of defining a bit field. I've tried various combinations (with `descr` too) to get this to work, but I can't figure it out.

Could it be that NumPy does not handle bit field types correctly?

### Reproduce the code example:

```python
import numpy as np


class wrapper:
    pass


interface = {
    ""shape"": (3,),
    ""typestr"": ""|t1"",
    ""version"": 3,
}
wrapper.__array_interface__ = interface

result = np.array(wrapper, copy=False)
```


### Error message:

```shell
Traceback (most recent call last):
  File ""/home/stijn/code/polars/py-polars/repro.py"", line 19, in <module>
    result = np.array(wrapper, copy=False)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: data type '|t1' not understood
```


### Python and NumPy Versions:

1.26.3
3.12.0 (main, Nov 16 2023, 08:47:32) [GCC 11.4.0]

### Runtime Environment:

[{'numpy_version': '1.26.3',
  'python': '3.12.0 (main, Nov 16 2023, 08:47:32) [GCC 11.4.0]',
  'uname': uname_result(system='Linux', node='frosty', release='6.1.0-1029-oem', version='#29-Ubuntu SMP PREEMPT_DYNAMIC Tue Jan  9 21:07:34 UTC 2024', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Prescott',
  'filepath': '/home/stijn/code/polars/.venv/lib/python3.12/site-packages/numpy.libs/libopenblas64_p-r0-0cf96a72.3.23.dev.so',
  'internal_api': 'openblas',
  'num_threads': 12,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.23.dev'}]
None

### Context for the issue:

I am trying to improve interoperability between the Polars dataframe library and NumPy.",2024-02-02 14:55:39,,DOC: Array interface protocol does not recognize bit field typestr and could mention AttributeError use,['04 - Documentation']
25737,open,stefanv,"### Proposed new feature or change:

- OpenBLAS wheels
  Reduction: %
  Related issue:
- Random / Cython
  Reduction: %
  Related issue:
- Increase minimum SIMD architecture
  Reduction: %
  Related issue:",2024-01-31 19:00:28,,ENH: Reduce size of distributed wheels,['03 - Maintenance']
25733,open,rgommers,"This is a follow-up to gh-22021. `np.object` was removed several releases back. It is the nicer name though, and we should probably bring it back so `object_` and `object` are the same thing (and longer-term we can prefer `object`.

It is another alias in the namespace, but underscored names really are no good. Since `object`, while not our favorite dtype, is here to stay it deserves the obvious name without an underscore.

When this is done, it'd be good to look at the PR that closed gh-22021 and do a similar thing (making the preferred name the actual one in the code, rather than the alias).",2024-01-31 11:37:51,,Bring back `np.object` to be an alias to `np.object_` ,['17 - Task']
25728,open,broukema,"### Describe the issue:

SUMMARY:

A beginner using f2py will expect that if the fortran backend lists parameters including an array size and an array, in that order, then a python front end that uses the fortran backend compiled by f2py should also list the array size and array in that same order. However, doing this gives a fatal error in python. The user is required to either omit the array size or provide the two parameters in the reverse order.

REPRODUCE:

With appropriate file names:
````
gfortran bug0.f; ./a.out; f2py -c -m param_order bug0.f; python3 bug0py.py
````
gives
````
nine =         9
`````
from running the fortran main program but
````
Traceback (most recent call last):
  File ""/MYDIR/bug0py.py"", line 7, in <module>
    param_order.get_fact(k, j_size, j_array)
param_order.error: (shape(j_array, 0) == j_size) failed for 1st keyword j_size: get_fact:j_size=45
````
from running the python front end.

Commenting out the line `param_order.get_fact(k, j_size, j_array)` and uncommenting either of the lines
````
#param_order.get_fact(k, j_array, j_size)
#param_order.get_fact(k, j_array)
````
and doing `gfortran bug0.f; ./a.out; f2py -c -m param_order bug0.f; python3 bug0py.py` again gives 
````
nine =      9
````
from both the fortran main program and the python front end.


### Reproduce the code example:

fortran `bug0.f`:
````
      program learn_f2py
      integer :: k
      integer, dimension(3) :: j_array
      k = 55
      j_size = 3
      j_array(2) = 845
      call get_fact(k, j_size, j_array)
      end

      subroutine get_fact(k, j_size, j_array)
!f2py integer, intent(in) :: k, j_size
      integer, dimension(j_size) :: j_array
!f2py intent(in) :: j_array
! implicit typing
      nine = (k + j_array(2))/100
      write(6,'(a,i10)')'nine =',nine
      return
      end
````

python `bug0py.py`:
````
import param_order
k = 55
j_array = (45, 845, 1845) # python list object
j_size = 3

#This fails fatally, despite the parameters being in the correct order:
param_order.get_fact(k, j_size, j_array)

#This works, despite the parameters being in the wrong order:
#param_order.get_fact(k, j_array, j_size)

#This works:
#param_order.get_fact(k, j_array)
````


### Error message:

````
Traceback (most recent call last):
  File ""/MYDIR/bug0py.py"", line 7, in <module>
    param_order.get_fact(k, j_size, j_array)
param_order.error: (shape(j_array, 0) == j_size) failed for 1st keyword j_size: get_fact:j_size=45
````


### Python and NumPy Versions:

````
Debian bookworm
ii  python3-numpy                                 1:1.24.2-1                              amd64
ii  python3                                       3.11.2-1+b1                             amd64
````


### Context for the issue:

DISCUSSION:

This behaviour is not obvious to the beginning user and ( see https://github.com/numpy/numpy/issues/25616 ) is not documented. The minimal working example that I have shown gives an error message that quickly gives a clue to what is wrong, since it is clear that `j_size` was supposed to be 3 but is seen by python as 45, and we know where the 45 comes from. However, if `j_array` starts with the value 0, the user is less likely to guess the explanation.

Apart from improved documentation, an alternative to heavy refactoring of `f2py` could be to provide a warning or error during the `f2py` compile step that an array size is provided earlier than the array itself and that the user should consider either (i) removing the array size in the python call, or (ii) placing it after the array in both the fortran and python sides. If the user is expected to leave the fortran side as unchanged as possible, then (ii) will not be an option, b ut (i) will be.
",2024-01-30 14:07:30,,BUG: f2py python front end gives a fatal error when array size is given before array in agreement with fortran backend,"['00 - Bug', 'component: numpy.f2py']"
25724,open,beauxq,"### Issue with current documentation:

https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.Polynomial.html#numpy.polynomial.polynomial.Polynomial

This page mentions `coef` as a parameter, but it doesn't say anything about `coef` as a property.

`coef` does not give the coefficients of the polynomial, as demonstrated by this code:

```python3
import numpy as np
from numpy.polynomial import Polynomial

inputs = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0]
outputs = [1.0, 2.0, 4.0, 8.0, 16.0, 32.0]

degree = 5
poly_fit = Polynomial.fit(inputs, outputs, degree)

coefficients = poly_fit.coef

print(""Coefficients:"", coefficients)
# Coefficients: [5.66015625 9.80338542 8.3984375  4.8828125  2.44140625 0.81380208]

# evaluate the polynomial at x=0
# (using 3 different methods)

print(np.polyval(coefficients, 0))  # 0.8138020833332782
print(np.polyval(coefficients[::-1], 0))  # 5.6601562500000036
print(poly_fit(0))  # 1.000000000000024
```

None of the values from `coef` are the constant x**0 coefficient (that should be something close to 1).
`poly_fit(0)` correctly evaluates the polynomial at zero. But how do I see the coefficients?

### Idea or request for content:

_No response_",2024-01-30 03:27:10,,DOC: How do I get the coefficients from a `Polynomial` object?,['04 - Documentation']
25698,open,ziofil,"### Proposed new feature or change:

I have implemented in about 30 LOC the following functionality for einsum:
```python
import numpy as np
ab = np.random.normal(size=(2,3))
abcdefgh = np.random.normal(size=(2,3,4,5,6,7,8,9))
contracted = np.einsum('ab,abcdefgh->(cd)e(fgh)', ab, abcdefgh)  # <--- look at return string

assert contracted.shape == (4*5, 6, 7*8*9)
assert np.allclose(contracted, np.einsum('ab,abcdefgh->cdefgh', ab, abcdefgh).reshape(contracted.shape))
```

Note the return part of the string `(cd)e(fgh)`: the idea is to use parentheses to enclose indices that we would like to reshape together.

I don't have time to follow up on docstring and tests. You can find the updated einsum function [here](https://github.com/ziofil/numpy/blob/ce1ba5b0eea7b48566f1c54d40e838261586dd33/numpy/_core/einsumfunc.py#L1420)",2024-01-26 11:07:15,,ENH: Adding reshape notation to einsum string,['unlabeled']
25693,open,mhvk,"### Proposed new feature or change:

A number of points arose in discussion of #25347 that were deemed better done in follow-up, to not distract from the meat of that (very large) PR. This issue is to remind us of those.
1. [x] The doc for `StringDType` has a size argument that was needed in development but not for actual release. It can be removed. This should be done before the NumPy 2.0 release because it is public API. (Though might it be useful for a short version that only uses arena? see below. Probably can put it back if needed...)
    - https://github.com/numpy/numpy/pull/25856
3. [x] The `add` ufunc needs a promoter so addition with a python string works.
4. [ ] Add a cython interface for the stringdtype C API.
5. [x] It is likely better to use a flag for strings ""long strings"" (stored outside of the numpy array proper) instead of one for short ones (stored inside), so that an all-zero entry correctly implies a zero-length string (see https://github.com/numpy/numpy/pull/25347#issuecomment-1910321015)
6. [ ] Refactor the flags in the vstring implementation to use bitfields. This will improve clarity and eliminate complicated  bitflag math.
7. [ ] Possibly, the arena should have more recoverable flags/size information (see https://github.com/numpy/numpy/pull/25347#issuecomment-1910321015)
8. [ ] Investigate refactoring `new_stringdtype_instance` into `tp_init`
9. [ ] Replace the long `#define` in `casts.c` with templating (or `.c.src`)
10. [x] Replace ufunc wrappers with passing functions into `*auxdata` (see [here](https://github.com/numpy/numpy/pull/25347#discussion_r1471915449), [here](https://github.com/numpy/numpy/pull/25347#discussion_r1471928789), [here](https://github.com/numpy/numpy/pull/25347#discussion_r1471930959), and [here](https://github.com/numpy/numpy/pull/25347#discussion_r1471932261)) [e.g., `minimum` and `maximum`; the various comparison functions; the templated unary functions; `find`, `rfind` and maybe `count`; `startswith` and `endswith`; `lstrip`, `rstrip` and `strip`, plus `whitespace` versions]. Attempt in gh-25796
11. [ ] Check `array2string` formatter overrides.
12. [x] Adjust error messages referring to ""object array"" (e.g., `a.view('2u8')` currently errors with `""Cannot change data-type for object array.""`).
13. [ ] Have some helper functions that make it easy for `StringDType` ufuncs to use `out` arguments, also for in-place operations.
14. [ ] See whether null-handling code in ufunc loops and casts can be consolidated into a helper function to reduce code duplication.

Things possibly for longer term
- Support in structured arrays (perhaps not super useful, but could be seen as similar to `object`).
- Expose more of the currently private `NpyString` API. This will depend on feedback from users.
- Fix `longdouble` to string, which is listed as broken in a `TODO` item in `casts.c`. isn't `dragon` able to deal with it?
- Add a DType API callback that triggers when the initial filling of a newly created array (e.g. after `PyArray_FromAny` finishes filling a new array). We could use this to trim the arena buffer to the exact size needed by the array. Currently we over-allocate because the buffer grows exponentially with a growth factor of 1.25.
- Might it make sense on 64bit systems, where normally the size is 16 bytes, to have a 8-byte version (short strings up to 7, only arena allocations for long ones; might use the `size` argument...).
- In principle, `.view(StringDType())` could be possible in some cases (e.g., to change the NA behaviour). Would need to share the allocator (and thus have reference counts for that...).
- Dealing with array scalars vs `str` scalars - see also more general discussion about array scalars in gh-24897.

Small warts, possibly not solvable
- `StringDType` is added to `add_dtype_helper` late in the initialization of `multiarraymodule`; can this be easier?
- Can the cases of having and not having gil be factored out, so that one doesn't get the kind of hybrid stuff in `load_non_nullable_string` with its `has_gil` argument.
- to have `dtype.hasobject` be true is logical but not quite the right name.
",2024-01-25 22:01:17,,TSK: Follow-up things for stringdtype,['17 - Task']
25677,open,ngoldbaum,"### Describe the issue:

For both signed and unsigned integers the default fill value is 99999, while for floats it is 1e20.

This is problematic for (u)int[8,16] as well as half floats, which do not contain the default fill value in their valid range.

### Reproduce the code example:

```python
>>> arr = np.ma.array([1, 2, 3], mask=[1, 0, 1], dtype=np.int8)
>>> arr.filled()
array([63,  2, 63], dtype=int8)

>>> arr = np.ma.array([1, 2, 3], mask=[1, 0, 1], dtype=np.float16)
>>> arr.filled()
/Users/goldbaum/Documents/numpy/build-install/usr/lib/python3.11/site-packages/numpy/ma/core.py:3873: RuntimeWarning: overflow encountered in cast
  np.copyto(result, fill_value, where=m)
Out[16]: array([inf,  2., inf], dtype=float16)
```


### Error message:

```shell
N/A
```


### Python and NumPy Versions:

Numpy 2.0 dev on python 3.11.

### Runtime Environment:

N/A

### Context for the issue:

I don't think this is an urgent but didn't see an issue describing this behavior so I'm filing this for future searchers.

That said, this does complicate the NEP 50 implementation because we need to have a number of workarounds so that this continues to work. I think it would be better to choose a default fill value that fits in the range of the data (`[i,f]info.max`?) for these types, but I have no idea what that entails for backward compatibility.",2024-01-24 15:47:52,,BUG: Masked array default fill value can overflow,['00 - Bug']
25654,open,JulianBothorn,"### Issue with current documentation:

Python 3.11.4, numpy 1.26.3, macOS Sonoma 14.2.1 (Intel)

The documentation does not explain how to generate a c wrapper that uses custom directives in a pyf file or it is not possible and the documentation contradicts itself. 

Im trying to move a [fortran project](https://github.com/scottransom/pyslalib)  from numpy.disutils to meson. The previous developers created a long pyf file with custom function annotations. The example for how to move to meson requires the [c wrapper generation step](https://numpy.org/doc/stable/f2py/buildtools/meson.html#automating-wrapper-generation).  [Extension module construction](https://numpy.org/doc/stable/f2py/usage.html#extension-module-construction) mentions using pyf files as a source for c wrapper generation.

 Running ```poetry run Python3 -m numpy.f2py -m slalib --lower slalib.pyf addet.f cc2s.f dc62s.f dr2af.f dvn.f etrms.f h2fk5.f nutc80.f prebn.f rverot.f tpv2c.f afin.f cc62s.f dcc2s.f dr2tf.f dvxv.f euler.f hfk5z.f oap.f prec.f rvgalc.f ue2el.f airmas.f cd2tf.f dcmpf.f drange.f e2h.f evp.f idchf.f oapqk.f preces.f rvlg.f ue2pv.f altaz.f cldj.f dcs2c.f dranrm.f earth.f fitxy.f idchi.f obs.f precl.f rvlsrd.f unpcd.f amp.f clyd.f dd2tf.f ds2c6.f ecleq.f fk425.f imxv.f pa.f prenut.f rvlsrk.f v2tp.f ampqk.f combn.f de2h.f ds2tp.f ecmat.f fk45z.f intin.f pav.f pv2el.f s2tp.f vdv.f aop.f cr2af.f deuler.f dsep.f ecor.f fk524.f invf.f pcd.f pv2ue.f sep.f veri.f aoppa.f cr2tf.f dfltin.f dsepv.f eg50.f fk52h.f kbj.f pda2h.f pvobs.f sepv.f vers.f aoppat.f cs2c.f dh2e.f dt.f el2ue.f fk54z.f m2av.f pdq2h.f pxy.f slalib-f2pywrappers.f vn.f aopqk.f cs2c6.f dimxv.f dtf2d.f epb.f fk5hz.f map.f permut.f range.f smat.f vxv.f atmdsp.f ctf2d.f djcal.f dtf2r.f epb2d.f flotin.f mappa.f pertel.f ranorm.f subet.f wait.f atms.f ctf2r.f djcl.f dtp2s.f epco.f galeq.f mapqk.f pertue.f rcc.f supgal.f xy2xy.f atmt.f daf2r.f dm2av.f dtp2v.f epj.f galsup.f mapqkz.f planel.f rdplan.f svd.f zd.f av2m.f dafin.f dmat.f dtps2c.f epj2d.f ge50.f moon.f planet.f refco.f svdcov.f bear.f dat.f dmoon.f dtpv2c.f epv.f geoc.f mxm.f plante.f refcoq.f svdsol.f caf2r.f dav2m.f dmxm.f dtt.f eqecl.f gmst.f mxv.f plantu.f refro.f tp2s.f caldj.f dbear.f dmxv.f dv2tp.f eqeqx.f gmsta.f nut.f pm.f refv.f tp2v.f calyd.f dbjin.f dpav.f dvdv.f eqgal.f h2e.f nutc.f polmo.f refz.f tps2c.f gresid.F random.F```
 
 Results in a c wrapper that does not respect the directives in the pyf file. 
 For example the sla_refro function has parameter 10 (ref) marked as intent(out) but the c wrapper keeps the parameter
 
 [Other options](https://numpy.org/doc/stable/f2py/usage.html#other-options) mentions that numpy.f2py -m can't be used with pyf files explaining the behaviour but contradicting the extension module construction section.
 
a simpler way to reproduce this is to alter the fib example file like so:

```
C FILE: FIB1.F
      SUBROUTINE FIB(A,N)
C
C     CALCULATE FIRST N FIBONACCI NUMBERS
C
      INTEGER N
      INTEGER A
      A = N + 1

      END
C END FILE FIB1.F
```
editing the pyf file to include 
```
python module fibx ! in 
    interface  ! in :fibx
        subroutine fib(a,n) ! in :fibx:fib1.f
            integer intent(out) :: a
            integer :: n
        end subroutine fib
    end interface 
end python module fibx
```
and finally running ```poetry run Python3 -m numpy.f2py -m fibx --lower fib1.pyf fib1.f```

The resulting .c file still has ""a"" marked as an input

 p.s. I believe that the pyf file is correct as [The smart way](https://numpy.org/doc/stable/f2py/f2py.getting-started.html#the-smart-way) results in a correct wrap.

### Idea or request for content:

Either an example of how to generate a c wrapper using a pyf file or the mention of signature files should be removed from [Extension module construction](https://numpy.org/doc/stable/f2py/usage.html#extension-module-construction) to avoid confusion",2024-01-22 13:53:35,,DOC: f2py command to generate c wrapper respecting pyf file directives,"['04 - Documentation', 'component: numpy.f2py']"
25647,open,ArgoHA,"### Describe the issue:

Numpy float value is not rounded with python round function in f-string, but is rounded in every other scenario. In the code example below, first print row doesn't work as expected, but second one does.
Not sure if this is a bug and if it is a numpy bug, please let me know what exactly is happening here.

### Reproduce the code example:

```python
import numpy as np

a = 0.123456
b = np.float32(a)

print(f""Python float: {round(a, 3)}, Numpy float: {round(b, 3)}"")
print(""Python float:"", round(a, 3), ""Numpy float:"", round(b, 3))
```


### Error message:

```shell
Python float: 0.123, Numpy float: 0.12300000339746475
Python float: 0.123 Numpy float: 0.123
```


### Python and NumPy Versions:

Python 3.11.4
np.__version__ 1.26.2

### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2024-01-21 08:21:15,,BUG: Scalar `__format__` should use dragon4 printing for floats,['00 - Bug']
25644,open,mcabbott,"### Describe the issue:

The docs for [`np.geomspace`](https://numpy.org/devdocs/reference/generated/numpy.geomspace.html#numpy-geomspace) state:

> If the inputs or dtype are complex, the output will follow a logarithmic spiral in the complex plane. (There are an infinite number of spirals passing through two points; the output will follow the shortest such path.)

But the function does not appear to obey this rule. I'm not sure how it is in fact choosing branch cuts.

Searching a bit, this may be altered by #25441 but I believe that is not in any released version yet.

### Reproduce the code example:

```python
import numpy as np

np.geomspace(-1j, 0.001 + 1j, 5)   # semicircle to the right, via +1
np.geomspace(-1j, -0.001 + 1j, 5)  # semicircle to the left, via -1, as documented

x = 1.2 + 3.4j  # also x = 1+1j
delta = 0.01j
np.geomspace(x, -x + delta, 5)  # crosses real line near 3.5, middle point 3.4-1.2j
np.geomspace(x, -x - delta, 5)  # goes the same way!
```


### Error message:

No error message. Numerical result is:

```python
>>> x = 1.2 + 3.4j  # also x = 1+1j
>>> delta = 0.01j
>>> np.geomspace(x, -x + delta, 5)  # crosses real line near 3.5, middle point 3.4-1.2j
array([ 1.2       +3.4j       ,  3.2509223 +1.5538648j ,
        3.39499673-1.20000116j,  1.55032927-3.24738677j,
       -1.2       -3.39j      ])
>>> np.geomspace(x, -x - delta, 5)  # goes the same way!
array([ 1.2       +3.4j       ,  3.25445784+1.55740034j,
        3.40499673-1.20000115j,  1.56093588-3.25799337j,
       -1.2       -3.41j      ])
```


### Python and NumPy Versions:

```
1.26.3
3.11.7 (main, Jan 16 2024, 14:42:22) [Clang 14.0.0 (clang-1400.0.29.202)]
```

### Runtime Environment:

```
[{'numpy_version': '1.26.3',
  'python': '3.11.7 (main, Jan 16 2024, 14:42:22) [Clang 14.0.0 '
            '(clang-1400.0.29.202)]',
  'uname': uname_result(system='Darwin', node='ArmBook.local', release='21.6.0', version='Darwin Kernel Version 21.6.0: Sun Nov  6 23:29:57 PST 2022; root:xnu-8020.240.14~1/RELEASE_ARM64_T8101', machine='arm64')},
 {'simd_extensions': {'baseline': ['NEON', 'NEON_FP16', 'NEON_VFPV4', 'ASIMD'],
                      'found': ['ASIMDHP'],
                      'not_found': ['ASIMDFHM']}},
 {'architecture': 'armv8',
  'filepath': '/opt/homebrew/lib/python3.11/site-packages/numpy/.dylibs/libopenblas64_.0.dylib',
  'internal_api': 'openblas',
  'num_threads': 8,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.23.dev'}]
None
```

### Context for the issue:

",2024-01-20 18:06:47,,BUG: branch choices for `geomspace` on complex arguments,['00 - Bug']
25642,open,loqs,"### Describe the issue:

SVML includes assembler that requires instrumentation / annotation to support CET. Somewhat similar to annotation for non-executable stack support https://github.com/numpy/SVML/commit/dd60c04e13f922676f2fd80189341ef2d9237ef4.  For specific requirements see https://www.intel.com/content/dam/develop/external/us/en/documents/catc17-introduction-intel-cet-844137.pdf or the CET chapter of Intel® 64 and IA-32 Architectures Software Developer’s Manual.  I did not report this on https://github.com/numpy/SVML as it has no issue tracker.

### Reproduce the code example:

```python
LDFLAGS='-Wl,-z,cet-report=error' python -m build --wheel --no-isolation -Csetup-args=""-Dblas=cblas"" -Csetup-args=""-Dlapack=lapack""
```


### Error message:

```shell
[497/498] Linking target numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so
FAILED: numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so 
c++  -o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/meson-generated_arraytypes.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/meson-generated_einsum.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/meson-generated_einsum_sumprod.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/meson-generated_lowlevel_strided_loops.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/meson-generated_nditer_templ.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/meson-generated_scalartypes.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/meson-generated_loops.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/meson-generated_matmul.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/meson-generated_scalarmath.c.o ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_acos_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_acos_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_acos_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_acosh_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_acosh_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_acosh_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_asin_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_asin_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_asin_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_asinh_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_asinh_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_asinh_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_atan2_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_atan2_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_atan2_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_atan_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_atan_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_atan_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_atanh_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_atanh_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_atanh_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_cbrt_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_cbrt_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_cbrt_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_cos_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_cos_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_cos_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_cosh_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_cosh_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_cosh_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_exp2_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_exp2_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_exp2_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_exp_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_exp_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_exp_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_expm1_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_expm1_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_expm1_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_log10_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_log10_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_log10_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_log1p_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_log1p_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_log1p_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_log2_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_log2_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_log2_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_log_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_log_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_log_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_pow_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_pow_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_pow_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_sin_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_sin_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_sin_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_sinh_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_sinh_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_sinh_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_tan_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_tan_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_tan_d_ha.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_tanh_d_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_tanh_s_la.s ../../numpy/core/src/umath/svml/linux/avx512/svml_z0_tanh_d_ha.s numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_abstractdtypes.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_alloc.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_arrayobject.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_array_coercion.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_array_method.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_array_assign_scalar.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_array_assign_array.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_arrayfunction_override.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_buffer.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_calculation.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_compiled_base.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_common.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_common_dtype.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_convert.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_convert_datatype.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_conversion_utils.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_ctors.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_datetime.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_datetime_strings.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_datetime_busday.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_datetime_busdaycal.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_descriptor.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_dlpack.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_dtypemeta.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_dragon4.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_dtype_transfer.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_dtype_traversal.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_experimental_public_dtype_api.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_flagsobject.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_getset.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_hashdescr.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_item_selection.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_iterators.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_legacy_dtype_implementation.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_mapping.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_methods.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_multiarraymodule.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_nditer_api.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_nditer_constr.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_nditer_pywrap.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_number.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_refcount.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_sequence.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_shape.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_scalarapi.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_strfuncs.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_temp_elide.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_typeinfo.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_usertypes.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_vdot.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_npysort_quicksort.cpp.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_npysort_mergesort.cpp.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_npysort_timsort.cpp.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_npysort_heapsort.cpp.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_npysort_radixsort.cpp.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_npysort_selection.cpp.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_npysort_binsearch.cpp.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_textreading_conversions.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_textreading_field_types.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_textreading_growth.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_textreading_readtext.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_textreading_rows.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_textreading_stream_pyobject.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_textreading_str_to_int.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_multiarray_textreading_tokenize.cpp.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_npymath_arm64_exports.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_common_array_assign.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_common_mem_overlap.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_common_npy_argparse.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_common_npy_hashtable.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_common_npy_longdouble.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_common_ucsnarrow.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_common_ufunc_override.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_common_numpyos.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_common_npy_cpu_features.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_common_cblasfuncs.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_common_python_xerbla.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_umath_ufunc_type_resolution.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_umath_clip.cpp.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_umath_dispatching.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_umath_extobj.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_umath_legacy_array_method.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_umath_override.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_umath_reduction.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_umath_ufunc_object.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_umath_umathmodule.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_umath_string_ufuncs.cpp.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_umath_wrapping_array_method.c.o numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p/src_umath__scaled_float_dtype.c.o -Wl,--as-needed -Wl,--allow-shlib-undefined -Wl,-O1 -shared -fPIC -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -flto=auto -Wl,-z,cet-report=error -march=x86-64 -mtune=generic -O2 -pipe -fno-plt -fexceptions -Wp,-D_FORTIFY_SOURCE=2 -fstack-clash-protection -fcf-protection -Wformat -Werror=format-security -Wp,-D_GLIBCXX_ASSERTIONS -g -ffile-prefix-map=/build/python-numpy/src=/usr/src/debug/python-numpy -flto=auto -ffat-lto-objects -Wl,--start-group numpy/core/libnpymath.a numpy/core/lib_multiarray_umath_mtargets.a /usr/lib/libcblas.so -Wl,--end-group
../../numpy/core/src/multiarray/experimental_public_dtype_api.c:389:1: warning: type of ‘PyUFunc_AddLoopFromSpec’ does not match original declaration [-Wlto-type-mismatch]
  389 | PyUFunc_AddLoopFromSpec(PyUFuncObject *ufunc, PyObject *info, int ignore_duplicate);
      | ^
../../numpy/core/src/umath/dispatching.c:154:1: note: type mismatch in parameter 3
  154 | PyUFunc_AddLoopFromSpec(PyObject *ufunc, PyArrayMethod_Spec *spec)
      | ^
../../numpy/core/src/umath/dispatching.c:154:1: note: type ‘void’ should match type ‘int’
../../numpy/core/src/umath/dispatching.c:154:1: note: ‘PyUFunc_AddLoopFromSpec’ was previously declared here
../../numpy/core/src/multiarray/compiled_base.h:15:1: warning: type of ‘arr_interp_complex’ does not match original declaration [-Wlto-type-mismatch]
   15 | arr_interp_complex(PyObject *, PyObject *const *, Py_ssize_t, PyObject *, PyObject *);
      | ^
../../numpy/core/src/multiarray/compiled_base.c:667:1: note: type mismatch in parameter 5
  667 | arr_interp_complex(PyObject *NPY_UNUSED(self), PyObject *const *args, Py_ssize_t len_args,
      | ^
../../numpy/core/src/multiarray/compiled_base.c:667:1: note: ‘arr_interp_complex’ was previously declared here
../../numpy/core/src/multiarray/compiled_base.h:13:1: warning: type of ‘arr_interp’ does not match original declaration [-Wlto-type-mismatch]
   13 | arr_interp(PyObject *, PyObject *const *, Py_ssize_t, PyObject *, PyObject *);
      | ^
../../numpy/core/src/multiarray/compiled_base.c:497:1: note: type mismatch in parameter 5
  497 | arr_interp(PyObject *NPY_UNUSED(self), PyObject *const *args, Py_ssize_t len_args,
      | ^
../../numpy/core/src/multiarray/compiled_base.c:497:1: note: ‘arr_interp’ was previously declared here
/usr/bin/ld: /tmp/ccdesNTJ.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/cc1AdOSy.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/cclPkj1Q.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccPROftU.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccZcsBiE.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/cchxKNoC.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/cc6RJeDn.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/cc4djqBJ.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccnjTr5k.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccWovo3i.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/cc6bj2ev.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccrtRE3y.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccUWVVhv.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/cc6zPwzb.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccF4pYxP.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccKQ7Wfm.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccVvCbmC.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccG8crii.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccekIpup.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccaFudNQ.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/cclk12ZQ.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/cc2J849w.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/cc1iLD1R.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/cct0Jz6v.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccRnDONn.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccbCrjVs.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccwKfhtB.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/cceKWOn4.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccQgNpvd.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccNl7wUH.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccnPS8mt.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccyHBNNE.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccIv0Gp1.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccpYsIHP.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccOh9mIQ.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccxDwKHZ.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccIcXIIS.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccWdLJQV.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccrXlQKb.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccYk7ged.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/cchOzA5h.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccEtSdJB.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccjpU3B8.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/cchbjt8w.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccNNbOym.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccx7J5Ka.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccrCwfxe.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccbpwazx.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/cc8HoZ5c.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccpc4fDs.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/cc4geSEV.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccncpsKo.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccPqNxBH.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccKrR1eP.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/cceXnMkA.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccgLeIaP.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccYOkuj5.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/cc4jwQ6C.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccawrqPz.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/cc8mrCQY.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccf6MxzS.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccT7LwbJ.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccASmgX3.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccYl0HY1.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/cc3c0DQB.o: error: missing IBT and SHSTK properties
/usr/bin/ld: /tmp/ccucSyfa.o: error: missing IBT and SHSTK properties
collect2: error: ld returned 1 exit status
[498/498] Linking target numpy/random/_generator.cpython-311-x86_64-linux-gnu.so
ninja: build stopped: subcommand failed.
```


### Python and NumPy Versions:

```
>>> import sys, numpy; print(numpy.__version__); print(sys.version)
1.26.3
3.11.6 (main, Nov 14 2023, 09:36:21) [GCC 13.2.1 20230801]
```

### Runtime Environment:

```
>>> import numpy; print(numpy.show_runtime())
[{'numpy_version': '1.26.3',
  'python': '3.11.6 (main, Nov 14 2023, 09:36:21) [GCC 13.2.1 20230801]',
  'uname': uname_result(system='Linux', node='arch', release='6.6.12-1-stable', version='#1 SMP Fri, 19 Jan 2024 15:03:04 +0000', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL',
                                    'AVX512_SPR']}}]
None
```

### Context for the issue:

Can be worked around by disabling SVML with `-Csetup-args=""-Ddisable-svml=true""`
See also https://github.com/numpy/numpy/issues/24221 for how the issue can be produced at run time if LDFLAGS='-Wl,-z,cet-report=error' is not used to detect such issues at build time.",2024-01-20 14:54:15,,BUG: SVML lacks CET support,['00 - Bug']
25640,open,ngoldbaum,"### Proposed new feature or change:

Right now `np.append` will convert both operands to an array first, then combine the result. This produces suboptimal results for stringdtype and a scalar operand:

```
Pdb) np.append(stringdtype_array, ""hello"")
*** ValueError: Cannot find common instance for unequal dtype instances
```

If instead we cast the second argument to the dtype of the first argument, this would succeed.

Note that this problem is particular to `StringDType`, but relying on numpy coercing the arguments to built-in dtypes is probably less forward looking than casting to the dtype of the first argument.",2024-01-19 21:55:38,,ENH: np.append should cast appended values to the dtype of the first argument,['unlabeled']
25637,open,seberg,"The new ufuncs added by Marten in gh-25536 could use two improvements:
1. Using C++ should help cut down a lot on code duplication
2. Using the new API would make some paths nicer (e.g. error handling).  It would also allow to improve the way temporary memory is managed.  (The place to look at is any of the string ufuncs.  These are not gufuncs, but that shouldn't really matter much.)

Some of these improvements were discussed on the PR.",2024-01-19 16:53:33,,"ENH,MAINT: FFT Gufuncs could be improved with C++ and new API",['unlabeled']
25635,open,crusaderky,"### Describe the issue:

In numpy 2.0 nightly, applying sqrt() to a fully masked scalar array undoes the mask.
numpy 1.26.3 behaves correctly.
Other ufuncs seem to behave correctly (but I haven't performed a thorough test for all of them).

### Reproduce the code example:

```python
>>> import numpy as np
>>> a = np.ma.masked_array(1, mask=True)
>>> type(a)
numpy.ma.core.MaskedArray

>>> np.sqrt(a)
# numpy 2.0
masked_array(data=1.,
             mask=False,
             fill_value=1e+20)
# numpy 1.26.3
masked_array(data=--,
             mask=True,
       fill_value=1e+20,
            dtype=float64)

>>> np.sqrt(np.ma.masked)
# numpy 2.0
masked_array(data=0.,
             mask=False,
             fill_value=1e+20)
# numpy 1.26.3
masked_array(data=--,
             mask=True,
       fill_value=1e+20,
            dtype=float64)
```


### Error message:

_No response_

### Python and NumPy Versions:

CPython 3.12.1 x86_64
numpy 2.0.0.dev0+git20240113.d2f60ff

from
python -m pip install --no-deps --pre -i https://pypi.anaconda.org/scientific-python-nightly-wheels/simple numpy 
as of 2024-01-19



### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2024-01-19 14:40:03,,BUG: `sqrt(ma.masked)` returns non-masked array,"['00 - Bug', '06 - Regression']"
25626,open,QuLogic,"### Describe the issue:

6 out of 16 tests fail in `test_print.py` when running on Windows on ARM. They appear to all be related to complex numbers. If I've read the results correctly, it only affects `np.cdouble` and `np.clongdouble` and not `np.complex64`.

### Reproduce the code example:

```python
pytest numpy/_core/tests/test_print.py
```


### Error message:

```shell
__________ test_complex_types[complex128] __________

tp = <class 'numpy.complex128'>

    @pytest.mark.parametrize('tp', [np.complex64, np.cdouble, np.clongdouble])
    def test_complex_types(tp):
        """"""Check formatting of complex types.

            This is only for the str function, and only for simple types.
            The precision of np.float32 and np.longdouble aren't the same as the
            python float precision.

        """"""
        for x in [0, 1, -1, 1e20]:
            assert_equal(str(tp(x)), str(complex(x)),
                         err_msg='Failed str formatting for type %s' % tp)
            assert_equal(str(tp(x*1j)), str(complex(x*1j)),
                         err_msg='Failed str formatting for type %s' % tp)
>           assert_equal(str(tp(x + x*1j)), str(complex(x + x*1j)),
                         err_msg='Failed str formatting for type %s' % tp)
E           AssertionError:
E           Items are not equal: Failed str formatting for type <class 'numpy.complex128'>
E            ACTUAL: '(1+0j)'
E            DESIRED: '(1+1j)'

tp         = <class 'numpy.complex128'>
x          = 1

numpy\_core\tests\test_print.py:65: AssertionError
__________ test_complex_types[clongdouble] __________

tp = <class 'numpy.clongdouble'>

    @pytest.mark.parametrize('tp', [np.complex64, np.cdouble, np.clongdouble])
    def test_complex_types(tp):
        """"""Check formatting of complex types.

            This is only for the str function, and only for simple types.
            The precision of np.float32 and np.longdouble aren't the same as the
            python float precision.

        """"""
        for x in [0, 1, -1, 1e20]:
            assert_equal(str(tp(x)), str(complex(x)),
                         err_msg='Failed str formatting for type %s' % tp)
            assert_equal(str(tp(x*1j)), str(complex(x*1j)),
                         err_msg='Failed str formatting for type %s' % tp)
>           assert_equal(str(tp(x + x*1j)), str(complex(x + x*1j)),
                         err_msg='Failed str formatting for type %s' % tp)
E           AssertionError:
E           Items are not equal: Failed str formatting for type <class 'numpy.clongdouble'>
E            ACTUAL: '(1+0j)'
E            DESIRED: '(1+1j)'

tp         = <class 'numpy.clongdouble'>
x          = 1

numpy\_core\tests\test_print.py:65: AssertionError
__________ test_complex_inf_nan[complex128] __________

dtype = <class 'numpy.complex128'>

    @pytest.mark.parametrize('dtype', [np.complex64, np.cdouble, np.clongdouble])
    def test_complex_inf_nan(dtype):
        """"""Check inf/nan formatting of complex types.""""""
        TESTS = {
            complex(np.inf, 0): ""(inf+0j)"",
            complex(0, np.inf): ""infj"",
            complex(-np.inf, 0): ""(-inf+0j)"",
            complex(0, -np.inf): ""-infj"",
            complex(np.inf, 1): ""(inf+1j)"",
            complex(1, np.inf): ""(1+infj)"",
            complex(-np.inf, 1): ""(-inf+1j)"",
            complex(1, -np.inf): ""(1-infj)"",
            complex(np.nan, 0): ""(nan+0j)"",
            complex(0, np.nan): ""nanj"",
            complex(-np.nan, 0): ""(nan+0j)"",
            complex(0, -np.nan): ""nanj"",
            complex(np.nan, 1): ""(nan+1j)"",
            complex(1, np.nan): ""(1+nanj)"",
            complex(-np.nan, 1): ""(nan+1j)"",
            complex(1, -np.nan): ""(1+nanj)"",
        }
        for c, s in TESTS.items():
>           assert_equal(str(dtype(c)), s)
E           AssertionError:
E           Items are not equal:
E            ACTUAL: '(inf+0j)'
E            DESIRED: '(inf+1j)'

TESTS      = {(inf+0j): '(inf+0j)', infj: 'infj', (-inf+0j): '(-inf+0j)', -infj: '-infj', ...}
c          = (inf+1j)
dtype      = <class 'numpy.complex128'>
s          = '(inf+1j)'

numpy\_core\tests\test_print.py:99: AssertionError
__________ test_complex_inf_nan[clongdouble] __________

dtype = <class 'numpy.clongdouble'>

    @pytest.mark.parametrize('dtype', [np.complex64, np.cdouble, np.clongdouble])
    def test_complex_inf_nan(dtype):
        """"""Check inf/nan formatting of complex types.""""""
        TESTS = {
            complex(np.inf, 0): ""(inf+0j)"",
            complex(0, np.inf): ""infj"",
            complex(-np.inf, 0): ""(-inf+0j)"",
            complex(0, -np.inf): ""-infj"",
            complex(np.inf, 1): ""(inf+1j)"",
            complex(1, np.inf): ""(1+infj)"",
            complex(-np.inf, 1): ""(-inf+1j)"",
            complex(1, -np.inf): ""(1-infj)"",
            complex(np.nan, 0): ""(nan+0j)"",
            complex(0, np.nan): ""nanj"",
            complex(-np.nan, 0): ""(nan+0j)"",
            complex(0, -np.nan): ""nanj"",
            complex(np.nan, 1): ""(nan+1j)"",
            complex(1, np.nan): ""(1+nanj)"",
            complex(-np.nan, 1): ""(nan+1j)"",
            complex(1, -np.nan): ""(1+nanj)"",
        }
        for c, s in TESTS.items():
>           assert_equal(str(dtype(c)), s)
E           AssertionError:
E           Items are not equal:
E            ACTUAL: '(inf+0j)'
E            DESIRED: '(inf+1j)'

TESTS      = {(inf+0j): '(inf+0j)', infj: 'infj', (-inf+0j): '(-inf+0j)', -infj: '-infj', ...}
c          = (inf+1j)
dtype      = <class 'numpy.clongdouble'>
s          = '(inf+1j)'

numpy\_core\tests\test_print.py:99: AssertionError
__________test_complex_type_print[complex128] __________

tp = <class 'numpy.complex128'>

    @pytest.mark.parametrize('tp', [np.complex64, np.cdouble, np.clongdouble])
    def test_complex_type_print(tp):
        """"""Check formatting when using print """"""
        # We do not create complex with inf/nan directly because the feature is
        # missing in python < 2.6
        for x in [0, 1, -1, 1e20]:
            _test_redirected_print(complex(x), tp)

        if tp(1e16).itemsize > 8:
            _test_redirected_print(complex(1e16), tp)
        else:
            ref = '(1e+16+0j)'
            _test_redirected_print(complex(1e16), tp, ref)

>       _test_redirected_print(complex(np.inf, 1), tp, '(inf+1j)')

tp         = <class 'numpy.complex128'>
x          = 1e+20

numpy\_core\tests\test_print.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

x = (inf+1j), tp = <class 'numpy.complex128'>, ref = '(inf+1j)'

    def _test_redirected_print(x, tp, ref=None):
        file = StringIO()
        file_tp = StringIO()
        stdout = sys.stdout
        try:
            sys.stdout = file_tp
            print(tp(x))
            sys.stdout = file
            if ref:
                print(ref)
            else:
                print(x)
        finally:
            sys.stdout = stdout

>       assert_equal(file.getvalue(), file_tp.getvalue(),
                     err_msg='print failed for type%s' % tp)
E       AssertionError:
E       Items are not equal: print failed for type<class 'numpy.complex128'>
E        ACTUAL: '(inf+1j)\n'
E        DESIRED: '(inf+0j)\n'

file       = <_io.StringIO object at 0x000001FEED9B0640>
file_tp    = <_io.StringIO object at 0x000001FEED9B0280>
ref        = '(inf+1j)'
stdout     = <_io.TextIOWrapper name='<tempfile._TemporaryFileWrapper object at 0x000001FEEAA7FA40>' mode='r+' encoding='utf-8'>
tp         = <class 'numpy.complex128'>
x          = (inf+1j)

numpy\_core\tests\test_print.py:118: AssertionError
__________ test_complex_type_print[clongdouble] __________

tp = <class 'numpy.clongdouble'>

    @pytest.mark.parametrize('tp', [np.complex64, np.cdouble, np.clongdouble])
    def test_complex_type_print(tp):
        """"""Check formatting when using print """"""
        # We do not create complex with inf/nan directly because the feature is
        # missing in python < 2.6
        for x in [0, 1, -1, 1e20]:
            _test_redirected_print(complex(x), tp)

        if tp(1e16).itemsize > 8:
            _test_redirected_print(complex(1e16), tp)
        else:
            ref = '(1e+16+0j)'
            _test_redirected_print(complex(1e16), tp, ref)

>       _test_redirected_print(complex(np.inf, 1), tp, '(inf+1j)')

tp         = <class 'numpy.clongdouble'>
x          = 1e+20

numpy\_core\tests\test_print.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

x = (inf+1j), tp = <class 'numpy.clongdouble'>, ref = '(inf+1j)'

    def _test_redirected_print(x, tp, ref=None):
        file = StringIO()
        file_tp = StringIO()
        stdout = sys.stdout
        try:
            sys.stdout = file_tp
            print(tp(x))
            sys.stdout = file
            if ref:
                print(ref)
            else:
                print(x)
        finally:
            sys.stdout = stdout

>       assert_equal(file.getvalue(), file_tp.getvalue(),
                     err_msg='print failed for type%s' % tp)
E       AssertionError:
E       Items are not equal: print failed for type<class 'numpy.clongdouble'>
E        ACTUAL: '(inf+1j)\n'
E        DESIRED: '(inf+0j)\n'

file       = <_io.StringIO object at 0x000001FEED9B0C40>
file_tp    = <_io.StringIO object at 0x000001FEED9B0B80>
ref        = '(inf+1j)'
stdout     = <_io.TextIOWrapper name='<tempfile._TemporaryFileWrapper object at 0x000001FEEAA7FA40>' mode='r+' encoding='utf-8'>
tp         = <class 'numpy.clongdouble'>
x          = (inf+1j)

numpy\_core\tests\test_print.py:118: AssertionError
```


### Python and NumPy Versions:

2.0.0.dev0+git20240118.a7c6be5
3.12.1 (tags/v3.12.1:2305ca5, Dec  7 2023, 22:12:47) [MSC v.1937 64 bit (ARM64)]


### Runtime Environment:

```
[{'numpy_version': '2.0.0.dev0+git20240118.a7c6be5',
  'python': '3.12.1 (tags/v3.12.1:2305ca5, Dec  7 2023, 22:12:47) [MSC v.1937 '
            '64 bit (ARM64)]',
  'uname': uname_result(system='Windows', node='mars', release='11', version='10.0.22621', machine='ARM64')},
 {'simd_extensions': {'baseline': ['NEON', 'NEON_FP16', 'NEON_VFPV4', 'ASIMD'],
                      'found': [],
                      'not_found': []}}]
None
```
(PS, this function prints already, so you can remove `print` from the PR template.)

### Context for the issue:

I've started working on building Matplotlib on Windows on Arm.",2024-01-19 08:38:15,,BUG: Complex printing tests fail on Windows ARM64,['00 - Bug']
25623,open,JasonGross,"### Proposed new feature or change:

https://github.com/scipy/scipy/pull/19549 is blocked on having `argmax` support all `axis` arguments that `max` supports.  The specification of tuple argmax is that `np.take_along_axis(x, np.argmax(x, axis=axis, keepdims=True), axis=axis)` should always equal `np.max(x, axis=axis, keepdims=True)`.

See also https://github.com/numpy/numpy/issues/25622 and https://github.com/numpy/numpy/issues/9283, where @eric-wieser eventually proposes this constraint in https://github.com/numpy/numpy/issues/9283#issuecomment-419615634",2024-01-18 23:52:46,,"ENH: `np.argmax`, `np.argmin`, `np.take_along_axis`, and `np.put_along_axis` should support tuple `axis` a la `np.max` and `np.min`",['unlabeled']
25622,open,JasonGross,"### Proposed new feature or change:

Consider the code
```python
import numpy as np
x = np.array([[0, 1], [2, 3]])
x_argmax1 = np.argmax(x, axis=None, keepdims=True)
x_argmax2 = np.argmax(x, axis=None, keepdims=False)
print(np.take_along_axis(x, x_argmax1, axis=None))
print(np.take_along_axis(x, x_argmax2, axis=None))
```
They both give
```
File ~/.local64/mambaforge/envs/default/lib/python3.11/site-packages/numpy/lib/shape_base.py:170, in take_along_axis(arr, indices, axis)
    167     arr_shape = arr.shape
    169 # use the fancy index
--> 170 return arr[_make_along_axis_idx(arr_shape, indices, axis)]

File ~/.local64/mambaforge/envs/default/lib/python3.11/site-packages/numpy/lib/shape_base.py:32, in _make_along_axis_idx(arr_shape, indices, axis)
     30     raise IndexError('`indices` must be an integer array')
     31 if len(arr_shape) != indices.ndim:
---> 32     raise ValueError(
     33         ""`indices` and `arr` must have the same number of dimensions"")
     34 shape_ones = (1,) * indices.ndim
     35 dest_dims = list(range(axis)) + [None] + list(range(axis+1, indices.ndim))

ValueError: `indices` and `arr` must have the same number of dimensions
```
This can be worked around by using
```python
print(np.take_along_axis(x, x_argmax1.flatten(), axis=None))
print(np.take_along_axis(x, x_argmax2.flatten(), axis=None))
```
but it would be nice if `np.take_along_axis(x, np.argmax(x, axis=axis, keepdims=True), axis=axis)` were always equal to `np.max(x, axis=axis, keepdims=True)`",2024-01-18 23:43:45,,"ENH: `np.argmax` and `np.argmin` with `axis=None, keepdims=True` should be compatible with `np.take_along_axis` and `np.put_along_axis`",['unlabeled']
25621,open,rgommers,"In-place operators typically behave the same as their out-of-place equivalents (excluding what happens with views). For example, if `x` and `y` are  `float64` arrays, then `x += y` and `x = x + y` are equivalent statements. However, something unexpected may happen when the right-hand operand has a dtype that would normally cause upcasting when combined with the left-hand operand according to NumPy's casting rules. In such cases, the in-place version doesn't upcast the left-hand operand, but downcasts the right-hand operand. To illustrate the difference:

```python
>>> # The in-place version:
>>> a = np.array(1, dtype=np.int8)
>>> a += np.array(2**12, dtype=np.int16)
>>> a
array(1, dtype=int8)

>>> # The out-of-place version:
>>> a + np.array(2**12, dtype=np.int16)
4097
>>> (a + np.array(2**12, dtype=np.int16)).dtype
dtype('int16')
```
This yields silently incorrect results, and seems undesirable.

It is already forbidden to do this with dtypes that aren't the same kind:
```python
>>> a += np.array(2**12, dtype=np.float16)
...
UFuncTypeError: Cannot cast ufunc 'add' output from dtype('float16') to dtype('int8') with casting rule 'same_kind'
```

It seems like it would be safer if the behavior would only allow `'safe'` rather than `'same_kind'` casting. Making that change would avoid the silently incorrect results, and recover the property that in-place and out-of-place statements act the same.

This came up in the review of NEP 56 (gh-25542). We decided to punt on it there, since it's a little unclear how impactful deprecating unsafe casts are, and there is no hurry in making this change. It does seem desirable though.

The right course of action here is probably to implement this change in a branch, and then seeing what fails in test suites of downstream projects to assess the impact.",2024-01-18 21:11:41,,Deprecating in-place operations where the out-of-place equivalent would upcast?,"['component: numpy._core', '07 - Deprecation']"
25616,open,broukema,"### Issue with current documentation:

https://numpy.org/doc/stable/f2py does not seem to state that (i)  multiple intent(out) parameters should be given as a list at the python level, and (ii) array sizes explicitly listed in the fortran subroutine have to be either ignored or shifted to the end of the python parameter list.

### Idea or request for content:

The documentation of `f2py` would be clearer if there were examples illustrating how (i) multiple `intent(out)` fortran parameters can be given as a python `list` left-hand value, and probably more importantly (ii) array sizes defined in the fortran subroutine e.g. with `dimension(n)` should either be ignored at the python level or optionally listed after the array parameters. Although case (i) for a single `intent(out)` parameter is given, I couldn't find the case of multiple `intent(out)` parameters or array sizes anywhere in the [main f2py web documentation](https://numpy.org/doc/stable/f2py).

With `python3 3.11.2-1+b1` `python3-numpy 1:1.24.2-1` on Debian/bookworm, the following fortran and python program illustrate (i) and (ii). Compiling and running either the fortran program alone with `gfortran 12.2.0-14` or compiling with `f2py` give return values of 0 to the shell for me, i.e. both pass all checks.

Case (ii) seems to be a feature, not a bug - since the python calling procedure has to know the size of the array before feeding it to the fortran side, the size does not have to be stated. But it would be useful to have it in the main documentation rather than have users discover it by trial and error. :)

````
! learn_f2py - simple program for checking how/if f2py works - fortran backend
!
!     Copyright (C) 2024 Boud Roukema
!     All rights reserved.
! 
! Redistribution and use in source and binary forms, with or without
! modification, are permitted provided that the following conditions are
! met:
! 
!     * Redistributions of source code must retain the above copyright
!        notice, this list of conditions and the following disclaimer.
! 
!     * Redistributions in binary form must reproduce the above
!        copyright notice, this list of conditions and the following
!        disclaimer in the documentation and/or other materials provided
!        with the distribution.
! 
!     * Neither the name of the NumPy Developers nor the names of any
!        contributors may be used to endorse or promote products derived
!        from this software without specific prior written permission.
! 
! THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
! ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
! LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
! A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
! OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
! SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
! LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
! DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
! THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
! (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
! OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
!------------------------------------
      program learn_f2py
      integer :: k
      integer, dimension(3) :: j_array

      m_expected = 720

      k = 55
      j_size = 3
      j_array(2) = 845
      
      call get_fact(k, j_size, j_array, m_result, mplus9)
      if(m_result .eq. m_expected)then
         i_pass = 0
      else
         i_pass = 1
      endif
      write(6,'(a,i10)')'i_pass = ',i_pass

#ifdef __GFORTRAN__
      stop(i_pass)
#endif
      end

      subroutine get_fact(k, j_size, j_array, m_result, mplus9)
!f2py integer, intent(out) :: m_result, mplus9
!f2py integer, intent(in) :: k, j_size
      integer, dimension(j_size) :: j_array
!f2py intent(in) :: j_array
! implicit typing
      m_result=1                   ! to hold the result; override input value
      n=6
      do i=1,n
         !m_result = m_result*i
         m_result = mult(m_result,i)
      enddo

      nine = (k + j_array(2))/100 ! should be (55 + 845)/100 = 9
      
      mplus9 = m_result + nine ! illustrate how to get a second return value
      write(6,'(a,i10,a,i10)')'learn_f2py: ',n,'!  = ',m_result
      write(6,'(a,i10,a,i10)')'learn_f2py: ',n,'! + 9  = ',mplus9

      return
      end

      function mult(m,i)
!f2py intent(in) m,i
      mult = m*i
!      i=i+1 ! modifying an input value is allowed in fortran, but unwise 
      return
      end
````

````
# ! learn_f2py - simple program for checking how/if f2py works - python frontend
#
#     Copyright (C) 2024 Boud Roukema
#     All rights reserved.
# 
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are
# met:
# 
#     * Redistributions of source code must retain the above copyright
#        notice, this list of conditions and the following disclaimer.
# 
#     * Redistributions in binary form must reproduce the above
#        copyright notice, this list of conditions and the following
#        disclaimer in the documentation and/or other materials provided
#        with the distribution.
# 
#     * Neither the name of the NumPy Developers nor the names of any
#        contributors may be used to endorse or promote products derived
#        from this software without specific prior written permission.
# 
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
#------------------------------------

import sys
import numpy as np

# import the .so shared object library that includes the subroutine get_fact()
import learn_f2py

print(learn_f2py.__doc__)

print(""Python frontend starting ..."")
# implicit declarations, python style
m_expected = 720

k = 55
#j_array = (45, 845, 1845) # python list object
j_array = np.array([45, 845, 1845]) # numpy array object
j_size = 3

# Although the fortran subroutine is declared to have five parameters,
# one of them is an array size, and two of them are outputs; so the
# python frontend gives the two outputs as the left-hand side of the
# call to the subroutine, and can choose to *not* inform the fortran
# subroutine of the array size via j_size:
m_result_py, mplus9_py = learn_f2py.get_fact(k, j_array)

print(""python: m_result_py, mplus9_py = "", m_result_py,mplus9_py)

if(m_result_py == m_expected):
    pass_fail_code = 0
else:
    pass_fail_code = 1

# This time we will explicitly feed the array size to the fortran subroutine:
m_result_py, mplus9_py = learn_f2py.get_fact(k, j_array, j_size)

print(""python: m_result_py, mplus9_py = "", m_result_py,mplus9_py)

if(m_result_py == m_expected):
    pass_fail_code += 0
else:
    pass_fail_code += 2
    
print(""pass_fail_code = "",pass_fail_code)

sys.exit(pass_fail_code)
````


I licensed these with the standard Numpy BSD-3-clause licence, so that anyone is welcome to copy/paste/edit into Numpy documentation if desired under ([the default Numpy LICENSE.txt](https://github.com/numpy/numpy/blob/main/LICENSE.txt), i.e. for licensing purposes, I would be considered one of the Numpy developers for this minute contribution).
",2024-01-18 20:03:09,,DOC: f2py needs to explain fortran vs python differences in parameter lists,"['04 - Documentation', 'component: numpy.f2py']"
25612,open,MartinKaercher,"Hi all,
I hope this is the right spot for bringing this up. `numpy.polynomial.polynomial.polyfit` requires `x`-values to be dimension `(M,)` and `y`-values to be either `(M,)` or `(M,K)` where in the latter case a polynomial is fitted `K`-times.
Now, weights `w` have to be specified with dimension `(M,)` regardless of the dimension of `y`-values. I would expect that when using `y`-values with dimension `(M,K)` also separate weights for each of the `K` data sets are allowed hence the dimension of `w` being `(M,K)`.
Is there any reason why this is not implemented?
My guess is, by looking at the source code in `polyutils.py`, that because the Vandermonde matrix has to be multiplied by weights  a new Vandermonde matrix for each of the `K` datasets would be necessary, probably requiring a `for` loop.
Currently I am exactly doing that in my project, a `for` loop over the `K` datasets.
Thanks in advance for your insights.",2024-01-18 09:24:02,,DISCUSS: Dimensions of weights used in `numpy.polynomial.polynomial.polyfit`,['unlabeled']
25606,open,evanberkowitz,"### Describe the issue:

I expected reductions of subclasses of ndarray to return the same type as reductions of ndarray itself.  For example, in the snippet shown below, I the `.sum()` and `.std()` of an integer-valued ndarray have type `numpy.int64` and `numpy.float64`.  But, the same reductions of a subclass instead give values wrapped by the subclass, instead of the 'bare' data types.

### Reproduce the code example:

```python
import numpy as np

a = np.arange(10)
print(type(a.sum()))
# <class 'numpy.int64'>
print(type(a.std()))
# <class 'numpy.float64'>

class RealisticInfoArray(np.ndarray):
    # Just lifted from
    # https://numpy.org/doc/stable/user/basics.subclassing.html

    def __new__(cls, input_array, info=None):
        # Input array is an already formed ndarray instance
        # We first cast to be our class type
        obj = np.asarray(input_array).view(cls)
        # add the new attribute to the created instance
        obj.info = info
        # Finally, we must return the newly created object:
        return obj

    def __array_finalize__(self, obj):
        # see InfoArray.__array_finalize__ for comments
        if obj is None: return
        self.info = getattr(obj, 'info', None)

b = RealisticInfoArray(a)
print(type(b.sum()))
# <class '__main__.RealisticInfoArray'>
print(type(b.std()))
# <class '__main__.RealisticInfoArray'>

# Here I was expecting, as above
# <class 'numpy.int64'>
# <class 'numpy.float64'>
# or even maybe int and float, respectively.
```


### Error message:

_No response_

### Python and NumPy Versions:

```
1.24.4
3.9.13 (main, Aug 25 2022, 18:29:29) 
[Clang 12.0.0 ]
```

### Runtime Environment:

I think this is not what was hoped for, and may be an issue unto itself, but that's for a different time.

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/var/folders/xs/n80hfp_j4zx4z6gfqjf05wwr0000gp/T/ipykernel_19584/667756739.py in <module>
----> 1 import numpy; print(numpy.show_runtime())

~/.local/lib/python3.9/site-packages/numpy/lib/utils.py in show_runtime()
     86     try:
     87         from threadpoolctl import threadpool_info
---> 88         config_found.extend(threadpool_info())
     89     except ImportError:
     90         print(""WARNING: `threadpoolctl` not found in system!""

~/opt/anaconda3/lib/python3.9/site-packages/threadpoolctl.py in threadpool_info()
    122     In addition, each module may contain internal_api specific entries.
    123     """"""
--> 124     return _ThreadpoolInfo(user_api=_ALL_USER_APIS).todicts()
    125 
    126 

~/opt/anaconda3/lib/python3.9/site-packages/threadpoolctl.py in __init__(self, user_api, prefixes, modules)
    338 
    339             self.modules = []
--> 340             self._load_modules()
    341             self._warn_if_incompatible_openmp()
    342         else:

~/opt/anaconda3/lib/python3.9/site-packages/threadpoolctl.py in _load_modules(self)
    369         """"""Loop through loaded libraries and store supported ones""""""
    370         if sys.platform == ""darwin"":
--> 371             self._find_modules_with_dyld()
    372         elif sys.platform == ""win32"":
    373             self._find_modules_with_enum_process_module_ex()

~/opt/anaconda3/lib/python3.9/site-packages/threadpoolctl.py in _find_modules_with_dyld(self)
    426 
    427             # Store the module if it is supported and selected
--> 428             self._make_module_from_path(filepath)
    429 
    430     def _find_modules_with_enum_process_module_ex(self):

~/opt/anaconda3/lib/python3.9/site-packages/threadpoolctl.py in _make_module_from_path(self, filepath)
    513             if prefix in self.prefixes or user_api in self.user_api:
    514                 module_class = globals()[module_class]
--> 515                 module = module_class(filepath, prefix, user_api, internal_api)
    516                 self.modules.append(module)
    517 

~/opt/anaconda3/lib/python3.9/site-packages/threadpoolctl.py in __init__(self, filepath, prefix, user_api, internal_api)
    604         self.internal_api = internal_api
    605         self._dynlib = ctypes.CDLL(filepath, mode=_RTLD_NOLOAD)
--> 606         self.version = self.get_version()
    607         self.num_threads = self.get_num_threads()
    608         self._get_extra_info()

~/opt/anaconda3/lib/python3.9/site-packages/threadpoolctl.py in get_version(self)
    644                              lambda: None)
    645         get_config.restype = ctypes.c_char_p
--> 646         config = get_config().split()
    647         if config[0] == b""OpenBLAS"":
    648             return config[1].decode(""utf-8"")

AttributeError: 'NoneType' object has no attribute 'split'
```

### Context for the issue:

This causes my software to serialize the resulting reductions in an unexpected, erroneous way.

If the behavior is intended, the documentation should explain how to remove the subclass type to coax plain data upon reduction.

If the behavior is not intended, it should be fixed.  Unfortunately, a change to the example that addresses my issue has proved beyond my tinkering because of numpy's unusual `__new__` and `__array_finalize__` approach.",2024-01-17 17:00:29,,"DOC: See if we should make a note of `__array_wrap__(..., return_scalar=True)` in subclassing docs",['04 - Documentation']
25597,open,rgommers,"`numpy.distutils` has been deprecated for a while, and there are better alternatives now. NumPy itself has switched to Meson. The `f2py` docs go above and beyond already and show how to use Meson, CMake and scikit-build: https://numpy.org/doc/stable/f2py/buildtools/index.html#build-systems.

Here is one example that is still using `numpy.distutils`: https://numpy.org/doc/stable/user/c-info.ufunc-tutorial.html. There may be others. It'd be fine to keep some docs around that use `setuptools` instead of `numpy.distutils`. The docs should probably simply use Meson in most places, and in cases that are more critical (like for `f2py`) mention or show that both Meson/meson-python and CMake/scikit-build-core are good options.",2024-01-16 17:38:27,,DOC: remove `numpy.distutils` usages from user-facing docs,"['17 - Task', '04 - Documentation', 'component: numpy.distutils', 'component: documentation']"
25589,open,mboll,"### Describe the issue:

This might be better described as ""unexpected behavior."" If you have an masked ndarray whose mask if comprised of only `False` values, the power operation will shrink the mask array down to a single bool. Other math operations do not do this (`+`, `-`, `/`, `*`), i.e., they preserve the shape of the mask array.

This introduces errors when you index the mask later on, expecting it to be an ndarray but it is just a bool.

The cause appears to be from [this line](https://github.com/numpy/numpy/blob/d35cd07ea997f033b2d89d349734c61f5de54b0d/numpy/ma/core.py#L6969) in the `numpy.ma.power` definition. The default `shrink` argument in `numpy.ma.mask_or` is `True`, and we are meeting those criteria in this case.

### Reproduce the code example:

```python
import numpy

# create an example masked array with an all-False mask
a = numpy.array((1, 2, 3))  # arbitrary data
b = numpy.zeros(3)  # all-False mask

m = numpy.ma.masked_array(a, b)


# preview the masked array, its mask has 3 Falses

m

# result:
# masked_array(data=[1, 2, 3],
#              mask=[False, False, False],
#        fill_value=999999)


# the multiply operator preserves the mask

m * 4


# result:
# masked_array(data=[4, 8, 12],
#              mask=[False, False, False],
#        fill_value=999999)


# the power operator shrinks the mask

m ** 2

# result:
# masked_array(data=[1, 4, 9],
#              mask=False,
#        fill_value=999999)
```


### Error message:

```shell
By default there is no error, but you will get an indexing error if you try to slice the mask in the future.
```


### Python and NumPy Versions:

Python 3.7 and 3.10.
Numpy 1.17, 1.24, 1.26

### Runtime Environment:

_No response_

### Context for the issue:

I came across this issue when simulating a vectorized grid of line-sphere intersections. I slice a section of the mask for future calculations, which raises an indexing exception in cases where the entire mask is False, because the power operations shrunk the mask from an array to a bool.

It is indeed possible to get around this by making a check on the mask after the power operation, but I lost some time trying to figure this out and I think other users may encounter similar problems.",2024-01-15 17:43:02,,"BUG: ma.power unexpectedly shrinks masks, but other operators don't",['00 - Bug']
25588,open,lorentzenchr,"### Describe the issue:

`np.take(a, indices, out=out)` fails if `out.dtype` is not exactly `a.dtype`. The doc says about `out`:
> It should be of the appropriate shape and dtype.

But what is *appropriate*?

### Reproduce the code example:

```python
import numpy as np
a = np.arange(3).astype(np.int32)
indices = np.arange(2)
out = np.zeros_like(indices, dtype=np.int64)
np.take(a, indices, out=out)
```


### Error message:

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
File ~/github/numpy/build-install/usr/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:59, in _wrapfunc(obj, method, *args, **kwds)
     58 try:
---> 59     return bound(*args, **kwds)
     60 except TypeError:
     61     # A TypeError occurs if the object does have such a method in its
     62     # class, but its signature is not identical to that of NumPy's. This
   (...)
     66     # Call _wrapit from within the except clause to ensure a potential
     67     # exception has a traceback chain.

TypeError: Cannot cast array data from dtype('int64') to dtype('int32') according to the rule 'safe'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
Cell In[4], line 1
----> 1 np.take(a, indices, out=out)

File ~/github/numpy/build-install/usr/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:192, in take(a, indices, axis, out, mode)
     95 @array_function_dispatch(_take_dispatcher)
     96 def take(a, indices, axis=None, out=None, mode='raise'):
     97     """"""
     98     Take elements from an array along an axis.
     99 
   (...)
    190            [5, 7]])
    191     """"""
--> 192     return _wrapfunc(a, 'take', indices, axis=axis, out=out, mode=mode)

File ~/github/numpy/build-install/usr/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:68, in _wrapfunc(obj, method, *args, **kwds)
     59     return bound(*args, **kwds)
     60 except TypeError:
     61     # A TypeError occurs if the object does have such a method in its
     62     # class, but its signature is not identical to that of NumPy's. This
   (...)
     66     # Call _wrapit from within the except clause to ensure a potential
     67     # exception has a traceback chain.
---> 68     return _wrapit(obj, method, *args, **kwds)

File ~/github/numpy/build-install/usr/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:45, in _wrapit(obj, method, *args, **kwds)
     43 except AttributeError:
     44     wrap = None
---> 45 result = getattr(asarray(obj), method)(*args, **kwds)
     46 if wrap:
     47     if not isinstance(result, mu.ndarray):

TypeError: Cannot cast array data from dtype('int64') to dtype('int32') according to the rule 'safe'
```


### Python and NumPy Versions:

'numpy_version': '2.0.0.dev0+git20240115.bbdd595'

### Runtime Environment:

not important

### Context for the issue:

`out = np.take(a, indices)` works, but passing the `out` arg explicitly saves intermediate memory and could be a tiny bit faster.",2024-01-15 14:48:47,,BUG: np.take cast to out argument,['00 - Bug']
25587,open,Laubeee,"### Issue with current documentation:

I JUST ran into this issue #22104 and it took me way too long to find that github-issue to finally see an example of how to use the Polynomial correctly. 

### Idea or request for content:

Adding an example (like the one given in #22104) [here](https://numpy.org/doc/stable/reference/routines.polynomials.html#quick-reference) and [here](https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.polyval.html), and perhaps a mention [here](https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.Polynomial.html#numpy.polynomial.polynomial.Polynomial) (e.g. adding ""evaluating the polynomial at positions"" instead of, or in additon to, ""Call self as a function"") would have been very helpful.

P.S. also the explanation
> Polynomial.fit uses scaled and shifted polynomials for numerical reasons. If you want the usual polynomials, you need to use the convert method

would be good to have in the docs.",2024-01-15 12:36:50,,DOC: missing examples for np.polynomial.polynomial.polyval,['04 - Documentation']
25580,open,lagru,"### Describe the issue:

I'm currently working on adapting skimage for NumPy 2.0 during which I was told to replace our usages of the former `np.core.numerictypes.obj2sctype(x)` with the equivalent `np.dtype(x).type`. However, it seems that if `x` is an generic dtype (I think that's what they are called) the result differs. The example below shows the difference on the current nightly wheel of NumPy.

### Reproduce the code example:

```python
import numpy as np
from numpy._core.numerictypes import obj2sctype
assert obj2sctype(np.floating), np.dtype(np.floating).type
```


### Error message:

```shell
(numpy.floating, numpy.float64)
```


### Python and NumPy Versions:

2.0.0.dev0+git20240109.6e3b923
3.12.1 | packaged by conda-forge | (main, Dec 23 2023, 08:03:24) [GCC 12.3.0]

### Runtime Environment:

<details><summary>Details</summary>
<p>

```
[{'numpy_version': '2.0.0.dev0+git20240109.6e3b923',
  'python': '3.12.1 | packaged by conda-forge | (main, Dec 23 2023, 08:03:24) '
            '[GCC 12.3.0]',
  'uname': uname_result(system='Linux', node='hue', release='6.6.10-arch1-1', version='#1 SMP PREEMPT_DYNAMIC Fri, 05 Jan 2024 16:20:41 +0000', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Haswell',
  'filepath': '/home/lg/.local/lib/micromamba/envs/skimagedev312/lib/python3.12/site-packages/numpy.libs/libopenblas64_p-r0-0cf96a72.3.23.dev.so',
  'internal_api': 'openblas',
  'num_threads': 8,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.23.dev'}]
None
```

</p>
</details> 

### Context for the issue:

I guess this is kind of an obscure edge case we are hitting in skimage, and I think I can figure out a way around it. But I still wanted to raise the issue in case this was something that was missed. Feel free to close this, if this isn't something to worry about. Though, I'd be curious if there is an alternative in NumPy 2.0 with which the original output can be revoverd.

Slightly related https://github.com/numpy/numpy/issues/17325.",2024-01-12 23:34:09,,BUG: `dtype(<generic>).type` has different output than deprecated `obj2sctype`?,['00 - Bug']
25564,open,pearu,"The issue of np.linalg.eigh returning wrong results or crashing is still real (using numpy 1.26.2):
```python
>>> import numpy as np
>>> n=32767
>>> b=np.random.rand(n)
>>> m_32767=np.diag(b)
>>> m_32767.shape
(32767, 32767)
>>> V_32767=np.linalg.eigh(m_32767)
 ** On entry to DSTEDC parameter number  8 had an illegal value
 ** On entry to DORMTR parameter number 12 had an illegal value
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/pearu/miniconda3/envs/jax-cuda-dev/lib/python3.11/site-packages/numpy/linalg/linalg.py"", line 1487, in eigh
    w, vt = gufunc(a, signature=signature, extobj=extobj)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/pearu/miniconda3/envs/jax-cuda-dev/lib/python3.11/site-packages/numpy/linalg/linalg.py"", line 118, in _raise_linalgerror_eigenvalues_nonconvergence
    raise LinAlgError(""Eigenvalues did not converge"")
numpy.linalg.LinAlgError: Eigenvalues did not converge
```
where the exception ""Eigenvalues did not converge"" is very likely wrong and misleading. With n == 32766, the above example works fine.

The underlying problem is that when computing `lwork` for the lapack `syevd` function using expression `1 + 6 * n + 2 * n * n`, it will overflow when `n == 32767`. This problem is not unique to `syevd` but it exists for all lapack functions that work array sizes are quadratic wrt input sizes.

While switching to lapack implementations that uses 64-bit integer inputs, the overflow issue is seemingly resolved but in fact it is just harder to reproduce because the critical `n` size will be `2147483647` where the issue re-merges when the `lwork` expression above will overflow for int64.

I have implemented a solution to the same problem in JAX (https://github.com/google/jax/pull/19288) that will lead to an overflow exception rather than wrong results or crashes. I think something similar is appropriate for NumPy as well.

_Originally posted by @pearu in https://github.com/numpy/numpy/issues/13956#issuecomment-1885191876_
            ",2024-01-10 17:49:39,,BUG: Silent int32 overflow in lapack work size computation leads to wrong exception,"['00 - Bug', 'component: numpy.linalg']"
25556,open,ngoldbaum,"When I try to build the current numpy main branch, gcc crashes with an internal compiler error inside highway's `vqsort`:



```
$ spin build --clean --  -Dbuildtype=debug -Db_sanitize=address
<snipping output until the error>
[185/333] Compiling C++ object numpy/_...c_npysort_highway_qsort.dispatch.cpp.o
FAILED: numpy/_core/libhighway_qsort.dispatch.h_SVE.a.p/src_npysort_highway_qsort.dispatch.cpp.o 
c++ -Inumpy/_core/libhighway_qsort.dispatch.h_SVE.a.p -Inumpy/_core -I../numpy/_core -Inumpy/_core/include -I../numpy/_core/include -I../numpy/_core/src/common -I../numpy/_core/src/multiarray -I../numpy/_core/src/npymath -I../numpy/_core/src/umath -I../numpy/_core/src/highway -I/home/goldbaum/.pyenv/versions/3.11.7-debug/include/python3.11d -I/home/goldbaum/numpy/build/meson_cpu -fdiagnostics-color=always -fsanitize=address -fno-omit-frame-pointer -Wall -Winvalid-pch -std=c++17 -O0 -g -fPIC -DNPY_INTERNAL_BUILD -DHAVE_NPY_CONFIG_H -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -D__STDC_VERSION__=0 -fno-exceptions -fno-rtti -O3 -DNPY_HAVE_NEON_VFPV4 -DNPY_HAVE_NEON_FP16 -DNPY_HAVE_NEON -DNPY_HAVE_ASIMD -DNPY_HAVE_ASIMDHP -DNPY_HAVE_SVE -march=armv8.2-a+sve+fp16 -DNPY_MTARGETS_CURRENT=SVE -MD -MQ numpy/_core/libhighway_qsort.dispatch.h_SVE.a.p/src_npysort_highway_qsort.dispatch.cpp.o -MF numpy/_core/libhighway_qsort.dispatch.h_SVE.a.p/src_npysort_highway_qsort.dispatch.cpp.o.d -o numpy/_core/libhighway_qsort.dispatch.h_SVE.a.p/src_npysort_highway_qsort.dispatch.cpp.o -c ../numpy/_core/src/npysort/highway_qsort.dispatch.cpp
during GIMPLE pass: sanopt
In file included from ../numpy/_core/src/npysort/highway_qsort.dispatch.cpp:3:
../numpy/_core/src/highway/hwy/contrib/sort/vqsort-inl.h: In function \u2018void hwy::N_SVE::detail::Recurse(D, Traits, T*, size_t, T*, uint64_t*, size_t) [with D = hwy::N_SVE::Simd<long unsigned int, 32, 0>; Traits = hwy::N_SVE::detail::SharedTraits<hwy::N_SVE::detail::TraitsLane<hwy::N_SVE::detail::OrderAscending<long unsigned int> > >; T = long unsigned int]\u2019:
../numpy/_core/src/highway/hwy/contrib/sort/vqsort-inl.h:1571:19: internal compiler error: in asan_expand_mark_ifn, at asan.c:3726
 1571 | HWY_NOINLINE void Recurse(D d, Traits st, T* HWY_RESTRICT keys,
      |                   ^~~~~~~
0xffff930073fb __libc_start_call_main
	../sysdeps/nptl/libc_start_call_main.h:58
0xffff930074cb __libc_start_main_impl
	../csu/libc-start.c:392
Please submit a full bug report,
with preprocessed source if appropriate.
Please include the complete backtrace with any bug report.
See <file:///usr/share/doc/gcc-11/README.Bugs> for instructions.
```

```
goldbaum@ubuntuvm:~/numpy$ gcc --version
gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Copyright (C) 2021 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```

@Mousius should I be reporting this upstream to highway (or maybe ubuntu or gcc?).",2024-01-09 16:50:25,,BUG: cannot build numpy on arm64 Ubuntu 22.04 with asan enabled,['00 - Bug']
25548,open,vcolindv,"### Describe the issue:

In the  numpy.array, mean method on int array is computed in float64 as described in documentation.
The behavior for masked_array is different, it is computing mean using array datatype. As a consequence, the resulting mean value is wrong where max_value is exceeded.

In my opinion , this could be either a feature of masked_array which is not documented and , to my view, a sneaky  bug.

As a work-around, user may add dtype=np.float64 parameter.

### Reproduce the code example:

```python
import numpy as np
a1 = np.ones((2000,2000), dtype=np.int16)
a1=a1*3000
a1.mean()
assert(a1.mean() == 3000.0)
# a1.mean() is correct: 3000.0
ma1 = np.ma.array(a1)
ma1.mean()
assert(ma1.mean() == 3000.0)
# ma1.mean() is -221.225472 instead of 3000.0
```


### Error message:

```shell
n/a
```


### Python and NumPy Versions:

1.21.6
3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)]

I have tested other versions with similar outputs.

### Runtime Environment:

_No response_

### Context for the issue:

unexpected result without error emitted leading to incorrect simulation.",2024-01-08 09:42:14,,BUG: masked_array mean/sum not aligned with array,['00 - Bug']
25498,open,xor2k,"### Proposed new feature or change:

Recently I have suggested some new dtypes and during that discussion, we have also discussed the .npy file format, see #25374. As requested, I hereby open a new issue.

New datatypes for efficient storage (#25374) like ragged arrays might not be in sight anytime soon in Numpy, but what caught the least controversy was that .npy so far has been appendable (unless when using object dtype), my pull request for a minor change in the .npy header got accepted (#20321) and I have created a library to do the actual appending ([npy-append-array](https://github.com/xor2k/npy-append-array)). However, with how the upcoming stringdtype is currently specified (compare [NEP 55](https://numpy.org/neps/nep-0055-string_dtype.html)), if there is going to be a new .npy format version 4 with data on the side (compare parameter `sidecar_size`), .npy files will not be appendable anymore (at least if stringdtype is used). Serialization for stringdtype has not been implemented or specified in-detail yet. A new Numpy file format version will be a breaking change and Numpy version 2.0 is on the horizon. Given these circumstances, I suggest a new option for .npy files: zip. According to the file format list on my Ubuntu 22.04, 56 popular file formats are based on zip:

`cat /usr/share/mime/subclasses | grep ""application/zip"" | wc -l`

This not only includes office file formats, but e.g. even Windows executables .exe files are .zip underneath - and they use it (amongst others) to distinguish between program code and strings. So plain zip may be considered to be super widespread among file formats and it is natively supported by Python.

How to do Arrays with zip? The simpliest way is to define a file hierarchy:
```
index.npy
strings.npy (optional)
```
So basically, what `np.load` would need to check is if the .npy file is a zip, then try to load `index.npy` within that file and further resources (like `strings.npy`) if required accordingly.

Depending on whether or not stringdtype can have its UTF8 string data in a regular .npy array (e.g. dtype char), the introduction of a new .npy file format version would even not be necessary. Theoretically, even users with old Numpy versions could just load the `index.npy` and `strings.npy` individually and be fine (although this may not matter). So the Numpy files within the zip file would still be of the current format (e.g. 1.0 or 3.0).

How does this make stringdtype files appendable? Analogous to zip, one could handle directories. So if a user tries to load a directory with `np.load`, it could look for an `index.npy` and load it instead. Now, both `index.npy` and `strings.npy` can be appended individually.

Therefore, the definition of what .npy files are would change:
* They can either be of the Numpy core file format as specified in [NEP 1](https://numpy.org/neps/nep-0001-npy-format.html)
* Or be zip, then they contain some `index.npy` of the Numpy core file format
* Or they can be a directory, then again the `index.npy` of the Numpy core format would be used

In the future, those .npy zip files may contain more data. What comes to my mind (besides ragged arrays that already caused controversy) could maybe a common file format with Pandas to store dataframes? (I can already see the bullets flying towards me for that suggestion :smile:) Currently, as far as I know, Pandas main file format is HDF, which is rather complex and causes a lot of issues. Especially with Numpy supporting stringdtype, some interesting options may open up here. But no matter what future use zip for Numpy might or might not have, it is a very simple and standard thing to do and it provides a lot of flexibility.

And finally, as a cherry on top so to say, zip can be (optionally) compressed. In many cases, compression can reduce the size of array data significantly, not only if the array is sparse. Users could profit a lot from that.

Any thoughts about that?

Merry Christmas everybody, Michael",2023-12-26 13:52:32,,ENH: zip-based .npy for stringdtype and future extensions,['unlabeled']
25496,open,h-vetinari,"This is similar to #9023 but worse, because it's less of a corner case, and more of a full edge. 

Whereas `assert_equal` ""only"" treats nans in object arrays incorrectly, its not even possible to put object arrays into `assert_allclose` at the most basic level:
```
>>> from numpy.testing import assert_allclose
>>> n = np.array([1], dtype=object)
>>> assert_allclose(n.astype(int), n)
[...]
TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
```

This makes it unnecessarily hard to write parametric tests for various corner cases (which are already hard to write at the best of times...).",2023-12-26 09:11:24,,BUG: `assert_allclose` cannot handle object arrays,['component: numpy.testing']
25472,open,rgommers,"The test suite has gradually become a bit slower again, and this makes CI jobs take longer - which now is an additional hassle because of the cost (as of now, $2.75 per wheel build run, see https://github.com/numpy/numpy/issues/24280#issuecomment-1867993272). Some jobs are quite slow, and the PyPy on Windows one is ridiculously slow, taking 1h 22m.

Here are the top 200 slowest tests for the `-m full` test suite (which is what the wheel builds run):

<details>

```
====================================== slowest 200 durations ======================================
11.75s call     numpy/lib/tests/test_io.py::TestSaveTxt::test_large_zip
7.19s call     numpy/random/tests/test_extending.py::test_cython
6.73s call     numpy/_core/tests/test_mem_overlap.py::test_may_share_memory_easy_fuzz
6.41s call     numpy/_core/tests/test_multiarray.py::TestDot::test_huge_vectordot[complex128]
6.02s call     numpy/linalg/tests/test_linalg.py::TestCond::test_generalized_sq_cases
5.39s call     numpy/distutils/tests/test_build_ext.py::test_multi_fortran_libs_link
5.28s call     numpy/_core/tests/test_mem_overlap.py::test_may_share_memory_harder_fuzz
5.01s setup    numpy/_core/tests/test_cython.py::test_is_timedelta64_object
4.99s call     numpy/_core/tests/test_multiarray.py::TestBool::test_count_nonzero_all
3.90s call     numpy/_core/tests/test_mem_overlap.py::test_diophantine_fuzz
3.58s call     numpy/lib/tests/test_io.py::TestSavezLoad::test_big_arrays
3.46s call     numpy/tests/test_warnings.py::test_warning_calls
3.39s call     numpy/lib/tests/test_format.py::test_large_archive
3.18s call     numpy/_core/tests/test_mem_overlap.py::TestUFunc::test_unary_ufunc_call_fuzz
2.88s call     numpy/testing/tests/test_utils.py::TestAssertNoGcCycles::test_asserts
2.88s call     numpy/testing/tests/test_utils.py::TestAssertNoGcCycles::test_passes
2.82s call     numpy/_core/tests/test_dtype.py::TestDTypeMakeCanonical::test_structured
2.36s call     numpy/lib/tests/test_io.py::test_load_refcount
2.24s call     numpy/lib/tests/test_function_base.py::TestLeaks::test_frompyfunc_leaks[bound-20]
2.24s call     numpy/lib/tests/test_function_base.py::TestLeaks::test_frompyfunc_leaks[unbound-0]
2.23s call     numpy/random/tests/test_generator_mt19937.py::TestIntegers::test_integers_small_dtype_chisquared[50000000-5000-uint16-6500.0]
2.10s call     numpy/random/tests/test_generator_mt19937.py::TestRandomDist::test_dirichlet_moderately_small_alpha
2.04s call     numpy/linalg/tests/test_linalg.py::TestDet::test_generalized_sq_cases
1.90s setup    numpy/f2py/tests/test_character.py::TestCharacterString::test_input[1]
1.83s setup    numpy/f2py/tests/test_parameter.py::TestParameters::test_constant_real_single
1.82s setup    numpy/f2py/tests/test_return_character.py::TestFReturnCharacter::test_all_f77[t0]
1.81s setup    numpy/f2py/tests/test_return_logical.py::TestFReturnLogical::test_all_f77[t0]
1.81s setup    numpy/f2py/tests/test_crackfortran.py::TestDimSpec::test_array_size[n]
1.80s setup    numpy/f2py/tests/test_return_integer.py::TestFReturnInteger::test_all_f77[t0]
1.80s setup    numpy/f2py/tests/test_callback.py::TestF77Callback::test_all[t]
1.80s setup    numpy/f2py/tests/test_callback.py::TestF90Callback::test_gh17797
1.80s setup    numpy/f2py/tests/test_return_complex.py::TestFReturnComplex::test_all_f77[t0]
1.80s setup    numpy/f2py/tests/test_return_real.py::TestFReturnReal::test_all_f77[t0]
1.80s setup    numpy/f2py/tests/test_regression.py::TestModuleAndSubroutine::test_gh25337
1.78s setup    numpy/f2py/tests/test_character.py::TestCharacter::test_input[c]
1.77s setup    numpy/f2py/tests/test_common.py::TestCommonWithUse::test_common_gh19161
1.77s setup    numpy/f2py/tests/test_string.py::TestDocStringArguments::test_example
1.76s setup    numpy/f2py/tests/test_character.py::TestMiscCharacter::test_gh18684
1.76s setup    numpy/f2py/tests/test_common.py::TestCommonBlock::test_common_block
1.76s setup    numpy/f2py/tests/test_callback.py::TestF77CallbackPythonTLS::test_all[t]
1.76s setup    numpy/f2py/tests/test_assumed_shape.py::TestAssumedShapeSumExample::test_all
1.76s setup    numpy/f2py/tests/test_assumed_shape.py::TestF2cmapOption::test_all
1.76s setup    numpy/f2py/tests/test_quoted_character.py::TestQuotedCharacter::test_quoted_character
1.75s setup    numpy/f2py/tests/test_string.py::TestFixedString::test_intent_in
1.75s setup    numpy/f2py/tests/test_size.py::TestSizeSumExample::test_all
1.75s setup    numpy/f2py/tests/test_crackfortran.py::TestCrackFortran::test_gh2848
1.75s setup    numpy/f2py/tests/test_f2cmap.py::TestF2Cmap::test_gh15095
1.75s setup    numpy/f2py/tests/test_return_real.py::TestCReturnReal::test_all[t4]
1.75s setup    numpy/f2py/tests/test_callback.py::TestGH25211::test_gh25211
1.75s setup    numpy/f2py/tests/test_data.py::TestDataMultiplierF77::test_data_stmts
1.74s setup    numpy/f2py/tests/test_semicolon_split.py::TestMultiline::test_multiline
1.74s setup    numpy/f2py/tests/test_isoc.py::TestISOC::test_c_double
1.74s call     numpy/linalg/tests/test_linalg.py::TestPinv::test_generalized_nonsq_cases
1.74s setup    numpy/f2py/tests/test_value_attrspec.py::TestValueAttr::test_gh21665
1.74s setup    numpy/f2py/tests/test_character.py::TestStringOptionalInOut::test_gh24662
1.74s setup    numpy/f2py/tests/test_crackfortran.py::TestFunctionReturn::test_function_rettype
1.74s setup    numpy/f2py/tests/test_mixed.py::TestMixed::test_all
1.74s setup    numpy/f2py/tests/test_module_doc.py::TestModuleDocString::test_module_docstring
1.74s setup    numpy/f2py/tests/test_callback.py::TestGH18335::test_gh18335
1.73s setup    numpy/f2py/tests/test_abstract_interface.py::TestAbstractInterface::test_abstract_interface
1.73s setup    numpy/f2py/tests/test_data.py::TestDataWithCommentsF77::test_data_stmts
1.73s setup    numpy/f2py/tests/test_kind.py::TestKind::test_int
1.73s setup    numpy/f2py/tests/test_regression.py::TestNumpyVersionAttribute::test_numpy_version_attribute
1.73s setup    numpy/f2py/tests/test_crackfortran.py::TestNoSpace::test_module
1.73s setup    numpy/f2py/tests/test_crackfortran.py::TestExternal::test_external_as_statement
1.73s setup    numpy/f2py/tests/test_character.py::TestBCCharHandling::test_gh25286
1.73s setup    numpy/f2py/tests/test_crackfortran.py::TestUnicodeComment::test_encoding_comment
1.73s setup    numpy/f2py/tests/test_character.py::TestNewCharHandling::test_gh25286
1.73s call     numpy/testing/tests/test_utils.py::TestAssertNoGcCycles::test_fails
1.73s setup    numpy/f2py/tests/test_character.py::TestStringAssumedLength::test_gh24008
1.73s setup    numpy/f2py/tests/test_docs.py::TestDocAdvanced::test_asterisk1
1.73s call     numpy/tests/test_ctypeslib.py::TestAsArray::test_reference_cycles
1.72s setup    numpy/f2py/tests/test_character.py::TestStringScalarArr::test_char
1.72s setup    numpy/f2py/tests/test_string.py::TestString::test_char
1.72s setup    numpy/f2py/tests/test_data.py::TestDataF77::test_data_stmts
1.72s setup    numpy/f2py/tests/test_block_docstring.py::TestBlockDocString::test_block_docstring
1.72s setup    numpy/f2py/tests/test_semicolon_split.py::TestCallstatement::test_callstatement
1.71s setup    numpy/f2py/tests/test_data.py::TestData::test_data_stmts
1.71s setup    numpy/f2py/tests/test_regression.py::TestIntentInOut::test_inout
1.71s setup    numpy/f2py/tests/test_regression.py::TestNegativeBounds::test_negbound
1.69s call     numpy/_core/tests/test_extint128.py::test_divmod_128_64
1.69s call     numpy/_core/tests/test_mem_overlap.py::TestUFunc::test_binary_ufunc_1d_manual
1.64s call     numpy/_core/tests/test_regression.py::TestRegression::test_structarray_title
1.59s call     numpy/_core/tests/test_nditer.py::test_iter_buffered_reduce_reuse
1.58s call     numpy/_core/tests/test_multiarray.py::TestDot::test_huge_vectordot[float64]
1.54s setup    numpy/f2py/tests/test_array_from_pyobj.py::TestIntent::test_in_out
1.18s call     numpy/array_api/tests/test_array_object.py::test_operators
1.17s call     numpy/linalg/tests/test_linalg.py::TestPinv::test_generalized_sq_cases
1.04s call     numpy/linalg/tests/test_linalg.py::test_sdot_bug_8577
1.00s call     numpy/linalg/tests/test_linalg.py::TestEigvals::test_generalized_sq_cases
0.84s call     numpy/linalg/tests/test_linalg.py::TestInv::test_generalized_sq_cases
0.83s call     numpy/_core/tests/test_multiarray.py::TestPickling::test_roundtrip
0.81s call     numpy/_core/tests/test_ufunc.py::TestUfunc::test_identityless_reduction_huge_array
0.80s call     numpy/linalg/tests/test_linalg.py::TestSolve::test_generalized_sq_cases
0.72s call     numpy/linalg/tests/test_linalg.py::TestEig::test_generalized_sq_cases
0.71s setup    numpy/_core/tests/test_array_interface.py::test_cstruct
0.68s call     numpy/_core/tests/test_mem_overlap.py::TestUFunc::test_unary_gufunc_fuzz
0.67s call     numpy/_core/tests/test_multiarray.py::TestArrayFinalize::test_lifetime_on_error
0.67s call     numpy/_core/tests/test_multiarray.py::TestAlignment::test_various_alignments
0.66s call     numpy/linalg/tests/test_linalg.py::TestSVD::test_generalized_sq_cases
0.66s call     numpy/_core/tests/test_scalarmath.py::TestModulus::test_float_modulus_exact
0.65s call     numpy/f2py/tests/test_crackfortran.py::TestNameArgsPatternBacktracking::test_nameargspattern_backtracking[@)@bind                         @(@]
0.64s setup    numpy/_core/tests/test_mem_policy.py::test_set_policy
0.62s call     numpy/lib/tests/test_format.py::test_huge_header_npz
0.60s call     numpy/random/tests/test_generator_mt19937_regressions.py::TestRegression::test_shuffle_of_array_of_different_length_strings
0.58s teardown numpy/typing/tests/test_typing.py::test_extended_precision
0.58s call     numpy/random/tests/test_regression.py::TestRegression::test_shuffle_of_array_of_different_length_strings
0.57s call     numpy/random/tests/test_randomstate_regression.py::TestRegression::test_shuffle_of_array_of_different_length_strings
0.57s call     numpy/random/tests/test_regression.py::TestRegression::test_shuffle_of_array_of_objects
0.57s call     numpy/random/tests/test_randomstate_regression.py::TestRegression::test_shuffle_of_array_of_objects
0.55s call     numpy/random/tests/test_generator_mt19937_regressions.py::TestRegression::test_shuffle_of_array_of_objects
0.54s call     numpy/_core/tests/test_limited_api.py::test_limited_api
0.49s call     numpy/_core/tests/test_dtype.py::TestDTypeMakeCanonical::test_make_canonical_hypothesis
0.47s call     numpy/_core/tests/test_multiarray.py::TestUnicodeEncoding::test_round_trip
0.47s call     numpy/_core/tests/test_mem_overlap.py::test_internal_overlap_fuzz
0.46s call     numpy/_core/tests/test_numeric.py::TestCreationFuncs::test_full
0.45s setup    numpy/f2py/tests/test_array_from_pyobj.py::TestSharedMemory::test_hidden[DOUBLE]
0.44s call     numpy/_core/tests/test_numeric.py::TestClip::test_clip_property
0.42s call     numpy/_core/tests/test_multiarray.py::TestCreation::test_zeros_big
0.42s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_take_and_repeat[<structured subarray 1>]
0.42s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape0-index0-2-<subarray in field>]
0.42s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape1-index1-4-<structured subarray 1>]
0.42s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_item_setting[<structured subarray 2>]
0.41s call     numpy/_core/tests/test_mem_policy.py::test_owner_is_base
0.41s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape0-index0-2-<subarray>]
0.41s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape1-index1-4-<subarray>]
0.41s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_take_and_repeat[<subarray in field>]
0.41s call     numpy/random/tests/test_generator_mt19937.py::TestIntegers::test_integers_small_dtype_chisquared[10000000-2500-int16-3300.0]
0.41s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape2-index2-2-<subarray in field>]
0.41s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape0-index0-2-<structured subarray 1>]
0.41s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape1-index1-4-<subarray in field>]
0.41s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape0-index0-2-<structured subarray 2>]
0.41s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_item_setting[<subarray in field>]
0.40s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_item_setting[<subarray>]
0.40s call     numpy/_core/tests/test_cpu_features.py::TestEnvPrivation::test_runtime_feature_selection
0.40s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_take_and_repeat[<subarray>]
0.39s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_take_and_repeat[<structured subarray 2>]
0.39s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape1-index1-4-<structured subarray 2>]
0.39s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_item_setting[<structured subarray 1>]
0.38s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape3-index3-2-<structured subarray 2>]
0.38s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape2-index2-2-<structured subarray 2>]
0.38s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape2-index2-2-<subarray>]
0.38s call     numpy/_core/tests/test_mem_overlap.py::TestUFunc::test_unary_ufunc_1d_manual
0.38s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape3-index3-2-<subarray in field>]
0.37s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape3-index3-2-<subarray>]
0.37s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape2-index2-2-<structured subarray 1>]
0.37s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape3-index3-2-<structured subarray 1>]
0.35s call     numpy/random/tests/test_randomstate.py::test_integer_repeat[multinomial]
0.35s call     numpy/_core/tests/test_regression.py::TestRegression::test_leak_in_structured_dtype_comparison
0.34s call     numpy/_core/tests/test_arrayprint.py::TestArray2String::test_any_text
0.34s call     numpy/_core/tests/test_multiarray.py::TestDot::test_accelerate_framework_sgemv_fix
0.30s call     numpy/_core/tests/test_multiarray.py::test_partition_fp[float16-N95]
0.28s call     numpy/_core/tests/test_mem_overlap.py::TestUFunc::test_unary_ufunc_call_complex_fuzz
0.27s call     numpy/_core/tests/test_multiarray.py::TestMethods::test_partition
0.26s call     numpy/_core/tests/test_umath.py::TestAbsoluteNegative::test_abs_neg_blocked
0.26s call     numpy/_core/tests/test_multiarray.py::TestMethods::test_sort_degraded
0.24s call     numpy/lib/tests/test_function_base.py::TestQuantile::test_quantile_monotonic_hypo
0.24s call     numpy/_core/tests/test_api.py::test_copyto_permut
0.23s call     numpy/ma/tests/test_core.py::TestMaskedArrayMathMethods::test_mean_overflow
0.23s call     numpy/_core/tests/test_mem_policy.py::test_switch_owner[0]
0.23s call     numpy/_core/tests/test_multiarray.py::TestCTypes::test_ctypes_as_parameter_holds_reference
0.23s call     numpy/tests/test_reloading.py::test_full_reimport
0.23s call     numpy/_core/tests/test_multiarray.py::TestCTypes::test_ctypes_data_as_holds_reference[overlapping]
0.23s call     numpy/_core/tests/test_multiarray.py::TestCTypes::test_ctypes_data_as_holds_reference[readonly]
0.23s call     numpy/_core/tests/test_scalarmath.py::TestBaseMath::test_blocked
0.22s call     numpy/_core/tests/test_multiarray.py::TestCTypes::test_ctypes_data_as_holds_reference[empty]
0.22s call     numpy/_core/tests/test_multiarray.py::TestCTypes::test_ctypes_data_as_holds_reference[structured]
0.22s call     numpy/_core/tests/test_multiarray.py::TestCTypes::test_ctypes_data_as_holds_reference[empty-2d]
0.22s call     numpy/_core/tests/test_multiarray.py::TestCTypes::test_ctypes_data_as_holds_reference[1d]
0.22s call     numpy/_core/tests/test_mem_policy.py::test_switch_owner[1]
0.22s call     numpy/_core/tests/test_multiarray.py::TestCTypes::test_ctypes_data_as_holds_reference[2d]
0.22s call     numpy/_core/tests/test_mem_policy.py::test_switch_owner[None]
0.22s call     numpy/_core/tests/test_multiarray.py::TestCTypes::test_ctypes_data_as_holds_reference[object]
0.22s call     numpy/_core/tests/test_indexing.py::TestMultiIndexingAutomated::test_multidim
0.22s call     numpy/lib/tests/test_histograms.py::TestHistogramOptimBinNums::test_scott_vs_stone
0.21s call     numpy/lib/tests/test_nanfunctions.py::TestNanFunctions_Median::test_float_special
0.20s call     numpy/lib/tests/test_format.py::test_huge_header[r]
0.20s call     numpy/_core/tests/test_extint128.py::test_safe_binop
0.20s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_create_delete[ones-1-<subarray in field>]
0.20s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_create_delete[ones-1-<structured subarray 1>]
0.20s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_create_delete[ones-1-<subarray>]
0.19s call     numpy/_core/tests/test_umath.py::TestAVXFloat32Transcendental::test_sincos_float32
0.19s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_create_delete[zeros-0-<subarray>]
0.19s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_create_delete[zeros-0-<structured subarray 2>]
0.19s call     numpy/array_api/tests/test_set_functions.py::test_inverse_indices_shape[unique_inverse]
0.19s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_create_delete[ones-1-<structured subarray 2>]
0.19s call     numpy/tests/test_scripts.py::test_pep338
0.18s call     numpy/_core/tests/test_arrayprint.py::TestArray2String::test_refcount
0.18s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_create_delete[zeros-0-<subarray in field>]
0.18s call     numpy/_core/tests/test_multiarray.py::TestIO::test_largish_file[path_obj]
0.18s call     numpy/_core/tests/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_create_delete[zeros-0-<structured subarray 1>]
0.18s call     numpy/fft/tests/test_helper.py::TestFFTShift::test_equal_to_original
0.18s call     numpy/_core/tests/test_multiarray.py::TestIO::test_largish_file[string]
0.18s call     numpy/distutils/tests/test_exec_command.py::TestExecCommand::test_basic
0.18s call     numpy/_core/tests/test_umath_accuracy.py::TestAccuracy::test_validate_transcendentals
0.18s call     numpy/tests/test_scripts.py::test_f2py[f2py]
0.18s call     numpy/_core/tests/test_multiarray.py::TestBool::test_count_nonzero
0.17s call     numpy/linalg/tests/test_linalg.py::TestEighCases::test_generalized_herm_cases
0.17s call     numpy/tests/test_public_api.py::test_import_lazy_import[testing]
0.17s call     numpy/linalg/tests/test_linalg.py::TestCond::test_sq_cases
============== 45452 passed, 376 skipped, 36 xfailed, 4 xpassed in 343.20s (0:05:43) ==============
```

</details>

One of the top offenders is `f2py`, there is already a separate issue for that: gh-25134. Separating out those tests so they don't run at all on wheel builds will take care of that problem.

For the rest we should go through and deal with some of the tests in the above list case by case. I'm having a look at `TestStructuredObjectRefcounting` now, which is one of the worst tests.",2023-12-22 19:41:23,,Doing something about slow tests again,"['17 - Task', 'component: CI']"
25460,open,rgommers,"Failure in today's aarch64 linux wheel build run:

```
___________________________ TestLog.test_log_values ____________________________
[gw0] linux -- Python 3.10.13 /tmp/tmp.gkfDdM/venv/bin/python
self = <numpy._core.tests.test_umath.TestLog object at 0xffff9d34f8e0>
    def test_log_values(self):
        x = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]
        y = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
        for dt in ['f', 'd', 'g']:
            log2_ = 0.69314718055994530943
            xf = np.array(x, dtype=dt)
            yf = np.array(y, dtype=dt)*log2_
            assert_almost_equal(np.log(xf), yf)
    
        # test aliasing(issue #17761)
        x = np.array([2, 0.937500, 3, 0.947500, 1.054697])
        xf = np.log(x)
        assert_almost_equal(np.log(x, out=x), xf)
    
        # test log() of max for dtype does not raise
        for dt in ['f', 'd', 'g']:
            with np.errstate(all='raise'):
                x = np.finfo(dt).max
>               np.log(x)
E               FloatingPointError: overflow encountered in log
dt         = 'g'
log2_      = 0.6931471805599453
self       = <numpy._core.tests.test_umath.TestLog object at 0xffff9d34f8e0>
x          = np.longdouble('1.189731495357231765085759326628007e+4932')
xf         = array([ 0.69314718, -0.06453852,  1.09861229, -0.05392834,  0.05325352])
y          = [0, 1, 2, 3, 4, 5, ...]
yf         = array([0.        , 0.69314718, 1.38629436, 2.07944154, 2.77258872,
       3.4657359 , 4.15888308, 4.85203026, 5.54517744, 6.23832463,
       6.93147181], dtype=float128)
../venv/lib/python3.10/site-packages/numpy/_core/tests/test_umath.py:1359: FloatingPointError
```

This used to pass until ~1.5 months ago, which is when we had the last successful nightly wheel build. It's a pretty niche failure, so I'm going to skip the `longdouble` check for now.",2023-12-22 15:25:09,,BUG: `np.log(max_value_for_longdouble)` fails on Linux aarch64,"['00 - Bug', 'component: numpy._core']"
25447,open,rgommers,"First time I've seen this one, it occurred in the PyPy on Windows wheel build job (see [CI log](https://github.com/numpy/numpy/actions/runs/7290847257/job/19868540074?pr=25446)):
```
  ERROR f2py/tests/test_abstract_interface.py - PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\Users\\RUNNER~1\\AppData\\Local\\Temp\\tmpye0_j0h4\\meson.build'
  ERROR gw1
```

Full traceback:

<details>

```
 ___________ ERROR collecting f2py/tests/test_abstract_interface.py ____________
  ..\venv-test\lib\site-packages\_pytest\runner.py:341: in from_call
      result: Optional[TResult] = func()
          cls        = <class '_pytest.runner.CallInfo'>
          duration   = 10.200289099999999
          excinfo    = <ExceptionInfo PermissionError(13, 'The process cannot access the file because it is being used by another process') tblen=24>
          func       = <function pytest_make_collect_report.<locals>.<lambda> at 0x0000027f94cf08e0>
          precise_start = 26.6201467
          precise_stop = 36.8204358
          reraise    = None
          result     = None
          start      = 1703177221.1964283
          stop       = 1703177231.3974376
          when       = 'collect'
  ..\venv-test\lib\site-packages\_pytest\runner.py:372: in <lambda>
      call = CallInfo.from_call(lambda: list(collector.collect()), ""collect"")
          collector  = <Module test_abstract_interface.py>
  ..\venv-test\lib\site-packages\_pytest\python.py:531: in collect
      self._inject_setup_module_fixture()
          __class__  = <class '_pytest.python.Module'>
          self       = <Module test_abstract_interface.py>
  ..\venv-test\lib\site-packages\_pytest\python.py:545: in _inject_setup_module_fixture
      self.obj, (""setUpModule"", ""setup_module"")
          has_nose   = True
          self       = <Module test_abstract_interface.py>
  ..\venv-test\lib\site-packages\_pytest\python.py:310: in obj
      self._obj = obj = self._getobj()
          obj        = None
          self       = <Module test_abstract_interface.py>
  ..\venv-test\lib\site-packages\_pytest\python.py:528: in _getobj
      return self._importtestmodule()
          self       = <Module test_abstract_interface.py>
  ..\venv-test\lib\site-packages\_pytest\python.py:617: in _importtestmodule
      mod = import_path(self.path, mode=importmode, root=self.config.rootpath)
          importmode = 'prepend'
          self       = <Module test_abstract_interface.py>
  ..\venv-test\lib\site-packages\_pytest\pathlib.py:565: in import_path
      importlib.import_module(module_name)
          mode       = <ImportMode.prepend: 'prepend'>
          module_name = 'numpy.f2py.tests.test_abstract_interface'
          names      = ['numpy', 'f2py', 'tests', 'test_abstract_interface']
          p          = WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/cibw-run-52rc4ryf/pp39-win_amd64/venv-test/lib/site-packages/numpy/f2py/tests/test_abstract_interface.py')
          path       = WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/cibw-run-52rc4ryf/pp39-win_amd64/venv-test/lib/site-packages/numpy/f2py/tests/test_abstract_interface.py')
          pkg_path   = WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/cibw-run-52rc4ryf/pp39-win_amd64/venv-test/lib/site-packages/numpy')
          pkg_root   = WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/cibw-run-52rc4ryf/pp39-win_amd64/venv-test/lib/site-packages')
          root       = WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/cibw-run-52rc4ryf/pp39-win_amd64/test_cwd')
  ..\..\..\..\pypa\cibuildwheel\Cache\pypy3.9-v7.3.12-win64\Lib\importlib\__init__.py:127: in import_module
      return _bootstrap._gcd_import(name[level:], package, level)
          level      = 0
          name       = 'numpy.f2py.tests.test_abstract_interface'
          package    = None
  <frozen importlib._bootstrap>:1030: in _gcd_import
      ???
          level      = 0
          name       = 'numpy.f2py.tests.test_abstract_interface'
          package    = None
  <frozen importlib._bootstrap>:1007: in _find_and_load
      ???
          import_    = <function _gcd_import at 0x00007ff8f29dec70>
          module     = <object object at 0x00007ff8f29deb90>
          name       = 'numpy.f2py.tests.test_abstract_interface'
  <frozen importlib._bootstrap>:986: in _find_and_load_unlocked
      ???
          import_    = <function _gcd_import at 0x00007ff8f29dec70>
          name       = 'numpy.f2py.tests.test_abstract_interface'
          parent     = 'numpy.f2py.tests'
          parent_module = <module 'numpy.f2py.tests' from 'C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-52rc4ryf\\pp39-win_amd64\\venv-test\\lib\\site-packages\\numpy\\f2py\\tests\\__init__.py'>
          path       = ['C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-52rc4ryf\\pp39-win_amd64\\venv-test\\lib\\site-packages\\numpy\\f2py\\tests']
          spec       = ModuleSpec(name='numpy.f2py.tests.test_abstract_interface', loader=<_pytest.assertion.rewrite.AssertionRewritingHook o...emp\\cibw-run-52rc4ryf\\pp39-win_amd64\\venv-test\\lib\\site-packages\\numpy\\f2py\\tests\\test_abstract_interface.py')
  <frozen importlib._bootstrap>:680: in _load_unlocked
      ???
          module     = <module 'numpy.f2py.tests.test_abstract_interface' from 'C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-52rc4ryf\\pp39-win_amd64\\venv-test\\lib\\site-packages\\numpy\\f2py\\tests\\test_abstract_interface.py'>
          spec       = ModuleSpec(name='numpy.f2py.tests.test_abstract_interface', loader=<_pytest.assertion.rewrite.AssertionRewritingHook o...emp\\cibw-run-52rc4ryf\\pp39-win_amd64\\venv-test\\lib\\site-packages\\numpy\\f2py\\tests\\test_abstract_interface.py')
  ..\venv-test\lib\site-packages\_pytest\assertion\rewrite.py:178: in exec_module
      exec(co, module.__dict__)
          cache_dir  = WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/cibw-run-52rc4ryf/pp39-win_amd64/venv-test/lib/site-packages/numpy/f2py/tests/__pycache__')
          cache_name = 'test_abstract_interface.pypy39-pytest-7.4.0.pyc'
          co         = <code object <module> at 0x0000027f929f2a18, file ""C:\Users\runneradmin\AppData\Local\Temp\cibw-run-52rc4ryf\pp39-win_amd64\venv-test\lib\site-packages\numpy\f2py\tests\test_abstract_interface.py"", line 1>
          fn         = WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/cibw-run-52rc4ryf/pp39-win_amd64/venv-test/lib/site-packages/numpy/f2py/tests/test_abstract_interface.py')
          module     = <module 'numpy.f2py.tests.test_abstract_interface' from 'C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-52rc4ryf\\pp39-win_amd64\\venv-test\\lib\\site-packages\\numpy\\f2py\\tests\\test_abstract_interface.py'>
          ok         = True
          pyc        = WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/cibw-run-52rc4ryf/pp39-win_amd64/venv-test/lib/site-packages/numpy/f2py/tests/__pycache__/test_abstract_interface.pypy39-pytest-7.4.0.pyc')
          self       = <_pytest.assertion.rewrite.AssertionRewritingHook object at 0x0000027ffb8cf130>
          state      = <_pytest.assertion.AssertionState object at 0x0000027ffb8cfc58>
          write      = True
  ..\venv-test\lib\site-packages\numpy\f2py\tests\test_abstract_interface.py:4: in <module>
      from . import util
          Path       = <class 'pathlib.Path'>
          __builtins__ = <builtins>
          __cached__ = 'C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-52rc4ryf\\pp39-win_amd64\\venv-test\\lib\\site-packages\\numpy\\f2py\\tests\\__pycache__\\test_abstract_interface.pypy39.pyc'
          __doc__    = None
          __file__   = 'C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-52rc4ryf\\pp39-win_amd64\\venv-test\\lib\\site-packages\\numpy\\f2py\\tests\\test_abstract_interface.py'
          __loader__ = <_pytest.assertion.rewrite.AssertionRewritingHook object at 0x0000027ffb8cf130>
          __name__   = 'numpy.f2py.tests.test_abstract_interface'
          __package__ = 'numpy.f2py.tests'
          __spec__   = ModuleSpec(name='numpy.f2py.tests.test_abstract_interface', loader=<_pytest.assertion.rewrite.AssertionRewritingHook o...emp\\cibw-run-52rc4ryf\\pp39-win_amd64\\venv-test\\lib\\site-packages\\numpy\\f2py\\tests\\test_abstract_interface.py')
          pytest     = <module 'pytest' from 'C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-52rc4ryf\\pp39-win_amd64\\venv-test\\lib\\site-packages\\pytest\\__init__.py'>
          textwrap   = <module 'textwrap' from 'C:\\Users\\runneradmin\\AppData\\Local\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\textwrap.py'>
  ..\venv-test\lib\site-packages\numpy\f2py\tests\util.py:276: in <module>
      checker.check_compilers()
          CompilerChecker = <class 'numpy.f2py.tests.util.CompilerChecker'>
          IS_WASM    = False
          MesonBackend = <class 'numpy.f2py._backends._meson.MesonBackend'>
          Path       = <class 'pathlib.Path'>
          __builtins__ = <builtins>
          __cached__ = 'C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-52rc4ryf\\pp39-win_amd64\\venv-test\\lib\\site-packages\\numpy\\f2py\\tests\\__pycache__\\util.pypy39.pyc'
          __doc__    = '\nUtility functions for\n\n- building and importing modules on test time, using a temporary location\n- detecting if compilers are present\n- determining paths to tests\n\n'
          __file__   = 'C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-52rc4ryf\\pp39-win_amd64\\venv-test\\lib\\site-packages\\numpy\\f2py\\tests\\util.py'
          __loader__ = <_frozen_importlib_external.SourceFileLoader object at 0x0000027f94d094b0>
          __name__   = 'numpy.f2py.tests.util'
          __package__ = 'numpy.f2py.tests'
          __spec__   = ModuleSpec(name='numpy.f2py.tests.util', loader=<_frozen_importlib_external.SourceFileLoader object at 0x0000027f94d09...\\AppData\\Local\\Temp\\cibw-run-52rc4ryf\\pp39-win_amd64\\venv-test\\lib\\site-packages\\numpy\\f2py\\tests\\util.py')
          _cleanup   = <function _cleanup at 0x0000027f94d42a20>
          _memoize   = <function _memoize at 0x0000027f94d42c00>
          _module_dir = None
          _module_num = 5403
          asunicode  = <function asunicode at 0x0000027ffbe4d560>
          atexit     = <module 'atexit' (built-in)>
          build_code = <function _memoize.<locals>.wrapper at 0x0000027f94d42d40>
          build_module = <function _memoize.<locals>.wrapper at 0x0000027f94d42ca0>
          check_language = <function check_language at 0x0000027f94d42340>
          checker    = <numpy.f2py.tests.util.CompilerChecker object at 0x0000027f94d40950>
          concurrent = <module 'concurrent' from 'C:\\Users\\runneradmin\\AppData\\Local\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\concurrent\\__init__.py'>
          contextlib = <module 'contextlib' from 'C:\\Users\\runneradmin\\AppData\\Local\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\contextlib.py'>
          fortran77_code = ""\nC Example Fortran 77 code\n      PROGRAM HELLO\n      PRINT *, 'Hello, Fortran 77!'\n      END\n""
          fortran90_code = ""\n! Example Fortran 90 code\nprogram hello90\n  type :: greeting\n    character(len=20) :: text\n  end type greeting\n\n  type(greeting) :: greet\n  greet%text = 'hello, fortran 90!'\n  print *, greet%text\nend program hello90\n""
          get_module_dir = <function get_module_dir at 0x0000027f94d42ac0>
          get_temp_module_name = <function get_temp_module_name at 0x0000027f94d42b60>
          glob       = <module 'glob' from 'C:\\Users\\runneradmin\\AppData\\Local\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\glob.py'>
          import_module = <function import_module at 0x0000027ffa535e20>
          numpy      = <module 'numpy' from 'C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-52rc4ryf\\pp39-win_amd64\\venv-test\\lib\\site-packages\\numpy\\__init__.py'>
          os         = <module 'os' from 'C:\\Users\\runneradmin\\AppData\\Local\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\os.py'>
          pytest     = <module 'pytest' from 'C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-52rc4ryf\\pp39-win_amd64\\venv-test\\lib\\site-packages\\pytest\\__init__.py'>
          re         = <module 're' from 'C:\\Users\\runneradmin\\AppData\\Local\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\re.py'>
          shutil     = <module 'shutil' from 'C:\\Users\\runneradmin\\AppData\\Local\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\shutil.py'>
          subprocess = <module 'subprocess' from 'C:\\Users\\runneradmin\\AppData\\Local\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\subprocess.py'>
          sys        = <module 'sys' (built-in)>
          tempfile   = <module 'tempfile' from 'C:\\Users\\runneradmin\\AppData\\Local\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\tempfile.py'>
          temppath   = <function temppath at 0x0000027f85bf6020>
          textwrap   = <module 'textwrap' from 'C:\\Users\\runneradmin\\AppData\\Local\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\textwrap.py'>
  ..\venv-test\lib\site-packages\numpy\f2py\tests\util.py:269: in check_compilers
      self.has_c = futures[0].result()
          executor   = <concurrent.futures.thread.ThreadPoolExecutor object at 0x0000027f94d40988>
          futures    = [<Future at 0x27f94d409c0 state=finished raised PermissionError>, <Future at 0x27f94d40870 state=finished returned bool>, <Future at 0x27f94d40598 state=finished returned bool>]
          self       = <numpy.f2py.tests.util.CompilerChecker object at 0x0000027f94d40950>
  ..\..\..\..\pypa\cibuildwheel\Cache\pypy3.9-v7.3.12-win64\Lib\concurrent\futures\_base.py:446: in result
      return self.__get_result()
          self       = None
          timeout    = None
  ..\..\..\..\pypa\cibuildwheel\Cache\pypy3.9-v7.3.12-win64\Lib\concurrent\futures\_base.py:391: in __get_result
      raise self._exception
          self       = None
  ..\..\..\..\pypa\cibuildwheel\Cache\pypy3.9-v7.3.12-win64\Lib\concurrent\futures\thread.py:58: in run
      result = self.fn(*self.args, **self.kwargs)
          self       = None
  ..\venv-test\lib\site-packages\numpy\f2py\tests\util.py:229: in check_language
      shutil.rmtree(tmpdir)
          code_snippet = None
          f          = <_io.TextIOWrapper name='C:\\Users\\RUNNER~1\\AppData\\Local\\Temp\\tmpye0_j0h4\\meson.build' mode='w' encoding='UTF-8'>
          lang       = 'c'
          meson_file = 'C:\\Users\\RUNNER~1\\AppData\\Local\\Temp\\tmpye0_j0h4\\meson.build'
          runmeson   = CompletedProcess(args=['meson', 'setup', 'btmp'], returncode=0, stdout=b'The Meson build system\r\nVersion: 1.3.0\r\nS...\\runneradmin\\AppData\\Local\\Temp\\cibw-run-52rc4ryf\\pp39-win_amd64\\venv-test\\Scripts\\ninja.EXE\r\n', stderr=b'')
          tmpdir     = 'C:\\Users\\RUNNER~1\\AppData\\Local\\Temp\\tmpye0_j0h4'
  ..\..\..\..\pypa\cibuildwheel\Cache\pypy3.9-v7.3.12-win64\Lib\shutil.py:759: in rmtree
      return _rmtree_unsafe(path, onerror)
          ignore_errors = False
          onerror    = <function rmtree.<locals>.onerror at 0x0000027f94d42660>
          path       = 'C:\\Users\\RUNNER~1\\AppData\\Local\\Temp\\tmpye0_j0h4'
  ..\..\..\..\pypa\cibuildwheel\Cache\pypy3.9-v7.3.12-win64\Lib\shutil.py:629: in _rmtree_unsafe
      onerror(os.unlink, fullname, sys.exc_info())
          entries    = [<DirEntry 'btmp'>, <DirEntry 'meson.build'>]
          entry      = <DirEntry 'meson.build'>
          fullname   = 'C:\\Users\\RUNNER~1\\AppData\\Local\\Temp\\tmpye0_j0h4\\meson.build'
          onerror    = <function rmtree.<locals>.onerror at 0x0000027f94d42660>
          path       = 'C:\\Users\\RUNNER~1\\AppData\\Local\\Temp\\tmpye0_j0h4'
          scandir_it = <posix.ScandirIterator object at 0x0000027f94d21910>
  ..\..\..\..\pypa\cibuildwheel\Cache\pypy3.9-v7.3.12-win64\Lib\shutil.py:627: in _rmtree_unsafe
      os.unlink(fullname)
  E   PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\Users\\RUNNER~1\\AppData\\Local\\Temp\\tmpye0_j0h4\\meson.build'
          entries    = [<DirEntry 'btmp'>, <DirEntry 'meson.build'>]
          entry      = <DirEntry 'meson.build'>
          fullname   = 'C:\\Users\\RUNNER~1\\AppData\\Local\\Temp\\tmpye0_j0h4\\meson.build'
          onerror    = <function rmtree.<locals>.onerror at 0x0000027f94d42660>
          path       = 'C:\\Users\\RUNNER~1\\AppData\\Local\\Temp\\tmpye0_j0h4'
          scandir_it = <posix.ScandirIterator object at 0x0000027f94d21910>
  ____________________________ ERROR collecting gw1 _____________________________
  Different tests were collected between gw3 and gw1. The difference is:
  --- gw3
  
  +++ gw1
  
  @@ -32556,8 +32556,6 @@
  
   distutils/tests/test_system_info.py::TestSystemInfoReading::test_compile2
   distutils/tests/test_system_info.py::TestSystemInfoReading::test_overrides
   distutils/tests/test_system_info.py::test_distutils_parse_env_order
  -f2py/tests/test_abstract_interface.py::TestAbstractInterface::test_abstract_interface
  -f2py/tests/test_abstract_interface.py::TestAbstractInterface::test_parse_abstract_interface
   f2py/tests/test_array_from_pyobj.py::TestIntent::test_in_out
   f2py/tests/test_array_from_pyobj.py::TestSharedMemory::test_in_from_2seq[BOOL]
   f2py/tests/test_array_from_pyobj.py::TestSharedMemory::test_in_from_2casttype[BOOL]
  C:\Users\runneradmin\AppData\Local\Temp\cibw-run-52rc4ryf\pp39-win_amd64\venv-test\lib\site-packages\numpy\_pytesttester.py:143: DeprecationWarning: 
  
    `numpy.distutils` is deprecated since NumPy 1.23.0, as a result
    of the deprecation of `distutils` itself. It will be removed for
    Python >= 3.12. For older Python versions it will remain present.
    It is recommended to use `setuptools < 60.0` for those Python versions.
    For more details, see:
      https://numpy.org/devdocs/reference/distutils_status_migration.html 
  
  
    from numpy.distutils import cpuinfo
  To see why this happens see Known limitations in documentation
  ============================== warnings summary ===============================
  ..\..\..\..\pypa\cibuildwheel\Cache\pypy3.9-v7.3.12-win64\Lib\importlib\__init__.py:127
  ..\..\..\..\pypa\cibuildwheel\Cache\pypy3.9-v7.3.12-win64\Lib\importlib\__init__.py:127
  ..\..\..\..\pypa\cibuildwheel\Cache\pypy3.9-v7.3.12-win64\Lib\importlib\__init__.py:127
  ..\..\..\..\pypa\cibuildwheel\Cache\pypy3.9-v7.3.12-win64\Lib\importlib\__init__.py:127
    C:\Users\runneradmin\AppData\Local\pypa\cibuildwheel\Cache\pypy3.9-v7.3.12-win64\Lib\importlib\__init__.py:127: UserWarning: The numpy.array_api submodule is still experimental. See NEP 47.
      return _bootstrap._gcd_import(name[level:], package, level)
  
  ..\..\..\..\pypa\cibuildwheel\Cache\pypy3.9-v7.3.12-win64\Lib\importlib\__init__.py:127
  ..\..\..\..\pypa\cibuildwheel\Cache\pypy3.9-v7.3.12-win64\Lib\importlib\__init__.py:127
  ..\..\..\..\pypa\cibuildwheel\Cache\pypy3.9-v7.3.12-win64\Lib\importlib\__init__.py:127
  ..\..\..\..\pypa\cibuildwheel\Cache\pypy3.9-v7.3.12-win64\Lib\importlib\__init__.py:127
    C:\Users\runneradmin\AppData\Local\pypa\cibuildwheel\Cache\pypy3.9-v7.3.12-win64\Lib\importlib\__init__.py:127: DeprecationWarning: 
    
      `numpy.distutils` is deprecated since NumPy 1.23.0, as a result
      of the deprecation of `distutils` itself. It will be removed for
      Python >= 3.12. For older Python versions it will remain present.
      It is recommended to use `setuptools < 60.0` for those Python versions.
      For more details, see:
        https://numpy.org/devdocs/reference/distutils_status_migration.html 
    
    
      return _bootstrap._gcd_import(name[level:], package, level)
  
  -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
  =========================== short test summary info ===========================
  ERROR f2py/tests/test_abstract_interface.py - PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\Users\\RUNNER~1\\AppData\\Local\\Temp\\tmpye0_j0h4\\meson.build'
  ERROR gw1
  8 warnings, 2 errors in 55.76s
  Error: Command bash D:\a\numpy\numpy/tools/wheels/cibw_test_command.sh D:\a\numpy\numpy failed with code 1. None
```

</details>

@HaoZeke this looks like a bug in the test. The subclassing of `util.F2PyTest` seems to be causing the module to be attempted to be built more than once, which won't work when the test suite is run in parallel.",2023-12-21 17:15:15,,BUG: test error collecting `f2py/tests/test_abstract_interface.py`,"['00 - Bug', 'component: numpy.f2py']"
25436,open,cdeepali,"### Describe the issue:

Installation using pip fails on ppc64le. 

### Reproduce the code example:

```python
conda create -y -n buildenvgxx11 gxx_linux-ppc64le
conda activate buildenvgxx11
pip install numpy==1.23.5
```


### Error message:

```shell
INFO: compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Ibuild/src.linux-ppc64le-3.10/numpy/core/src/multiarray -Ibuild/src.linux-ppc64le-3.10/numpy/core/src/common -Ibuild/src.linux-ppc64le-3.10/numpy/core/src/umath -Inumpy/core/include -Ibuild/src.linux-ppc64le-3.10/numpy/core/include/numpy -Ibuild/src.linux-ppc64le-3.10/numpy/distutils/include -Ibuild/src.linux-ppc64le-3.10/numpy/core/src/npysort -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/opt/conda/include/python3.10 -Ibuild/src.linux-ppc64le-3.10/numpy/core/src/common -Ibuild/src.linux-ppc64le-3.10/numpy/core/src/npymath -c'
      extra options: '-O3 -mcpu=power10 -mtune=power10'
      INFO: powerpc64le-conda-linux-gnu-cc: build/src.linux-ppc64le-3.10/numpy/core/src/umath/loops_arithmetic.dispatch.vsx4.c
      INFO: powerpc64le-conda-linux-gnu-cc: build/src.linux-ppc64le-3.10/numpy/core/src/umath/loops_trigonometric.dispatch.vsx4.c
      INFO: powerpc64le-conda-linux-gnu-cc: build/src.linux-ppc64le-3.10/numpy/core/src/umath/loops_hyperbolic.dispatch.vsx4.c
      during RTL pass: expand
      In file included from build/src.linux-ppc64le-3.10/numpy/core/src/umath/loops_hyperbolic.dispatch.vsx4.c:11:
      numpy/core/src/umath/loops_hyperbolic.dispatch.c.src: In function 'FLOAT_tanh_VSX4':
      numpy/core/src/umath/loops_hyperbolic.dispatch.c.src:374:9: internal compiler error: in rs6000_sibcall_aix, at config/rs6000/rs6000.c:25670
        374 |         npy_clear_floatstatus_barrier((char*)dimensions);
            |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
      Please submit a full bug report,
      with preprocessed source if appropriate.
      See <https://gcc.gnu.org/bugs/> for instructions.
      during RTL pass: expand
      In file included from build/src.linux-ppc64le-3.10/numpy/core/src/umath/loops_arithmetic.dispatch.vsx4.c:11:
      numpy/core/src/umath/loops_arithmetic.dispatch.c.src: In function 'BYTE_divide_VSX4':
      numpy/core/src/umath/loops_arithmetic.dispatch.c.src:76:13: internal compiler error: in rs6000_sibcall_aix, at config/rs6000/rs6000.c:25670
         76 |             npy_set_floatstatus_overflow();
            |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
      Please submit a full bug report,
      with preprocessed source if appropriate.
      See <https://gcc.gnu.org/bugs/> for instructions.
      error: Command ""<myenv>buildenvgxx11/bin/powerpc64le-conda-linux-gnu-cc -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O3 -Wall -mcpu=power8 -fPIC -O3 -isystem /opt/conda/include -mcpu=power8 -fPIC -O3 -isystem /opt/conda/include -mcpu=power8 -mtune=power8 -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O3 -pipe -isystem <myenv>buildenvgxx11/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem <myenv>buildenvgxx11/include -fPIC -DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Ibuild/src.linux-ppc64le-3.10/numpy/core/src/multiarray -Ibuild/src.linux-ppc64le-3.10/numpy/core/src/common -Ibuild/src.linux-ppc64le-3.10/numpy/core/src/umath -Inumpy/core/include -Ibuild/src.linux-ppc64le-3.10/numpy/core/include/numpy -Ibuild/src.linux-ppc64le-3.10/numpy/distutils/include -Ibuild/src.linux-ppc64le-3.10/numpy/core/src/npysort -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/opt/conda/include/python3.10 -Ibuild/src.linux-ppc64le-3.10/numpy/core/src/common -Ibuild/src.linux-ppc64le-3.10/numpy/core/src/npymath -c build/src.linux-ppc64le-3.10/numpy/core/src/umath/loops_arithmetic.dispatch.vsx4.c -o build/temp.linux-ppc64le-3.10/build/src.linux-ppc64le-3.10/numpy/core/src/umath/loops_arithmetic.dispatch.vsx4.o -MMD -MF build/temp.linux-ppc64le-3.10/build/src.linux-ppc64le-3.10/numpy/core/src/umath/loops_arithmetic.dispatch.vsx4.o.d -O3 -mcpu=power10 -mtune=power10"" failed with exit status 1
      
      

      ########### EXT COMPILER OPTIMIZATION ###########
      INFO: Platform      :
        Architecture: ppc64le
        Compiler    : unix-like

      CPU baseline  :
        Requested   : 'min'
        Enabled     : VSX VSX2
        Flags       : -mcpu=power8
        Extra checks: VSX_ASM

      CPU dispatch  :
        Requested   : 'max -xop -fma4'
        Enabled     : VSX3 VSX4
        Generated   :
                    :
        VSX3        : VSX VSX2
        Flags       : -mcpu=power9 -mtune=power9
        Extra checks: none
        Detect      : VSX3
                    : build/src.linux-ppc64le-3.10/numpy/core/src/umath/loops_trigonometric.dispatch.c
                    :
        VSX4        : VSX VSX2 VSX3
        Flags       : -mcpu=power10 -mtune=power10
        Extra checks: VSX4_MMA
        Detect      : VSX4
                    : build/src.linux-ppc64le-3.10/numpy/core/src/umath/loops_arithmetic.dispatch.c
                    : build/src.linux-ppc64le-3.10/numpy/core/src/umath/loops_trigonometric.dispatch.c
                    : build/src.linux-ppc64le-3.10/numpy/core/src/umath/loops_hyperbolic.dispatch.c
                    : build/src.linux-ppc64le-3.10/numpy/core/src/umath/loops_modulo.dispatch.c
      INFO: CCompilerOpt.cache_flush[857] : write cache to path -> /tmp/pip-install-kw88t5sf/numpy_228008dbdfce4094a9475c5197e2ff88/build/temp.linux-ppc64le-3.10/ccompiler_opt_cache_ext.py
      INFO:
      ########### CLIB COMPILER OPTIMIZATION ###########
      INFO: Platform      :
        Architecture: ppc64le
        Compiler    : unix-like

      CPU baseline  :
        Requested   : 'min'
        Enabled     : VSX VSX2
        Flags       : -mcpu=power8
        Extra checks: VSX_ASM

      CPU dispatch  :
        Requested   : 'max -xop -fma4'
        Enabled     : VSX3 VSX4
        Generated   : none
      INFO: CCompilerOpt.cache_flush[857] : write cache to path -> /tmp/pip-install-kw88t5sf/numpy_228008dbdfce4094a9475c5197e2ff88/build/temp.linux-ppc64le-3.10/ccompiler_opt_cache_clib.py
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for numpy

```


### Python and NumPy Versions:

Numpy: 1.23.5, 1.26.2
GCC:  Anaconda GCC 11.2 

### Runtime Environment:

_No response_

### Context for the issue:

Unable to build Tensorflow on ppc64le - https://github.com/tensorflow/tensorflow/issues/62659",2023-12-20 13:51:05,,BUG: pip install fails on ppc64le after VSX4 introducion,"['00 - Bug', '32 - Installation', 'component: SIMD']"
25435,open,flexatone,"### Describe the issue:

The NumPy `isin` and `in1d` functions fail when `test_elements` is a length-1 unsigned integer with a value that overflows on cast to a signed integer. The functions perform as expected for values that do not overflow, or when the `test_elelements` array as more than 1 value.
 

### Reproduce the code example:

```python
>>> # `test_elements` is a single value in a uint64 array
>>> np.in1d(np.array([0, 1], dtype=np.int64), np.array((9223372036854775807,), dtype=np.uint64)) 
array([False, False])

>>> # as soon as `test_elements` contains a value that overflows, we get an `IndexError` 
>>> np.in1d(np.array([0, 1], dtype=np.int64), np.array((9223372036854775808,), dtype=np.uint64)) 
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ariza/.env312/lib/python3.12/site-packages/numpy/lib/arraysetops.py"", line 699, in in1d
    outgoing_array[basic_mask] = isin_helper_ar[ar1[basic_mask] -
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: arrays used as indices must be of integer (or boolean) type

>>> # however, if there is more than one number in that array, it works as expected
>>> np.in1d(np.array([0, 1], dtype=np.int64), np.array((9223372036854775808, 42), dtype=np.uint64))
array([False, False])

>>> # presumably all numbers between 9223372036854775808 and np.iinfo(np.uint64).max will fail
>>> np.in1d(np.array([0, 1], dtype=np.int64), np.array((np.iinfo(np.uint64).max,), dtype=np.uint64))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ariza/.env312/lib/python3.12/site-packages/numpy/lib/arraysetops.py"", line 699, in in1d
    outgoing_array[basic_mask] = isin_helper_ar[ar1[basic_mask] -
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: arrays used as indices must be of integer (or boolean) type

>>> # consistently, as soon as we have more than one number it works as expected
>>> np.in1d(np.array([0, 1], dtype=np.int64), np.array((np.iinfo(np.uint64).max, 42), dtype=np.uint64))
array([False, False])
```


### Error message:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ariza/.env312/lib/python3.12/site-packages/numpy/lib/arraysetops.py"", line 699, in in1d
    outgoing_array[basic_mask] = isin_helper_ar[ar1[basic_mask] -
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: arrays used as indices must be of integer (or boolean) type
```



### Python and NumPy Versions:

```
>>> import sys, numpy; print(numpy.__version__); print(sys.version)
1.26.2
3.12.0 (main, Dec  6 2023, 09:10:38) [GCC 9.4.0]
```


### Runtime Environment:

```
>>> import numpy; print(numpy.show_runtime())
WARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information
[{'numpy_version': '1.26.2',
  'python': '3.12.0 (main, Dec  6 2023, 09:10:38) [GCC 9.4.0]',
  'uname': uname_result(system='Linux', node='ariza-is-p15', release='5.15.0-71-generic', version='#78~20.04.1-Ubuntu SMP Wed Apr 19 11:26:48 UTC 2023', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}}]
None
```


### Context for the issue:

I found this error through a Hypothesis-based test run on [StaticFrame](https://github.com/static-frame/static-frame). It is a rarely encountered issue but relevant when having containers with heterogeneous array types.",2023-12-19 21:26:32,,BUG: `isin` and `in1d` fail when `test_elements` is a length-1 unsigned integer with a value that overflows on cast to a signed integer,['00 - Bug']
25424,open,dibnob44,"### Proposed new feature or change:

 I do not think f2py needs to cater for defined types, overloading and else structures where C and F95 differ too much-user may switch directly to C/C++ instead. I understand such structures would be permitted between f90 routines, gfortran or else compiler taking care, but not at python interface serviced by f2py. However, all  f90  scalar and array declarations should be properly processed, including  those involving integer expression and lower bounds for dimensions, and calling
for dynamic memory allocation and f2py should not complain on valid intrinsic subroutines (e.g. tiny() in my example).
To better track both fortran and f2py auto-generated C errors, effort should be made to better relate  lines in auto-generated C code to fortran constructs/line range. Line number in the C code does not tell much from where it comes in Fortran. The extent of compatibility with Fortran95 should be described  IN DETAIL in f2py documentation.",2023-12-18 20:09:32,,DOC: `f2py` compatibility with Fortran95 clarification,"['04 - Documentation', 'component: numpy.f2py']"
25421,open,dibnob44,"### Describe the issue:

Sorry as I cannot be more specific on the bug nature because there is no way to figure to what fortran feature is related
wrong syntax in the C code generated by f2py from fortran90 source, hence example files are quite long. As ""paste drop or click"" does not accept a valid TGZ file, you need to download archive from https://users.camk.edu.pl/alex/#software
tab ""m"". There are 3 .f90 files, to be compiled with build.sh script (note f2py<-->f2py3 name may need editing depending on your system configuration). They were compiled correctly
by numpy 1.21.5 f2py under linux (listing in enclosed archive) and the .so library worked OK in python, they no longer compile correctly in numpy 1.26.2. The error appears during compilation od C code generated by f2py (listing enclosed).
A MSWindow user claims on his installation numpy 1.23.5 compilation worked ok.
I also noticed f2py complained on valid intrinsic procedure tiny()-a separate issue. Included build.sh file may need editing
f2py<-->f2py3, depending on installation. gfortran compiles all 3 .f90 files without complain.

### Reproduce the code example:

```python
No short example feasible as bug appears in C code auto-generated by f2py with no clear link to fortran source. See numpy 1.21.5 and 1.26.2 compile listing in the archive put on web:
https://users.camk.edu.pl/alex/#software under tab ""m"". Also enclosed is compile script and f90 code, so that results may be reproduced under both numpy versions at your place.
```


### Error message:

_No response_

### Python and NumPy Versions:

numpy 1.21.5 and 1.26.2, Python 3.10.12 (for newer numpy)

### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2023-12-18 19:00:44,,BUG: f2py generates error C code out of fortran90,"['00 - Bug', 'component: numpy.f2py']"
25415,open,HaoZeke,"At the moment, since the switch to `meson` as the default, all the compile tests had to be marked slow. At first, it was assumed that the fixture based tests were at fault (https://github.com/numpy/numpy/pull/25252). However, on further investigation, the problem is that that the `meson` is quite a bit slower in general:

```bash
❯ hyperfine ""f2py --verbose -c _bufrlib.pyf -m _bufrlib -L$PWD -lbvers --backend distutils"" ""f2py --verbose -c _bufrlib.pyf -m _bufrlib -L$PWD -lbvers --backend meson""
Benchmark 1: f2py --verbose -c _bufrlib.pyf -m _bufrlib -L/blah/playground/numpy_bugs/gh-25266 -lbvers --backend distutils
  Time (mean ± σ):     380.0 ms ± 179.0 ms    [User: 531.0 ms, System: 941.1 ms]
  Range (min … max):   219.0 ms … 656.2 ms    10 runs
 
Benchmark 2: f2py --verbose -c _bufrlib.pyf -m _bufrlib -L/blah/playground/numpy_bugs/gh-25266 -lbvers --backend meson
  Time (mean ± σ):      1.512 s ±  0.042 s    [User: 1.964 s, System: 2.480 s]
  Range (min … max):    1.465 s …  1.588 s    10 runs
 
Summary
  f2py --verbose -c _bufrlib.pyf -m _bufrlib -L/blah/playground/numpy_bugs/gh-25266 -lbvers --backend distutils ran
    3.98 ± 1.88 times faster than f2py --verbose -c _bufrlib.pyf -m _bufrlib -L/blah/playground/numpy_bugs/gh-25266 -lbvers --backend meson
hyperfine    25.10s user 34.36s system 309% cpu 19.216 total
```

This would be very nice to fix, but it is also kind of **low prio** right now (in light of the `meson` backend still sort of being brought up to speed / tests are running on CI, just maybe not as often as they could). ",2023-12-18 07:07:44,,"MAINT, ENH: Try to speed up the `meson` backend",['component: numpy.f2py']
25396,open,lysnikolaou,"### Proposed new feature or change:

Calling `astype` on an ndarray with values that will overflow under the new dtype currently succeeds and the values *do* overflow.

```python3
In [1]: max_int32 = np.iinfo(np.int32).max

In [2]: np.array([max_int32 + 1]).astype(np.int32)
Out[2]: array([-2147483648], dtype=int32)
```

Do you think we could add a new casting option to `astype` that would check values for overflow? Something like the following (the exact option name and the resulting behavior - excpetion vs warning, etc. - is of course open to discussion):

```python3
In [3]: np.array([max_int32 + 1]).astype(np.int32, casting=""checkoverflow"")
---------------------------------------------------------------------------
OverflowError                                Traceback (most recent call last)
Cell In[3], line 1
----> 1 np.array([max_int32 + 1]).astype(np.int32, casting=""checkoverflow"")

OverflowError: Cannot cast array data from dtype('int64') to dtype('int32') because some values would overflow
```

This is already done for operations on scalars, but not on arrays, which I assume is due to the performance bottleneck it'd create, if we'd have to check integer overflow by hand. The same does not apply to `astype` though, since only the users that would be willing to sacrifice performance for checks would do so.

Thoughts?",2023-12-14 17:27:31,,ENH: Add casting option to `astype` that would raise in case of overflow,['unlabeled']
25391,open,rot8,"### Describe the issue:

From a mathematical standpoint the trace is [only defined for square matrices ](https://en.wikipedia.org/wiki/Trace_(linear_algebra)) or more generally for matrices in ℂ<sup>n</sup>.

However, the [numpy implementation](https://github.com/numpy/numpy/blob/v1.26.0/numpy/core/fromnumeric.py#L1700-L1761) returns the sum of entries where all indices are the same which for square matrices coincides with the trace but also returns wrong results disguised as trace if the matrix is not square. This can result in unwanted behavior as one could use the trace to check if the matrix is square, expecting an error if it is not.

I can see that this implementation of the trace may have its use cases. But I would strongly suggest modifying the default function to return only the mathematically correct trace and extending the function with a flag which allows non-square matrices.

```
import numpy as np

A = np.array([[1, 2, 3],
              [1, 2, 3]])

print(A.trace())
```
Result: `3`
Expected Result: `""AssertionError: Trace is not defined for non-square matrices""`

Further, I am interested if anyone knows why this implementation was chosen and what is actual use cases are.


### Reproduce the code example:

```python
import numpy as np

A = np.array([[1, 2, 3],
              [1, 2, 3]])

print(A.trace())
```


### Error message:

```shell
No Error
```


### Python and NumPy Versions:

numpy: 1.21.5
sys: 3.9.13


### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2023-12-14 08:15:07,,BUG: Trace returns results for non-square matrices,['00 - Bug']
25375,open,tylerjereddy,"I don't see an issue open for this at the moment, but the search for ""dtype repr"" returns a bunch of results. This came up in gh-25285 and over in SciPy at i.e., https://github.com/scipy/scipy/pull/19686.

Because different platforms/archs can interpret i.e., the single-character typecodes ""differently,"" we can end up with stuff like this, where the reprs are deceptive of the bit layout ""meaning"" (true type), even though the bits are in the same places:

```python
# arm64 Mac example
>>> np.array([0], dtype=""G"").dtype
dtype('complex128')
>>> np.array([0], dtype=""D"").dtype
dtype('complex128')
>>> np.array([0], dtype=""G"").dtype == np.array([0], dtype=""D"").dtype
True
>>> np.array([0], dtype=""G"").tobytes() == np.array([0], dtype=""D"").tobytes()
True
>>> np.issubdtype(np.array([0], dtype=""G"").dtype, np.clongdouble)
True
>>> np.issubdtype(np.array([0], dtype=""D"").dtype, np.clongdouble)
False
```

Perhaps `dtype` reprs might eventually reflect the bit layout ""meaning"" (the true type, rather than ""some type"" that represents just the same bit width?). It sounds like Sebastian might have been ""ok"" with some approach to resolving this, though I may have misunderstood.

",2023-12-12 20:08:35,,"ENH: granular dtype reprs, even when bitwise-identical",['01 - Enhancement']
25374,open,xor2k,"### Proposed new feature or change:

Based on https://github.com/numpy/numpy/pull/25347#issuecomment-1848700866

Of course, this concept is up for discussion, I'd like to have some feedback and therefore, I will also provide multiple options for various aspects.

Currently, Numpy does allow to store arbitrary data within arrays via the dtype object. However, it comes with a lot of overhead when writing contents to disk: contents need to be pickled, which consumes both a lot of space and CPU usage, since serialization is _actually_ necessary. Also, since the memory representation is different from the serialization, object array .npy files cannot be (efficiently) memory mapped.

On the other hand, there are Numpy datatypes (byte, int, float, double, ... and structs thereof) that can be written to disk very efficiently with barely any serialization necessary besides the creation of a header, which is just a few bytes long. Instead, memory is just dumped into a file without any processing which can happen at the speed of the interface. At the time of writing, modern consumer hardware (PCIE 5.0 SSDs with 4 lanes) exists that can read and write with more than 10 GBytes/s. Not too many years ago this used to be the speed of RAM, so memory mapping becomes even more viable nowadays. On the other hand, e.g. large language models (LLMs) and video consume memory like never before with no end in sight, so there will be demand. Even when not using larger arrays then the main memory, memory mapping is convenient by eliminating load times between program runs.

With certain applications however come problems: e.g. both text (LLMs) as well as video can vary in length (ragged arrays), which is currently only supported via the object dtype. While text is being worked on https://github.com/numpy/numpy/pull/25347 there is no solution for serialization and there is also no solution for ragged arrays (like e.g. video) yet.

This issue suggests how to handle certain datatypes which are currently only available via the object interface in a way that they can be (de)serialized efficiently - in other words ""dumped"" from memory and memory mapped. It would also be interesting to preserve the ability to append efficiently to .npy files and also to not modify the file format specified in [NEP 1](https://numpy.org/neps/nep-0001-npy-format.html), but just add new dtypes.

Appendability, although not explicitly mentioned in NEP 1 is quite a sought-after property, compare the discussion and the top answer to https://stackoverflow.com/a/30379177 This also shows that the .npy file format is popular not _despite_ but _because_ of its simplicity.

I would like to suggest the following dtypes:

1. Reference (alternatively: Enumeration, Reference, Pointer, Offset or Index)
Array Indices would refer to the first axis (0 for C order, -1 for Fortran order), which would also determine the final shape of the array. For example, if the referred-to array has the shape (100, 20, 30) ""array of images"" and C order, then a Reference would be a (20,30) ""image"". If the referring array has the shape (120, 5, 10), then the final shape would be (120, 5, 10, 20, 30). I would not recommend to use byte offsets since this makes it more difficult to do certain modifications to the referred-to array: for video one could modify all images (e.g. using a different crop, i.e. different dimensions) without invalidating the References (not possible with byte offsets).
2. Range (two references from 1. or one reference and one length)
To create ragged arrays, Ranges are required. This would require to modify the concept of a shape. Raggedness can happen in multiple dimensions, depending on how many levels of References/Ranges are used. For the shapes above, using Ranges instead of References would result in the final array to have a shape like (120, 5, 10, *, 20, 30) with * indicating varying lengths.
3. Strings (can be either zero delimited UNIX style strings, then it's a reference (see 1), otherwise a Range into a byte array)
3.1. Zero delimited is more space efficient both with references and storage
3.2. Ranges into byte arrays use more space, but certain operations can be faster, strings can contain zero (has advantages and disadvantages)
3.3. One could theoretically also implement both String variants and let the user decide

So basically, all of those dtypes are uint64 numbers or tuples with two elements of uint64 numbers. They can point to different arrays that can be stored in different .npy files and/or in memory, making these dtypes memory mappable. Since multiple regular .npy files are used, they can always be appended to. Doing many append operations on multiple files may stress the file system (fragmentation), but makes the implementation easier.

Additional .npy files can be organized in a hierarchical way (one .npy file and all other filenames derived from that) or with multiple files referring to each other like in Microsoft Excel, with the difference that references are not per-cell, but per column. Both options do not exclude each other. It would be possible to make filenames in dtypes just optional. If a filename is provided, it is option 2, otherwise option 1 and filenames are derived automatically.

# Option 1: Organize .npy hierarchically with automatic filenames
Let's say we save some ""main.npy"" which refers to other arrays. Then, all those other arrays can be saved as ""main.npy.heap1"", ""main.npy.heap2"", ... if the dtype from main.npy is struct and multiple struct fields are References, Ranges or Strings or just ""main.npy.heap"" for simple, non-struct dtypes. One could also use a different ending for Strings, like ""main.npy.strings"" and it would be a (regular .npy) byte array. Although the heap files do not end in .npy, they would be regular .npy files. When the file main.npy gets loaded, all files of all reference dtypes are loaded or memory mapped as well. Even without extra documentation, a user could quickly see that those files belong together, since they share the main .npy file's name. String dtypes would require such approach, if one did not want to embed strings into the .npy file format by introducing a new Numpy file format version and/or sacrificing appendability.

# Option 2: Excel-like cross referenced .npy files
If one array has a Reference, Range or String dtype that refers to another array, both could be saved in different files with the filenames being specified explicitly in the dtype description. This would be comparable to cell cross file references in Excel, just on a per-column basis, e.g. ""video.npy"" could have one Range datatype that refers to ""image.npy"" with ""image.npy"" containing all (uncompressed, maybe low res) images of all videos in their order. Another file ""video_events.npy"" could contain Ranges to parts of those video files where some events happen. So one could have multiple Reference/Range sets into the same data. With option 1 this would require ""image.npy"" to be duplicated, which might or might not be convenient/possible.

That's it for now. Any feedback, suggestions, questions?",2023-12-12 16:28:07,,ENH: new datatypes for efficient storage,"['01 - Enhancement', '57 - Close?']"
25363,open,AntoniosBarotsis,"### Issue with current documentation:

I have ~2~ 1 things to note:

- > Each parameter represents the coordinates of the array varying along a specific axis.

  This leads the reader to assume that `i` and `j` will just be indices they can use which is not the case.
- > ~then the parameters would be `array([[0, 0], [1, 1]])` and `array([[0, 1], [0, 1]])`~
  
  ~appears to have a typo as `[0,1]` is mentioned twice~ [see here](https://github.com/numpy/numpy/issues/25363#issuecomment-1853416556)

The function is defined [here](https://github.com/numpy/numpy/blob/v1.26.0/numpy/core/numeric.py#L1776-L1845), rendered docs [here](https://numpy.org/doc/stable/reference/generated/numpy.fromfunction.html).

### Idea or request for content:

I came across [this](https://stackoverflow.com/questions/18702105/parameters-to-numpys-fromfunction) SO question which describes the same issue I had. The [solution](https://stackoverflow.com/a/24900335/12756474) also solved my problem.

In my opinion, explaining what `i` and `j` actually are should be both present and clear. In my case, the fact that I have used functions with similar names in different programming languages made me assume that this would work the same which is not true, I would assume this is true for a lot of people which is why it should be very obvious that this is not the case.

This seems to have been an issue for a while, I would be interested in making a PR for this as long as some discussion is had about how the docs could be changed :)",2023-12-11 11:43:11,,DOC: Misleading docs for `np.fromfunction`,['04 - Documentation']
25362,open,pinusm,"### Describe the issue:

When I try to compute the weighted mean of an array that includes `inf`, and that element is given a weight of 0, I'm getting a RuntimeWarning, and a nan result.
This isn't the case when the weight is just slightly larger than zero.

Drilling this down, I reached np.multiply(), but I'm not familiar enough with python/C to dig deeper..

### Reproduce the code example:

```python
import sys

import numpy as np

print(np.average([1, 1], weights=[1, 0]))
# 1.0
print(np.average([np.inf, 1], weights=[1, 1]))
# inf
print(np.average([np.inf, 1], weights=[1, 0]))
# inf
print(np.average([np.inf, 1], weights=[0, 1]))
# nan
# RuntimeWarning: invalid value encountered in multiply avg = np.multiply(a, wgt,

eps = sys.float_info.epsilon
print(np.average([np.inf, 1], weights=[0+eps, 1]))
# inf
```


### Error message:

```shell
C:\Users\REDACTED\anaconda3\envs\REDACTED\lib\site-packages\numpy\lib\function_base.py:550: RuntimeWarning: invalid value encountered in multiply
  avg = np.multiply(a, wgt,
```


### Python and NumPy Versions:

1.26.2
3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]

### Runtime Environment:

[{'numpy_version': '1.26.2',
  'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, '
            '13:26:23) [MSC v.1916 64 bit (AMD64)]',
  'uname': uname_result(system='Windows', node='DESKTOP-FIBPK27', release='10', version='10.0.22631', machine='AMD64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'filepath': 'C:\\Users\\micha\\miniconda3\\envs\\np_bug\\Library\\bin\\mkl_rt.2.dll',
  'internal_api': 'mkl',
  'num_threads': 8,
  'prefix': 'mkl_rt',
  'threading_layer': 'intel',
  'user_api': 'blas',
  'version': '2023.1-Product'}]
None

### Context for the issue:

I came across this trying to use scipy's `gmean()` on arrays that contain 0, with 0 weights.
https://github.com/scipy/scipy/issues/19647

",2023-12-10 09:27:47,,BUG: average() returns nan when an `inf` value is given a 0 weight,['00 - Bug']
25351,open,flexatone,"### Proposed new feature or change:

Greetings!

Could `np.object_` be made generic? 

For example, if I have an object array of two-element integer tuples, it would be ideal if I could type hint the array with `np.dtype[np.object_[tuple[int, int]]]`; this would allow us to distinguish between object arrays by their contained types. (I.e., `np.dtype[np.object_[tuple[int, int]]]` is not the same as `np.dtype[np.object_[datetime.date]]`.)

I am happy to explore a PR if this seems like a good enhancement.

I posted this on the ""numpy-discussion"" forum but there was no feedback.



",2023-12-08 20:45:38,,ENH: Make ``np.object_`` generic for better object array type hinting,['unlabeled']
25306,open,timhoffm,"### Proposed new feature or change:

I suggest to deprecate this feature. This is essentially already alluded in ""we could consider deprecating even more"" in https://github.com/numpy/numpy/pull/13578#issuecomment-755400474.

It seems too permissive (allowing sloppy user code) and complicates NumPy code (e.g. #13003). Users should instead dereference the `.dtype` attribute explicitly (""explicit is better than implicit""), e.g.
``` 
np.array(data, dtype=obj.dtype)
# instead of
np.array(data, dtype=obj)
```

Also, the [current documentation](https://numpy.org/devdocs/reference/arrays.dtypes.html) is not exactly correct. It claims

> Any type object with a `.dtype` attribute

but NumPy explicitly disallows this for ndarrays - for good reason, because `np.array(data, dtype=other_array)` would be semantically sloppy.

OTOH, I can for example do this with a pandas.Series (`np.array(data, dtype=series)`). IMHO this should not work for pandas.Series or other objects. An additional unwanted side effect is that one can have quite strange comparions due to dtypes comparing True to valid data type specifications
```
>>> np.dtype('int64') == pd.Series([1, 2])
True
```
",2023-12-04 01:28:35,,DEP: Deprecate constructing dtypes from any object having a .dtype attribute,['unlabeled']
25304,open,timhoffm,"### Proposed new feature or change:

This was deprecated in #15534.

It seems appropriate to expire the deprecation in 2.0.",2023-12-04 00:51:56,,DEP: Expire deprecation of generic scalars as dtypes,['unlabeled']
25290,open,timhoffm,"### Describe the issue:

I would have assumed that for a scalar *element* input like `np.isin(1, [1, 2, 3])`, I'll get a simple scalar bool result. However, the result is a 0d bool array.

This expectation is in analogy to e.g. `np.exp(0)`, which also returns a simple float for a float input.

Preferable for consistency would be a change to returning simple bools for scalar input. But that may raise compatibility issues. I'm unclear whether you could go for this (maybe as part of the 2.0 transition?). Alternatively, this behaviour should at least be documented in https://numpy.org/doc/stable/reference/generated/numpy.isin.html#numpy.isin

### Reproduce the code example:

```python
import numpy as np
np.isin(1, [1, 2, 3])
# returns a 0d array(True)
```


### Error message:

_No response_

### Python and NumPy Versions:

3.11
1.26.2

### Runtime Environment:

_No response_

### Context for the issue:

_No response_",2023-12-01 09:24:08,,API: Return type of isin() for scalar inputs,['54 - Needs decision']
25288,open,loongson-zn,"https://github.com/numpy/numpy/pull/25215#issue-2004051415

Dear @rgommers @seiko2plus, about LSX instruction CI testing problem, I did some work.
First, ubuntu does not yet have a suitable toolchain, so ci (https://github.com/numpy/numpy/blob/main/.github/workflows/linux_qemu.yml) is not applicable now.
Second， I have verified through [LoongArch sbuildQEMU on debian community](https://wiki.debian.org/LoongArch/sbuildQEMU) .  I have stored gcc and qemu that need to be upgraded( supporting simd), [Click here](https://github.com/loongson-zn/qemu-debian)  have gcc and qemu binary,  rootfs can running on  x86. If you want to do it yourself, you can refer to [README.md](https://github.com/loongson-zn/qemu-debian/blob/master/README.md)
If both of you have other ideas, I would be happy to help.

Particularize:
[gcc master branch](https://gcc.gnu.org/git/gcc.git) and [qemu master branch](https://github.com/qemu/qemu.git)  already supports the extension vector instruction of LoongArch(LSX).
",2023-12-01 08:26:13,,Discuss the issue of Numpy CI testing for LSX instructions,"['05 - Testing', 'component: SIMD']"
25239,open,HaoZeke,"The issues with the CI test timings (https://github.com/numpy/numpy/pull/25111#issuecomment-1824873678, https://github.com/numpy/numpy/issues/25134#issuecomment-1824074863, etc.) for `f2py` are part of the older `unittest` style test framework. The compiler checks can (and should) be implemented as `pytest` fixtures of `session` scope.

In keeping with the discussion on https://github.com/numpy/numpy/pull/25111#issuecomment-1824873678.",2023-11-23 20:19:56,,TST: Use fixtures for `f2py` tests,"['05 - Testing', 'component: numpy.f2py', 'component: CI']"
25235,open,cbeytas,"### Proposed new feature or change:

**numpy.searchsorted**(a, v, side='left', sorter=None)
`sorter`: Optional array of integer indices that sort array a into ascending order. They are typically the result of argsort.

The `sorter` array must be the same size as the input array.
If it is not `ValueError` will be raised:
`ValueError: sorter.size must equal a.size`

Allowing `sorter` to be smaller could allow searching a subset of the input array whose indices have been pre-sorted separately.
",2023-11-23 15:52:12,,ENH: Allow searchsorted to use a sorter array that's smaller than the input array,['unlabeled']
25229,open,HaoZeke,"Consider the following (after the corrected handling of `ISO_C` in #25226):

```python
   Ignoring map {'integer':{'c_short':'short int'}}: 'short int' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'integer':{'c_long':'long int'}}: 'long int' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'integer':{'c_long_long':'long long int'}}: 'long long int' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'integer':{'c_signed_char':'signed char'}}: 'signed char' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'integer':{'c_size_t':'size_t'}}: 'size_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'integer':{'c_int8_t':'int8_t'}}: 'int8_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'integer':{'c_int16_t':'int16_t'}}: 'int16_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'integer':{'c_int32_t':'int32_t'}}: 'int32_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'integer':{'c_int64_t':'int64_t'}}: 'int64_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'integer':{'c_int_least8_t':'int_least8_t'}}: 'int_least8_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'integer':{'c_int_least16_t':'int_least16_t'}}: 'int_least16_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'integer':{'c_int_least32_t':'int_least32_t'}}: 'int_least32_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'integer':{'c_int_least64_t':'int_least64_t'}}: 'int_least64_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'integer':{'c_int_fast8_t':'int_fast8_t'}}: 'int_fast8_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'integer':{'c_int_fast16_t':'int_fast16_t'}}: 'int_fast16_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'integer':{'c_int_fast32_t':'int_fast32_t'}}: 'int_fast32_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'integer':{'c_int_fast64_t':'int_fast64_t'}}: 'int_fast64_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'integer':{'c_intmax_t':'intmax_t'}}: 'intmax_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'integer':{'c_intptr_t':'intptr_t'}}: 'intptr_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'integer':{'c_ptrdiff_t':'intptr_t'}}: 'intptr_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Mapping ""real(kind=c_float)"" to ""float""
	Mapping ""real(kind=c_double)"" to ""double""
	Ignoring map {'real':{'c_long_double':'long double'}}: 'long double' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'complex':{'c_float_complex':'float _Complex'}}: 'float _Complex' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'complex':{'c_double_complex':'double _Complex'}}: 'double _Complex' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'complex':{'c_long_double_complex':'long double _Complex'}}: 'long double _Complex' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Ignoring map {'logical':{'c_bool':'_Bool'}}: '_Bool' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']
	Mapping ""character(kind=c_char)"" to ""char""
```

Which can be seen by passing `verbose = True` to `process_f2cmap_dict` while mapping the `iso_c` values in `capi_maps.py`. To silence these, there are the (commented out) entries in `iso_c2py_map` in `_isocbind.py`.

The reason they are commented out is, otherwise, F2PY will internally construct and try to use ""construction"" functions like these:

```C
needs['short_from_pyobj'] = ['int_from_pyobj']
cfuncs['short_from_pyobj'] = """"""
static int
short_from_pyobj(short* v, PyObject *obj, const char *errmess) {
    int i = 0;
    if (int_from_pyobj(&i, obj, errmess)) {
        *v = (short)i;
        return 1;
    }
    return 0;
}
```

Which are defined in `cfuncs.py`. Now most of these (almost all) follow a pattern, they just cast and call `int_from_pyobj` or similar. However, for the `iso_c` (and really in general for the remaining C types) there should be similar functions. 

## Solution

This could be copy pasted, but really they should be generated on the fly out of templates like the `meson` backend and its `meson.build` backend. That would also finally allow for syntax highlighting, currently modifying the `cfuncs.py` C code is rather painful.


### Addenum

For #25226 the mapping of types isn't really required. Here are the changes needed to `_isocbind.py` to reproduce the issue:

```python
iso_c_binding_map = {
    'integer': {
        'c_int': 'int',
        'c_short': 'short int',
        'c_long': 'long int',
        'c_long_long': 'long long int',
        'c_signed_char': 'signed char',
        'c_size_t': 'size_t',
        'c_int8_t': 'int8_t',
        'c_int16_t': 'int16_t',
        'c_int32_t': 'int32_t',
        'c_int64_t': 'int64_t',
        'c_int_least8_t': 'int_least8_t',
        'c_int_least16_t': 'int_least16_t',
        'c_int_least32_t': 'int_least32_t',
        'c_int_least64_t': 'int_least64_t',
        'c_int_fast8_t': 'int_fast8_t',
        'c_int_fast16_t': 'int_fast16_t',
        'c_int_fast32_t': 'int_fast32_t',
        'c_int_fast64_t': 'int_fast64_t',
        'c_intmax_t': 'intmax_t',
        'c_intptr_t': 'intptr_t',
        'c_ptrdiff_t': 'intptr_t',
    },
    'real': {
        'c_float': 'float',
        'c_double': 'double',
        'c_long_double': 'long double'
    },
    'complex': {
        'c_float_complex': 'float _Complex',
        'c_double_complex': 'double _Complex',
        'c_long_double_complex': 'long double _Complex'
    },
    'logical': {
        'c_bool': '_Bool'
    },
    'character': {
        'c_char': 'char'
    }
}

# TODO: At some point these should be included, but then they'd need special
# handling in cfuncs.py e.g. needs[int64_t_from_pyobj] These are not very hard
# to add, since they all derive from the base `int_from_pyobj`, e.g. the way
# `short_from_pyobj` and others do

isoc_c2pycode_map = {
    'int': 'i',  # int
    'short int': 'h',  # short int
    'long': 'l',  # long int
    'long long': 'q',  # long long int
    'signed char': 'b',  # signed char
    'size_t': 'I',  # size_t (approx unsigned int)
    'int8_t': 'b',  # int8_t
    'int16_t': 'h',  # int16_t
    'int32_t': 'i',  # int32_t
    'int64_t': 'q',  # int64_t
    'int_least8_t': 'b',  # int_least8_t
    'int_least16_t': 'h',  # int_least16_t
    'int_least32_t': 'i',  # int_least32_t
    'int_least64_t': 'q',  # int_least64_t
    'int_fast8_t': 'b',  # int_fast8_t
    'int_fast16_t': 'h',  # int_fast16_t
    'int_fast32_t': 'i',  # int_fast32_t
    'int_fast64_t': 'q',  # int_fast64_t
    'intmax_t': 'q',  # intmax_t (approx long long)
    'intptr_t': 'q',  # intptr_t (approx long long)
    'ptrdiff_t': 'q',  # intptr_t (approx long long)
    'float': 'f',  # float
    'double': 'd',  # double
    'long double': 'g',  # long double
    'float _Complex': 'F',  # float  _Complex
    'double _Complex': 'D',  # double  _Complex
    'long double _Complex': 'D',  # very approximate complex
    '_Bool': 'i',  #  Bool but not really
    'char': 'c',   # char
}

iso_c2py_map = {
    'int': 'int',
    'short int': 'int',                 # forced casting
    'long': 'int',
    'long long': 'long',
    'signed char': 'int',           # forced casting
    'size_t': 'int',                # approx Python int
    'int8_t': 'int',                # forced casting
    'int16_t': 'int',               # forced casting
    'int32_t': 'int',
    'int64_t': 'long',
    'int_least8_t': 'int',          # forced casting
    'int_least16_t': 'int',         # forced casting
    'int_least32_t': 'int',
    'int_least64_t': 'long',
    'int_fast8_t': 'int',           # forced casting
    'int_fast16_t': 'int',          # forced casting
    'int_fast32_t': 'int',
    'int_fast64_t': 'long',
    'intmax_t': 'long',
    'intptr_t': 'long',
    'ptrdiff_t': 'long',
    'float': 'float',
    'double': 'float',              # forced casting
    'long double': 'float',         # forced casting
    'float _Complex': 'complex',     # forced casting
    'double _Complex': 'complex',
    'long double _Complex': 'complex', # forced casting
    '_Bool': 'bool',
    'char': 'bytes',                  # approx Python bytes
}

```",2023-11-22 21:53:36,,ENH: Add templates for `f2py` c functions and handle more C types,"['00 - Bug', '01 - Enhancement', 'component: numpy.f2py']"
25216,open,bersbersbers,"### Describe the issue:

Typing thinks `np.array("""")` cannot be compared to `""""`

### Reproduce the code example:

```python
from typing import reveal_type

import numpy as np
import numpy.typing as npt

X: npt.NDArray[np.str_] = np.array("""")
reveal_type(X)
print(X > """")
print(X > X)
```


### Error message:

```shell
(.venv) C:\Build\project>pyright bug.py
C:\Build\project\bug.py
  C:\Build\project\bug.py:7:13 - information: Type of ""X"" is ""ndarray[Any, dtype[str_]]""
  C:\Build\project\bug.py:8:7 - error: Operator "">"" not supported for types ""NDArray[Any]"" and ""Literal['']"" (reportGeneralTypeIssues)
  C:\Build\project\bug.py:9:7 - error: Operator "">"" not supported for types ""NDArray[Any]"" and ""NDArray[Any]"" (reportGeneralTypeIssues)
2 errors, 0 warnings, 1 information

(.venv) C:\Build\project>mypy bug.py
bug.py:7: note: Revealed type is ""numpy.ndarray[Any, numpy.dtype[numpy.str_]]""
bug.py:8: error: Unsupported operand types for > (""ndarray[Any, dtype[str_]]"" and ""str"")  [operator]
bug.py:8: note: Following member(s) of ""str"" have conflicts:
bug.py:8: note:     Expected:
bug.py:8: note:         def __contains__(self, object, /) -> bool
bug.py:8: note:     Got:
bug.py:8: note:         def __contains__(self, str, /) -> bool
bug.py:8: note:     Expected:
bug.py:8: note:         def __getitem__(self, int, /) -> _SupportsArray[dtype[object_]] | _NestedSequence[_SupportsArray[dtype[object_]]]
bug.py:8: note:     Got:
bug.py:8: note:         def __getitem__(self, SupportsIndex | slice, /) -> str
bug.py:8: note:     <2 more conflict(s) not shown>
bug.py:8: note:     Expected:
bug.py:8: note:         def __contains__(self, object, /) -> bool
bug.py:8: note:     Got:
bug.py:8: note:         def __contains__(self, str, /) -> bool
bug.py:8: note:     Expected:
bug.py:8: note:         def __getitem__(self, int, /) -> _SupportsArray[dtype[object_]] | _NestedSequence[_SupportsArray[dtype[object_]]]
bug.py:8: note:     Got:
bug.py:8: note:         def __getitem__(self, SupportsIndex | slice, /) -> str
bug.py:8: note: See https://mypy.rtfd.io/en/stable/_refs.html#code-operator for more info
bug.py:9: error: Unsupported operand types for > (""ndarray[Any, dtype[str_]]"" and ""ndarray[Any, dtype[str_]]"")  [operator]
Found 2 errors in 1 file (checked 1 source file)
```


### Runtime information:

1.26.2
3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]

### Context for the issue:

_No response_",2023-11-21 12:37:09,,TYP: false positives in string comparison between string arrays or literals,"['00 - Bug', 'Static typing']"
25206,open,rbavery,"### Describe the issue:

When I try to use npt.DTypeLike in a pydantic BaseModel, I get an error from pydantic that won't be fixed by pydantic

```
Please use `typing_extensions.TypedDict` instead of `typing.TypedDict` on Python < 3.12.

For further information visit https://errors.pydantic.dev/2.5/u/typed-dict-version
  File ""/opt/workspace/src/sedonaai/tasks/ml_model_schemas.py"", line 46, in <module>
    class TensorInfo(BaseModel):
pydantic.errors.PydanticUserError: Please use `typing_extensions.TypedDict` instead of `typing.TypedDict` on Python < 3.12.

For further information visit https://errors.pydantic.dev/2.5/u/typed-dict-version
```
this issue was first reported here:
https://github.com/pydantic/pydantic/issues/6645

and this is where TypedDict is used from the typing module: https://github.com/numpy/numpy/blob/main/numpy/_typing/_dtype_like.py#L8


### Reproduce the code example:

```python
from pydantic import BaseModel, Field, FilePath, field_validator
from typing import List, Tuple, Dict, Optional, Literal, Union
import numpy.typing as npt

class TensorInfo(BaseModel):
    dtype: npt.DTypeLike = Field(...)
    shape: Union[Tuple[int, ...], List[int]] = Field(...)

    class Config:
        arbitrary_types_allowed = True
```


### Error message:

```shell
Please use `typing_extensions.TypedDict` instead of `typing.TypedDict` on Python < 3.12.

For further information visit https://errors.pydantic.dev/2.5/u/typed-dict-version
  File ""/opt/workspace/src/sedonaai/tasks/ml_model_schemas.py"", line 46, in <module>
    class TensorInfo(BaseModel):
pydantic.errors.PydanticUserError: Please use `typing_extensions.TypedDict` instead of `typing.TypedDict` on Python < 3.12.

For further information visit https://errors.pydantic.dev/2.5/u/typed-dict-version
```



### Runtime information:

```
1.26.2
3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]
```

```
[{'numpy_version': '1.26.2',
  'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]',
  'uname': uname_result(system='Linux', node='0591b98d73a9', release='6.2.0-36-generic', version='#37~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Oct  9 15:34:04 UTC 2', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Haswell',
  'filepath': '/usr/local/lib/python3.10/dist-packages/numpy.libs/libopenblas64_p-r0-0cf96a72.3.23.dev.so',
  'internal_api': 'openblas',
  'num_threads': 12,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.23.dev'}]

```

### Context for the issue:

I'm trying to define pydantic models for a metadata standard for computer vision models that operate on satellite images. validating numpy types would be very useful so that users of the standard can easily check if their model inputs match the correct numpy type and dimension shape. Supporting this functionality with numpy.typing is desirable across python versions and so I don't need to reinvent the wheel on how to serialize and deserialize numpy types. ",2023-11-20 19:23:13,,Pydantic v2 and numpy.typing conflict when using npt.DtypeLike in pydantic.BaseModel with python <=3.11 ,['Static typing']
25199,open,HaoZeke,"Some older issues (e.g. #19441, https://github.com/numpy/numpy/issues/22211#issuecomment-1724289771) and parts of the documentation very briefly cover distribution of extensions with F2PY sources. It seems (anecdotally) that telling people to go look at SciPy hasn't been super helpful so, a proper documentation section would be nice.",2023-11-20 14:46:22,,DOC: Explain distribution for `f2py` with `meson-python`,"['04 - Documentation', 'component: numpy.f2py']"
25197,open,srepmub,"### Describe the issue:

assigning using a masked boolean array as index does not work as expected, as it may overwrite target elems for masked index elems.. 

I know we can use 'filled', but you need to be aware of the issue and this should not be necessary.

if backward compatibility is a concern, perhaps this could be fixed for numpy 2?

### Reproduce the code example:

```python
import numpy as np
import numpy.ma as ma

arr = np.ones((2,2))
mask = np.array([[False, False], [False, True]])

marr = ma.MaskedArray(arr, mask, dtype=bool)

print(marr)
arr[marr] = 2

print(arr) # all 2's but there should be one 1
```


### Error message:

_No response_

### Runtime information:

>>> import sys, numpy; print(numpy.__version__); print(sys.version) 
1.24.2
3.11.2 (main, Mar 13 2023, 12:18:29) [GCC 12.2.0]
>>> print(numpy.show_runtime())
[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Haswell',
  'filepath': '/usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.21.so',
  'internal_api': 'openblas',
  'num_threads': 12,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.21'}]
None
>>> 


### Context for the issue:

this now causes subtle issues without any warning. ",2023-11-20 10:13:47,,index assignment using masked boolean array unexpected behaviour,['00 - Bug']
25180,open,DavidAlphaFox,"### Steps to reproduce:

in the python's repl
import numpy

### Error message:

```shell
Python 3.10.13 (main, Oct  5 2023, 16:21:31) [Clang 13.0.0 ] on openbsd7
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'sscal_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'sswap_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'scopy_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zhemm_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'izamax_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zaxpy_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'isamax_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'snrm2_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ssyr_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'sger_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dsyr2k_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zdscal_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zgerc_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zgeru_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dgemm_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dcopy_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ztrsm_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zgemm_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zscal_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ztpmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zcopy_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zswap_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zgemv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dscal_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ccopy_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dswap_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ddot_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'daxpy_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'idamax_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ctrsm_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'cherk_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'cgemm_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dtbsv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dgemv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dger_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'sgemv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'saxpy_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ztpsv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'csscal_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'cher_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'drot_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'cscal_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'cswap_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'chpr_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'icamax_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ztrmm_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zher_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zhpr_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zdotc_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'sdot_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ctrmm_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'sspmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zdrot_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ctbmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'caxpy_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ctbsv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ztrmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'cdotc_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'cgemv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'cgerc_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ctrmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'strmm_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'chemm_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'cher2k_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dzasum_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'chemv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'cher2_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dznrm2_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zdotu_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'cgeru_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dtrmm_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dsyrk_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'strsm_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dnrm2_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'sgemm_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'srot_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dtrmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zhpmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dsyr_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ssyr2k_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ssyrk_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ctpsv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ctpmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dspmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'stpmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'stpsv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ssymv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'sspr2_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dtrsm_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'scasum_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dtrsv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'scnrm2_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dasum_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'sspr_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zher2k_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dsymv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dtpmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dtpsv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zgbmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ssyr2_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dsyr2_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'strmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zhpr2_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ztbmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ztbsv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'chbmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'csrot_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zhemv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ztrsv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ctrsv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'cdotu_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ssbmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dsymm_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'chpmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dspr_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'ssymm_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'srotm_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'sasum_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zherk_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'chpr2_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'drotm_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zher2_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dspr2_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dgbmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'zhbmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'stbmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'stbsv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'strsv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dtbmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'dsbmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'cgbmv_'
python:/usr/local/lib/liblapack.so.7.1: undefined symbol 'sgbmv_'
python:/home/david/.local/lib/python3.10/site-packages/numpy/linalg/_umath_linalg.cpython-310.so: undefined symbol 'scopy_'
python:/home/david/.local/lib/python3.10/site-packages/numpy/linalg/_umath_linalg.cpython-310.so: undefined symbol 'dcopy_'
python:/home/david/.local/lib/python3.10/site-packages/numpy/linalg/_umath_linalg.cpython-310.so: undefined symbol 'ccopy_'
python:/home/david/.local/lib/python3.10/site-packages/numpy/linalg/_umath_linalg.cpython-310.so: undefined symbol 'zcopy_'
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/david/.local/lib/python3.10/site-packages/numpy/__init__.py"", line 149, in <module>
    from . import lib
  File ""/home/david/.local/lib/python3.10/site-packages/numpy/lib/__init__.py"", line 23, in <module>
    from . import index_tricks
  File ""/home/david/.local/lib/python3.10/site-packages/numpy/lib/index_tricks.py"", line 12, in <module>
    import numpy.matrixlib as matrixlib
  File ""/home/david/.local/lib/python3.10/site-packages/numpy/matrixlib/__init__.py"", line 4, in <module>
    from . import defmatrix
  File ""/home/david/.local/lib/python3.10/site-packages/numpy/matrixlib/defmatrix.py"", line 12, in <module>
    from numpy.linalg import matrix_power
  File ""/home/david/.local/lib/python3.10/site-packages/numpy/linalg/__init__.py"", line 73, in <module>
    from . import linalg
  File ""/home/david/.local/lib/python3.10/site-packages/numpy/linalg/linalg.py"", line 35, in <module>
    from numpy.linalg import _umath_linalg
ImportError: Cannot load specified object
```


### Additional information:

_No response_",2023-11-19 01:02:05,,OpenBSD/amd64 7.4 undefined symbols during the import,['32 - Installation']
25179,open,HaoZeke,"Currently `f2py` has two separate paths within `f2py2e` which do ""almost the same thing"". `run_main` is supposed to do everything but `-c` and `run_compile()` is supposed to compile to a module. There is a lot of technical debt being carried here, `run_compile()` shouldn't be doing anything different from `run_main` in the first place. This also leads to subtle bugs since the results of `-c` and regular `f2py` runs are often different. It also makes it hard to test the CLI well (at the very least, doubles the number of possible tests).",2023-11-19 00:34:39,,MAINT: Unify `run_main()` and `run_compile()`,['component: numpy.f2py']
25159,open,AlexisEspinosaGayosso,"### Describe the issue:

I have a fortran subroutine that makes use of MPI:
```
subroutine sayhello
use mpi
implicit none


    integer :: comm, rank, size, ierr, namelength
    character(len=15) :: processorname

call MPI_INIT(ierr)
call MPI_Comm_size(MPI_COMM_WORLD, size, ierr)
call MPI_Comm_rank(MPI_COMM_WORLD, rank, ierr)
call MPI_GET_PROCESSOR_NAME(processorName, namelength, ierr)
print *, 'Hello, World! I am process ',rank,' of ',size,'.'

end subroutine sayhello
```

Then, I create the library with f2py:
```
$ module load PrgEnv-gnu/8.3.3 python/3.10.10 py-numpy/1.23.4 py-mpi4py/3.1.4-py3.10.10
$ LDSHARED=ftn F77=ftn F90=ftn f2py --f90exec=ftn --verbose -c helloworld.f90 -m helloworld
```

This creates the library without errors:
`helloworld.cpython-310-x86_64-linux-gnu.so`

But when loading the library, I get an error related to MPI libraries:
```
$ python -c ""import helloworld""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: /software/projects/bq2/espinosa/f2py/f2py_test/helloworld.cpython-310-x86_64-linux-gnu.so: undefined symbol: mpi_comm_rank_
```

When reviewing the output of the compilation, I can see that the ""ftn"" wrapper (which provides all the ""options machinery"" for compiling MPI code in the HPE-Cray) is used for compilation, but not for linking (partial output below):
```
INFO: customize Gnu95FCompiler
DEBUG: find_executable('gfortran')
INFO: Found executable /opt/cray/pe/gcc/12.2.0/bin/gfortran
INFO: customize Gnu95FCompiler using build_ext
********************************************************************************
<class 'numpy.distutils.fcompiler.gnu.Gnu95FCompiler'>
version_cmd     = ['/opt/cray/pe/gcc/12.2.0/bin/gfortran', '-dumpversion']
compiler_f77    = ['ftn', '-Wall', '-g', '-ffixed-form', '-fno-second-underscore', '-fPIC', '-O3', '-funroll-loops']
compiler_f90    = ['ftn', '-Wall', '-g', '-fno-second-underscore', '-fPIC', '-O3', '-funroll-loops']
compiler_fix    = ['ftn', '-Wall', '-g', '-ffixed-form', '-fno-second-underscore', '-Wall', '-g', '-fno-second-underscore', '-fPIC', '-O3', '-funroll-loops']
linker_so       = ['/opt/cray/pe/gcc/12.2.0/bin/gfortran', '-Wall', '-g', '-Wall', '-g', '-shared']
archiver        = ['/opt/cray/pe/gcc/12.2.0/bin/gfortran', '-cr']
ranlib          = ['/opt/cray/pe/gcc/12.2.0/bin/gfortran']
linker_exe      = ['/opt/cray/pe/gcc/12.2.0/bin/gfortran', '-Wall', '-Wall']
version         = LooseVersion ('12.2.0')
libraries       = ['gfortran']
library_dirs    = ['/opt/cray/pe/gcc/12.2.0/snos/lib/gcc/x86_64-suse-linux/12.2.0/../../../../lib64', '/opt/cray/pe/gcc/12.2.0/snos/lib/gcc/x86_64-suse-linux/12.2.0/../../../../lib64', '/software/setonix/2023.08/software/linux-sles15-zen3/gcc-12.2.0/python-3.10.10-bk4mjnuv6ufkvy3gb5h62l65dgv6zost/lib']
object_switch   = '-o '
compile_switch  = '-c'
include_dirs    = ['/software/setonix/2023.08/software/linux-sles15-zen3/gcc-12.2.0/python-3.10.10-bk4mjnuv6ufkvy3gb5h62l65dgv6zost/include/python3.10']
```
The `linker_so` is calling `gfortran` directly, so it is not using all the libraries and paths that the ""ftn"" wrapper calls.

Issue is very similar (if not exactly the same) as that in:
#16481 (f2py undefined symbol with PGI fortran compiler and MPI calls)
but now when using the gfortran compiler (or ""ftn"" wrapper) in HPE-Cray EX cluster. On that similar issue, the explicit use of `<F90>` in the settings for ""linker_so"" made the trick. So I went into `gnu.py` file and noticed that the mentioned fix does not apply here, as `<F90>` setting is already there:
```
possible_executables = ['gfortran', 'f95']
    executables = {
        'version_cmd'  : [""<F90>"", ""-dumpversion""],
        'compiler_f77' : [None, ""-Wall"", ""-g"", ""-ffixed-form"",
                          ""-fno-second-underscore""],
        'compiler_f90' : [None, ""-Wall"", ""-g"",
                          ""-fno-second-underscore""],
        'compiler_fix' : [None, ""-Wall"",  ""-g"",""-ffixed-form"",
                          ""-fno-second-underscore""],
        'linker_so'    : [""<F90>"", ""-Wall"", ""-g"",],
        'archiver'     : [""ar"", ""-cr""],
        'ranlib'       : [""ranlib""],
        'linker_exe'   : [None, ""-Wall""]
    }
```

Then, I modified the `gnu.py` file and add all the paths and libraries that the `ftn` wrapper would have called in the linking step:
```
'linker_so'    : [""<F90>"", ""-Wall"", ""-g"",
                          ""-I/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/include"",
                          ""-I/opt/cray/pe/libsci/23.02.1.1/GNU/9.1/x86_64/include"",
                          ""-I/opt/cray/pe/dsmml/0.2.2/dsmml//include"",
                          ""-I/opt/cray/xpmem/2.5.2-2.4_3.47__gd0f7936.shasta/include"",
                          ""-L/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/lib"",
                          ""-L/opt/cray/pe/libsci/23.02.1.1/GNU/9.1/x86_64/lib"",
                          ""-L/opt/cray/pe/dsmml/0.2.2/dsmml//lib"",
                          ""-L/opt/cray/xpmem/2.5.2-2.4_3.47__gd0f7936.shasta/lib64"",
                          ""-ldl"",
                          ""-Wl,--as-needed,-lsci_gnu_82_mpi,--no-as-needed"",
                          ""-Wl,--as-needed,-lsci_gnu_82,--no-as-needed"",
                          ""-Wl,--as-needed,-lmpifort_gnu_91,--no-as-needed"",
                          ""-Wl,--as-needed,-lmpi_gnu_91,--no-as-needed"",
                          ""-Wl,--as-needed,-ldsmml,--no-as-needed"",
                          ""-lxpmem""],
```

With that change, the creation of the library again finished without errors, but when calling the library in python the same error happens:
```
$ python -c ""import helloworld""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: /software/projects/bq2/espinosa/f2py/f2py_test/helloworld.cpython-310-x86_64-linux-gnu.so: undefined symbol: mpi_comm_rank_
```
So, it seems that even if the `ftn` wrapper was used in the linking step, the same error may persists. What would be the error source then?

(Using a ctypes approach works for the same kind of MPI test function, so I'm wondering what could be going wrong with the f2py approach?)

### Reproduce the code example:

All explained in the Description section above (including code snippets)


### Error message:

```shell
$ python -c ""import helloworld""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: /software/projects/bq2/espinosa/f2py/f2py_test/helloworld.cpython-310-x86_64-linux-gnu.so: undefined symbol: mpi_comm_rank_
```

### Runtime information:

```
$ python -c ""import sys, numpy; print(numpy.__version__); print(sys.version)""
1.23.4
3.10.10 (main, Aug 25 2023, 00:41:58) [GCC 12.2.0 20220819 (HPE)]
```

### Context for the issue:

Users already count with postprocessing sets of scripts and functions that make use of f2py with fortran-MPI code. This error is affecting their workflows! Currently they are moving their data to an external cluster to proceed with their postprocessing which is, definitively, not ideal!",2023-11-16 05:25:53,,"BUG: f2py undefined symbol for MPI application in HPE-Cray cluster with gfortran compiler (or ""ftn"" wrapper)","['00 - Bug', 'component: numpy.f2py']"
25134,open,mattip,"From #25073 where [@HaoZeke pointed out](https://github.com/numpy/numpy/pull/25073#issuecomment-1807161922) CI is currently not properly testing f2py on windows. 

> I believe the solution is to have a new CI job, where (some) f2py tests should be tested the way users would use them, i.e. by generating the meson build directory with the skeleton template and then running meson within with the right environment variables.

",2023-11-13 05:53:31,,"CI, TST: have a f2py-specific CI run and properly skip f2py tests elsewhere","['05 - Testing', 'component: numpy.f2py']"
25096,open,Bug-Find,"Add: This is the first time I report a bug of NumPy. I don't know why the title has ""np.split(a_list, an_int) raises an exception"", because I set a different title before.

### Describe the issue:

There are two groups of issues.
1. np.fill_diagonal, np.diag, and np.diagonal can cause unexpected value assigning when flipping diagonals. 

An important underlying behavior about the underlying code of np.fill_diagonal should be as I interpret in the example below:
In a wrong flipping of the diagonal of the array:
[[0, 1]
 [2, 3]],
the underlying code of numpy first assigns the value 3 to the top-left corner of the array. Then the underlying code treats the top-left corner has the value 3 rather than the original value 0 and assign 3 to the bottom-right of the array.

But if use the subscript of array instead of np.fill_diagonal, the behavior looks to be different, as in the last example in the first code group below.

2. The document of np.array_split doesn't say that the np.split doesn't allows lists, whereas np.array_split allows lists and has weird behavior on some lists.

### Reproduce the code example:

```python
# the first code group
import numpy as np
np.__version__
a = np.arange(4).reshape(2, 2)
arr = a.copy()
arr
np.fill_diagonal(arr, np.flip(np.diag(arr.copy()))) # a correct flip
arr
arr = a.copy()
arr
np.fill_diagonal(arr, np.flip(np.diag(arr))) # has a bug
arr
# If use np.diagonal instead of np.diag in the above, the result will be the same.
arr = a.copy()
arr
np.fill_diagonal(arr, np.flip(np.diagonal(arr)))
arr
# Another example on a 5 by 5 matrix
a = np.diag([1,2,3,4,5])
arr = np.fliplr(a.copy())
# a correct assigning similar to a way of assigning anti-diagonal by np.fliplr that is showed near the end of the document of np.fill_diagonal
np.fill_diagonal(np.fliplr(arr), np.diag(np.flip(np.fliplr(arr.copy()))))
arr
arr = np.fliplr(a.copy())
arr
np.fill_diagonal(np.fliplr(arr), np.diag(np.flip(np.fliplr(arr)))) # has a bug
arr
arr = np.fliplr(a.copy())
np.fill_diagonal(np.fliplr(arr), np.diagonal(np.flip(np.fliplr(arr)))) # has a bug
arr
# A similar bug on Assigning
arr = np.fliplr(a.copy())
idx = np.arange(arr.shape[0])
idx
arr[-idx-1, idx] = np.diag(np.fliplr(arr.copy())) # a correct flip
arr
arr = np.fliplr(a.copy())
arr
arr[-idx-1, idx] = np.diag(np.fliplr(arr)) # bug
arr
arr = np.fliplr(a.copy())
arr[-idx-1, idx] = np.diagonal(np.fliplr(arr)) # bug
arr

# the second code group
List = [range(4), range(4)]
np.array_split(List, 2, axis=1)
List = [[0,1,2,3], [0,1,2,3]]
np.array_split(List, 2, axis=1)
np.split(List, 2, axis=1) # as the document of np.split says, the first positional argument should be ndarray, so this code will cause error, but the error message doesn't tell users to use ndarray
import sys, numpy; print(numpy.__version__); print(sys.version)
print(numpy.show_runtime())
```


### Error message:

```shell
Traceback (most recent call last):
  File ""D:\Lib\site-packages\numpy\lib\shape_base.py"", line 859, in split
    len(indices_or_sections)
TypeError: object of type 'int' has no len()

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#52>"", line 1, in <module>
    np.split(List, 2, axis=1) # as the document of np.split says, the first positional argument should be ndarray, so this code will cause error, but the error message doesn't tell users to use ndarray
  File ""D:\Lib\site-packages\numpy\lib\shape_base.py"", line 862, in split
    N = ary.shape[axis]
AttributeError: 'list' object has no attribute 'shape'
```


### Runtime information:

1.25.2
3.11.5 (tags/v3.11.5:cce6ba9, Aug 24 2023, 14:38:34) [MSC v.1936 64 bit (AMD64)]
[{'numpy_version': '1.25.2',
  'python': '3.11.5 (tags/v3.11.5:cce6ba9, Aug 24 2023, 14:38:34) [MSC v.1936 '
            '64 bit (AMD64)]',
  'uname': uname_result(system='Windows', node='LAPTOP-G4EA37VA', release='10', version='10.0.22621', machine='AMD64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Zen',
  'filepath': 'D:\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll',
  'internal_api': 'openblas',
  'num_threads': 12,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.23.dev'}]
None

### Context for the issue:

_No response_",2023-11-09 12:36:14,,"BUG: `np.split(a_list, an_int)` raises an exception",['00 - Bug']
25053,open,YiqunChen1999,"### Describe the issue:

Calling np.histogram(data, bins=10000) raise IndexError: index 10008 is out of bounds for axis 0 with size 10001. The data is attached here.
[data.zip](https://github.com/numpy/numpy/files/13235543/data.zip)


### Reproduce the code example:

```python
# unzip the data.zip and get the data.pkl first.

import numpy as np
import pickle

with open('data.pkl', 'rb') as fp:
    data = pickle.load(fp)

np.histogram(data, bins=10000)
```


### Error message:

```shell
Traceback (most recent call last):
  File ""/xxxx/lib/python3.10/site-packages/numpy/lib/histograms.py"", line 844, in histogram
    decrement = tmp_a < bin_edges[indices]
IndexError: index 10008 is out of bounds for axis 0 with size 10001
```


### Runtime information:

`python -c ""import sys, numpy; print(numpy.__version__); print(sys.version)""` gives:
```
1.24.3
3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]
```

`python -c ""import numpy; print(numpy.show_runtime())""` gives
```
WARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information
[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}}]
None
```

### Context for the issue:

_No response_",2023-11-02 07:31:28,,BUG: np.histogram raise IndexError,['00 - Bug']
25034,open,rgommers,"_Transferred from https://github.com/numpy/numpy/issues/22711#issuecomment-1785115279 (bug report by @ashish-2022)._:

I get this error when compiling numpy 1.26.1 with intel MKL on Windows (Python 3.11.6). is there any extra environment variables I need to set for this?

```
 [7/489] Compiling C object numpy/core/libnpymath.a.p/meson-generated_ieee754.c.obj
      FAILED: numpy/core/libnpymath.a.p/meson-generated_ieee754.c.obj
      ""icl"" ""-Inumpy\core\libnpymath.a.p"" ""-Inumpy\core"" ""-I..\..\numpy\core"" ""-Inumpy\core\include"" ""-I..\..\numpy\core\include"" ""-I..\..\numpy\core\src\npymath"" ""-I..\..\numpy\core\src\common"" ""-IC:\DE-Python\Include"" ""-ID:\Script_VMC\numpy-1.26.1\.mesonpy-okitud_x\build\meson_cpu"" ""-DNDEBUG"" ""/MD"" ""/nologo"" ""/showIncludes"" ""/utf-8"" ""/W2"" ""/Qstd:c99"" ""/O3"" ""/arch:SSE3"" ""-DNPY_HAVE_SSE2"" ""-DNPY_HAVE_SSE"" ""-DNPY_HAVE_SSE3"" ""-DMS_WIN64="" ""/Fdnumpy\core\libnpymath.a.p\meson-generated_ieee754.c.pdb"" /Fonumpy/core/libnpymath.a.p/meson-generated_ieee754.c.obj ""/c"" numpy/core/libnpymath.a.p/ieee754.c
      icl: remark #10441: The Intel(R) C++ Compiler Classic (ICC) is deprecated and will be removed from product release in the second half of 2023. The Intel(R) oneAPI DPC++/C++ Compiler (ICX) is the recommended compiler moving forward. Please transition to use this compiler. Use '/Qdiag-disable:10441' to disable this message.
      ../../numpy/core/include/numpy/npy_common.h(388): catastrophic error: #error directive: npy_cdouble definition is not compatible with C99 complex definition !         Please contact NumPy maintainers and give detailed information about your         compiler and platform
        #error npy_cdouble definition is not compatible with C99 complex definition ! \
         ^

      compilation aborted for numpy/core/libnpymath.a.p/ieee754.c (code 4)
```",2023-10-30 13:40:43,,`npy_cdouble` related error when compiling 1.26.1 with Intel compilers on Windows,['32 - Installation']
25024,open,djhoese,"### Proposed new feature or change:

I'm not sure if this is a bug (it is intended to work already) or a feature request (it isn't expected to work). For context I made an issue with the xarray library and it seemed like a good idea to move that discussion here.

https://github.com/pydata/xarray/issues/8388

Bottom line is I would like mypy to understand the typing annotations and ""generic""-ness of the below:

```python
import xarray as xr
import numpy as np

def compute_relative_azimuth(sat_azi: xr.DataArray, sun_azi: xr.DataArray) -> xr.DataArray:
    abs_diff = np.absolute(sun_azi - sat_azi)
    ssadiff = np.minimum(abs_diff, 360 - abs_diff)
    return ssadiff

```

```bash
$ mypy ./xarray_mypy.py
xarray_mypy.py:7: error: Incompatible return value type (got ""ndarray[Any, dtype[Any]]"", expected ""DataArray"")  [return-value]
Found 1 error in 1 file (checked 1 source file)
```

I'm wondering if this would work if this below overload definition was swapped with the `_SupportsArrayUFunc` overload below it:

https://github.com/numpy/numpy/blob/eae0b8bc7cebda2c0e32c0552bda21ff64cf990f/numpy/_typing/_ufunc.pyi#L95-L107",2023-10-29 00:57:03,,ENH: Improved support for typing with __array_function__ usage,['Static typing']
25023,open,LeoVasanko,"### Describe the issue:

I was trying to implement my own random generator but encountered a crash when trying to use it with a Generator. Mine is based on CFFI so that was a suspect but the simplified test case below crashes any machine where I try it even though it doesn't do anything dangerous.


### Reproduce the code example:

```python
import numpy as np

class FakeRandom(np.random.BitGenerator):
    def __init__(self, seed=None):
        super().__init__(seed)

    def random_raw(self, size=None):
        return 0

    def spawn(self, n):
        raise NotImplementedError

bg = FakeRandom()
gen = np.random.Generator(bg)
print(gen)    # Prints Generator(FakeRandom)
gen.random()  # SIGSEGV
```


### Error message:

```shell
Traceback:

#0  0x0000000000000000 in ?? ()
#1  0x00007fffa5fa6867 in random_standard_uniform_fill ()
   from /home/user/.local/share/hatch/env/virtual/randquik/45ZJMGs_/randquik/lib/python3.10/site-packages/numpy/random/_generator.cpython-310-x86_64-linux-gnu.so
#2  0x00007fffa61d08d8 in __pyx_f_5numpy_6random_7_common_double_fill ()
   from /home/user/.local/share/hatch/env/virtual/randquik/45ZJMGs_/randquik/lib/python3.10/site-packages/numpy/random/_common.cpython-310-x86_64-linux-gnu.so
#3  0x00007fffa5f805f7 in __pyx_pw_5numpy_6random_10_generator_9Generator_15random ()
   from /home/user/.local/share/hatch/env/virtual/randquik/45ZJMGs_/randquik/lib/python3.10/site-packages/numpy/random/_generator.cpython-310-x86_64-linux-gnu.so
#4  0x00005555556d4dde in ?? ()
#5  0x000055555569cf52 in _PyEval_EvalFrameDefault ()
```
```


### Runtime information:

Python 3.10.12, WSL2, Numpy 1.26.1 - SIGSEGV
Python 3.11.0, Windows, Numpy 1.24.1 - just terminates with no error message
Python 3.11.2, Linux, Numpy 1.25.2 - SIGSEGV


### Context for the issue:

My random generator is of better quality and runs faster than MT or PCG64 (which are also very fast in Numpy implementation).
",2023-10-28 19:37:38,,BUG: np.random.Generator(MyBitGenerator).random() segfaults every time,['00 - Bug']
25007,open,ShinjanM,"### Describe the issue:

Hi, 

While computing the eigenvectors of Hermitian matrices using numpy.linalg.eigh the eigenvectors are not coming out to be orthonormal. The eigenvectors are orthonormal using the scipy.linalg.eigh routine.

A jupyter notebook with a simple example is attached.

[Compare_diag.pdf](https://github.com/numpy/numpy/files/13173924/Compare_diag.pdf)


### Reproduce the code example:

```python
import numpy as np
import scipy as sp

def create_random_hermitian_mat(dim):
    a = np.random.random((dim,dim)) + 1j*np.random.random((dim,dim))
    b = np.conj(a).T + a
    return b

a = create_random_hermitian_mat(4)
u, v = np.linalg.eigh(a)
print(""Numpy Results = \n"", np.matmul(np.conj(v).T, v))
u_sp, v_sp = sp.linalg.eigh(a)
print(""\n Scipy results = \n"", np.matmul(np.conj(v_sp).T, v_sp))
```

Output:
```
Numpy Results = 
[[ 2.   +0.j    0.   +0.j    -0.   -0.j    1.586+1.586j]
[ 0.   +0.j    2.   +0.j    0.   +0.j    -0.651-0.651j]
[-0.   +0.j    0.   -0.j    2.   +0.j    -1.027-1.027j]
[1.586-1.586j -0.651+0.651j -1.027+1.027j  5.   +0.j   ]]

Scipy Results =
[[ 1.+0.j -0.-0.j -0.-0.j  0.-0.j]
[-0.+0.j  1.+0.j  0.+0.j -0.+0.j]
[-0.+0.j  0.-0.j  1.+0.j  0.+0.j]
[ 0.+0.j -0.-0.j  0.-0.j  1.+0.j]]
```


### Runtime information:


Numpy version. 
```
'1.26.0'
```

Numpy configuration
```
Build Dependencies:
  blas:
    detection method: pkgconfig
    found: true
    include directory: /Users/shinjan/Programs/miniforge3/envs/scicomp/include
    lib directory: /Users/shinjan/Programs/miniforge3/envs/scicomp/lib
    name: blas
    pc file directory: /Users/shinjan/Programs/miniforge3/envs/scicomp/lib/pkgconfig
    version: 3.9.0
  lapack:
    detection method: pkgconfig
    found: true
    include directory: /Users/shinjan/Programs/miniforge3/envs/scicomp/include
    lib directory: /Users/shinjan/Programs/miniforge3/envs/scicomp/lib
    name: lapack
    pc file directory: /Users/shinjan/Programs/miniforge3/envs/scicomp/lib/pkgconfig
    version: 3.9.0
Compilers:
  c:
    commands: arm64-apple-darwin20.0.0-clang
    linker: ld64
    name: clang
    version: 15.0.7
  c++:
    commands: arm64-apple-darwin20.0.0-clang++
    linker: ld64
    name: clang
    version: 15.0.7
  cython:
    commands: cython
    linker: cython
    name: cython
    version: 3.0.2
Machine Information:
  build:
    cpu: aarch64
    endian: little
    family: aarch64
    system: darwin
  cross-compiled: true
  host:
    cpu: arm64
    endian: little
    family: aarch64
    system: darwin
Python Information:
  path: /Users/shinjan/Programs/miniforge3/envs/scicomp/bin/python
  version: '3.11'
SIMD Extensions:
  baseline:
  - NEON
  - NEON_FP16
  - NEON_VFPV4
  - ASIMD
  found:
  - ASIMDHP
  not found:
  - ASIMDFHM

```

Scipy Version
```
'1.11.3'
```


Scipy Configuration
```
Build Dependencies:
  blas:
    detection method: pkgconfig
    found: true
    include directory: /Users/shinjan/Programs/miniforge3/envs/scicomp/include
    lib directory: /Users/shinjan/Programs/miniforge3/envs/scicomp/lib
    name: blas
    openblas configuration: unknown
    pc file directory: /Users/shinjan/Programs/miniforge3/envs/scicomp/lib/pkgconfig
    version: 3.9.0
  lapack:
    detection method: pkgconfig
    found: true
    include directory: /Users/shinjan/Programs/miniforge3/envs/scicomp/include
    lib directory: /Users/shinjan/Programs/miniforge3/envs/scicomp/lib
    name: lapack
    openblas configuration: unknown
    pc file directory: /Users/shinjan/Programs/miniforge3/envs/scicomp/lib/pkgconfig
    version: 3.9.0
  pybind11:
    detection method: pkgconfig
    include directory: /Users/shinjan/Programs/miniforge3/envs/scicomp/include
    name: pybind11
    version: 2.11.1
Compilers:
  c:
    commands: arm64-apple-darwin20.0.0-clang
    linker: ld64
    name: clang
    version: 15.0.7
  c++:
    commands: arm64-apple-darwin20.0.0-clang++
    linker: ld64
    name: clang
    version: 15.0.7
  cython:
    commands: cython
    linker: cython
    name: cython
    version: 0.29.36
  fortran:
    commands: /Users/runner/miniforge3/conda-bld/scipy-split_1696467662374/_build_env/bin/arm64-apple-darwin20.0.0-gfortran
    linker: ld64
    name: gcc
    version: 12.3.0
  pythran:
    include directory: ../../_build_env/venv/lib/python3.11/site-packages/pythran
    version: 0.14.0
Machine Information:
  build:
    cpu: x86_64
    endian: little
    family: x86_64
    system: darwin
  cross-compiled: true
  host:
    cpu: arm64
    endian: little
    family: aarch64
    system: darwin
Python Information:
  path: /Users/shinjan/Programs/miniforge3/envs/scicomp/bin/python
  version: '3.11'

```


### Context for the issue:

Erroneous eigenvectors from numpy.linalg.eigh ",2023-10-26 07:05:39,,BUG: Eigenvectors generated via linalg.eigh are not orthonormal in v1.26.0,['00 - Bug']
25004,open,rgommers,"The cibuildwheel config contains `CFLAGS=""-fno-strict-aliasing""` [here](https://github.com/numpy/numpy/blob/44efd35ab74b8aac6b580a927b0d540344551610/pyproject.toml#L180), specifically for building wheels on Linux and macOS. I've looked in the history of that, but couldn't really determine why this was added in this way. It doesn't make too much sense that we'd need it for our own wheel builds, but that users who build from source on the same platforms would not need it. So my guess is that it was added there to fix some particular problem and it was hard to add compiler flags with `numpy.distutils` in a more generic way, so we left it at patching up an issue found when testing wheels.

Does anyone remember?

I'd expect that if we need this flag, we need it for C++ too and not only for C. And that we should add it in the build system for compilers that support the flag, rather than only for wheels.
",2023-10-25 21:23:08,,"`-fno-strict-aliasing` usage - is it needed, and if so do we default to it?",['unlabeled']
24994,open,Luca-Blight,"### Proposed new feature or change:

There is a newer package that has been implemented by other major packages such as FastAPI and Llama index which has overwhelming performance gains. I would like to make the change to Numpy if that's okay?

More info:
https://docs.astral.sh/ruff/",2023-10-24 21:51:05,,Linting Package Replacement: Pycodestyle to Ruff,['16 - Development']
24989,open,vadimkantorov,"### Proposed new feature or change:

@rgommers This has been discussed several times:
- https://github.com/numpy/numpy/issues/16243
- https://github.com/pytorch/pytorch/issues/52262

I propose to review it once again to potentially introduce a breaking change or some solution for this in NumPy 2.0 or Array API. It's most of time inconvenient to have the return type change as a function of numel() - usually we do some array processing after and having conditions complicates the consuming code.

Maybe a new argument `force=True` argument can be introduced with existing behavior `force=False` as default, and `atleast1d().tolist()` behavior if `force=True` is passed. OR maybe a new method `aslist()` can be introduced and `tolist()` be deprecated/produce-warnings and removed in a few years.",2023-10-23 21:52:51,,ENH: Create obvious way of getting a single 1-D list with all array elements,"['01 - Enhancement', '62 - Python API']"
24976,open,mhvk,"### Proposed new feature or change:

(Discussed in https://github.com/numpy/numpy/pull/23912#discussion_r1365994920)

The function `np.clip` arguably has surprising casting behaviour:
```
a = np.arange(5, dtype='u1')
np.clip(a, -1, 3)
# OverflowError with NEP 50
np.clip(a, np.int64(-1), np.int64(3))
# array([0, 1, 2, 3, 3])  
# int64 dtype with NEP 50
# (before NEP 50, both examples gave int16)
```
I would naively have expected for the output dtype to always be the same as the input one. That this does not happen is because internally `np.clip` calls a ufunc: https://github.com/numpy/numpy/blob/d885b0b546cbe2ab8401622c8d4db984ec5ea31b/numpy/_core/_methods.py#L92-L101
and these treat the arguments symmetrically.

It is possible to get the output dtype by setting `out` or `dtype`, but in the current implementation that still gives either the `OverflowError` or casting errors:
```
np.clip(a, np.int64(-1), np.int64(3), out=a)  # or dtype=a.dtype
# UFuncTypeError: Cannot cast ufunc 'clip' output from dtype('int64') to dtype('uint8') with casting rule 'same_kind'
```
adding `casting=""unsafe""` gives the wrong answer, because `-1` becomes `255`.

I think it should be possible to make the `np.clip` function (probably not the `ufunc`) cast the `min` and `max` to `a.dtype`, but ensure that the valid ranges are respected (i.e., negative integers would become 0 if the dtype is unsigned). This would be similar to what was done in #24915, i.e., ideally we have the behaviour of `np.clip` be identical to
```
min = -1
max = 3
out = a.copy()
out[a<min] = min
out[a>max] = max
return out
```
(which still gives an out-of-bound error for `min=-1` because of `__setitem__`, but works for `min=np.int64(-1)`)

But perhaps this is more work than is warranted. 



",2023-10-21 15:50:34,,ENH: Ensure that output of np.clip has the same dtype as the main array,['unlabeled']
24963,open,graingert,"### Proposed new feature or change:

currently the [numpy.dtype.flags](https://numpy.org/devdocs/reference/generated/numpy.dtype.flags.html) bitmasks are exposed in numpy._core.multiarray

the flag numpy._core.multiarray.LIST_PICKLE is [used by distributed](https://github.com/dask/distributed/blob/7ea3bff22897f46b0cea340ec47c67c407e5b12a/distributed/protocol/numpy.py#L25) to determine how best to pickle an ndarray so it would be great if there were a public namespace to access these flags, eg `numpy.dtypes.LIST_PICKLE`",2023-10-19 15:37:21,,ENH: expose numpy.dtype.flags bitmasks in numpy.dtypes,"['01 - Enhancement', 'Numpy 2.0 API Changes']"
24926,open,dpinol,"### Proposed new feature or change:

Currently _stack_dispatcher enforces that arrays argument of np.stack is a sequence with __get_item__.
imho this restriction is not required, since first line of np.stack is 
`arrays = [asanyarray(arr) for arr in arrays]`

thanks",2023-10-14 06:11:12,,ENH: Drop requirement of stack arrays to be a sequence,['01 - Enhancement']
24921,open,DLumi,"### Issue with current documentation:

Not sure if that's documentation, but it kind of is.
So when you call `pip show numpy`, you are supposed to see a brief overview of what the package is about, right?

Here's how it was for 1.23.0 (and still is for many other packages):
```
pip show numpy
Name: numpy
Version: 1.23.0
Summary: NumPy is the fundamental package for array computing with Python.
Home-page: https://www.numpy.org
Author: Travis E. Oliphant et al.
Author-email:
License: BSD
Location: c:\anaconda3\envs\converterenv\lib\site-packages
Requires:
Required-by: contourpy, coremltools, h5py, imageio, Keras-Preprocessing, matplotlib, onnx, onnxruntime, onnxruntime-gpu, opencv-python, opt-einsum, pandas, PyWavelets, scikit-image
, scikit-learn, scipy, tensorboard, tensorflow, tensorflow-hub, tensorflow-model-optimization, tf2onnx, tifffile, torchvision
```

Well from some point in time (I did not bother to check all the way down, but it works in 1.26.0) the WHOLE license is included, for some reason. Here's the snipped of what it looks like:
```
Name: numpy
Version: 1.26.0
Summary: Fundamental package for array computing in Python
Home-page: https://numpy.org
Author: Travis E. Oliphant et al.
Author-email:
License: Copyright (c) 2005-2023, NumPy Developers.
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
/~a thousand lines of the actual license go here~/
Location: C:\Anaconda3\envs\Py11\Lib\site-packages
Requires:
Required-by: coremltools, h5py, ml-dtypes, onnx, onnxruntime-gpu, opencv-python, opt-einsum, scikit-learn, scipy, tensorboard, tensorflow-hub, tensorflow-intel, tf2onnx
```

So this actually clogs the terminal output quite a lot.
I'm pretty sure that this line causes the issue: https://github.com/numpy/numpy/blob/67539a40cb13bad56a650809bf10a49e905a250d/pyproject.toml#L22

But that has to be verified.

### Idea or request for content:

I would be glad to see a brief summary of the License instead of the actual text. Similar to how it was before.",2023-10-13 13:08:47,,DOC: license text clogs pip show command,['04 - Documentation']
24907,open,GalaxySnail,"### Describe the issue:

Failed to build numpy 1.26.0 or main branch with `PYTHONSAFEPATH=1` environment variable.

Please refer to: https://docs.python.org/3/using/cmdline.html#envvar-PYTHONSAFEPATH

### Reproduce the code example:

```shell
export PYTHONSAFEPATH=1
pip wheel --no-binary numpy numpy==1.26.0
# or main branch
pip wheel --no-binary numpy git+https://github.com/numpy/numpy
```


### Error message:

```python
Traceback (most recent call last):
  File ""/tmp/pip-wheel-vdqy4dbr/numpy_1f47512b2a0a4ad3899813685753144f/.mesonpy-2dh000d4/build/../../numpy/core/code_generators/generate_ufunc_api.py"", line 4, in <module>
    import genapi
ModuleNotFoundError: No module named 'genapi'
```


<details>

<summary>full meson log</summary>

```shell
The Meson build system
Version: 1.2.99
Source dir: /tmp/pip-wheel-vdqy4dbr/numpy_1f47512b2a0a4ad3899813685753144f
Build dir: /tmp/pip-wheel-vdqy4dbr/numpy_1f47512b2a0a4ad3899813685753144f/.mesonpy-2dh000d4/build
Build type: native build
Project name: NumPy
Project version: 1.26.0
C compiler for the host machine: cc (gcc 13.2.1 ""cc (GCC) 13.2.1 20230801"")
C linker for the host machine: cc ld.bfd 2.41.0
C++ compiler for the host machine: c++ (gcc 13.2.1 ""c++ (GCC) 13.2.1 20230801"")
C++ linker for the host machine: c++ ld.bfd 2.41.0
Cython compiler for the host machine: cython (cython 3.0.3)
Host machine cpu family: x86_64
Host machine cpu: x86_64
Program python found: YES (/usr/bin/python)
Found pkg-config: /usr/bin/pkg-config (1.8.1)
Run-time dependency python found: YES 3.11
Has header ""Python.h"" with dependency python-3.11: YES
Compiler for C supports arguments -fno-strict-aliasing: YES
Test features ""SSE SSE2 SSE3"" : Supported
Test features ""SSSE3"" : Supported
Test features ""SSE41"" : Supported
Test features ""POPCNT"" : Supported
Test features ""SSE42"" : Supported
Test features ""AVX"" : Supported
Test features ""F16C"" : Supported
Test features ""FMA3"" : Supported
Test features ""AVX2"" : Supported
Test features ""AVX512F"" : Supported
Test features ""AVX512CD"" : Supported
Test features ""AVX512_KNL"" : Supported
Test features ""AVX512_KNM"" : Supported
Test features ""AVX512_SKX"" : Supported
Test features ""AVX512_CLX"" : Supported
Test features ""AVX512_CNL"" : Supported
Test features ""AVX512_ICL"" : Supported
Test features ""AVX512_SPR"" : Supported
Configuring npy_cpu_dispatch_config.h using configuration
Message:
CPU Optimization Options
  baseline:
    Requested : min
    Enabled   : SSE SSE2 SSE3
  dispatch:
    Requested : max -xop -fma4
    Enabled   : SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512F AVX512CD AVX512_KNL AVX512_KNM AVX512_SKX AVX512_CLX AVX512_CNL AVX512_ICL AVX512_SPR

Library m found: YES
Run-time dependency openblas found: YES
Checking if ""CBLAS"" with dependency openblas: links: YES
Dependency openblas found: YES unknown (cached)
Program _build_utils/process_src_template.py found: YES (/usr/bin/python /tmp/pip-wheel-vdqy4dbr/numpy_1f47512b2a0a4ad3899813685753144f/numpy/_build_utils/process_src_template.py)
Program _build_utils/tempita.py found: YES (/tmp/pip-wheel-vdqy4dbr/numpy_1f47512b2a0a4ad3899813685753144f/numpy/_build_utils/tempita.py)
Configuring __config__.py using configuration
Checking for size of ""short"" : 2
Checking for size of ""int"" : 4
Checking for size of ""long"" : 8
Checking for size of ""long long"" : 8
Checking for size of ""float"" : 4
Checking for size of ""double"" : 8
Checking for size of ""long double"" : 16
Checking for size of ""off_t"" : 8
Checking for size of ""Py_intptr_t"" with dependency python-3.11: 8
Checking for size of ""PY_LONG_LONG"" with dependency python-3.11: 8
Has header ""complex.h"" : YES
Checking for type ""complex float"" : YES
Checking for size of ""struct {float __x; float __y;}"" : 8
Checking for type ""complex double"" : YES
Checking for size of ""struct {double __x; double __y;}"" : 16
Checking for type ""complex long double"" : YES
Checking for size of ""struct {long double __x; long double __y;}"" : 32
Checking for function ""sin"" : YES
Checking for function ""cos"" : YES
Checking for function ""tan"" : YES
Checking for function ""sinh"" : YES
Checking for function ""cosh"" : YES
Checking for function ""tanh"" : YES
Checking for function ""fabs"" : YES
Checking for function ""floor"" : YES
Checking for function ""ceil"" : YES
Checking for function ""sqrt"" : YES
Checking for function ""log10"" : YES
Checking for function ""log"" : YES
Checking for function ""exp"" : YES
Checking for function ""asin"" : YES
Checking for function ""acos"" : YES
Checking for function ""atan"" : YES
Checking for function ""fmod"" : YES
Checking for function ""modf"" : YES
Checking for function ""frexp"" : YES
Checking for function ""ldexp"" : YES
Checking for function ""expm1"" : YES
Checking for function ""log1p"" : YES
Checking for function ""acosh"" : YES
Checking for function ""asinh"" : YES
Checking for function ""atanh"" : YES
Checking for function ""rint"" : YES
Checking for function ""trunc"" : YES
Checking for function ""exp2"" : YES
Checking for function ""copysign"" : YES
Checking for function ""nextafter"" : YES
Checking for function ""strtoll"" : YES
Checking for function ""strtoull"" : YES
Checking for function ""cbrt"" : YES
Checking for function ""log2"" : YES
Checking for function ""pow"" : YES
Checking for function ""hypot"" : YES
Checking for function ""atan2"" : YES
Checking for function ""csin"" : YES
Checking for function ""csinh"" : YES
Checking for function ""ccos"" : YES
Checking for function ""ccosh"" : YES
Checking for function ""ctan"" : YES
Checking for function ""ctanh"" : YES
Checking for function ""creal"" : YES
Checking for function ""cimag"" : YES
Checking for function ""conj"" : YES
Checking for function ""cabs"" : YES
Checking for function ""cabsf"" : YES
Checking for function ""cabsl"" : YES
Checking for function ""cacos"" : YES
Checking for function ""cacosf"" : YES
Checking for function ""cacosl"" : YES
Checking for function ""cacosh"" : YES
Checking for function ""cacoshf"" : YES
Checking for function ""cacoshl"" : YES
Checking for function ""carg"" : YES
Checking for function ""cargf"" : YES
Checking for function ""cargl"" : YES
Checking for function ""casin"" : YES
Checking for function ""casinf"" : YES
Checking for function ""casinl"" : YES
Checking for function ""casinh"" : YES
Checking for function ""casinhf"" : YES
Checking for function ""casinhl"" : YES
Checking for function ""catan"" : YES
Checking for function ""catanf"" : YES
Checking for function ""catanl"" : YES
Checking for function ""catanh"" : YES
Checking for function ""catanhf"" : YES
Checking for function ""catanhl"" : YES
Checking for function ""cexp"" : YES
Checking for function ""cexpf"" : YES
Checking for function ""cexpl"" : YES
Checking for function ""clog"" : YES
Checking for function ""clogf"" : YES
Checking for function ""clogl"" : YES
Checking for function ""cpow"" : YES
Checking for function ""cpowf"" : YES
Checking for function ""cpowl"" : YES
Checking for function ""csqrt"" : YES
Checking for function ""csqrtf"" : YES
Checking for function ""csqrtl"" : YES
Checking for function ""csin"" : YES (cached)
Checking for function ""csinf"" : YES
Checking for function ""csinl"" : YES
Checking for function ""csinh"" : YES (cached)
Checking for function ""csinhf"" : YES
Checking for function ""csinhl"" : YES
Checking for function ""ccos"" : YES (cached)
Checking for function ""ccosf"" : YES
Checking for function ""ccosl"" : YES
Checking for function ""ccosh"" : YES (cached)
Checking for function ""ccoshf"" : YES
Checking for function ""ccoshl"" : YES
Checking for function ""ctan"" : YES (cached)
Checking for function ""ctanf"" : YES
Checking for function ""ctanl"" : YES
Checking for function ""ctanh"" : YES (cached)
Checking for function ""ctanhf"" : YES
Checking for function ""ctanhl"" : YES
Checking for function ""isfinite"" : YES
Header ""Python.h"" has symbol ""isfinite"" with dependency python-3.11: YES
Checking for function ""isinf"" : YES
Header ""Python.h"" has symbol ""isinf"" with dependency python-3.11: YES
Checking for function ""isnan"" : YES
Header ""Python.h"" has symbol ""isnan"" with dependency python-3.11: YES
Checking for function ""signbit"" : YES
Header ""Python.h"" has symbol ""signbit"" with dependency python-3.11: YES
Checking for function ""fallocate"" : YES
Header ""Python.h"" has symbol ""HAVE_FTELLO"" with dependency python-3.11: YES
Header ""Python.h"" has symbol ""HAVE_FSEEKO"" with dependency python-3.11: YES
Checking for function ""strtold_l"" : NO
Checking for function ""strtold_l"" : NO
Checking for function ""backtrace"" : YES
Checking for function ""madvise"" : YES
Has header ""features.h"" : YES
Has header ""xlocale.h"" : NO
Has header ""dlfcn.h"" : YES
Has header ""execinfo.h"" : YES
Has header ""libunwind.h"" : YES
Has header ""sys/mman.h"" : YES
Compiler for C supports arguments -O3: YES
Has header ""endian.h"" : YES
Has header ""sys/endian.h"" : NO
Header ""inttypes.h"" has symbol ""PRIdPTR"" : YES
Compiler for C supports function attribute visibility:hidden: YES
Configuring config.h using configuration
Configuring _numpyconfig.h using configuration
Configuring npymath.ini using configuration
Configuring mlib.ini using configuration
Generating multi-targets for ""_umath_tests.dispatch.h""
  Enabled targets: AVX2, SSE41, baseline
Generating multi-targets for ""argfunc.dispatch.h""
  Enabled targets: AVX512_SKX, AVX2, SSE42, baseline
Generating multi-targets for ""simd_qsort.dispatch.h""
  Enabled targets: AVX512_SKX
Generating multi-targets for ""simd_qsort_16bit.dispatch.h""
  Enabled targets: AVX512_SPR, AVX512_ICL
Generating multi-targets for ""loops_arithm_fp.dispatch.h""
  Enabled targets: FMA3__AVX2, baseline
Generating multi-targets for ""loops_arithmetic.dispatch.h""
  Enabled targets: AVX512_SKX, AVX512F, AVX2, SSE41, baseline
Generating multi-targets for ""loops_comparison.dispatch.h""
  Enabled targets: AVX512_SKX, AVX512F, AVX2, SSE42, baseline
Generating multi-targets for ""loops_exponent_log.dispatch.h""
  Enabled targets: AVX512_SKX, AVX512F, FMA3__AVX2, baseline
Generating multi-targets for ""loops_hyperbolic.dispatch.h""
  Enabled targets: AVX512_SKX, FMA3__AVX2, baseline
Generating multi-targets for ""loops_logical.dispatch.h""
  Enabled targets: AVX512_SKX, AVX2, baseline
Generating multi-targets for ""loops_minmax.dispatch.h""
  Enabled targets: AVX512_SKX, AVX2, baseline
Generating multi-targets for ""loops_modulo.dispatch.h""
  Enabled targets: baseline
Generating multi-targets for ""loops_trigonometric.dispatch.h""
  Enabled targets: AVX512F, FMA3__AVX2, baseline
Generating multi-targets for ""loops_umath_fp.dispatch.h""
  Enabled targets: AVX512_SKX, baseline
Generating multi-targets for ""loops_unary.dispatch.h""
  Enabled targets: AVX512_SKX, AVX2, baseline
Generating multi-targets for ""loops_unary_fp.dispatch.h""
  Enabled targets: SSE41, baseline
Generating multi-targets for ""loops_unary_fp_le.dispatch.h""
  Enabled targets: SSE41, baseline
Generating multi-targets for ""loops_unary_complex.dispatch.h""
  Enabled targets: AVX512F, FMA3__AVX2, baseline
Generating multi-targets for ""loops_autovec.dispatch.h""
  Enabled targets: AVX2, baseline
Generating multi-targets for ""_simd.dispatch.h""
  Enabled targets: SSE42, AVX2, FMA3, FMA3__AVX2, AVX512F, AVX512_SKX, baseline
Build targets in project: 101

NumPy 1.26.0

  User defined options
    Native files: /tmp/pip-wheel-vdqy4dbr/numpy_1f47512b2a0a4ad3899813685753144f/.mesonpy-2dh000d4/build/meson-python-native-file.ini
    buildtype   : release
    b_ndebug    : if-release
    b_vscrt     : md

Found ninja-1.11.1 at /usr/bin/ninja
+ /usr/bin/ninja
[1/498] Generating numpy/core/npy_math_internal.h with a custom command
[2/498] Generating numpy/__init__.pxd with a custom command
[3/498] Generating 'numpy/core/libnpymath.a.p/ieee754.c'
[4/498] Generating numpy/__init__.py with a custom command
[5/498] Generating numpy/__init__.cython-30.pxd with a custom command
[6/498] Generating 'numpy/core/libnpymath.a.p/npy_math_complex.c'
[7/498] Generating numpy/core/_umath_doc_generated with a custom command
[8/498] Generating numpy/core/__umath_generated with a custom command
[9/498] Generating numpy/core/__ufunc_api with a custom command
FAILED: numpy/core/__ufunc_api.c numpy/core/__ufunc_api.h
/usr/bin/python ../../numpy/core/code_generators/generate_ufunc_api.py -o numpy/core
Traceback (most recent call last):
  File ""/tmp/pip-wheel-vdqy4dbr/numpy_1f47512b2a0a4ad3899813685753144f/.mesonpy-2dh000d4/build/../../numpy/core/code_generators/generate_ufunc_api.py"", line 4, in <module>
    import genapi
ModuleNotFoundError: No module named 'genapi'
[10/498] Generating numpy/core/__multiarray_api with a custom command
FAILED: numpy/core/__multiarray_api.c numpy/core/__multiarray_api.h
/usr/bin/python ../../numpy/core/code_generators/generate_numpy_api.py -o numpy/core --ignore numpy/core/__umath_generated.c
Traceback (most recent call last):
  File ""/tmp/pip-wheel-vdqy4dbr/numpy_1f47512b2a0a4ad3899813685753144f/.mesonpy-2dh000d4/build/../../numpy/core/code_generators/generate_numpy_api.py"", line 5, in <module>
    import genapi
ModuleNotFoundError: No module named 'genapi'
[11/498] Compiling C object numpy/core/libnpymath.a.p/meson-generated_ieee754.c.o
[12/498] Compiling C object numpy/core/libnpymath.a.p/src_npymath_npy_math.c.o
[13/498] Compiling C object numpy/core/libnpymath.a.p/meson-generated_npy_math_complex.c.o
[14/498] Compiling C++ object numpy/core/libnpymath.a.p/src_npymath_halffloat.cpp.o
ninja: build stopped: subcommand failed.
```

</details>

### Runtime information:

```
>>> print(sys.version)
3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]
```

### Context for the issue:

_No response_",2023-10-12 11:39:17,,BUG: failed to build with `PYTHONSAFEPATH=1` environment variable,['00 - Bug']
24900,open,Rodrigo-Tenorio,"### Describe the issue:

Whenever reading complex numbers using `genfromtxt`, numpy silently returns `nan`.

I've seen similar issues discussed in other places (e.g. https://github.com/numpy/numpy/issues/7895),
but I haven't found any issue for this specific case.

### Reproduce the code example:

```python
import numpy as np

data = np.array([0 + 0j, 1 + 1j])
np.savetxt(outfile := ""./array_in_file.txt"", data)
loaded_data = np.genfromtxt(outfile)

print(data)
print(loaded_data)
```


### Error message:

```shell
[0.+0.j 1.+1.j]
[nan nan]
```


### Runtime information:

>>> import sys, numpy; print(numpy.__version__); print(sys.version)
1.26.0
3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]

>>> print(numpy.show_runtime())
WARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information
[{'numpy_version': '1.26.0',
  'python': '3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]',
  'uname': uname_result(system='Linux', node='pop-os', release='6.2.6-76060206-generic', version='#202303130630~1683753207~22.04~77c1465 SMP PREEMPT_DYNAMIC Wed M', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}}]
None


### Context for the issue:

The current behavior is to silently parse a ""numerical value"" as a `nan`.
I understand this is the default case for other non-numerical values such as strings,
but I would have expected complex numbers to behave more closely to numbers.

I'm unaware of the complexity of this function, but would it be useful to add a minor
warning for the case of parsing complex numbers as `nan`?",2023-10-11 14:27:31,,BUG: `genfromtxt` silently returns nan whenever reading complex values,"['00 - Bug', '57 - Close?']"
24898,open,akiss-ic,"### Proposed new feature or change:

While the documentation does state that the `xp` to `np.interp` must be monotonically increasing and that if they are decreasing the results are meaningless, the user is not warned if a monotonically decreasing series is provided. It would be trivial to check this and at least warn the user. The failure mode often produces reasonable but erroneous results so it often results in bugs. I have hit this several times while using `np.interp`, especially as it's MATLAB equivalent of `interp1` does not require it to be monotonically decreasing. 

An even better solution would be to detect if `xp` is monotonically decreasing and then just flip it and do the interpolation anyway, removing this unnecessary restriction. ",2023-10-10 14:40:10,,ENH: Check for increasing monotonicity in interp,['01 - Enhancement']
24897,open,mhvk,"### Proposed new feature or change:

This may come too late, but numpy 2.0 may be a good opportunity to remove an irritating wart, that `ufunc(array_scalar_arguments)` returns a numpy scalar rather than an array scalar. There is a very long-standing PR (#14489) that introduces an option to avoid this (using `out=...`), but perhaps it is time to just ditch the conversion to scalar? 

See gh-13105 for discussion. As a specific example of why it is unhandy, it mentions:
> #13100 raises a case where `np.fix` resorts to calling `np.asanyarray(np.ceil(x, out=out))` in order to ensure that the result is an array. Unfortunately, this has a number of draw-backs:
> 
> * It discards duck-types, like `dask` arrays
> * It changes the dtype of 0d object arrays containing array-likes

The [first comment](https://github.com/numpy/numpy/issues/13105#issuecomment-471382946) on the issue by @rgommers suggests just removing the cast to scalar. To me, this still makes the most sense.

EDIT: the simplest implementation of this is to not use `PyArray_Return` in ufuncs, or perhaps adjust `PyArray_Return` to remove the try of converting to scalars altogether. ",2023-10-10 14:06:28,,ENH: No longer auto-convert array scalars to numpy scalars in ufuncs (and elsewhere?),['Numpy 2.0 API Changes']
24865,open,mattip,The [build report](https://numpy.org/devdocs/reference/simd/build-options.html#build-report) section should be updated with the more recent meson changes. The `log_example.txt` is no longer relevant.,2023-10-05 09:01:00,,DOC: SIMD page refers to outdated Build Report,"['04 - Documentation', 'component: SIMD']"
24862,open,charris,"3,9-v7.3.13 was released Sept 29 and seems to have made it into azure. 

See https://dev.azure.com/numpy/numpy/_build/results?buildId=32240&view=logs&jobId=95a72139-892c-5f80-4f09-2f9a3201b7ef for the error.

The error message is:
```
>                   assert_equal(arr_method(obj), NotImplemented, err_msg)
E                   TypeError: operand 'MyType' does not support ufuncs (__array_ufunc__=None)
```
Looks like PyPy is failing to return `NotImplemented`.",2023-10-04 20:06:13,,"Failing test with PyPy 3,.9-v7.3.13 on Windows.",['24 - PyPy']
24859,open,ngoldbaum,"Currently, users will see seg faults if a user-defined dtype is passed somewhere where `copyswap` is used internally to copy array elements.

If we require that user-defined dtypes implement a single-element copy operation we will be able to implement a default copyswap implementation for user-defined dtypes which can be used where copyswap is used internally in NumPy.",2023-10-04 17:36:49,,ENH: dtype API needs an API slot for a single-element copy,['component: numpy.dtype']
24857,open,mattip,"In PR #24839, we began using scipy-openblas wheels in the CI build/test cycles. In order to use these in wheel building as well, we need to declare a dependency, so that `pip install numpy` in all its variations (build a wheel, build an sdist, install a wheel, install from an sdist) will
- install `scipy-openblas` befor compiling
- ship a modified `_distributor_init.py` (or add a `_distributor_init_local.py`) to import `scipy-openblas` before `numpy`
- find the `scipy-openblas.pc` file when building
- declare a dependency in the metadata of the numpy wheel/sdist

This is tricky since (take from the short discussion starting in [this comment](https://github.com/numpy/numpy/pull/24749#discussion_r1332844224)
- There are two variants: scipy-openblas32 and scipy-openblas64. Which to use as the canonical dependency? It seems at least for NumPy this is easy: always use the ILP64 package.
- How to allow changing that choice for a different variant (if not the 32/64 problem, then say MKL, or Accelerate, or None for platforms where OpenBLAS is not appropriate)?

We cannot currently have different metadata in the sdist than we have in the wheel on PyPI. There are some ideas in the draft [PEP 725](https://peps.python.org/pep-0725/). Or perhaps we could have a meta-package `scipy-openblasX`?",2023-10-04 13:07:11,,"ENH, BLD: use wheels to get BLAS implementations","['01 - Enhancement', '14 - Release', 'component: distribution']"
24842,open,anntzer,"### Describe the issue:

Consider the complex array in the attached npy file (bad.npy in bad.zip), sort it by absolute value, and assert that the absolute value is increasing.  The assertion will occasionally fail (around 1-5 times per 100 iterations).
[bad.zip](https://github.com/numpy/numpy/files/12781432/bad.zip)
It is also possible to see the failure by running the loop from within an ipython shell, but it is much rarer (once per 1000 to 10000 iterations).

This occurs on a conda python + pip-installed numpy (a mix which may or may not be a good idea, I know); AFAICT this happens with numpy 1.25-1.26 but not on numpy 1.24.

### Reproduce the code example:

```python
for i in $(seq 0 99); do
    echo ""$i""
    python -c 'import numpy as np; nz = np.load(""/tmp/bad.npy""); z = nz[np.argsort(abs(nz))]; assert np.diff(abs(z)).min() >= 0'
done
```


### Error message:

```shell
AssertionError
```


### Runtime information:

1.26.0
3.11.3 | packaged by conda-forge | (main, Apr  6 2023, 08:58:31) [Clang 14.0.6 ]

[{'numpy_version': '1.26.0',
  'python': '3.11.3 | packaged by conda-forge | (main, Apr  6 2023, 08:58:31) '
            '[Clang 14.0.6 ]',
  'uname': uname_result(system='Darwin', node='ICR-XYN62WVT43', release='22.6.0', version='Darwin Kernel Version 22.6.0: Wed Jul  5 22:17:35 PDT 2023; root:xnu-8796.141.3~6/RELEASE_ARM64_T8112', machine='arm64')},
 {'simd_extensions': {'baseline': ['NEON', 'NEON_FP16', 'NEON_VFPV4', 'ASIMD'],
                      'found': ['ASIMDHP'],
                      'not_found': ['ASIMDFHM']}},
 {'architecture': 'armv8',
  'filepath': '/Users/antony/.local/lib/python3.11/site-packages/numpy/.dylibs/libopenblas64_.0.dylib',
  'internal_api': 'openblas',
  'num_threads': 8,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.23.dev'},
 {'architecture': 'armv8',
  'filepath': '/Users/antony/.local/lib/python3.11/site-packages/scipy/.dylibs/libopenblas.0.dylib',
  'internal_api': 'openblas',
  'num_threads': 8,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.21.dev'},
 {'architecture': 'armv8',
  'filepath': '/opt/homebrew/Cellar/openblas/0.3.24/lib/libopenblasp-r0.3.24.dylib',
  'internal_api': 'openblas',
  'num_threads': 8,
  'prefix': 'libopenblas',
  'threading_layer': 'openmp',
  'user_api': 'blas',
  'version': '0.3.24'},
 {'filepath': '/opt/homebrew/Cellar/gcc/13.2.0/lib/gcc/current/libgomp.1.dylib',
  'internal_api': 'openmp',
  'num_threads': 8,
  'prefix': 'libgomp',
  'user_api': 'openmp',
  'version': None}]

### Context for the issue:

_No response_",2023-10-02 09:35:27,,BUG: Occasional failure to sort complex numbers by absolute value on M2 macs,"['00 - Bug', 'component: SIMD']"
24841,open,rossburton,"### Describe the issue:

`numpy.distutils.ccompiler_opt` uses a list of regexs to determine features about the compile:

https://github.com/numpy/numpy/blob/5ca26b1b9df5e8c89923a2bb723a5ab95b6fe09b/numpy/distutils/ccompiler_opt.py#L993

However the `cc_has_debug` regex simply checks that the complete compile command _contains_ 'od', as it's a case-insensitive comparison.

It's very common for distributions to pass interesting compile flags such as `-fdebug-prefix-map` which will contain the build path, i.e. an arbitrary build.  This detection is therefore over-eager to detect a debug build.

Concrete example: we pass `-fdebug-prefix-map` to relocate source paths in the binaries. If we build in a directory containing ""od"" then different compiler flags (specifically, an extra `-O3`) are used compared to a directory that does not contain ""od"".

### Reproduce the code example:

```python
Build with compiler flags containing 'od'. Build again with flags not containing 'od'. Binaries will be different.
```


### Error message:

_No response_

### Runtime information:

NumPy 1.26.

### Context for the issue:

_No response_",2023-10-02 08:46:49,,BUG: distutils.ccompiler_opt over-eager to detect cc_has_debug,['00 - Bug']
24836,open,for13to1,"### Describe the issue:

With passing ""prefix"" and ""suffix"" to `np.array2string` calling, the string returned by `np.array2string` does not match what it was expected to be: the prefix and suffix did not change.

### Reproduce the code example:

```python
import numpy as np

a = np.arange(9).reshape(3, 3)
print(a)
s = np.array2string(a, separator=', ', prefix='{', suffix='}')
print(s)
```


### Error message:

```shell
No Error exported, but just the result is not as expected
```


### Runtime information:

[{'numpy_version': '1.26.0',
  'python': '3.10.10 | packaged by Anaconda, Inc. | (main, Mar 21 2023, '
            '18:39:17) [MSC v.1916 64 bit (AMD64)]',
  'uname': uname_result(system='Windows', node='DESKTOP-1HQPULJ', release='10', version='10.0.22621', machine='AMD64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'filepath': 'C:\\Users\\for13to1\\.conda\\envs\\py3a\\Library\\bin\\mkl_rt.2.dll',
  'internal_api': 'mkl',
  'num_threads': 8,
  'prefix': 'mkl_rt',
  'threading_layer': 'intel',
  'user_api': 'blas',
  'version': '2023.1-Product'}]

### Context for the issue:


![image](https://github.com/numpy/numpy/assets/115892874/16332162-4d2c-4631-ae9c-bcaaa0115ba1)

the output of `print(a)` is:
```python
[[0 1 2]
 [3 4 5]
 [6 7 8]]
```

and the output of `print(s)` is:
```python
[[0, 1, 2],
  [3, 4, 5],
  [6, 7, 8]]
```

the latter one output should be like this (as I specified the `prefix` be `{`, the `suffix` be `}`:
```python
{{0, 1, 2},
  {3, 4, 5},
  {6, 7, 8}}
```

or at least, it is morre reasonable be like this or somthing (obviously, it is also somehow weird):
```python
{[0, 1, 2],
  [3, 4, 5],
  [6, 7, 8]}
```",2023-09-30 10:54:50,,"BUG: array2string's parameter ""prefix"" and ""suffix"" do not work",['00 - Bug']
24832,open,tatarize,"### Describe the issue:

In Windows 7,
Download 3.8.10 32-bit Python.
pip install numpy

Then in python:
`import numpy`

![image](https://github.com/numpy/numpy/assets/3302478/4ed81766-beba-4ae7-b49d-5eddbb76d054)


### Reproduce the code example:

```python
import numpy
```


### Error message:

```shell
Problem signature:
  Problem Event Name:	APPCRASH
  Application Name:	python.exe
  Application Version:	3.8.10150.1013
  Application Timestamp:	608fe058
  Fault Module Name:	_multiarray_umath.cp38-win32.pyd
  Fault Module Version:	0.0.0.0
  Fault Module Timestamp:	6498fabf
  Exception Code:	c000001d
  Exception Offset:	000269c9
  OS Version:	6.1.7601.2.1.0.256.1
  Locale ID:	1033
  Additional Information 1:	0a9e
  Additional Information 2:	0a9e372d3b4ad19135b953a78882e789
  Additional Information 3:	0a9e
  Additional Information 4:	0a9e372d3b4ad19135b953a78882e789

Read our privacy statement online:
  http://go.microsoft.com/fwlink/?linkid=104288&clcid=0x0409

If the online privacy statement is not available, please read our privacy statement offline:
  C:\Windows\system32\en-US\erofflps.txt
```


### Runtime information:

It installs 1.24.5 but the max compatibility is 1.23.5

### Context for the issue:

Generic `import numpy` installs an incompatible version.",2023-09-28 20:04:50,,BUG: Windows7 Python 32-bit 3.8 after plain `pip install numpy` will import crash.,"['00 - Bug', '32 - Installation']"
24829,open,pok1800,"### Steps to reproduce:

Hi Guy, 
     Please help to step parameter  for compile my cpu detail below. Compile is success. Can not support sse3.

ENV CFLAGS=""-march=native -msse2 -std=c99 -UNDEBUG -mtune=generic""

RUN pip3 install --user --no-cache-dir 'numpy@git+https://github.com/numpy/numpy/'

grep -m1 -A3 ""vendor_id"" /proc/cpuinfo
vendor_id       : GenuineIntel
cpu family      : 6
model           : 13
model name      : Intel(R) Celeron(R) M processor         1.50GHz

gcc -v -E -x c /dev/null -o /dev/null -march=native 2>&1 | grep /cc1 | grep mtune
 /usr/lib/gcc/i686-linux-gnu/7/cc1 -E -quiet -v -imultiarch i386-linux-gnu /dev/null -o /dev/null -march=pentium-m -mmmx -mno-3dnow -msse -msse2 -mno-sse3 -mno-ssse3 -mno-sse4a -mno-cx16 -mno-sahf -mno-movbe -mno-aes -mno-sha -mno-pclmul -mno-popcnt -mno-abm -mno-lwp -mno-fma -mno-fma4 -mno-xop -mno-bmi -mno-sgx -mno-bmi2 -mno-tbm -mno-avx -mno-avx2 -mno-sse4.2 -mno-sse4.1 -mno-lzcnt -mno-rtm -mno-hle -mno-rdrnd -mno-f16c -mno-fsgsbase -mno-rdseed -mno-prfchw -mno-adx -mfxsr -mno-xsave -mno-xsaveopt -mno-avx512f -mno-avx512er -mno-avx512cd -mno-avx512pf -mno-prefetchwt1 -mno-clflushopt -mno-xsavec -mno-xsaves -mno-avx512dq -mno-avx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi -mno-avx5124fmaps -mno-avx5124vnniw -mno-clwb -mno-mwaitx -mno-clzero -mno-pku -mno-rdpid --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=1024 -mtune=generic -fstack-protector-strong -Wformat -Wformat-security

### Error message:

```shell
RuntimeError: NumPy was built with baseline optimizations:
(SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2) but your machine doesn't support:
(SSE3).
```


### Additional information:

_No response_",2023-09-28 16:25:56,,how to fix compile numpy 1.26.0 in dockerfire for cpu x86 (pentium-m),['32 - Installation']
24821,open,smsaladi,"### Proposed new feature or change:

Sort single character (S1 or U1) arrays using an underlying integer representation to improve speed.

### Background: 

With a np.array of characters (U1/S1), np.unique is much faster when doing np.view as int -> np.unique -> np.view as U1/S1 for arrays of decent size. I would not have expected this since np.unique ""knows"" what's coming in as U1/S1 and could handle the view-stuff internally. I've played with this a number of ways (e.g. S1 vs U1; int32 vs int64; return_counts = True vs False; 100, 1000, or 10k elements) and seem to notice the same pattern. A short illustration below with U1, int32, return_counts = False, 10 vs 10k.

I posted this on the numpy listserv and got feedback from @ngoldbaum as follows (excerpt):

> Looking at a py-spy profile of a slightly modified version of the code you
shared, it seems the difference comes down to NumPy's sorting
implementation simply being faster for ints than unicode strings. In
particular, it looks like string_quicksort_<npy::unicode_tag, char> is two
or three times slower than quicksort_<npy::int_tag, int> when passed the
same data.
>
> We could probably add a special case in the sorting code to improve
performance for sorting single-character arrays. I have no idea if that
would be complicated or would make the code difficult to deal with. I'll
also note that string sorting is a more general problem than integer
sorting, since a generic string sort can't assume that it is handed
single-character strings.

https://mail.python.org/archives/list/numpy-discussion@python.org/thread/VF43XVYRFWVUVCMOFPEEI73EVUJ6MRVI/

I am posting it here in case it might be worth considering the enhancement.

```python
import numpy as np

charlist_10 = np.array(list('ASDFGHJKLZ'), dtype='U1')
charlist_10k = np.array(list('ASDFGHJKLZ' * 1000), dtype='U1')

def unique_basic(x):
    return np.unique(x)

def unique_view(x):
    return np.unique(x.view(np.int32)).view(x.dtype)

In [27]: %timeit unique_basic(charlist_10)
2.17 µs ± 40.7 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)

In [28]: %timeit unique_view(charlist_10)
2.53 µs ± 38.4 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)

In [29]: %timeit unique_basic(charlist_10k)
204 µs ± 4.61 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

In [30]: %timeit unique_view(charlist_10k)
66.7 µs ± 2.91 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)
```",2023-09-27 14:51:53,,ENH: Sorting single-character arrays (S1 / U1) using integer sorting,['01 - Enhancement']
24815,open,leonfoks,"### Describe the issue:

Im using the bit generators/Generator classes in parallel using mpi4py.
I instantiate PCG64DXSM with a fixed seed on multiple ranks, I then jump each state by its MPI rank.
I would expect the entropy of each jumped state to be deterministic if I use the same initial seed on each rank.

My little test function below shows this even just on a single core with 3 different instances.  If I run this method twice, the entropies after jumping are not consistent. The random_raw() values, however, ARE the same, which is confusing me even more.

Could someone please shed some light on this behaviour?  Is this expected?

Cheers,


### Reproduce the code example:

```python
def test_jump_seed(generator, seed):
    print('fixed seed: {}'.format(seed))
    bg = generator(seed = seed)
    s_check = bg.seed_seq.entropy
    print(""Initialized entropy equals fixed seed: {}"".format(s_check == seed))
    bg1 = bg.jumped(1)
    bg2 = bg.jumped(2)
    bg = bg.jumped(0)
    print('ent 0: {}\nent 1: {}\nent 2: {}'.format(bg.seed_seq.entropy, bg1.seed_seq.entropy, bg2.seed_seq.entropy))
    
    print('random draw from 0: {}'.format((bg.random_raw(), bg.random_raw(), bg.random_raw())))
    print('random draw from 1: {}'.format((bg1.random_raw(), bg1.random_raw(), bg1.random_raw())))
    print('random draw from 2: {}'.format((bg2.random_raw(), bg2.random_raw(), bg2.random_raw())))    


from numpy.random import PCG64DXSM
test_jump_seed(PCG64DXSM, 185147845955919253731470418070286414128)
test_jump_seed(PCG64DXSM, 185147845955919253731470418070286414128)

This results in the following

Not an error but here the output showing the behaviour.

first time
test_jump_seed(PCG64DXSM, 185147845955919253731470418070286414128)
fixed seed: 185147845955919253731470418070286414128
Initialized entropy equals fixed seed: True
ent 0: 8818521033502638969998011739757375928
ent 1: 276586473706694309202910464541218565095
ent 2: 6685832468418544397451491647484482421
random draw from 0: (16533588765141592909, 4218189437431980598, 12363468412188683268)
random draw from 1: (7254871951857873707, 7869898639939376462, 7791142928462258301)
random draw from 2: (13533774799967122937, 1569031215448962883, 10916170404904575733)

second time
test_jump_seed(PCG64DXSM, 185147845955919253731470418070286414128)
fixed seed: 185147845955919253731470418070286414128
Initialized entropy equals fixed seed: True
ent 0: 66283374068340491752624244213781575907
ent 1: 87969716045228665193716301454196756402
ent 2: 299128717460029393984061490474242083461
random draw from 0: (16533588765141592909, 4218189437431980598, 12363468412188683268)
random draw from 1: (7254871951857873707, 7869898639939376462, 7791142928462258301)
random draw from 2: (13533774799967122937, 1569031215448962883, 10916170404904575733)
```


### Error message:

_No response_

### Runtime information:

1.26.0
3.10.12 (main, Jul  5 2023, 15:34:07) [Clang 14.0.6 ]

### Context for the issue:

_No response_",2023-09-26 22:28:41,,BUG: `jumped` bit generators `.seed_seq` is not meaningful,"['00 - Bug', 'component: numpy.random', '57 - Close?']"
24774,open,erezinman,"### Proposed new feature or change:

When both arrays are sorted and the ""searched"" array is smaller than the ""searching"" array, then it might prudent to reverse the order of the search, and then update the original array. An example implementation in python that doesn't take the ""side' parameter into account (simple to remedy that):

```cython
import cython
import numpy as np


@cython.boundscheck(False)  # turn off bounds-checking for entire function
@cython.wraparound(False)  # turn off negative index wrapping for entire function
def sortedsearchsorted(a: cython.numeric[:], v: cython.numeric[:]) -> cython.longlong[:]:
    la = len(a)
    lv = len(v)
    if la > lv:
        return np.searchsorted(a, v)

    inverse: cython.longlong[:] = np.searchsorted(v, a) 
    result: cython.longlong[:] = np.empty(len(v), np.int64)    
    
    i: cython.Py_ssize_t = 0
    prev: cython.longlong = 0
    for i in range(la):
        result[prev:inverse[i] + 1] = i
        prev = inverse[i] + 1
        
    result[prev:] = la
    return np.asarray(result)
```

When the size of `a` is significantly larger than that of `v` you see clear results for that method. These will undoubtedly will be a lot more significant if written in C.  This will require an additional parameter `both_sorted` in the function's signature.",2023-09-22 00:05:03,,"ENH: An enhancement to searchsorted when both arrays are sorted, and one array is significantly smaller than the other",['01 - Enhancement']
24755,open,mkostousov,"### Proposed new feature or change:

Version 1.25.1, Python 3.12.re02
After enabling interpreters in Python C Api:

PyInterpreterConfig config = {
    .check_multi_interp_extensions = 1,
    .gil = PyInterpreterConfig_OWN_GIL,
};
PyThreadState *tstate = NULL;
PyStatus status = Py_NewInterpreterFromConfig(&tstate, &config);
if (PyStatus_Exception(status)) {
    return -1;
}

Import numpy throws an exception:
module numpy.core._multiarray._umath does not support loading in subinterpreters
",2023-09-20 14:55:16,,ENH: Please support subinterpreters,['01 - Enhancement']
24738,open,randolf-scholz,"### Proposed new feature or change:

For `numpy.typing.NDArray`, the argument needs to be a subtype of `numpy.generic`. However, this leads to problem since we can no longer specify concrete types for `object`-arrays, for example, `mypy` raises `[type-var]` for this code:

```python
import matplotlib.pyplot as plt
from matplotlib.figure import Figure
from matplotlib.axes import Axes
from numpy.typing import NDArray

axes: NDArray[Axes]  # ✘ mypy: raises type-var
fig: Figure
fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)

for ax in axes.flat:
    reveal_type(axes)  # Axes if the previous error is ignored, else ndarray
    ax.set_title(""A subplot"")
```

For `NDArray` of dtype `np.object_`, it should somehow wrap the given type. One approach could be to make `np.object_` generic and change methods overloads so that for object-type, it generally returns the wrapped type.",2023-09-18 16:32:20,,"ENH: [typing] `NDarray[plt.Axes]` not possible - Type argument of ""NDArray"" must be a subtype of ""generic""","['01 - Enhancement', '23 - Wish List', 'Static typing']"
24736,open,charris,"Cloning `numpy/doc` for document generation currently downloads about 400 MB, which is not trivial. Doing a ""blobless"" clone makes no difference. Maybe we could experiment with `git lfs`, which is supported by github. See https://git-lfs.com/.",2023-09-18 14:01:23,,The numpy/doc repo is getting huge.,['unlabeled']
24699,open,ngoldbaum,"On import, [`ml_dtypes`](https://github.com/jax-ml/ml_dtypes) adds new entries to `np.sctypeDict` so that e.g. `np.dtype(“int4”)` returns an int4 dtype defined outside NumPy.

Since jax currently documents this behavior to users and relies on it internally, I don’t think we can reasonably break it without a deprecation story and a migration story.

For deprecating it, we would keep a list of all the strings that NumPy accepts out of the box and if any other string is passed in and somehow we get back a valid dtype, we should raise a deprecation warning. I don’t know if there are other ways of injecting a string dtype name into NumPy’s internals without manipulating `sctypeDict` so this will catch any other shenanigans.

We should probably also deprecate `np.sctypeDict` too?

In a few releases after adding the deprecation, we could make it so `np.dtype` can only return dtype instances with a mapping defined out of the box in NumPy or via some as-yet unwritten mechanism to associate string names with dtypes, probably with some kind of support for namespacing.

As far as I know jax is the only downstream library that injects dtype names into the `np.dtype(""dtype_name"")` mechanism.

The deprecation should not be added until we have a clear migration story for the jax library and any possible impacts on jax users are considered.

xref https://github.com/numpy/numpy/pull/24376#issuecomment-1717908075 and the discussion that follows for context",2023-09-13 20:13:05,,DEP: Deprecate registering dtype names with np.sctypeDict?,['component: numpy.dtype']
24694,open,SebastianJL,"### Describe the issue:

Hi everyone,

I am loading data from a binary file. The file has a header and I read it with:

```python
header_type = np.dtype([('time', '>f8'), ('n', '>i4'), ('dims', '>i4'),
                       ('n_gas', '>i4'), ('n_dark', '>i4'), ('n_star', '>i4'), ('pad', '>i4')])
with open('test.tipsy', 'rb') as binary_in:
	header = np.fromfile(header = np.fromfile(binary_in, count=1, dtype=header_type)[0]
```
Note, how I extract the single element with `[0]` instead of using the array as is. header has the type `numpy.void`.

I then try to swap its byte order. This normally works on `numpy.ndarray` but seems to do something completely random on `numpy.void'

```python
print(header.tobytes().hex())
res = header.byteswap(inplace=False)
print(res.tobytes().hex())
```

output of several runs
```shell
3f947ae147ae147b000400000000000300000000000400000000000000000000
70cc49706c7f0000a0a33c491c5600000200000000000000040000006c7f0000

3f947ae147ae147b000400000000000300000000000400000000000000000000
50fbdfea127f0000a0b330d9e3550000020000000000000004000000127f0000

3f947ae147ae147b000400000000000300000000000400000000000000000000
107eca8d477f0000a05382ffa0550000020000000000000004000000477f0000
```
As you can see, while the header before `byteswap()` stays the same it's output seems to be completely random.

I don't really know why this method even exists on `numpy.void`. I couldn't find it when searching in the docs. Should this work? Or should the method just not be present?

### Reproduce the code example:

```python
import numpy as np

header_type = header_type = np.dtype([('time', '>f8'), ('n', '>i4'), ('dims', '>i4'), ('n_gas', '>i4'), ('n_dark', '>i4'), ('n_star', '>i4'), ('pad', '>i4')])
header = np.array([(31.2, 12, 1, 4, 4, 4, 0)], dtype=header_type)[0]
print(f""{type(header) = }"")
print(header.tobytes().hex())
header = header.byteswap(inplace=False)
print(header.tobytes().hex())
```


### Error message:

```shell
type(header) = <class 'numpy.void'>
403f3333333333330000000c0000000100000004000000040000000400000000
909719af607f0000a043b0e6cf55000002000000000000000085cd3a82fd0100
```


### Runtime information:

1.25.2
3.9.5 (default, Jun 23 2021, 13:33:38) 
[GCC 10.2.1 20210110]
WARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information
[{'numpy_version': '1.25.2',
  'python': '3.9.5 (default, Jun 23 2021, 13:33:38) \n[GCC 10.2.1 20210110]',
  'uname': uname_result(system='Linux', node='phoenix', release='6.1.0-10-amd64', version='#1 SMP PREEMPT_DYNAMIC Debian 6.1.38-1 (2023-07-14)', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}}]

### Context for the issue:

It just took me several hours to track down why the header wouldn't be written correctly in the little endian format. Now I can work around it but it would be nice if it just didn't happen.",2023-09-13 15:55:30,,BUG: Possible undefined behavior for numpy.void.byteswap(),['00 - Bug']
24689,open,JoostFWMaas,"### Describe the issue:

Using ""apply_along_axis(...)"" on an array of strings has unexpected results when the strings are of varying lengths. All strings are truncated at the length of the first string of the input array. ""apply_along_axis(...)"" in this case is supposed to be similar to (but faster than) an in-line for loop, but the behaviour is not the same.
 
```
Correct output: ['0' '1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16' '17' '18' '19']
Wrong output: ['0' '1' '2' '3' '4' '5' '6' '7' '8' '9' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1']
``` 


In the input array ""numbers"", the shortest string has a length of 1 and the longest string has a length of 2. In the incorrect output array ""wrong_joined_numbers"", all strings have a length of 1. In the output array ""correct_joined_numbers"", all of the entries are of the correct length and value.

### Reproduce the code example:

```python
import numpy as np

numbers = np.arange(20).astype(str)[:, np.newaxis]
correct_joined_numbers = np.array([''.join(ii) for ii in numbers])
wrong_joined_numbers = np.apply_along_axis(''.join, arr=numbers, axis=1)
print(f'Correct output: {correct_joined_numbers}')
print(f'Wrong output: {wrong_joined_numbers}')
```


### Error message:

_No response_

### Runtime information:

1.25.2
3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]
[{'numpy_version': '1.25.2',
  'python': '3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 '
            '64 bit (AMD64)]'

### Context for the issue:

_No response_",2023-09-12 19:15:20,,BUG: using apply_along_axis on array of strings of varying length truncates string lengths to length of first string,['00 - Bug']
24687,open,Mark2000,"### Proposed new feature or change:

Dynamicists frequently use the matrix representation of the cross product (https://en.m.wikipedia.org/wiki/Cross_product#Conversion_to_matrix_multiplication), and the lack of a skew symmetric operator in NumPy is a frequent source of annoyance.

I propose either adding a skew function to linalg that returns the skew symmetric matrix of a vector `np.cross(a, np.identity(a.shape[0]) * -1)` (from https://stackoverflow.com/questions/66707295/numpy-cross-product-matrix-function), or more radically, making the second argument `b` of np.cross optional and have `np.cross(a)` return the matrix representation of [a x]. For the latter, would need to consider the interaction with other kwargs and different input shapes. ",2023-09-12 15:43:10,,ENH: Skew symmetric/matrix cross product function,"['01 - Enhancement', 'component: numpy.linalg']"
24670,open,rgommers,"This follows up on this comment from @skirpichev: https://github.com/numpy/numpy/issues/23808#issuecomment-1711046374 about `f2py -c` needing `meson` and `ninja` for Python 3.12 (or with `--backend=meson` on older Python versions).

Right now we do not use `optional-dependencies` at all. It is also the future replacement for `test_requirements.txt` & co. Here is an example of how it can be used: https://github.com/scipy/scipy/blob/9f7549abcc61f5dc72ca000cef7bc43066c74527/pyproject.toml#L74-L109

In numpy it could have an extra `f2py` section, and that would allow installing the needed dependencies with `pip install numpy[f2py]`. Of course you have to know that that's possible, and at that point you can equally well do `pip install meson ninja` , so this isn't all that urgent. An extra complication is that `ninja` is a system dependency, and using it from PyPI is a bit hacky. That's for example why `meson` and `meson-python` do not have a direct dependency on `ninja`; meson-python first checks if it's installed on the system and dynamically adds it in non-isolated installs if it's missing.

I'd say we don't want to do this for 1.26.0, but we can consider populating the optional dependencies table for 2.0.",2023-09-08 09:58:14,,Consider populating `optional-dependencies` in pyproject.toml,"['03 - Maintenance', 'component: build']"
24642,open,aaronmondal,"### Describe the issue:

According to the spec the `stream` parameter for `__dlpack__` and `to_device` should be `Optional[Union[int, Any]]`. The current implementation just uses `None`.

### Reproduce the code example:

```python
from typing import Any, Protocol, Self

from numpy import array_api as xp

PyCapsule = Any

class Array(Protocol):
    def __dlpack__(*, stream: int | Any | None = None) -> PyCapsule:
        ...


def identity(array: Array) -> Array:
    return array

identity(xp.asarray([1]))  # Error.

class Array2(Protocol):
    def to_device(
        self,
        device: Any,
        /,
        *,
        stream: int | Any | None = None,
    ) -> Self:
        ...

def identity2(array: Array2) -> Array2:
    return array

identity2(xp.asarray([1]))  # Error
```


### Error message:

```shell
[Pyright] Argument of type ""Array"" cannot be assigned to parameter ""array"" of type ""Array"" in function ""identity""
  ""Array"" is incompatible with protocol ""Array""
    ""__dlpack__"" is an incompatible type
      Type ""(*, stream: None = None) -> PyCapsule"" cannot be assigned to type ""(*, stream: int | Any | None = None) -> PyCapsule""
        Keyword parameter ""stream"" of type ""int | Any | None"" cannot be assigned to type ""None""
          Type ""int | Any | None"" cannot be assigned to type ""None""

[Pyright] Argument of type ""Array"" cannot be assigned to parameter ""array"" of type ""Array2"" in function ""identity2""
  ""Array"" is incompatible with protocol ""Array2""
    ""to_device"" is an incompatible type
      Type ""(device: Device, /, stream: None = None) -> Array"" cannot be assigned to type ""(device: Any, /, *, stream: int | Any | None = None) -> Array""
        Keyword parameter ""stream"" of type ""int | Any | None"" cannot be assigned to type ""None""
          Type ""int | Any | None"" cannot be assigned to type ""None""
```


### Runtime information:

1.25.1
3.11.4 (main, Jun  6 2023, 22:16:46) [GCC 12.3.0]

### Context for the issue:

Loosely related to #24641 

`__dlpack__`: https://data-apis.org/array-api/latest/API_specification/generated/array_api.array.__dlpack__.html
`to_device`: https://data-apis.org/array-api/latest/API_specification/generated/array_api.array.to_device.html",2023-09-05 13:51:35,,TYP: array_api `stream` has wrong type,"['33 - Question', 'Static typing', 'component: numpy.array_api']"
24640,open,husisy,"### Describe the issue:

macos-arm (apple silicon M2) only, not reproducible on `ubuntu-22.04,AMD-R7`

call `np.linalg.eigh` on a hermitian complex matrix, the eigenvectors are wrong with accelerate and netlib blas, but correct with openblas.

| env | blas | eigh |
| :-: | :-: | :-: |
| `env00` | accelerate | wrong |
| `env01` | netlib | wrong |
| `env02` | openblas | correct |

```bash
conda create -y -n env00 -c conda-forge ""libblas=*=*accelerate"" numpy

conda create -y -n env01 -c conda-forge ""libblas=*=*netlib"" numpy

# default if no blas specified
conda create -y -n env02 -c conda-forge ""libblas=*=*openblas"" numpy
```

seems related issue: numpy/numpy#21950

### Reproduce the code example:

```python
import numpy as np
np0 = np.array([[0,1j],[-1j,0]])
EVL,EVC = np.linalg.eigh(np0)
print(EVC)
print(EVC.T.conj() @ EVC)
```


### Error message:

```shell
# wrong (accelerate,netlib), the first EVC is correct but not normalized, the second EVC is wrong in direction
# [[-0.70710678-0.70710678j  0.70710678+1.20710678j]
#  [ 0.70710678-0.70710678j  0.5       -0.70710678j]]
# [[ 2.        +0.j  -0.5       -0.5j]
#  [-0.5       +0.5j  2.70710678+0.j ]]

# correct (openblas)
# [[-0.70710678+0.j          0.70710678+0.j        ]
#  [ 0.        -0.70710678j  0.        -0.70710678j]]
# [[1.00000000e+00+0.j 2.23711432e-17+0.j]
#  [2.23711432e-17+0.j 1.00000000e+00+0.j]]
```


### Runtime information:

```Python
import sys, numpy
print(numpy.__version__); print(sys.version)
## same for all: env00 env01 env02
# 1.25.2
# 3.11.5 | packaged by conda-forge | (main, Aug 27 2023, 03:33:12) [Clang 15.0.7 ]
```

### Context for the issue:

_No response_",2023-09-05 05:53:22,,BUG: `np.linalg.eigh(complex)` give wrong eigenvectors with accelerate and netlib blas,"['00 - Bug', 'component: numpy.linalg']"
24632,open,Holt59,"### Describe the issue:

When using `np.interp` with a period, I get completely different results between Windows and Linux (see below).

### Reproduce the code example:

```python
import numpy as np

eps = float(np.finfo(np.float64).eps)

print(
    np.interp(
        [0.0, 120.0, 134.99, 135.0, 145.0, 239.99, 240.0, 241.0, 254.99, 359.99, 360.0],
        [
            0.0,
            135.0 - eps,
            135.0,
            240.0 - eps,
            240.0,
            255.0 - eps,
            255.0,
            360.0 - eps,
            360.0,
        ],
        [5.0, 5.0, 12.0, 12.0, 8.0, 8.0, 5.0, 5.0, 5.0],
        period=360,
    )
)

print(
    np.interp(
        [0.0, 120.0, 134.99, 135.0, 145.0, 239.99, 240.0, 241.0, 254.99, 359.99, 360.0],
        [
            0.0,
            135.0 - eps,
            135.0,
            240.0 - eps,
            240.0,
            255.0 - eps,
            255.0,
            360.0 - eps,
            360.0,
        ],
        [5.0, 5.0, 12.0, 12.0, 8.0, 8.0, 5.0, 5.0, 5.0],
        # period=360,
    )
)
```


### Outputs Windows vs. Linux:

```shell
# Windows (expected behavior)
[ 5.  5.  5. 12. 12. 12.  8.  8.  8.  5.  5.]
[ 5.  5.  5. 12. 12. 12.  8.  8.  8.  5.  5.]

# Linux
[ 5.         11.22222222 11.99948148  5.          5.66666667 11.99933333
  8.          8.          8.          5.          5.        ]
[ 5.  5.  5. 12. 12. 12.  8.  8.  8.  5.  5.]
```


### Runtime information:

Linux (docker python:3.10-slim):

```
1.25.2
3.10.12 (main, Jul 28 2023, 05:41:22) [GCC 12.2.0]
```

Windows (Windows 10):

```
1.25.2
3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]
```

### Context for the issue:

_No response_",2023-09-04 07:58:53,,BUG: Inconsistent behavior of np.interp between Windows and Linux with period,['00 - Bug']
24627,open,anntzer,"### Issue with current documentation:

AFAICT, the ""character+size"" typenames (""u1"", ""i4"", ""f8"", etc.) are not documented, whether at https://numpy.org/doc/stable/user/basics.types.html or at https://numpy.org/doc/stable/user/basics.types.html.

### Idea or request for content:

_No response_",2023-09-03 14:57:43,,DOC: character+size string typenames not documented,['04 - Documentation']
24585,open,kwsp,"### Describe the issue:

`np.recarray` ufuncs don't work, even though they are documented. E.g. [`np.recarray.sum`](https://numpy.org/doc/stable/reference/generated/numpy.recarray.sum.html#numpy.recarray.sum)

### Reproduce the code example:

```python
x = np.array([(1.0, 2), (3.0, 4)], dtype=[('x', '<f8'), ('y', '<i8')])
x = x.view(np.recarray)
x.sum()  # Raises numpy.core._exceptions._UFuncNoLoopError
```


### Error message:

```shell
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\user\miniconda3\envs\oct\lib\site-packages\numpy\core\_methods.py"", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
numpy.core._exceptions._UFuncNoLoopError: ufunc 'add' did not contain a loop with signature matching types (dtype((numpy.record, [('x', '<f8'), ('y', '<i8')])), dtype((numpy.record, [('x', '<f8'), ('y', '<i8')]))) -> None
```


### Runtime information:

1.25.2
3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]

### Context for the issue:

_No response_",2023-08-29 20:53:30,,ufuncs don't work on `recarray`,['04 - Documentation']
24576,open,eendebakpt,"### Describe the issue:

The latex presentation is automatically used in environments like Jupyter notebook and Spyder. For coefficients of object dtype an exception is generated since `np.signbit` is not available.

### Reproduce the code example:

```python
import fractions
from numpy.polynomial import Polynomial

f=fractions.Fraction(2,3)
p=Polynomial([f, f])
p._repr_latex_()
```


### Error message:

```shell
Traceback (most recent call last):

  Cell In[2], line 6
    p._repr_latex_()

  File C:\develop\env311\Lib\site-packages\numpy\polynomial\_polybase.py:468 in _repr_latex_
    elif not np.signbit(c):

TypeError: ufunc 'signbit' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
```


### Runtime information:

1.25.1
3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]

### Context for the issue:

_No response_",2023-08-29 08:08:37,,BUG: Using the latex presentation of a Polynomial with coefficients of object dtype raises an error,['00 - Bug']
24574,open,melissawm,"Currently, for a regular docs PR we should not skip GitHub actions since that is where the [artifacts redirector](https://github.com/numpy/numpy/blob/main/.github/workflows/circleci.yml) job is run.

We could add a `[docs only]` CI option that is equivalent to running only CircleCI, i.e.

`[skip azp][skip cirrus][skip travis][skip actions]`

**but** runs the artifacts redirector job.",2023-08-29 02:13:23,,"DOC, CI: Add a ""skip"" option to only run docs-related CI checks","['04 - Documentation', 'component: CI']"
24568,open,KrisMinchev,"### Proposed new feature or change:

The current default values for both `domain` and `window` parameters are both ``np.array([-1, 1])``. On the other hand, if we call ``__init__`` with 3 parameters, then `domain` and `window` always get cast to `double`. This leads to the following behaviour:
```python
>>> p1 = P(np.array([1,2,3]))
>>> p1
Polynomial([1., 2., 3.], domain=[-1,  1], window=[-1,  1], symbol='x')
>>> p2 = P(np.array([1,2,3]), np.array([-1, 1]), np.array([-1,1]))
>>> p2
Polynomial([1., 2., 3.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')
>>> p1.domain.dtype
dtype('int64')
>>> p2.domain.dtype
dtype('float64')
```
Changing the default values of the parameters ``domain`` and ``window`` to ``np.array([-1., 1.])`` will increase consistency between constructors ``Polynomial(coef)`` and ``Polynomial(coef, domain, window)``.",2023-08-28 15:58:06,,"ENH: Change default values for ""domain"" and ""window"" parameters of ``Polynomial`` class","['01 - Enhancement', 'component: numpy.polynomial']"
24565,open,LemonBoy,"### Proposed new feature or change:

Given a complex-valued matrix `C` of size `MxN` and a real-valued one `R` of size `NxQ` it should be possible to perform the `C @ R` operation between the two matrices using one strided `GEMM` call, exploiting the contiguous nature of the real and imaginary parts.

I suppose MATLAB is performing something similar as I've noticed a 2x difference in execution time between `C * R` and `(R' * C')'`.",2023-08-28 10:54:26,,ENH: Speed up complex-real matrix multiplication,['01 - Enhancement']
24548,open,misuzu,"### Proposed new feature or change:

Is it possible to run tests suite for 32-bit platforms in CI so it won't regress?

Here are tests failures for numpy 1.25.1 on i686-linux (32-bit userspace on 64-bit kernel):

```
FAILED lib/python3.10/site-packages/numpy/core/tests/test_ufunc.py::TestUfunc::test_identityless_reduction_huge_array - ValueError: Maximum allowed dimension exceeded
FAILED lib/python3.10/site-packages/numpy/core/tests/test_umath.py::TestRemainder::test_float_remainder_overflow - AssertionError: FloatingPointError not raised by divmod
FAILED lib/python3.10/site-packages/numpy/f2py/tests/test_kind.py::TestKind::test_int - AssertionError: selectedintkind(19): expected 16 but got -1
= 3 failed, 35327 passed, 1637 skipped, 1308 deselected, 30 xfailed, 5 xpassed, 341 warnings in 280.71s (0:04:40) =
```

Detailed output:

<details>

```
lib/python3.10/site-packages/numpy/typing/tests/test_runtime.py::TestRuntimeProtocol::test_issubclass[_NestedSequence] PASSED [100%]

=================================== FAILURES ===================================
_______________ TestUfunc.test_identityless_reduction_huge_array _______________

self = <numpy.core.tests.test_ufunc.TestUfunc object at 0xdb5bb418>

    @requires_memory(6 * 1024**3)
    def test_identityless_reduction_huge_array(self):
        # Regression test for gh-20921 (copying identity incorrectly failed)
>       arr = np.zeros((2, 2**31), 'uint8')
E       ValueError: Maximum allowed dimension exceeded

self       = <numpy.core.tests.test_ufunc.TestUfunc object at 0xdb5bb418>

lib/python3.10/site-packages/numpy/core/tests/test_ufunc.py:1622: ValueError
_________________ TestRemainder.test_float_remainder_overflow __________________

self = <numpy.core.tests.test_umath.TestRemainder object at 0xdb6e98c8>

    @pytest.mark.skipif(IS_WASM, reason=""fp errors don't work in wasm"")
    def test_float_remainder_overflow(self):
        a = np.finfo(np.float64).tiny
        with np.errstate(over='ignore', invalid='ignore'):
            div, mod = np.divmod(4, a)
            np.isinf(div)
            assert_(mod == 0)
        with np.errstate(over='raise', invalid='ignore'):
            assert_raises(FloatingPointError, np.divmod, 4, a)
        with np.errstate(invalid='raise', over='ignore'):
>           assert_raises(FloatingPointError, np.divmod, 4, a)

a          = 2.2250738585072014e-308
div        = inf
mod        = 0.0
self       = <numpy.core.tests.test_umath.TestRemainder object at 0xdb6e98c8>

lib/python3.10/site-packages/numpy/core/tests/test_umath.py:828: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../8hiddxp62lqzs9mrlizhn96akvzhvm39-python3-3.10.12/lib/python3.10/unittest/case.py:738: in assertRaises
    return context.handle('assertRaises', args, kwargs)
        args       = (<ufunc 'divmod'>, 4, 2.2250738585072014e-308)
        context    = None
        expected_exception = <class 'FloatingPointError'>
        kwargs     = {}
        self       = <numpy.testing._private.utils._Dummy testMethod=nop>
../8hiddxp62lqzs9mrlizhn96akvzhvm39-python3-3.10.12/lib/python3.10/unittest/case.py:200: in handle
    with self:
        args       = [4, 2.2250738585072014e-308]
        callable_obj = <ufunc 'divmod'>
        kwargs     = {}
        name       = 'assertRaises'
        self       = None
../8hiddxp62lqzs9mrlizhn96akvzhvm39-python3-3.10.12/lib/python3.10/unittest/case.py:223: in __exit__
    self._raiseFailure(""{} not raised by {}"".format(exc_name,
        exc_name   = 'FloatingPointError'
        exc_type   = None
        exc_value  = None
        self       = <unittest.case._AssertRaisesContext object at 0xc6dde5b0>
        tb         = None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.case._AssertRaisesContext object at 0xc6dde5b0>
standardMsg = 'FloatingPointError not raised by divmod'

    def _raiseFailure(self, standardMsg):
        msg = self.test_case._formatMessage(self.msg, standardMsg)
>       raise self.test_case.failureException(msg)
E       AssertionError: FloatingPointError not raised by divmod

msg        = 'FloatingPointError not raised by divmod'
self       = <unittest.case._AssertRaisesContext object at 0xc6dde5b0>
standardMsg = 'FloatingPointError not raised by divmod'

../8hiddxp62lqzs9mrlizhn96akvzhvm39-python3-3.10.12/lib/python3.10/unittest/case.py:163: AssertionError
______________________________ TestKind.test_int _______________________________

self = <numpy.f2py.tests.test_kind.TestKind object at 0xda77e400>

    def test_int(self):
        """"""Test `int` kind_func for integers up to 10**40.""""""
        selectedintkind = self.module.selectedintkind
    
        for i in range(40):
>           assert selectedintkind(i) == selected_int_kind(
                i
            ), f""selectedintkind({i}): expected {selected_int_kind(i)!r} but got {selectedintkind(i)!r}""
E           AssertionError: selectedintkind(19): expected 16 but got -1
E           assert -1 == 16
E            +  where -1 = <fortran function selectedintkind>(19)
E            +  and   16 = selected_int_kind(19)

i          = 19
selectedintkind = <fortran function selectedintkind>
self       = <numpy.f2py.tests.test_kind.TestKind object at 0xda77e400>

lib/python3.10/site-packages/numpy/f2py/tests/test_kind.py:20: AssertionError
=============================== warnings summary ===============================
lib/python3.10/site-packages/numpy/core/tests/test_numeric.py::TestNonarrayArgs::test_dunder_round_edgecases[2147483647--1]
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/core/tests/test_numeric.py:200: RuntimeWarning: invalid value encountered in cast
    assert_equal(round(val, ndigits), round(np.int32(val), ndigits))

lib/python3.10/site-packages/numpy/core/tests/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc
lib/python3.10/site-packages/numpy/core/tests/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc
lib/python3.10/site-packages/numpy/core/tests/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc
lib/python3.10/site-packages/numpy/core/tests/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc
lib/python3.10/site-packages/numpy/core/tests/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc
lib/python3.10/site-packages/numpy/core/tests/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc
lib/python3.10/site-packages/numpy/core/tests/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc
lib/python3.10/site-packages/numpy/core/tests/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/core/tests/test_umath.py:1935: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    x_f64 = np.float64(x_f32)

lib/python3.10/site-packages/numpy/core/tests/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc
lib/python3.10/site-packages/numpy/core/tests/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc
lib/python3.10/site-packages/numpy/core/tests/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/core/tests/test_umath.py:1944: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    assert_array_max_ulp(myfunc(x_f64), np.float64(y_true128),

lib/python3.10/site-packages/numpy/core/tests/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc
lib/python3.10/site-packages/numpy/core/tests/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc
lib/python3.10/site-packages/numpy/core/tests/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc
lib/python3.10/site-packages/numpy/core/tests/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc
lib/python3.10/site-packages/numpy/core/tests/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/core/tests/test_umath.py:1940: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    assert_equal(myfunc(x_f64), np.float64(y_true128))

lib/python3.10/site-packages/numpy/f2py/tests/test_return_integer.py: 20 warnings
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/f2py/tests/test_return_integer.py:16: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    assert t(array([123])) == 123

lib/python3.10/site-packages/numpy/f2py/tests/test_return_integer.py: 20 warnings
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/f2py/tests/test_return_integer.py:17: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    assert t(array([[123]])) == 123

lib/python3.10/site-packages/numpy/f2py/tests/test_return_integer.py: 20 warnings
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/f2py/tests/test_return_integer.py:18: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    assert t(array([123], ""b"")) == 123

lib/python3.10/site-packages/numpy/f2py/tests/test_return_integer.py: 20 warnings
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/f2py/tests/test_return_integer.py:19: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    assert t(array([123], ""h"")) == 123

lib/python3.10/site-packages/numpy/f2py/tests/test_return_integer.py: 20 warnings
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/f2py/tests/test_return_integer.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    assert t(array([123], ""i"")) == 123

lib/python3.10/site-packages/numpy/f2py/tests/test_return_integer.py: 20 warnings
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/f2py/tests/test_return_integer.py:21: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    assert t(array([123], ""l"")) == 123

lib/python3.10/site-packages/numpy/f2py/tests/test_return_integer.py: 20 warnings
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/f2py/tests/test_return_integer.py:22: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    assert t(array([123], ""B"")) == 123

lib/python3.10/site-packages/numpy/f2py/tests/test_return_integer.py: 20 warnings
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/f2py/tests/test_return_integer.py:23: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    assert t(array([123], ""f"")) == 123

lib/python3.10/site-packages/numpy/f2py/tests/test_return_integer.py: 20 warnings
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/f2py/tests/test_return_integer.py:24: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    assert t(array([123], ""d"")) == 123

lib/python3.10/site-packages/numpy/f2py/tests/test_return_real.py: 16 warnings
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/f2py/tests/test_return_real.py:23: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    assert abs(t(array([234])) - 234.0) <= err

lib/python3.10/site-packages/numpy/f2py/tests/test_return_real.py: 16 warnings
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/f2py/tests/test_return_real.py:24: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    assert abs(t(array([[234]])) - 234.0) <= err

lib/python3.10/site-packages/numpy/f2py/tests/test_return_real.py: 16 warnings
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/f2py/tests/test_return_real.py:25: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    assert abs(t(array([234]).astype(""b"")) + 22) <= err

lib/python3.10/site-packages/numpy/f2py/tests/test_return_real.py: 16 warnings
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/f2py/tests/test_return_real.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    assert abs(t(array([234], ""h"")) - 234.0) <= err

lib/python3.10/site-packages/numpy/f2py/tests/test_return_real.py: 16 warnings
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/f2py/tests/test_return_real.py:27: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    assert abs(t(array([234], ""i"")) - 234.0) <= err

lib/python3.10/site-packages/numpy/f2py/tests/test_return_real.py: 16 warnings
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/f2py/tests/test_return_real.py:28: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    assert abs(t(array([234], ""l"")) - 234.0) <= err

lib/python3.10/site-packages/numpy/f2py/tests/test_return_real.py: 16 warnings
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/f2py/tests/test_return_real.py:29: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    assert abs(t(array([234], ""B"")) - 234.0) <= err

lib/python3.10/site-packages/numpy/f2py/tests/test_return_real.py: 16 warnings
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/f2py/tests/test_return_real.py:30: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    assert abs(t(array([234], ""f"")) - 234.0) <= err

lib/python3.10/site-packages/numpy/f2py/tests/test_return_real.py: 16 warnings
  /nix/store/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1/lib/python3.10/site-packages/numpy/f2py/tests/test_return_real.py:31: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
    assert abs(t(array([234], ""d"")) - 234.0) <= err

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED lib/python3.10/site-packages/numpy/core/tests/test_ufunc.py::TestUfunc::test_identityless_reduction_huge_array - ValueError: Maximum allowed dimension exceeded
FAILED lib/python3.10/site-packages/numpy/core/tests/test_umath.py::TestRemainder::test_float_remainder_overflow - AssertionError: FloatingPointError not raised by divmod
FAILED lib/python3.10/site-packages/numpy/f2py/tests/test_kind.py::TestKind::test_int - AssertionError: selectedintkind(19): expected 16 but got -1
= 3 failed, 35327 passed, 1637 skipped, 1308 deselected, 30 xfailed, 5 xpassed, 341 warnings in 280.71s (0:04:40) =
```

</details>
",2023-08-26 05:40:21,,BUG: test failures on 32-bit platforms: i686 Linux and armv7l,['00 - Bug']
24530,open,jakevdp,"### Describe the issue:

Starting in numpy 1.25.0, it appears that the `ufunc.reduce` method does not recognize the `identity` argument for ufuncs created with `np.frompyfunc`

### Reproduce the code example:

```python
import numpy as np
import operator

print(np.__version__)

x = np.arange(10)
mask = x % 2 == 0
print(np.add.reduce(x, where=mask))

my_add = np.frompyfunc(operator.add, nin=2, nout=1, identity=0)
print(my_add.reduce(x, where=mask))
```


### Error message:

Result in numpy 1.24.4:

```shell
1.24.4
20
20
```
Result in numpy 1.25.0:
```shell
1.25.0
20
```
```pytb
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
[<ipython-input-7-a7d31df3c91a>](https://localhost:8080/#) in <cell line: 11>()
      9 
     10 my_add = np.frompyfunc(operator.add, nin=2, nout=1, identity=0)
---> 11 print(my_add.reduce(x, where=mask))

ValueError: reduction operation 'add (vectorized)' does not have an identity, so to use a where mask one has to specify 'initial'
```


### Runtime information:

```
>>> import sys, numpy; print(numpy.__version__); print(sys.version)
1.25.0
3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]
>>> print(numpy.show_runtime())
[{'numpy_version': '1.25.0',
  'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]',
  'uname': uname_result(system='Linux', node='e87715ee78c7', release='5.15.109+', version='#1 SMP Fri Jun 9 10:57:30 UTC 2023', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Haswell',
  'filepath': '/usr/local/lib/python3.10/dist-packages/numpy.libs/libopenblas64_p-r0-7a851222.3.23.so',
  'internal_api': 'openblas',
  'num_threads': 2,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.23'}]
None
```

### Context for the issue:

I am implementing `jax.numpy.frompyfunc` and had to special-case the tests in order to work around this bug: https://github.com/google/jax/pull/17275",2023-08-24 19:25:26,,BUG: ufuncs created with np.frompyfunc do not recognize identity in reduce in numpy 1.25.X,"['00 - Bug', '09 - Backport-Candidate']"
24525,open,maxnoe,"### Describe the issue:

Since version 1.24, the code example below results in a masked array where the data array and the mask array don't have the same shape

### Reproduce the code example:

```python
import numpy as np
print(np.__version__)

rng = np.random.default_rng(0)

data = rng.normal(size=(2, 101))

data[:, 2] = np.nan

std = np.ma.std(data, axis=1)
median = np.ma.median(data, axis=1)

print(""median:"")
print(repr(median))
print(""std:"")
print(repr(std))

deviation = data - median[:, np.newaxis]

comparison = deviation < 0.5 * std[:, np.newaxis]

print(comparison.shape, comparison.mask.shape)
print(comparison)
```


### Error message:


Output under 1.23:

```
1.23.5
median:
masked_array(data=[nan, nan],
             mask=False,
       fill_value=1e+20)
std:
masked_array(data=[--, --],
             mask=[ True,  True],
       fill_value=1e+20,
            dtype=float64)
(2, 101) (2, 101)
[[-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
  -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
  -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
  -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
  -- -- -- --]
 [-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
  -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
  -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
  -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
  -- -- -- --]]
```


Output under 1.25 (also 1.24):
```
1.25.2
median:
masked_array(data=[nan, nan],
             mask=False,
       fill_value=1e+20)
std:
masked_array(data=[--, --],
             mask=[ True,  True],
       fill_value=1e+20,
            dtype=float64)
(2, 101) (2, 1)
Traceback (most recent call last):
  File ""/home/mnoethe/test_numpy_ma_std.py"", line 23, in <module>
    print(comparison)
  File ""/home/mnoethe/.local/conda/envs/numpy-1.25/lib/python3.10/site-packages/numpy/ma/core.py"", line 3997, in __str__
    return str(self._insert_masked_print())
  File ""/home/mnoethe/.local/conda/envs/numpy-1.25/lib/python3.10/site-packages/numpy/ma/core.py"", line 3991, in _insert_masked_print
    _recursive_printoption(res, mask, masked_print_option)
  File ""/home/mnoethe/.local/conda/envs/numpy-1.25/lib/python3.10/site-packages/numpy/ma/core.py"", line 2437, in _recursive_printoption
    np.copyto(result, printopt, where=mask)
ValueError: could not broadcast where mask from shape (2,2) into shape (2,100)
```



### Runtime information:

[{'numpy_version': '1.25.2',
  'python': '3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) '
            '[GCC 12.3.0]',
  'uname': uname_result(system='Linux', node='e5b-dell-12', release='5.14.0-1051-oem', version='#58-Ubuntu SMP Fri Aug 26 05:50:00 UTC 2022', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL',
                                    'AVX512_SPR']}},
 {'architecture': 'Haswell',
  'filepath': '/home/mnoethe/.local/conda/envs/numpy-1.25/lib/libopenblasp-r0.3.23.so',
  'internal_api': 'openblas',
  'num_threads': 20,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.23'}]

### Context for the issue:

Most confusingly, the example above works fine with numpy 1.25 if the shape of the data array is `(2, 100)` (just one element smaller in the last dimension).",2023-08-24 16:57:42,,BUG: masked std and median on unmasked array result in invalid masked array,"['00 - Bug', 'component: numpy.ma']"
24480,open,eendebakpt,"### Proposed new feature or change:

The `numpy.polynomial` classes have quite some overhead in the calculations. This is a collection of ideas to reduce the overhead

* https://github.com/numpy/numpy/pull/24473
* https://github.com/numpy/numpy/pull/24467
* For several operations on polynomials (e.g. addition, substraction) the `ABCPolyBase._get_coefficients` takes time.  The check `np.all(self.domain == other.domain)` can be replaced with either `np.array_equal(self.domain, other.domain).all()` or `(self.domain == other.domain).all()` (and similar for the `window`). For benchmark

```
from numpy.polynomial import Polynomial
import timeit

p = Polynomial([1,2,3])
q = Polynomial([3,2,1])

dt=timeit.timeit('p + p - q', globals={'p': p, 'q': q}, number=40_000)
print(dt)
```
the timings are
```
2.51 seconds (main)
2.35 seconds (np.array_equal)
2.2 seconds (.all() )
```

The fastest option does assume the `domain` already is a numpy array. Current main and the `np.array_equal` option cast (with `asarray`). https://github.com/numpy/numpy/pull/24499

* The `numpy.polynomial.polyutils.as_series` is called in many places. Also in internal methods where many of the input checks (e.g. on input dimensions) are not required. We can add a flag `internal : bool` to `as_series` than will skip several of the checks. Draft PR: #24531


Profiling results on main (2023-8-22) for `p * q` with `p=Polynomial([1,2,3]); q=Polynomial([3,0,3])`:
![np_polynomial_mul](https://github.com/numpy/numpy/assets/883786/db20517a-19cd-46e0-b7e4-957b4cce381b)

",2023-08-21 15:31:25,,ENH: Reduce overhead of numpy.polynomial,"['01 - Enhancement', 'component: numpy.polynomial']"
24478,open,Itayazolay,"### Describe the issue:

According to `np.asarray(a)` [docs](https://numpy.org/doc/stable/reference/generated/numpy.asarray.html#numpy-asarray), it may return the original array or a copy.
In the case below, while `np.asarray(a, dtype='object') is not a`, - `np.shares_memory(np.asarray(a, dtype='object'), a)` given that a is copied.



### Reproduce the code example:

```python
import numpy as np
import pickle
t = np.array([None, None, 123], dtype='object')
tp = pickle.loads(pickle.dumps(t))
g = np.asarray(tp, dtype='object')

assert g is not tp
assert np.shares_memory(g, tp)  # Unexpected
g[0] = '1'
assert tp[0] == '1'  # Unexpected


# The issue seems to be originated here
assert not np.dtype('O') is pickle.loads(pickle.dumps(np.dtype('O'))) 
assert np.dtype('O') == pickle.loads(pickle.dumps(np.dtype('O')))
```


### Error message:

_No response_

### Runtime information:

1.25.2
3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:58:44) [GCC 11.3.0]
[{'numpy_version': '1.25.2',
  'python': '3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:58:44) '
            '[GCC 11.3.0]',
  'uname': uname_result(system='Linux', node='itay-jether', release='6.2.0-26-generic', version='#26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2',
                                'AVX512F',
                                'AVX512CD',
                                'AVX512_SKX',
                                'AVX512_CLX',
                                'AVX512_CNL',
                                'AVX512_ICL'],
                      'not_found': ['AVX512_KNL', 'AVX512_KNM']}},
 {'architecture': 'SkylakeX',
  'filepath': '/home/itay/miniforge3/envs/jpy310/lib/python3.10/site-packages/numpy.libs/libopenblas64_p-r0-5007b62f.3.23.dev.so',
  'internal_api': 'openblas',
  'num_threads': 8,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.23.dev'},
 {'architecture': 'SkylakeX',
  'filepath': '/home/itay/miniforge3/envs/jpy310/lib/python3.10/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so',
  'internal_api': 'openblas',
  'num_threads': 8,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.18'},
 {'architecture': 'SkylakeX',
  'filepath': '/home/itay/miniforge3/envs/jpy310/lib/python3.10/site-packages/cvxopt.libs/libopenblasp-r0-5c2b7639.3.23.so',
  'internal_api': 'openblas',
  'num_threads': 8,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.23'},
 {'architecture': 'Prescott',
  'filepath': '/home/itay/miniforge3/envs/jpy310/lib/python3.10/site-packages/scs.libs/libopenblas-r0-f650aae0.3.3.so',
  'internal_api': 'openblas',
  'num_threads': 1,
  'prefix': 'libopenblas',
  'threading_layer': 'disabled',
  'user_api': 'blas',
  'version': None},
 {'filepath': '/home/itay/miniforge3/envs/jpy310/lib/python3.10/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0',
  'internal_api': 'openmp',
  'num_threads': 8,
  'prefix': 'libgomp',
  'user_api': 'openmp',
  'version': None}]

### Context for the issue:

I'm using pandas and have pickled dataframes, when using `df.astype(str)` the dataframe is changed in-place
see [issue](https://github.com/pandas-dev/pandas/issues/54654) in pandas",2023-08-21 14:10:12,,BUG: np.asarray return a copy with shared memory,['00 - Bug']
24470,open,rossbar,"### Issue with current documentation:

As noted in the [backward compatibility section of NEP 51](https://numpy.org/neps/nep-0051-scalar-representation.html#backward-compatibility), one of the places where the updated scalar representations is likely to cause issues is in (vanilla) doctests in downstream libraries. Since vanilla doctest performs test evaluation by string matching, any test that returns a numpy scalar will be changing (and thus failing) when going from numpy v1. -> v2.0, e.g.

```python
>>> a = np.array([1, 2, 3])
>>> a[0]
1
```

will become:

```python
>>> a = np.array([1, 2, 3])
>>> a[0]
np.int64(1)
```

NEP 51 doesn't really offer concrete advice on how to address this upcoming compatibility break. The current text:

> It may be necessary to adopt tools for doctest testing to allow approximate value checking for the new representation.

This is generally good advice as tools like `pytest-doctestplus` should be more robust to the scalar repr change, but there are likely many libraries that depend on numpy which use vanilla doctest for whom adopting a new doctesting framework represents a non-trivial amount of effort.

Therefore, I think it's worthwhile to consider alternatives to aid in downstream projects transitioning to the new NumPy 2.0 scalar reprs.

### Background

This issue popped up for NetworkX, which tests against the numpy nightly wheels. See networkx/networkx#6846.

The way that NetworkX decided to deal with this was to use `np.set_printoptions` to configure the doctest runner to pin numpy reprs back to an older version. Networkx uses `pytest` to run doctests with the `--doctest-modules` flag, so we were able to update the test configuration with something like:

```python
# in conftest.py

@pytest.fixture(autouse=True)
def np_legacy_scalar_repr(doctest_namespace):
    import numpy as np
    np.set_printoptions(legacy=""1.21"")
```

See networkx/networkx#6856 for details.

The advantages of this approach are:
 - doctests can continue to run & check that outputs are numerically correct (ignoring the repr change)
 - allows libraries to avoid complicating their CI with multiple jobs for `numpy<2.0` and `numpy>=2.0` for the sake of doctests
 - Transitioning to 2.0 is straightforward: when numpy 2.0 becomes the minimum supported version for the library, revert the legacy print config & update any doctests that are returning numpy scalars to use the 2.0 version.

### Idea or request for content:

I *think* the above generalizes to downstream projects which depend on vanilla doctest. If anyone has feedback I'd love to hear it! If this seems like a sensible plan, then I'd propose to add it to the documentation; perhaps in the ""transitioning to numpy 2.0"" document.

Related to this: will `printoptions` get a `""1.26""` legacy option?

",2023-08-20 23:23:39,,NEP 51 transition for downstream libraries using doctest,['04 - Documentation']
24428,open,hoodmane,"I am trying to understand pep3118 since it is essentially undocumented, see the discussion here: https://discuss.python.org/t/question-pep-3118-format-strings-and-the-buffer-protocol/31264/7 

@mattip @seberg @rgommers @pitrou

Numpy implements a large subset of it in `numpy/core/_internal.py`. I think the parser in `_internal.py` implements the following lark grammar:

<details>
<summary>Lark grammar for numpy's _dtype_from_pep3118</summary>

```
?start: root
root: entry+
?entry: ( array | padding | _normal_entry ) name?

struct: ""T{"" entry* ""}""
padding: ""x""
name:  "":"" IDENTIFIER "":""

array: shape _normal_entry
shape: ""("" _shape_body "")""
_shape_body: (NUMBER "","")* NUMBER

_normal_entry: byteorder? repeat? ( struct | prim )
byteorder: BYTEORDER
repeat: NUMBER
prim: PRIMITIVE

IDENTIFIER: /[^:^\s]+/
NUMBER: (""0""..""9"")+
BYTEORDER: ""@"" | ""="" | ""<"" | "">"" | ""^"" | ""!""
PRIMITIVE: ""Zf"" | ""Zd"" | ""Zg"" | /[?cbBhHiIlLqQfdgswOx]/

%ignore /\s+/
```
</details>

There are a few things I think are weird about this grammar:

#### The location of the byte order marks in relation to shapes

The pep says:

> Endian-specification (‘!’, ‘@’,’=’,’>’,’<’, ‘^’) is also allowed inside the string so that it can change if needed. The previously-specified endian string is in force until changed. The default endian is ‘@’ which means native data-types and alignment. If un-aligned, native data-types are requested, then the endian specification is ‘^’.

This is completely ambiguous about where these marks can go. Prior to pep3118 it seems that the marks are only allowed at the very start of the format string. It seems to me that the most logical location would be that one is allowed between each pair of adjacent entries. But `_dtype_from_pep3118` expects them to come *between* the shape and the primitive:
```
>>> _dtype_from_pep3118(""@(3,1)i"") # @ before shape not allowed
ValueError: Unknown PEP 3118 data type specifier '(3,1)i'
>>> _dtype_from_pep3118(""(3,1)@i"") # @ between shape and i allowed
dtype(('<i4', (3, 1)))
```
This would sort of make sense if the mark only affected the current entry but it also affects all following ones, making the location a bit perplexing. This becomes particularly noticeable when you look at the parse trees: since it affects all following entries, it should come next to the entries but the parser grammar above makes the order mark a child of a particular entry.

I think this is a bug which should be fixed by the following patch:
<details>
<summary>patch</summary>

```patch
--- a/numpy/core/_internal.py
+++ b/numpy/core/_internal.py
@@ -673,12 +673,6 @@ def __dtype_from_pep3118(stream, is_subdtype):
         if stream.consume('}'):
             break
 
-        # Sub-arrays (1)
-        shape = None
-        if stream.consume('('):
-            shape = stream.consume_until(')')
-            shape = tuple(map(int, shape.split(',')))
-
         # Byte order
         if stream.next in ('@', '=', '<', '>', '^', '!'):
             byteorder = stream.advance(1)
@@ -686,6 +680,12 @@ def __dtype_from_pep3118(stream, is_subdtype):
                 byteorder = '>'
             stream.byteorder = byteorder
 
+        # Sub-arrays (1)
+        shape = None
+        if stream.consume('('):
+            shape = stream.consume_until(')')
+            shape = tuple(map(int, shape.split(',')))
+
         # Byte order characters also control native vs. standard type sizes
         if stream.byteorder in ('@', '^'):
             type_map = _pep3118_native_map
```


</details>

#### `(4)h` vs `4h` vs `hhhh`

In the struct module documentation it says:
>  the format string '4h' means exactly the same as 'hhhh'.

But `_dtype_from_pep3118` disagrees: it gives the same output for `4h` and `(4)h` but both are different from the output for `hhhh`. Then there is the issue of `(4)4h`, which is treated as a array of 4 arrays of 4 h's, so not the same as `(4,4)h`. Also, perplexingly `(4)(4)h` is a syntax error. I think `(4)4h` should be the same as `(4)T{hhhh}`.

Also as I said, it seems to me that it makes more sense to allow arbitrary nested arrays like `(4)(4)h` to mean the current thing that `(4)4h` means.

#### Arrays of padding

I think it's weird that `_dtype_from_pep3118` accepts arrays of padding like `(4, 4)x`. Isn't this properly rendered as `16x`? It gives the same output. My grammar doesn't allow it.

#### Named padding

Is it intended that can be named? If you need a name for it, is it padding anymore?

### A lark grammar with my suggested modifications:

<details>
<summary>Details</summary>


```
?start: root
root: _entry+
_entry: byteorder? entry
?entry:  padding | (_normal_entry  name?)

padding: NUMBER? ""x""
name:  "":"" IDENTIFIER "":""

_normal_entry: array | (repeat? ( struct | prim ))

struct: ""T{"" _entry* ""}""

array: shape _normal_entry
shape: ""("" _shape_body "")""
_shape_body: (NUMBER "","")* NUMBER


byteorder: BYTEORDER
repeat: NUMBER
prim: PRIMITIVE


IDENTIFIER: /[^:^\s]+/
NUMBER: (""0""..""9"")+
BYTEORDER: ""@"" | ""="" | ""<"" | "">"" | ""^"" | ""!""
PRIMITIVE: ""Zf"" | ""Zd"" | ""Zg"" | /[?cbBhHiIlLqQfdgswOx]/

%ignore /\s+/
```

</details>",2023-08-16 12:26:58,,Questions about pep3118 format strings,['unlabeled']
24424,open,bsipocz,"Once a downstream CI starts to fail due to a new nightly version, it would be extremely helpful to be able to go back a few versions for debugging and testing.

Somewhat related (but I'm happy to open a separate issue for it), is  that while I understand that currently there are technical limitations, having the git hash in the version would be super useful, too. But even having the date would be nicer than the extremely generic `dev0`",2023-08-15 20:22:40,,CI: provide older version of dev wheels rather than overwriting the latest,"['14 - Release', 'component: CI']"
24418,open,softwaredoug,"### Describe the issue:

Tests are failing on Mac OSX, m1, Python 3.11.4. Seems to occur both on clang and gcc builds.

Specifically the test_mem_policy tests give an error seemingly related to an inability to build the test code in those tests

```
Undefined symbols for architecture arm64:
  ""_main"", referenced from:
     implicit entry/start for main executable
ld: symbol(s) not found for architecture arm64
```

And, subsequently, perhaps caused by this initial build failure

```
> Visual Studio environment is needed to run Ninja. It is recommended to use Meson wrapper:
/Users/douglas.turnbull/ws/numpy/venv/bin/meson compile -C .

> ERROR: Could not detect Ninja v1.8.2 or newer
```

### Reproduce the code example:

```
 python runtests.py -v -m full    
```

but also, this recreates the issue faster

```python
python -m pytest numpy/core/tests/test_mem_policy.py
```


### Error message:

```shell
Snippet from meson log

Detecting Apple linker via: `cc -Wl,-v` -> 1
stderr:
@(#)PROGRAM:ld  PROJECT:ld64-857.1
BUILD 23:13:29 May  7 2023
configured to support archs: armv6 armv7 armv7s arm64 arm64e arm64_32 i386 x86_64 x86_64h armv6m armv7k armv7m armv7em
Library search paths:
.
/opt/homebrew/lib
/usr/local/lib
/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/lib
Framework search paths:
/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/System/Library/Frameworks/
Undefined symbols for architecture arm64:
  ""_main"", referenced from:
     implicit entry/start for main executable
ld: symbol(s) not found for architecture arm64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
```


### Runtime information:

I'm running Python 3.11.4 installed with asdf. I'm running in a virtual environment created with `python3 -m venv venv`.

```
>>> import sys, numpy; print(numpy.__version__); print(sys.version)
2.0.0.dev0+git20230813.104addf
3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]
```

```
>>> print(numpy.show_runtime())
WARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information
[{'numpy_version': '2.0.0.dev0+git20230813.104addf',
  'python': '3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 '
            '(clang-1403.0.22.14.1)]',
  'uname': uname_result(system='Darwin', node='[REDACTED]', release='22.6.0', version='Darwin Kernel Version 22.6.0: Wed Jul  5 22:22:05 PDT 2023; root:xnu-8796.141.3~6/RELEASE_ARM64_T6000', machine='arm64')},
 {'simd_extensions': {'baseline': ['NEON', 'NEON_FP16', 'NEON_VFPV4', 'ASIMD'],
                      'found': ['ASIMDHP', 'ASIMDDP'],
                      'not_found': ['ASIMDFHM']}}]
```

### Context for the issue:

I was trying to get started building/testing a pull request adding popcount to numpy - #21429 and seeing if I could help finish it. I'm new to numpy development, but not to python / C development.

Largely I can ignore these failures for my case (I think) but wanted to flag/share them.",2023-08-14 20:32:12,,BUG: test_mem_policy failing without ninja,['00 - Bug']
24397,open,daniilS,"### Describe the issue:

`np.vectorize` doesn't work when used on a method inside a class definition, whether it's used as a function or as a decorator. The docstring for `vectorize` says that `pyfunc` is ""A python function or method"".

#9349 was working on a fix by making `vectorize` follow the descriptor protocol, but was closed when #23514 made `np.vectorize` work as a decorator. However, #23514 didn't implement the `vectorize.__get__` method which would fix this.

### Reproduce the code example:

```python
import numpy as np

class C:
    def __init__(self, y):
        self.y = y
        self.inside_init = np.vectorize(self.pyfunc)  # current workaround
    
    @np.vectorize
    def decorated(self, x):
        return x + self.y
    
    def pyfunc(self, x):
        return x + self.y
    
    inside_class = np.vectorize(pyfunc)

c = C(1)
a = np.array([1, 2])

c.inside_init(a)  # works
c.decorated(a)  # error
c.inside_class(a)  # error
```


### Error message:

```shell
[...]
TypeError: C.decorated() missing 1 required positional argument: 'x'
[...]
TypeError: C.pyfunc() missing 1 required positional argument: 'x'
```


### Runtime information:

1.25.2
3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]
[{'numpy_version': '1.25.2',
  'python': '3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 '
            '64 bit (AMD64)]',
  'uname': uname_result(system='Windows', node='LAPTOP-11P3HCJO', release='10', version='10.0.19045', machine='AMD64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}}]

### Context for the issue:

_No response_",2023-08-11 16:43:08,,BUG: np.vectorize doesn't work inside class definition,['01 - Enhancement']
24394,open,seberg,"The scalar converters in f2py are from a different age and time and are crazy forgiving.  For example, conversion to double amounts to something like:
```
def to_double(obj):
    try:
       return float(obj)
    except:
       pass
    try:
       return complex(obj).real
    except:
       pass
    if not isinstance(obj, (str, bytes)):
       return to_double(obj[0])
    
    raise  # the except: pass is not correct above of course.
```
it allows complex values, and nested sequences and doesn't even check if that nested sequence has only length one at least.

I ran into this because of our deprecation warning on `float(np.array([1.]))`, in f2py code paths SciPy doesn't notice the warning when raising as an error, since it just continues to the sequence unpacking.

Not sure what we should do, it might be nice to very gently deprecate this, downstream may want to explicitly allow 1-element arrays in some cases for example.
(Optimizers are one example where allowing 1-element arrays seems rather acceptable, since you know you are minimizing a scalar value.)",2023-08-11 12:31:24,,BUG: f2py scalar converters are too forgiving,['component: numpy.f2py']
24388,open,patquem,"### Describe the issue:

Hi,
Surprisingly, when going from 2D to 3D on large arrays, a zero-padding becomes very CPU time consuming.
Is it normal ?
Patrick

### Reproduce the code example:

```python
import numpy as np
import time

size = 1000
arr2d = np.random.random((size, size))
arr3d = np.random.random((size, size, size))

t0 = time.perf_counter()
arr2d_pad = np.pad(arr2d, pad_width=0)
t1 = time.perf_counter()
arr3d_pad = np.pad(arr3d, pad_width=0)
t2 = time.perf_counter()

print(arr2d_pad.all() == arr2d.all())
print(arr3d_pad.all() == arr3d.all())

print(f""zero-padding in 2D: {t1-t0}s"")
print(f""zero-padding in 3D: {t2-t1}s"")
```


### Error message:

```shell
True
True
zero-padding in 2D: 0.012127699999837205s
zero-padding in 3D: 3.2117499000014504s
```


### Runtime information:

1.24.3
3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]

### Context for the issue:

_No response_",2023-08-10 14:05:55,,ENH: np.pad is time consuming in 3D ... for nothing ,"['00 - Bug', '57 - Close?']"
24368,open,Aryo-Zare,"### Proposed new feature or change:

I had made a suggestion to the matplotlib commuity (the link below) regarding some enhancement in histogram view :
https://github.com/matplotlib/matplotlib/issues/26415
Briefly it’s about plotting a data with negative values in logarithmic scale (their abstract values) and also keeping the bins sizes to be logarithmically arranged (details of the solution in the link above).
They had mentioned that because they use np.histogram for their data processing, they do not want to further process data at plotting level. Hence I’m contacting the numpy community here for implementing this.

",2023-08-09 11:02:01,,"ENH: np.histogram : negative values , logarithmic scale & logarithmically sized bins.",['unlabeled']
24355,open,charris,"This:

```
../numpy/core/src/multiarray/alloc.c: In function ‘PyDataMem_RENEW’:
../numpy/core/src/multiarray/alloc.c:274:9: warning: pointer ‘ptr’ may be used after ‘realloc’ [-Wuse-after-free]
  274 |         PyTraceMalloc_Untrack(NPY_TRACE_DOMAIN, (npy_uintp)ptr);
      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../numpy/core/src/multiarray/alloc.c:272:14: note: call to ‘realloc’ here
```

There are more in NumPy 1.26,x but I will look into fixing them from main.
",2023-08-07 15:34:51,,Compiler warnings using gcc 13.,"['16 - Development', '03 - Maintenance']"
24345,open,eendebakpt,"### Proposed new feature or change:

The performance of numpy operations involving scalars can be improved by adding a simple cache for some of the arrays generated in the `convert_ufunc_arguments`. A prototype implementation is https://github.com/numpy/numpy/compare/main...eendebakpt:scalar_no_alloc_proto
 
A benchmark of the prototype (`i` an integer, `f` a float and `x` a small numpy array):
```
i + x: Mean +- std dev: [main] 688 ns +- 36 ns -> [pr] 522 ns +- 25 ns: 1.32x faster
f + x: Mean +- std dev: [main] 706 ns +- 57 ns -> [pr] 517 ns +- 21 ns: 1.36x faster
np.sin(2*np.pi*x + phi): Mean +- std dev: [main] 2.06 us +- 0.11 us -> [pr] 1.61 us +- 0.05 us: 1.28x faster
s + x: Mean +- std dev: [main] 723 ns +- 47 ns -> [pr] 742 ns +- 44 ns: 1.03x slower

Benchmark hidden because not significant (2): x + x, x**2

Geometric mean: 1.14x faster
```

<details><summary>Benchmark script</summary>

```
import pyperf
import numpy as np
#print(np, np.__version__)

np._set_promotion_state('weak')

setup=""""""
import numpy as np
np._set_promotion_state('weak')

x = np.array([1., 2., 3.])

i = 3
f = 1.1

s = np.float64(20.)
phi = np.pi/4
""""""


runner = pyperf.Runner()
runner.timeit(name=""i + x"", stmt=""_ = f + x"",setup=setup)
runner.timeit(name=""f + x"", stmt=""_ = f + x"",setup=setup)
runner.timeit(name=""np.sin(2*np.pi*x + phi)"", stmt=""_ = np.sin(2*np.pi*x + phi)"",setup=setup)
runner.timeit(name=""s + x"", stmt=""_ = s + x"",setup=setup)
runner.timeit(name=""x + x"", stmt=""_ = s + x"",setup=setup)
runner.timeit(name=""x**2"", stmt=""_ = x**2"",setup=setup)
```
</details>

The basic ideas for this approach can be found in a mailing list thread: https://mail.python.org/archives/list/numpy-discussion@python.org/thread/DPATA4IOVBSXC5VNY7IBYIKLXL2ZGI36/#DPATA4IOVBSXC5VNY7IBYIKLXL2ZGI36

Notes:

* The performance gain from the prototype is due to prevention of the allocation and deallocation of a numpy array. For a scalar argument to a ufunc such as a python `float`, the scalar argument is converted internally to a python array
with `PyArray_FromAny` in `convert_ufunc_arguments`. The allocation of a temporary numpy array is expensive, so in the prototype we keep track of the generated array and re-use it if possible.
* Profiling `1.1 + x` with valgrind results in (without this PR and the promotion state from NEP50 set to `weak`):

![scalar_alloc](https://github.com/numpy/numpy/assets/883786/73db21d6-dd6b-417f-ba8a-d9cc5c959134)

Without this PR about 25% if the runtime is spend in allocating a deallocating a temporary numpy array.

* The performance of the `PyArray_FromAny` could perhaps be improved by using freelists of generated arrays. This is more complex as it is unclear when generated arrays will be available again. So one has to keep a longer list of arrays and check reference counts. In the prototype from this PR there is a single array in the freelist, and it will be free after the ufunc execution is complete.
An advantage of improving the `PyArray_FromAny` is that for larger arrays `x, y, z` in the expression `np.sin( (x+y)+z)` the intermediate result for `x+y` will be available quickly, so one can avoid allocation and deallocation of larger arrays. The prototype here only deals with scalars.
* The prototype is not complete yet. Some open items are proper initialization, thread safety and fast path for numpy scalars.
",2023-08-05 21:16:02,,ENH: [RFC] Improve performance of numpy operations involving scalars,['unlabeled']
24339,open,shaunc,"### Describe the issue:

It is my understanding that the fields of an aligned recarray should be appriate to each of the datatypes in the record. However, when a field containing a first subfield w a large type follows an array with an odd number of elements of a small type, the 2nd field is misaligned.

### Reproduce the code example:
```python
import numpy as np

dtx = np.dtype(np.int8, align=True)
ax = np.array([1], dtype=dtx)
dty = np.dtype((np.record, [('a', np.int64)]), align=True)
ay = np.rec.array([2],dtype=dty)
values = [ax, ay]
fields = [('x', dtx, (1,)), ('y', dty)]
rec_array = np.rec.fromarrays(
    values, dtype=np.dtype(fields, align=True)
)
print(rec_array.dtype.fields['y'])
# prints: (dtype((numpy.record, [('a', '<i8')])), 1)
#
# Note that ""1"" alignment can't be right for subfield ""a"" of record ""y""

from numpy.lib.recfunctions import repack_fields
rec_array = repack_fields(rec_array, align=True, recurse=True)
print(rec_array.dtype.fields['y'])
# still prints: (dtype((numpy.record, [('a', '<i8')])), 1)


### Error message:

When this record array is passed to numba cuda, it results in an alignment error.

### Runtime information:

In [3]: import sys, numpy; print(numpy.__version__); print(sys.version)
1.24.3
3.11.3 | packaged by conda-forge | (main, Apr  6 2023, 08:57:19) [GCC 11.3.0]


### Context for the issue:

GPU programming is increasingly important, but requires proper alignment to handle structured data.",2023-08-04 16:50:47,,BUG: alignment not enforced in rec.fromarrays,['00 - Bug']
24336,open,chriseclectic,"### Describe the issue:

I have been running into issues with trying to use Numpy object arrays to contruct ND-Arrays of Python objects where I want to take advantage of the ND-indexing and other array features, but the values of the array are Python objects, which themselves have an `__array__` method defined, but the `__array__` method is inefficient and should be avoided.

In these cases I am not able to construct an object array from a nested list via slicing without invoking the objects `__array__` method.

A simplified example of where this is a problem is as follows:

Consider a skeleton implementation of a Python class for N-qubit Pauli matrices. These objects have an efficient representation (for simplicity I will just use the length N string labels), but can also be represented inefficiently as shape (2 ** N, 2 **N) complex matrices. Suppose this class defines an `__array__` method to convert to these inefficient complex arrays like shown in the bellow code example.

Now suppose I want to construct an `object` ND-array of Paulis.  The simplest case would be

```python
arr1 = np.empty([1], dtype=object)
arr1[0] = Pauli(""X"")
```
This does not invoke the `Pauli.__array__` method and returns `array([Pauli(X)], dtype=object)`.

However if I try and set via slicing 

```python
arr2 = np.empty([1], dtype=object)
arr2[:] = [Pauli(""X"")]
# Prints: Pauli.__array__ called
```
 This invokes the array method, even though the returned array is an object array with the original objects as values (`np.all(arr1 == arr2)` is True).
 
Complete code shown bellow, where you can see where this becomes a problem for example by setting `nq=20` which would require ~17TB memory per Pauli for each `__array__` return.

### Reproduce the code example:

```python
import numpy as np

class Pauli:
    """"""Basic N-qubit Pauli""""""

    _mats = {
        ""I"": np.eye(2, dtype=complex),
        ""X"": np.array([[0, 1], [1, 0]], dtype=complex),
        ""Y"": np.array([[0, -1j], [0, 1j]], dtype=complex),
        ""Z"": np.array([[1, 0], [0, -1]], dtype=complex),
    }

    def __init__(self, label):
        """"""Initialize a Pauli from a string label""""""
        self.label = label
     
    def __repr__(self):
        return f""Pauli({self.label})""
    
    def to_matrix(self):
        """"""Convert to a (2**N, 2**N) complex matrix""""""
        ret = np.eye(1, dtype=complex)
        for i in self.label:
            ret = np.kron(self._mats[i], ret)
        return ret

    def __array__(self, dtype=None):
        # Print to know when the `__array__` method is invoked
        print(""Pauli.__array__ called"")
        mat = self.to_matrix()
        if dtype:
            mat = mat.astype(dtype)
        return mat

    def __eq__(self, other):
        if isinstance(other, Pauli):
            return self.label == other.label
        return False

# Create an array-like nested list of Pauli objects
nq = 1
paulis = [[Pauli(nq * ""I""), Pauli(nq * ""X"")],
          [Pauli(nq * ""Y""), Pauli(nq * ""Z"")]]

# Initialize and empty array and set values to nested list elements
arr = np.empty([2, 2], dtype=object)
arr[:] = paulis

# This print `Pauli.__array__ called` 4 times, so invokes it for each Pauli
# Still returns an object array with the `Pauli` objects as values.
```


### Error message:

_No response_

### Runtime information:

My main numpy runtime is
```
1.23.5
3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:01:00) 
[Clang 13.0.1 ]
```

I also tried on Numpy 1.25.2 and get same results.


### Context for the issue:

This is probably quite a specific edge case, but it is something I've been struggling with for awhile and have had to come up with various messy work arounds to avoid the issue.",2023-08-04 15:11:42,,BUG: Setting an object array via slicing with nested lists invokes the objects `__array__` methods unnecessarily,['00 - Bug']
24313,open,dentalfloss1,"### Describe the issue:

As of numpy 1.25, the following deprecation warning appears when setting the dtype of an array that will overflow:

> /home/sarah/dep.py:6: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 248 to int8 will fail in the future.
For the old behavior, usually:
    np.array(value).astype(dtype)
will give the desired result (the cast overflows).
  arr = np.array(dat, dtype=mydtype)

However, if the dtype is a multipart structured array, the suggested fix duplicates all the values, thus changing the shape of the array, as can be seen in the example code below.


### Reproduce the code example:

```python
import numpy as np 
mydtype = [('re','i1'),('im','i1')]

dat = [(2,158),(6,856),(248,35)]

arr = np.array(dat, dtype=mydtype)
# works but is deprecated

arr2 = np.array(dat).astype(mydtype)
print(arr,arr2)

print(""Is arr equal to arr2 ? "", np.array_equal(arr,arr2))
```


### Error message:

```
sarah@sarah-optiplex-ubuntu:~$ python dep.py
/home/sarah/dep.py:6: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 158 to int8 will fail in the future.
For the old behavior, usually:
    np.array(value).astype(dtype)
will give the desired result (the cast overflows).
  arr = np.array(dat, dtype=mydtype)
/home/sarah/dep.py:6: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 856 to int8 will fail in the future.
For the old behavior, usually:
    np.array(value).astype(dtype)
will give the desired result (the cast overflows).
  arr = np.array(dat, dtype=mydtype)
/home/sarah/dep.py:6: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 248 to int8 will fail in the future.
For the old behavior, usually:
    np.array(value).astype(dtype)
will give the desired result (the cast overflows).
  arr = np.array(dat, dtype=mydtype)
[( 2, -98) ( 6,  88) (-8,  35)] [[(  2,   2) (-98, -98)]
 [(  6,   6) ( 88,  88)]
 [( -8,  -8) ( 35,  35)]]
Is arr equal to arr2 ?  False 
```

### Runtime information:

import sys, numpy; print(numpy.__version__); print(sys.version) : 
1.25.1
3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]


print(numpy.show_runtime()) :

WARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information
[{'numpy_version': '1.25.1',
  'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]',
  'uname': uname_result(system='Linux', node='sarah-optiplex-ubuntu', release='5.15.0-78-generic', version='#85-Ubuntu SMP Fri Jul 7 15:25:09 UTC 2023', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}}]
None



### Context for the issue:

Working around this bug requires multiple difficult-to-read lines of code, when it should by working similarly to how the deprecated code worked. ",2023-08-01 16:52:49,,BUG: astype changes the shape of structured arrays,['00 - Bug']
24290,open,rousku,"### Describe the issue:

The problem is that `import numpy` hangs forever if sys.stdin is being read simultaneously in a separate thread AND the script that imports numpy is started with `Popen`, which is configured to forward stdin, stdout, stderr to subprocess.PIPE. 

To reproduce the issue, run `python reproduce.py`. It should print ""Numpy imported"" but hangs forever instead.
""Numpy imported"" is printed if the line that spawns wait_for_stop is commented.

### Reproduce the code example:

python reproduce.py:
```
import subprocess
import sys

proc = subprocess.Popen([sys.executable, 'numpy_script.py'],
                        stdin=subprocess.PIPE,
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE,
                        bufsize=0,
                        creationflags=subprocess.CREATE_NO_WINDOW)

print(proc.stdout.read())
```

numpy_script.py:
```
import sys
import _thread
import threading


def wait_for_stop():
    sys.stdin.read()
    _thread.interrupt_main()


threading.Thread(target=wait_for_stop, daemon=True).start()

import numpy

print('Numpy imported')
```

### Error message:

_No response_

### Runtime information:

```
>>> import sys, numpy; print(numpy.__version__); print(sys.version)
1.25.1
3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]
```

```
>>> print(numpy.show_runtime())
[{'numpy_version': '1.25.1',
  'python': '3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 '
            '64 bit (AMD64)]',
  'uname': uname_result(system='Windows', node='FI-L-7376992', release='10', version='10.0.19044', machine='AMD64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Haswell',
  'filepath': 'C:\\Users\\ADMIN\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\numpy-bug-Txf2mLmS-py3.10\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll',
  'internal_api': 'openblas',
  'num_threads': 12,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.23'}]
None
>>>
```

### Context for the issue:

`import numpy` should never hang.",2023-07-30 10:04:06,,"BUG: ""import numpy""  hangs",['00 - Bug']
24284,open,OrbitalMechanic,"### Describe the issue:

I'm attempting to compile the subroutines for the DIVA differential equations solver from Math 77. The errors occur during the C language compilation phase. One of the error messages follows.

/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:792:12: error: expected ';' after expression
    divaf_t divaf_cb = { Py_None, NULL, 0 };
           ^
           ;

The source code is attached in the file below.

[diva.f.txt](https://github.com/numpy/numpy/files/12205354/diva.f.txt)


### Reproduce the code example:

```python
See the file above for the source code.
```


### Error message:

```shell
The error messages and tracebacks follow.

INFO: compile options: '-DNPY_DISABLE_OPTIMIZATION=1 -I/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9 -I/Users/user/opt/anaconda3/lib/python3.9/site-packages/numpy/core/include -I/Users/user/opt/anaconda3/include/python3.9 -c'
INFO: clang: /var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c
INFO: clang: /var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/fortranobject.c
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:385:9: warning: variable 'capi_j' set but not used [-Wunused-but-set-variable]
    int capi_j,capi_i = 0;
        ^
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:581:9: warning: variable 'capi_j' set but not used [-Wunused-but-set-variable]
    int capi_j,capi_i = 0;
        ^
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:792:12: error: expected ';' after expression
    divaf_t divaf_cb = { Py_None, NULL, 0 };
           ^
           ;
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:792:5: error: use of undeclared identifier 'divaf_t'
    divaf_t divaf_cb = { Py_None, NULL, 0 };
    ^
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:792:13: error: use of undeclared identifier 'divaf_cb'
    divaf_t divaf_cb = { Py_None, NULL, 0 };
            ^
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:792:24: error: expected expression
    divaf_t divaf_cb = { Py_None, NULL, 0 };
                       ^
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:793:5: error: use of undeclared identifier 'divaf_t'
    divaf_t *divaf_cb_ptr = &divaf_cb;
    ^
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:793:14: error: use of undeclared identifier 'divaf_cb_ptr'
    divaf_t *divaf_cb_ptr = &divaf_cb;
             ^
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:793:30: error: use of undeclared identifier 'divaf_cb'
    divaf_t *divaf_cb_ptr = &divaf_cb;
                             ^
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:795:5: error: use of undeclared identifier 'divaf_typedef'
    divaf_typedef divaf_cptr;
    ^
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:796:12: error: expected ';' after expression
    divao_t divao_cb = { Py_None, NULL, 0 };
           ^
           ;
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:796:5: error: use of undeclared identifier 'divao_t'
    divao_t divao_cb = { Py_None, NULL, 0 };
    ^
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:796:13: error: use of undeclared identifier 'divao_cb'
    divao_t divao_cb = { Py_None, NULL, 0 };
            ^
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:796:24: error: expected expression
    divao_t divao_cb = { Py_None, NULL, 0 };
                       ^
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:797:5: error: use of undeclared identifier 'divao_t'
    divao_t *divao_cb_ptr = &divao_cb;
    ^
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:797:14: error: use of undeclared identifier 'divao_cb_ptr'
    divao_t *divao_cb_ptr = &divao_cb;
             ^
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:797:30: error: use of undeclared identifier 'divao_cb'
    divao_t *divao_cb_ptr = &divao_cb;
                             ^
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:799:5: error: use of undeclared identifier 'divao_typedef'
    divao_typedef divao_cptr;
    ^
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:822:72: error: use of undeclared identifier 'divaf_cb'
        capi_kwlist,&tspecs_capi,&y_capi,&f_capi,&kord_capi,&neq_capi,&divaf_cb.capi,&divao_cb.capi,&idimt_capi,&idimy_capi,&idimf_capi,&idimk_capi,&iopt_capi,&PyTuple_Type,&divaf_xa_capi,&PyTuple_Type,&divao_xa_capi))
                                                                       ^
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:822:87: error: use of undeclared identifier 'divao_cb'
        capi_kwlist,&tspecs_capi,&y_capi,&f_capi,&kord_capi,&neq_capi,&divaf_cb.capi,&divao_cb.capi,&idimt_capi,&idimy_capi,&idimf_capi,&idimk_capi,&iopt_capi,&PyTuple_Type,&divaf_xa_capi,&PyTuple_Type,&divao_xa_capi))
                                                                                      ^
/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c:911:22: error: use of undeclared identifier 'divaf_cb'
if(F2PyCapsule_Check(divaf_cb.capi)) {
                     ^
fatal error: too many errors emitted, stopping now [-ferror-limit=]
2 warnings and 20 errors generated.
error: Command ""clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/user/opt/anaconda3/include -arch x86_64 -I/Users/user/opt/anaconda3/include -fPIC -O2 -isystem /Users/user/opt/anaconda3/include -arch x86_64 -ftrapping-math -DNPY_DISABLE_OPTIMIZATION=1 -I/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9 -I/Users/user/opt/anaconda3/lib/python3.9/site-packages/numpy/core/include -I/Users/user/opt/anaconda3/include/python3.9 -c /var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c -o /var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.o -MMD -MF /var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.o.d"" failed with exit status 1
(base) user@Mac-Pro diva.f_for_f2py % 
(base) user@Mac-Pro diva.f_for_f2py % cp /var/folders/2r/4bw6nw0x58z0_ybx632_h14m0000gq/T/tmpsa0z02z5/src.macosx-10.9-x86_64-3.9/divamodule.c doit.c
(base) user@Mac-Pro diva.f_for_f2py %
```


### Runtime information:

The command used to run f2py is:

f2py3  --fcompiler=gnu95 --f77exec=/usr/local/bin/gfortran -c -m diva diva.f   

### Context for the issue:

I need to use the DIVA integrator for a Python script I'm writing to integrate orbits. DIVA aligns with the form of the equations of motion being used as well as the ability to numerically integrate the variational equations simultaneously. ",2023-07-28 23:51:33,,BUG: f2py generates errors during C language phase,"['00 - Bug', 'component: numpy.f2py']"
24280,open,rgommers,This is getting kinda annoying: `[skip cirrus]` is broken and the wheel build and other CI jobs are running way too often. It was probably broken by the addition of other logic like always triggering on PRs with build-related label.,2023-07-28 16:18:31,,Cirrus CI free usage going away - job runtime issues & credits,['component: CI']
24251,open,mattip,"running either 
```
python runtests.py --bench-compare <commit1> <commit2>
```

or 
```
spin bench --compare <commit1> <commit2>
```

will end up calling 
```
cd benchmark; asv continuous --factor 1.05 --bench main <commit1> <commit2>
```
Then asv creates a virtualenv for each commit, and uses a vanilla `pip install` to build NumPy. While this can work on macOS and linux (and use the system OpenBLAS), it will fail to correctly use OpenBLAS on windows, since the build also needs to
- copy the OpenBLAS DLL into the correct place
- build the `_distributor_init.py` file. In CI this is done via additional post-install code:

Is there a way to, if OpenBLAS is found, to do this as part of the build?

Here is what the various CI invocations to do these steps look like: 
https://github.com/numpy/numpy/blob/374e73d0690d5caa6dca45af8b181b0472594498/.github/workflows/windows_meson.yml#L63-L72
https://github.com/numpy/numpy/blob/374e73d0690d5caa6dca45af8b181b0472594498/.github/workflows/windows_clangcl.yml#L66-L75

(on azure this also shows the complicated command line needed)
https://github.com/numpy/numpy/blob/374e73d0690d5caa6dca45af8b181b0472594498/azure-steps-windows.yml#L38-L48

https://github.com/numpy/numpy/blob/374e73d0690d5caa6dca45af8b181b0472594498/tools/wheels/cibw_before_build.sh#L39-L56",2023-07-24 14:07:43,,asv fails to run on windows when pkg-config and OpenBLAS are set up properly,['28 - Benchmark']
24221,open,jurobystricky,"### Describe the issue:

We tested some Python apps  in anticipation of CET support being merged into upstream Linux kernel in the near future.
https://www.phoronix.com/news/Linux-6.4-Shadow-Stack-Coming
While testing CentOS Stream 9 with a custom kernel with shadow stack enabled and Glibc with CET support, we were not able to import ""numpy"". 



### Reproduce the code example:

```python
$ python3
Python 3.9.17 (main, Jun 26 2023, 00:00:00) 
[GCC 11.4.1 20230605 (Red Hat 11.4.1-2)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy
Traceback (most recent call last):
...
```


### Error message:

```shell
Traceback (most recent call last):
  File ""/home/bkc/.local/lib/python3.9/site-packages/numpy/core/__init__.py"", line 23, in <module>
    from . import multiarray
  File ""/home/bkc/.local/lib/python3.9/site-packages/numpy/core/multiarray.py"", line 10, in <module>
    from . import overrides
  File ""/home/bkc/.local/lib/python3.9/site-packages/numpy/core/overrides.py"", line 8, in <module>
    from numpy.core._multiarray_umath import (
ImportError: /home/bkc/.local/lib/python3.9/site-packages/numpy/core/../../numpy.libs/libquadmath-96973f99.so.0.0.0: rebuild shared object with SHSTK support enabled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/bkc/.local/lib/python3.9/site-packages/numpy/__init__.py"", line 139, in <module>
    from . import core
  File ""/home/bkc/.local/lib/python3.9/site-packages/numpy/core/__init__.py"", line 49, in <module>
    raise ImportError(msg)
ImportError: 

IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!

Importing the numpy C-extensions failed. This error can happen for
many reasons, often due to issues with your setup or how NumPy was
installed.

We have compiled some common reasons and troubleshooting tips at:

    https://numpy.org/devdocs/user/troubleshooting-importerror.html

Please note and check the following:

  * The Python version is: Python3.9 from ""/usr/bin/python3""
  * The NumPy version is: ""1.25.1""

and make sure that they are the versions you expect.
Please carefully study the documentation linked above for further help.

Original error was: /home/bkc/.local/lib/python3.9/site-packages/numpy/core/../../numpy.libs/libquadmath-96973f99.so.0.0.0: rebuild shared object with SHSTK support enabled
```


### Runtime information:

>>> import sys
>>> print(sys.version)
3.9.17 (main, Jun 26 2023, 00:00:00) 
[GCC 11.4.1 20230605 (Red Hat 11.4.1-2)]


### Context for the issue:

There is a workaround for this, setting the environmental variable:
**GLIBC_TUNABLES=glibc.cpu.x86_shstk=permissive**
will disable CET functionality and allows loading numpy shared libraries.
However, the prudent solution is to actually rebuild all numpy shared libraries with SHSTK support enabled.
Thanks.",2023-07-20 03:45:37,,BUG: libquadmath-96973f99.so.0.0.0: rebuild shared object with SHSTK support enabled,['00 - Bug']
24209,open,ngoldbaum,"Once all these issues are fixed the CI job running with the sanitizers turned on can have `UBSAN_OPTIONS=halt_on_error=1` in its `spin test` invocation so that new UB doesn't get introduced in the future.

* `NULL` array shape passed to memcpy

```
numpy/array_api/tests/test_creation_functions.py::test_asarray_copy 
../numpy/core/src/multiarray/array_coercion.c:1047:13: runtime error: null pointer passed as argument 2, which is declared to never be null
```

Happening because `memcpy` is receiving a `NULL` second argument, ultimately because the array coercion machinery is getting passed an incompletely initialized array object created internally inside numpy as part of ufunc reduction.

There's another similar issue with subarrays:

```
numpy/core/tests/test_arrayprint.py::TestArray2String::test_structure_format_mixed 
../numpy/core/src/multiarray/ctors.c:674:13: runtime error: null pointer passed as argument 2, which is declared to never be null
```

Another one in `TestBool`:

```
numpy/core/tests/test_multiarray.py::TestBool::test_cast_from_void 
../numpy/core/src/multiarray/scalarapi.c:276:9: runtime error: null pointer passed as argument 2, which is declared to never be null
```

This one seems to be from not doing error handling for the `scalar_value` function.

* `NULL` passed to qsort

```
numpy/core/tests/test_datetime.py::TestDateTime::test_datetime_busday_offset 
../numpy/core/src/multiarray/datetime_busdaycal.c:234:5: runtime error: null pointer passed as argument 1, which is declared to never be null
```

* Misaligned cast in boolean indexing

```
numpy/array_api/tests/test_set_functions.py::test_inverse_indices_shape[unique_all] 
../numpy/core/src/multiarray/common.h:288:31: runtime error: load of misaligned address 0x6080000c75a2 for type 'unsigned int', which requires 4 byte alignment
0x6080000c75a2: note: pointer points here
 00 00  01 01 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00
              ^ 
../numpy/core/src/multiarray/common.h:288:31: runtime error: load of misaligned address 0x6080000c75a1 for type 'unsigned int', which requires 4 byte alignment
0x6080000c75a1: note: pointer points here
 00 00 00  01 01 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00
              ^ 
numpy/core/tests/test_api.py::test_copyto_fromscalar 
../numpy/core/src/multiarray/common.h:288:31: runtime error: load of misaligned address 0x60200000c6f2 for type 'unsigned int', which requires 4 byte alignment
0x60200000c6f2: note: pointer points here
 00 00  00 01 00 00 00 01 00 00  00 00 00 00 00 00 00 00  02 11 00 00 10 00 00 00  14 00 80 5b 00 00
              ^ 
numpy/core/tests/test_mem_overlap.py::TestUFunc::test_unary_ufunc_1d_manual 
../numpy/core/src/multiarray/common.h:288:31: runtime error: load of misaligned address 0x6190020f597f for type 'unsigned int', which requires 4 byte alignment
0x6190020f597f: note: pointer points here
 fc fd fe ff 00  01 02 03 04 05 06 07 08  09 0a 0b 0c 0d 0e 0f 10  11 12 13 14 15 16 17 18  19 1a 1b
             ^ 

```


The first report is coming from a Python line doing something like:

```
np.array([False])[np.array([True])]
```

Somehow this creates a C iterator in numpy's boolean indexing internals that has a stride of 1, ultimately leading to a cast from uninitialized data (I think). I can't reproduce this outside the array API tests so it's something a bit more complicated than just the snippet above.

* Invalid bit shift

```
numpy/lib/tests/test_mixins.py::TestNDArrayOperatorsMixin::test_forward_binary_methods 
../numpy/core/src/npymath/npy_math_internal.h.src:657:18: runtime error: left shift of negative value -1
```

```
numpy/core/tests/test_ufunc.py::TestUfunc::test_ufunc_at_basic[a1] 
../numpy/core/src/umath/_rational_tests.c:54:24: runtime error: left shift of 1 by 31 places cannot be represented in type 'int'
```


* Int overflows in datetime casts

Quite a few of these, just an example:

```
numpy/core/tests/test_casting_unittests.py::TestCasting::test_time_to_time[M8[ms]-M8[ns]-Casting.safe-None-1000000-1] 
../numpy/core/src/umath/loops_autovec.dispatch.c.src:76:9: runtime error: signed integer overflow: -9223372036854775808 * 1000000 cannot be represented in type 'long int'
../numpy/core/src/multiarray/dtype_transfer.c:865:25: runtime error: signed integer overflow: 9223372036854775807 * 1000000 cannot be represented in type 'long int'

numpy/core/tests/test_casting_unittests.py::TestCasting::test_time_to_time[M8[4D]-M8[1M]-Casting.same_kind-None-None-denom8]
../numpy/core/src/multiarray/datetime.c:478:8: runtime error: signed integer overflow: 4 * 9223372036854775807 cannot be represented in type 'long int'
```

* Int overflow in einsum tests

```
numpy/core/tests/test_einsum.py::TestEinsum::test_einsum_broadcast 
../numpy/core/src/multiarray/einsum_sumprod.c.src:620:33: runtime error: signed integer overflow: 9223365439786057728 + 13194139533312 cannot be represented in type 'long int'
../numpy/core/src/multiarray/arraytypes.c.src:3789:13: runtime error: signed integer overflow: 9223365439786057728 + 13194139533312 cannot be represented in type 'long int'
PASSED
```

* Int overflow in int128 tests

```
numpy/core/tests/test_extint128.py::test_safe_binop 
../numpy/core/src/common/npy_extint128.h:21:14: runtime error: signed integer overflow: -9223372036854775808 + -9223372036854775808 cannot be represented in type 'long int'
../numpy/core/src/common/npy_extint128.h:35:14: runtime error: signed integer overflow: -9223372036854775808 - 9223372036854775807 cannot be represented in type 'long int'
../numpy/core/src/common/npy_extint128.h:56:14: runtime error: signed integer overflow: -9223372036854775808 * -9223372036854775808 cannot be represented in type 'long int'

```

* Int overflow in TestWritebackIfCopy

```
numpy/core/tests/test_multiarray.py::TestWritebackIfCopy::test_choose_mod_raise 
../numpy/core/src/multiarray/iterators.c:1302:44: runtime error: signed integer overflow: -3617008641903833651 * 3 cannot be represented in type 'long int'
```

* Int overflow in nditer tests

```
numpy/core/tests/test_nditer.py::test_iter_too_large_with_multiindex 
../numpy/core/src/multiarray/nditer_api.c:497:20: runtime error: signed integer overflow: 1152921504606846976 * 1024 cannot be represented in type 'long int'
```

* Int overflows in NEP 50 tests

```
numpy/core/tests/test_nep50_promotions.py::test_nep50_weak_integers[i] 
../numpy/core/src/umath/scalarmath.c.src:62:14: runtime error: signed integer overflow: 100 + 2147483647 cannot be represented in type 'int'
../numpy/core/src/umath/loops_autovec.dispatch.c.src:76:9: runtime error: signed integer overflow: 100 + 2147483647 cannot be represented in type 'int'
numpy/core/tests/test_nep50_promotions.py::test_nep50_weak_integers[l] 
../numpy/core/src/umath/scalarmath.c.src:62:14: runtime error: signed integer overflow: 100 + 9223372036854775807 cannot be represented in type 'long int'
../numpy/core/src/umath/loops_autovec.dispatch.c.src:76:9: runtime error: signed integer overflow: 100 + 9223372036854775807 cannot be represented in type 'long int'
numpy/core/tests/test_nep50_promotions.py::test_nep50_weak_integers[q]
 ../numpy/core/src/umath/scalarmath.c.src:62:14: runtime error: signed integer overflow: 100 + 9223372036854775807 cannot be represented in type 'long long int'
../numpy/core/src/umath/loops_autovec.dispatch.c.src:76:9: runtime error: signed integer overflow: 100 + 9223372036854775807 cannot be represented in type 'long long int'
```

* Int overflows in overflow tests

```
numpy/core/tests/test_scalarmath.py::test_scalar_integer_operation_overflow[--i] 
../numpy/core/src/umath/scalarmath.c.src:71:14: runtime error: signed integer overflow: -2147483648 - 2147483647 cannot be represented in type 'int'

numpy/core/tests/test_scalarmath.py::test_scalar_integer_operation_overflow[--l] 
../numpy/core/src/umath/scalarmath.c.src:71:14: runtime error: signed integer overflow: -9223372036854775808 - 9223372036854775807 cannot be represented in type 'long int'

numpy/core/tests/test_scalarmath.py::test_scalar_integer_operation_overflow[--q] 
../numpy/core/src/umath/scalarmath.c.src:71:14: runtime error: signed integer overflow: -9223372036854775808 - 9223372036854775807 cannot be represented in type 'long long int'

numpy/core/tests/test_umath.py::TestRationalFunctions::test_gcd_overflow 
../numpy/core/src/npymath/npy_math_internal.h.src:636:33: runtime error: negation of -9223372036854775808 cannot be represented in 

numpy/core/tests/test_ufunc.py::test_ufunc_input_floatingpoint_error[0] 
../numpy/core/src/umath/loops_autovec.dispatch.c.src:76:9: runtime error: signed integer overflow: -9223372036854775808 + -9223372036854775808 cannot be represented in type 'long int'
```

Presumably where we intentionally want overflow to happen we need to check for overflow before actually doing an overflowing operation in C.

* Int overflow in ufunc at inner loops

```
numpy/core/tests/test_ufunc.py::TestUfunc::test_ufunc_at_inner_loops[multiply-i] 
../numpy/core/src/umath/loops.c.src:461:29: runtime error: signed integer overflow: 6058426 * 397 cannot be represented in type 'int'
../numpy/core/src/umath/loops_autovec.dispatch.c.src:76:9: runtime error: signed integer overflow: 6058426 * 397 cannot be represented in type 'int'
```",2023-07-18 19:08:15,,BUG: undefined behavior detected by ubsan in sanitizer CI runs,"['00 - Bug', 'Project', 'sprintable - C']"
24194,open,peterdsharpe,"### Describe the issue:

When `np.geomspace(start, stop)` is called with large values of `stop` (greater than or equal to $2^{64}$), an AttributeError is thrown. 

For example, `np.geomspace(1, 10 ** 20)` gives an error. Expected behavior is to not error and instead to yield identical output to `np.logspace(0, 20)` (which functions correctly).

Interestingly, `np.geomspace(1, 10 ** 19)` gives correct behavior and does not yield an error. `np.geomspace(1, 10 ** 21)` does yield an error, as do higher values of `stop`.

Reproducibility of this bug is verified to persist across multiple computers, multiple NumPy versions, and multiple Python versions (see below).

### Reproduce the code example:

```python
import numpy as np
np.geomspace(1, 10 ** 20)
```


### Error message:

```shell
In [3]: np.geomspace(1, 10 ** 20)
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
AttributeError: 'int' object has no attribute 'log10'

The above exception was the direct cause of the following exception:

TypeError                                 Traceback (most recent call last)
<ipython-input-3-dfd50b4fec30> in <module>
----> 1 np.geomspace(1, 10 ** 20)

~\anaconda3\lib\site-packages\numpy\core\function_base.py in geomspace(start, stop, num, endpoint, dtype, axis)
    443         _nx.negative(out_sign, out=out_sign, where=both_negative)
    444
--> 445     log_start = _nx.log10(start)
    446     log_stop = _nx.log10(stop)
    447     result = logspace(log_start, log_stop, num=num,

TypeError: loop of ufunc does not support argument 0 of type int which has no callable log10 method
```


### Runtime information:

Bug reproduces on Computer 1:

NumPy 1.25.1
Python 3.9.16 (main, Jan 11 2023, 16:16:36) [MSC v.1916 64 bit (AMD64)]

-----

Bug also confirmed to reproduce on Computer 2:

NumPy 1.24.3
Python 3.10.11 | packaged by Anaconda, Inc. | (main, Apr 20 2023, 18:56:50) [MSC v.1916 64 bit (AMD64)]

### Context for the issue:

Usage of `np.geomspace(1, 10 ** 20)` seems like a core, significant error in the basic functionality of one of NumPy's spacing functions.

While one can of course work around this by running `np.logspace(1, 20)` (which does not produce an error), the `geomspace` function is widely used enough that I would think this bug would affect a ton of NumPy users.",2023-07-16 16:09:52,,BUG: `np.geomspace()` raises AttributeError if `stop >= 2 ** 64`,['00 - Bug']
24157,open,ngoldbaum,"It looks like many of numpy's test dependencies haven't been updated since dependabot was disabled in 2021, see #20268.

In the meantime, github [fixed the issue](https://github.blog/changelog/2022-11-07-dependabot-pull-requests-off-by-default-for-forks/) causing spam PRs on forks of NumPy at the end of last year.

Would it make sense to turn dependabot on again now that forks won't be spammed with PRs anymore? I think existing forks may need to toggle a setting, new forks should have that setting turned off by default.

If there's no appetite for turning on dependabot again, would a PR that updates the test dependencies be OK?",2023-07-10 15:34:58,,CI: Re-enable dependabot or manually update test dependencies?,['component: CI']
24155,open,benmaier,"### Describe the issue:

Generally, a piecewise function can take values of any type as an input and return numbers of any type as an output. However, in its current implementation `numpy.piecewise` forces the output type to be equal to the input type, wich can lead to erroneous output.

This is because the result of the evaluation is being prepared with `output = numpy.zeros_like(input)`, which copies the `dtype` of the input (cf: https://github.com/numpy/numpy/blob/v1.25.0/numpy/lib/function_base.py#L751 )

Maybe this is actually desired behavior and not a bug, which is a viewpoint I would kindly disagree with :)

### Reproduce the code example:

```python
In [4]: if True:
   ...:     import numpy as np
   ...:
   ...:     f = lambda x: 1/x
   ...:
   ...:     xint = 10
   ...:     xflt = 10.
   ...:     result_int_input = np.piecewise(xint, (xint<0, xint>=0), (0, f))
   ...:     result_flt_input = np.piecewise(xflt, (xflt<0, xflt>=0), (0, f))
   ...:

In [5]: result_int_input
Out[5]: array(0)

In [6]: result_flt_input
Out[6]: array(0.1)
```


### Error message:

_No response_

### Runtime information:

    1.23.2
    3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]

### Context for the issue:

It took me 20min and looking into numpy's source to find out why my code is failing.",2023-07-10 13:50:22,,"BUG: numpy.piecewise returns data of the same type as the input type, which is undesirable",['00 - Bug']
24145,open,fellideal,"### Proposed new feature or change:

`Einsum` has been fantastic for computing tensor contractions. However, there are two additional supports I believe would greatly enhance its functionality and efficiency.

(1) Symmetries:
Let's consider a tensor contraction such as `A[a,b,c,d,e] * B[d,e,f,g,h] = C[a,b,c,f,g,h]`. If we have symmetries such as `A[a,b,c,d,e] = A[b,a,c,d,e]`, or `A[a,b,c,d,e]=A[a,b,c,e,d]`, being able to utilize these symmetries could significantly enhance computational efficiency.

A package called `ctf` provides some support for this, but it seems to be only for symmetric/antisymmetric for adjacent indices. The link below leads to its GitHub repository:

https://github.com/cyclops-community/ctf

Further details can be found in the following documentation:

https://solomon2.web.engr.illinois.edu/ctf_python/ctf.html#module-ctf.core

However, from what I've seen, the efficiency isn't as good as one might hope (as noted in the following GitHub issue):

https://github.com/cyclops-community/ctf/issues/136

(2) Parallelization:
Although it seems that `tensordot` supports parallelization
https://stackoverflow.com/questions/23650449/numpy-np-einsum-array-multiplication-using-multiple-cores

, when exporting MKL/OpenBLAS/OpenMP together with `einsum` for general contractions, the actual speed up is not quite realized, despite an increase in CPU load.

For binary tensor contraction, there are some packages, such as:

https://github.com/jackkamm/einsum2

However, for more general applications (more than two), it seems we're still lacking comprehensive solutions.

If these two features could be incorporated - symmetries and parallelization - into `einsum`, I believe it would greatly enhance its functionality and computational efficiency.",2023-07-08 16:02:32,,ENH: Symmetries and parallelization support of einsum,['unlabeled']
24115,open,Zac-HD,"Under Python 3.11, though for some reason I haven't investigated _not_ 3.10,

```python
import numpy.testing
```
triggers the following `EncodingWarning`
```
$ python -X warn_default_encoding demo.py
numpy/testing/_private/utils.py:1251: EncodingWarning: 'encoding' argument not specified.
  output = subprocess.run(cmd, capture_output=True, text=True)
```

It would be great if Numpy added this to CI and fixed however many errors that turns up; with Pytest 7.4.0 and later running clean it's now reasonably practical to do so.  

You may want a two-stage approach where you fix all the current cases in Numpy first (like https://github.com/HypothesisWorks/hypothesis/pull/3619), and then revisit enforcing it in CI once upstream projects have cleaned up their own warnings (like https://github.com/HypothesisWorks/hypothesis/pull/3691, where I hit this issue).",2023-07-04 22:11:14,,ENH: use `PYTHONWARNDEFAULTENCODING=1` to fix optional encoding warnings in Python 3.10+,['sprintable']
24108,open,alonme,"### Proposed new feature or change:

Currently the error emitted by calling `np.sign` on a `None` object is unclear

```
In [2]: np.sign(None)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In [2], line 1
----> 1 np.sign(None)

TypeError: '<' not supported between instances of 'NoneType' and 'int'
```

I think that a better error should be raised.

Would like to try and fix - if this is accepted",2023-07-04 15:14:40,,ENH: Improve error emitted by np.sign(None),['03 - Maintenance']
24084,open,jan-wassenberg,"### Proposed new feature or change:

Continuing from and separately to https://github.com/numpy/numpy/pull/24018, let's discuss SIMD intrinsics here.

I understand there is an ongoing effort to replace(?) the NEP38 macros with a C++ wrapper (https://github.com/numpy/numpy/pull/21057).

> part that is because Sayed has momentum for the ""custom"" universal intrinsics

Unfortunately it seems there is some duplication of effort :( [Highway](https://github.com/google/highway) has been under development since 2017 with contributions from about a dozen engineers, and open sourced in 2019 with several dozen open-source collaborators since then (mostly bugfixes).

Wouldn't we get further by collaborating, perhaps by extending Highway with Numpy-specific operations? This would make it easier to share code and onboard developers (Highway has 2.6k Github stars) as opposed to a custom wrapper only used by Numpy.

Wouldn't it also make sense to benefit from all the ongoing maintenance efforts? This is quite costly (multiple patches per week) given all the platforms/compilers to support.

> @jan-wassenberg , I think this would just be something like MaskedGatherLoad and MaskedBlendedStore

Thanks, makes sense. FYI it is possible to emulate these by IfThenElse on the indices, replacing invalid ones with a safe/dummy address. (We found this to have no observable perf impact on x86.)
We can soon add native versions of these, though, because it would be more obvious/convenient for user code.
",2023-06-30 08:12:07,,ENH: C++ SIMD wrapper,['component: SIMD']
24050,open,h-vetinari,"### Describe the issue:

I suspect this is part of the larger issue of consistency (resp. lack thereof) around scalars vs. 0-dim arrays.

But it's worse in this case, because now any user of the various `assert_*` functions from `numpy.testing` will be themselves unable to ensure that the behaviour is as intended.

### Reproduce the code example:

```python
>>> import numpy as np
>>> from numpy.testing import assert_equal
>>> assert_equal(1.0, np.array(1, dtype=np.int64))  # passes?!?
>>> assert_equal(1.0, np.array(2, dtype=np.int64))  # changed value -> fails
Traceback (most recent call last):
[...]
```


### Error message:

_No response_

### Runtime information:

```
>>> import sys, numpy; print(numpy.__version__); print(sys.version)
1.25.0
3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:34:57) [MSC v.1936 64 bit (AMD64)]
```

```
>>> print(numpy.show_runtime())
WARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information
[{'numpy_version': '1.25.0',
  'python': '3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:34:57) '
            '[MSC v.1936 64 bit (AMD64)]',
  'uname': uname_result(system='Windows', release='10', version='10.0.22621', machine='AMD64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}}]
None
```

### Context for the issue:

`numpy.testing` is a key component for ensuring people can build their own libraries on top of numpy. ""equal"" should actually mean equal type, shape, content, etc. (as it usually does...), also for 0-dim arrays.",2023-06-26 15:22:49,,BUG: assert_equal does not distinguish scalars from 0-dim arrays,['01 - Enhancement']
24035,open,Kai-Striega,"### Proposed new feature or change:

[np-23762](https://github.com/numpy/numpy/pull/23762) added an `.mT` attribute for both arrays and masked arrays. There should be a matching function for this. Because:

* of the analogue to `.T` 
* the [Array API](https://data-apis.org/array-api/latest/API_specification/generated/array_api.matrix_transpose.html) has it.

I propose the name `matrix_transpose`, this seems like the logical choice and matches the Array API, but am happy to change if anyone feels strongly that it should be something different. There is already a C level function `PyArray_MatrixTranspose` defined in `numpy/core/src/multiarray/shape.c`. This issue is to implement a Python level version of that function.

I'm happy to work on this. I will need some help finding the correct way to correct way to implement the addition.
",2023-06-24 08:50:55,,ENH: Add `np.matrix_transpose` ,['unlabeled']
24032,open,Semnodime,"### Describe the issue:

`numpy.histogram` returns negative values for some bins when the data is provided as list of numerical strings.


### Reproduce the code example:

```python
import numpy as np
data =  [100]*10 + [200]*14
bins = list(range(0, 2050, 50))

# the string version
print(list(np.histogram([str(x) for x in data], bins=bins)[0]))
# [24, -24, 10, 0, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14]

# the int version
print(list(np.histogram([int(x) for x in data], bins=bins)[0]))
# [0, 0, 10, 0, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
```


### Error message:

_No response_

### Runtime information:

1.21.5
3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0]


### Context for the issue:

I post this issue to throw light on the fact that some issue might lie deeper within numpy and cause other bugs even if the `numpy.histogram` function specificly has never been designed to work with a list of strings.
The reason this is significant IMHO is that the result is quite similar yet faulty instead of raising an error or being total garbage: it poses the risk of the resulting data being trusted and not being recognized as errorneous.",2023-06-23 20:20:22,,BUG: numpy.histogram should raise an error for string arrays,"['00 - Bug', 'sprintable']"
24019,open,adrinjalali,"### Steps to reproduce:

In `numpy>=1.25`, this is the behavior:

```py
>>> import numpy as np
>>> type(np.mean)
<class 'numpy._ArrayFunctionDispatcher'>
>>> import inspect
>>> inspect.isfunction(np.mean)
False
>>> inspect.ismethod(np.mean)
False
>>> from numpy import _ArrayFunctionDispatcher
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name '_ArrayFunctionDispatcher' from 'numpy' (/home/adrin/miniforge3/envs/skops/lib/python3.11/site-packages/numpy/__init__.py)
```

I would think `inspect.isfunction(np.mean)` should return `True`, maybe?

Also, it's odd that the `type(np.mean)` returns something which cannot be imported, and the actual import is `from numpy.core._multiarray_umath import _ArrayFunctionDispatcher`.

Ended up observing this since our dispatch mechanism in `skops` which was dispatching `ufunc` was ignored since these are no more `ufunc`s (I think they were before?)

",2023-06-22 13:39:31,,Attributes on the objects dispatched via _ArrayFunctionDispatcher,['component: __array_function__']
23993,open,Hank0626,"### Describe the issue:

when I try to use inspect.getfullargspec to get np.any, raises `unsupported callable`

### Reproduce the code example:

```python
import numpy as np
import inspect
import sys
print(np.__version__); print(sys.version)
inspect.getfullargspec(getattr(np, 'any'))
```


### Error message:

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
File ~/anaconda3/lib/python3.10/inspect.py:1285, in getfullargspec(func)
   1268 try:
   1269     # Re: `skip_bound_arg=False`
   1270     #
   (...)
   1282     # getfullargspec() historically ignored __wrapped__ attributes,
   1283     # so we ensure that remains the case in 3.3+
-> 1285     sig = _signature_from_callable(func,
   1286                                    follow_wrapper_chains=False,
   1287                                    skip_bound_arg=False,
   1288                                    sigcls=Signature,
   1289                                    eval_str=False)
   1290 except Exception as ex:
   1291     # Most of the times 'signature' will raise ValueError.
   1292     # But, it can also raise AttributeError, and, maybe something
   1293     # else. So to be fully backwards compatible, we catch all
   1294     # possible exceptions here, and reraise a TypeError.

File ~/anaconda3/lib/python3.10/inspect.py:2467, in _signature_from_callable(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)
   2466 if _signature_is_builtin(obj):
-> 2467     return _signature_from_builtin(sigcls, obj,
   2468                                    skip_bound_arg=skip_bound_arg)
   2470 if isinstance(obj, functools.partial):

File ~/anaconda3/lib/python3.10/inspect.py:2274, in _signature_from_builtin(cls, func, skip_bound_arg)
   2273 if not s:
-> 2274     raise ValueError(""no signature found for builtin {!r}"".format(func))
   2276 return _signature_fromstr(cls, func, s, skip_bound_arg)

ValueError: no signature found for builtin <function any at 0x102cd8a30>

The above exception was the direct cause of the following exception:

TypeError                                 Traceback (most recent call last)
Cell In[7], line 1
----> 1 inspect.getfullargspec(getattr(np, 'any'))

File ~/anaconda3/lib/python3.10/inspect.py:1295, in getfullargspec(func)
   1285     sig = _signature_from_callable(func,
   1286                                    follow_wrapper_chains=False,
   1287                                    skip_bound_arg=False,
   1288                                    sigcls=Signature,
   1289                                    eval_str=False)
   1290 except Exception as ex:
   1291     # Most of the times 'signature' will raise ValueError.
   1292     # But, it can also raise AttributeError, and, maybe something
   1293     # else. So to be fully backwards compatible, we catch all
   1294     # possible exceptions here, and reraise a TypeError.
-> 1295     raise TypeError('unsupported callable') from ex
   1297 args = []
   1298 varargs = None

TypeError: unsupported callable
```


### Runtime information:

1.25.0
3.10.9 (main, Mar  1 2023, 12:20:14) [Clang 14.0.6 ]

### Context for the issue:

_No response_",2023-06-19 13:30:57,,BUG: Unsupported callable in ``inspect.getfullargspec`` of ``np.any``,['01 - Enhancement']
23988,open,rgommers,"The failing test is:
```
FAILED lib/tests/test_format.py::test_python2_python3_interoperability
```
with:
```
  E         File ""<unknown>"", line 1
  E           {'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           
  E                                                              ^
  E       SyntaxError: invalid decimal literal
```

Full traceback:

<details>

```
____________________ test_python2_python3_interoperability _____________________
  
  fp = <_io.BufferedReader name='/tmp/tmp.KtBhpBVwgx/venv/lib/python3.12/site-packages/numpy/lib/tests/data/win64python2.npy'>
  version = (1, 0), max_header_size = 10000
  
      def _read_array_header(fp, version, max_header_size=_MAX_HEADER_SIZE):
          """"""
          see read_array_header_1_0
          """"""
          # Read an unsigned, little-endian short int which has the length of the
          # header.
          import ast
          import struct
          hinfo = _header_size_info.get(version)
          if hinfo is None:
              raise ValueError(""Invalid version {!r}"".format(version))
          hlength_type, encoding = hinfo
      
          hlength_str = _read_bytes(fp, struct.calcsize(hlength_type), ""array header length"")
          header_length = struct.unpack(hlength_type, hlength_str)[0]
          header = _read_bytes(fp, header_length, ""array header"")
          header = header.decode(encoding)
          if len(header) > max_header_size:
              raise ValueError(
                  f""Header info length ({len(header)}) is large and may not be safe ""
                  ""to load securely.\n""
                  ""To allow loading, adjust `max_header_size` or fully trust ""
                  ""the `.npy` file using `allow_pickle=True`.\n""
                  ""For safety against large resource use or crashes, sandboxing ""
                  ""may be necessary."")
      
          # The header is a pretty-printed string representation of a literal
          # Python dictionary with trailing newlines padded to a ARRAY_ALIGN byte
          # boundary. The keys are strings.
          #   ""shape"" : tuple of int
          #   ""fortran_order"" : bool
          #   ""descr"" : dtype.descr
          # Versions (2, 0) and (1, 0) could have been created by a Python 2
          # implementation before header filtering was implemented.
          #
          # For performance reasons, we try without _filter_header first though
          try:
  >           d = ast.literal_eval(header)
  
  ast        = <module 'ast' from '/opt/python/cp312-cp312/lib/python3.12/ast.py'>
  encoding   = 'latin1'
  fp         = <_io.BufferedReader name='/tmp/tmp.KtBhpBVwgx/venv/lib/python3.12/site-packages/numpy/lib/tests/data/win64python2.npy'>
  header     = ""{'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \n""
  header_length = 70
  hinfo      = ('<H', 'latin1')
  hlength_str = b'F\x00'
  hlength_type = '<H'
  max_header_size = 10000
  struct     = <module 'struct' from '/opt/python/cp312-cp312/lib/python3.12/struct.py'>
  version    = (1, 0)
  
  ../venv/lib/python3.12/site-packages/numpy/lib/format.py:625: 
  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
  
  node_or_string = ""{'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \n""
  
      def literal_eval(node_or_string):
          """"""
          Evaluate an expression node or a string containing only a Python
          expression.  The string or node provided may only consist of the following
          Python literal structures: strings, bytes, numbers, tuples, lists, dicts,
          sets, booleans, and None.
      
          Caution: A complex expression can overflow the C stack and cause a crash.
          """"""
          if isinstance(node_or_string, str):
  >           node_or_string = parse(node_or_string.lstrip("" \t""), mode='eval')
  
  node_or_string = ""{'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \n""
  
  /opt/python/cp312-cp312/lib/python3.12/ast.py:66: 
  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
  
  source = ""{'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \n""
  filename = '<unknown>', mode = 'eval'
  
      def parse(source, filename='<unknown>', mode='exec', *,
                type_comments=False, feature_version=None):
          """"""
          Parse the source into an AST node.
          Equivalent to compile(source, filename, mode, PyCF_ONLY_AST).
          Pass type_comments=True to get back type comments where the syntax allows.
          """"""
          flags = PyCF_ONLY_AST
          if type_comments:
              flags |= PyCF_TYPE_COMMENTS
          if feature_version is None:
              feature_version = -1
          elif isinstance(feature_version, tuple):
              major, minor = feature_version  # Should be a 2-tuple.
              if major != 3:
                  raise ValueError(f""Unsupported major version: {major}"")
              feature_version = minor
          # Else it should be an int giving the minor version for 3.x.
  >       return compile(source, filename, mode, flags,
                         _feature_version=feature_version)
  E         File ""<unknown>"", line 1
  E           {'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           
  E                                                              ^
  E       SyntaxError: invalid decimal literal
  
  feature_version = -1
  filename   = '<unknown>'
  flags      = 1024
  mode       = 'eval'
  source     = ""{'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \n""
  type_comments = False
  
  /opt/python/cp312-cp312/lib/python3.12/ast.py:52: SyntaxError
  
  During handling of the above exception, another exception occurred:
  
      @pytest.mark.xfail(IS_WASM, reason=""Emscripten NODEFS has a buggy dup"")
      def test_python2_python3_interoperability():
          fname = 'win64python2.npy'
          path = os.path.join(os.path.dirname(__file__), 'data', fname)
          with pytest.warns(UserWarning, match=""Reading.*this warning\\.""):
  >           data = np.load(path)
  
  fname      = 'win64python2.npy'
  path       = '/tmp/tmp.KtBhpBVwgx/venv/lib/python3.12/site-packages/numpy/lib/tests/data/win64python2.npy'
  
  ../venv/lib/python3.12/site-packages/numpy/lib/tests/test_format.py:535: 
  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
  ../venv/lib/python3.12/site-packages/numpy/lib/npyio.py:453: in load
      return format.read_array(fid, allow_pickle=allow_pickle,
          N          = 6
          _ZIP_PREFIX = b'PK\x03\x04'
          _ZIP_SUFFIX = b'PK\x05\x06'
          allow_pickle = False
          encoding   = 'ASCII'
          fid        = <_io.BufferedReader name='/tmp/tmp.KtBhpBVwgx/venv/lib/python3.12/site-packages/numpy/lib/tests/data/win64python2.npy'>
          file       = '/tmp/tmp.KtBhpBVwgx/venv/lib/python3.12/site-packages/numpy/lib/tests/data/win64python2.npy'
          fix_imports = True
          magic      = b'\x93NUMPY'
          max_header_size = 10000
          mmap_mode  = None
          own_fid    = True
          pickle_kwargs = {'encoding': 'ASCII', 'fix_imports': True}
          stack      = <contextlib.ExitStack object at 0x7fdf547276e0>
  ../venv/lib/python3.12/site-packages/numpy/lib/format.py:786: in read_array
      shape, fortran_order, dtype = _read_array_header(
          allow_pickle = False
          fp         = <_io.BufferedReader name='/tmp/tmp.KtBhpBVwgx/venv/lib/python3.12/site-packages/numpy/lib/tests/data/win64python2.npy'>
          max_header_size = 10000
          pickle_kwargs = {'encoding': 'ASCII', 'fix_imports': True}
          version    = (1, 0)
  ../venv/lib/python3.12/site-packages/numpy/lib/format.py:628: in _read_array_header
      header = _filter_header(header)
          ast        = <module 'ast' from '/opt/python/cp312-cp312/lib/python3.12/ast.py'>
          encoding   = 'latin1'
          fp         = <_io.BufferedReader name='/tmp/tmp.KtBhpBVwgx/venv/lib/python3.12/site-packages/numpy/lib/tests/data/win64python2.npy'>
          header     = ""{'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \n""
          header_length = 70
          hinfo      = ('<H', 'latin1')
          hlength_str = b'F\x00'
          hlength_type = '<H'
          max_header_size = 10000
          struct     = <module 'struct' from '/opt/python/cp312-cp312/lib/python3.12/struct.py'>
          version    = (1, 0)
  ../venv/lib/python3.12/site-packages/numpy/lib/format.py:575: in _filter_header
      for token in tokenize.generate_tokens(StringIO(s).readline):
          StringIO   = <class '_io.StringIO'>
          last_token_was_number = False
          s          = ""{'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \n""
          token      = TokenInfo(type=55 (OP), string='(', start=(1, 50), end=(1, 51), line=""{'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \n"")
          token_string = '('
          token_type = 55
          tokenize   = <module 'tokenize' from '/opt/python/cp312-cp312/lib/python3.12/tokenize.py'>
          tokens     = [TokenInfo(type=55 (OP), string='{', start=(1, 0), end=(1, 1), line=""{'descr': '<f8', 'fortran_order': False, 'shape':...er'"", start=(1, 17), end=(1, 32), line=""{'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \n""), ...]
  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
  
  source = <built-in method readline of _io.StringIO object at 0x7fdf44818c40>
  encoding = None, extra_tokens = True
  
      def _generate_tokens_from_c_tokenizer(source, encoding=None, extra_tokens=False):
          """"""Tokenize a source reading Python code as unicode strings using the internal C tokenizer""""""
          if encoding is None:
              it = _tokenize.TokenizerIter(source, extra_tokens=extra_tokens)
          else:
              it = _tokenize.TokenizerIter(source, encoding=encoding, extra_tokens=extra_tokens)
  >       for info in it:
  E         File ""<string>"", line 1
  E           {'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           
  E                                                              ^
  E       SyntaxError: invalid decimal literal
  
  encoding   = None
  extra_tokens = True
  info       = (55, '(', (1, 50), (1, 51), ""{'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \n"")
  it         = <_tokenize.TokenizerIter object at 0x7fdf45f009d0>
  source     = <built-in method readline of _io.StringIO object at 0x7fdf44818c40>
  
  /opt/python/cp312-cp312/lib/python3.12/tokenize.py:526: SyntaxError
```

</details>

```python
>>> import numpy as np
>>> np.load('/usr/lib/python3.11/site-packages/numpy/lib/tests/data/win64python2.npy')
array([1., 1.])
```

Should be due to the `tokenize` rewrite in Python 3.12, which has a known behavior change where invalid syntax is no longer parsed. Cc @lysnikolaou for visibility. It looks like this was a binary `.npy` file saved in Python 2.x with a `2L` long int literal, and in Python 3 that isn't valid syntax. I think the fix is to delete the file and the test. @rkern or anyone else, any objections to that?",2023-06-19 09:32:41,,`test_format.py::test_python2_python3_interoperability` fails on Python 3.12,"['00 - Bug', 'component: numpy.lib']"
23975,open,mattip,"Currently `test_new_policy` in `core/tests/test_mem_policy.py` sets a memory policy and then runs the `core` and `ma` test suite. This can be confusing, since any test failure in those suites will appear twice: once in the regular tests, and once in the recursive `test_new_policy` tests. It would be better to have a separate CI run for that test.",2023-06-18 13:28:59,,TST: refactor test_new_policy test,"['17 - Task', '05 - Testing']"
23973,open,fyellin,"### Describe the issue:

Numpy does not allow you to use the buffer-protocol to get a read-write buffer for numpy record.  It requires you to ask for the record read-only.  This is in spite of the fact that the pointer returned using the buffer protocol does, in fact, point to a read-write piece of memory and one ought to be able to modify it.

Using the buffer protocol is the recommended way to read and write ""array-like objects"" in C.  To use the buffer protocol for a numpy record, you have to lie and request a read-only buffer, and then write to it anyway.  The older __array_interface__ works just fine.

### Reproduce the code example:

```python
>>> import numpy as np
>>> x = np.zeros(2, dtype='d,d,i')

# memoryview mimics the behavior of the buffer protocol
>>> memoryview(x).readonly
False
>>> memoryview(x[0]).readonly
True

On the other hand

>>> x.__array_interface__['data']   # returns tuple (address, read-only)
(105553143159680, False)
>>> x[0].array_interface__['data']
(105553143159680, False)

I also wrote some C code which I linked in and called from Python

void foo(PyObject *object) {
    Py_buffer view;
    PyObject_GetBuffer(object, &view, PyBUF_CONTIG_RO);
    if (view.obj) {
        double *data = (double *)view.buf;
        *data += 1;
    }
    PyBuffer_Release(&view);
}

both foo(x) and foo(x[0]) correctly implemented an element of the array. The buffer really does point do the actual data and not a copy of it.

Using PyBUF_CONTIG gives on error on foo(x[0]).
```


### Error message:

_No response_

### Runtime information:

numpy.__version__
'1.25.0'
>>> sys.version
'3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]'
>>> numpy.show_runtime()
[{'numpy_version': '1.25.0',
  'python': '3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 '
            '(clang-1300.0.29.30)]',
  'uname': uname_result(system='Darwin', node='Franks-14-Mac-Book-Pro-2.local', release='22.4.0', version='Darwin Kernel Version 22.4.0: Mon Mar  6 20:59:28 PST 2023; root:xnu-8796.101.5~3/RELEASE_ARM64_T6000', machine='arm64')},
 {'simd_extensions': {'baseline': ['NEON', 'NEON_FP16', 'NEON_VFPV4', 'ASIMD'],
                      'found': ['ASIMDHP', 'ASIMDDP'],
                      'not_found': ['ASIMDFHM']}},
 {'architecture': 'armv8',
  'filepath': '/Users/fy/Library/Python/3.10/lib/python/site-packages/numpy/.dylibs/libopenblas64_.0.dylib',
  'internal_api': 'openblas',
  'num_threads': 10,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.23'}]
>>> 


### Context for the issue:

I should be able to use the buffer protocol to read and write a numpy record without lying about my intentions to write to it.",2023-06-18 07:24:53,,BUG: Buffer protocol doesn't support read-write access for numpy records,['00 - Bug']
23934,open,jbrockmendel,"### Proposed new feature or change:

I discussed this with @seberg a few weeks ago.  In pandas we have a hotspot in the cython code where we slice an ndarray.  AFAICT the slicing itself is fast, but having to make a python-space call is slow.  bc the relevant call happens inside a loop, this adds up.

Specific request: a numpy C-API function for ndarray[i:j]",2023-06-13 15:03:32,,ENH: C-API function for slicing ndarray[i:j],"['Project', '63 - C API']"
23929,open,Nitin-Gahlawat,"### Issue with current documentation:

**Related Issue**: #23786

The Gitpod support has been removed from the newer version of NumPy, as concluded in the above issue. Consequently, the Gitpod section will be removed from the upcoming version of the documentation.

As requested by @Mukulikaa and @rossbar in the pull request generated for the related issue https://github.com/numpy/doc/pull/15, a deprecated note has been added to the [Gitpod development guide](https://numpy.org/doc/stable/dev/development_gitpod.html) to inform users about the removal of Gitpod support.

**Changes/solution for the above problem**
I have generated a pull request to implement these changes, which can be found at: https://github.com/numpy/doc/pull/16. I am eagerly awaiting the community's review and approval of the pull request for the merge.

### Idea or request for content:

_No response_",2023-06-13 05:25:40,,DOC: Adding a depreciated note on Gitpod development guide.,['04 - Documentation']
23928,open,ngvananh2508,"### Describe the issue:

When I ran these code lines, it appeared this issue: 



This is the adata_pb.X array:

![image](https://github.com/numpy/numpy/assets/130348585/5fc36940-7eed-4668-aa66-ff2c37559b8e)


Thank you so much for helping me!




### Reproduce the code example:

```python
import numpy as np
adata_pb.layers[""counts""] = adata_pb.X.copy()
adata_pb.obs[""lib_size""] = np.sum(adata_pb.layers[""counts""], axis = 1) 
adata_pb.obs[""log_lib_size""] = np.log(adata_pb.obs[""lib_size""])
```


### Error message:

```shell
AttributeError                            Traceback (most recent call last)
AttributeError: 'numpy.float32' object has no attribute 'log'

The above exception was the direct cause of the following exception:

TypeError                                 Traceback (most recent call last)
Cell In[53], line 1
----> 1 adata_pb.obs[""log_lib_size""] = np.log(adata_pb.obs[""lib_size""])

File ~/anaconda3/lib/python3.10/site-packages/pandas/core/generic.py:2113, in NDFrame.__array_ufunc__(self, ufunc, method, *inputs, **kwargs)
   2109 @final
   2110 def __array_ufunc__(
   2111     self, ufunc: np.ufunc, method: str, *inputs: Any, **kwargs: Any
   2112 ):
-> 2113     return arraylike.array_ufunc(self, ufunc, method, *inputs, **kwargs)

File ~/anaconda3/lib/python3.10/site-packages/pandas/core/arraylike.py:402, in array_ufunc(self, ufunc, method, *inputs, **kwargs)
    399 elif self.ndim == 1:
    400     # ufunc(series, ...)
    401     inputs = tuple(extract_array(x, extract_numpy=True) for x in inputs)
--> 402     result = getattr(ufunc, method)(*inputs, **kwargs)
    403 else:
    404     # ufunc(dataframe)
    405     if method == ""__call__"" and not kwargs:
    406         # for np.<ufunc>(..) calls
    407         # kwargs cannot necessarily be handled block-by-block, so only
    408         # take this path if there are no kwargs

TypeError: loop of ufunc does not support argument 0 of type numpy.float32 which has no callable log method
```


### Runtime information:

I am using python 3.10.9 and numpy 1.23.5

### Context for the issue:

_No response_",2023-06-13 02:29:12,,AttributeError: 'numpy.float32' object has no attribute 'log',['00 - Bug']
23899,open,Zac-HD,"TL;DR: either `np.array([""str"", b""bytes""])` should be an error for mixing string types, or `np.array([""str"", b""non-ascii: \xff""])` should not crash.  This is probably low priority though.

```pycon
$ python  # version 3.10
>>> import numpy as np
>>> np.__version__
'1.23.5'

# Looks like we coerce input to string types, whether unicode or bytes
>>> np.array([1.337, ""str""])
array(['1.337', 'str'], dtype='<U32')
>>> np.array([1.337, b""bytes""])
array([b'1.337', b'bytes'], dtype='|S32')

# and if we _mix_ unicode and bytes (uh-oh!), we silently get unicode
>>> np.array([""str"", b""bytes""])
array(['str', 'bytes'], dtype='<U5')

# but this groundlessly assumes that the bytestring is valid ASCII, and boom:
>>> np.array([""str"", b""non-ascii: \xff""])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
UnicodeDecodeError: 'ascii' codec can't decode byte 0xff in position 11: ordinal not in range(128)
```

This is pretty surprising, because in general I expect bytestrings can contain _arbitrary_ bytes, e.g. packed structs, and Numpy is generally pretty good about that.  Plausibly related issues:

- https://github.com/numpy/numpy/issues/12075
- https://github.com/numpy/numpy/issues/6315
- https://github.com/numpy/numpy/issues/4162#issuecomment-32602496
- https://github.com/HypothesisWorks/hypothesis/pull/3670#discussion_r1222564734









",2023-06-08 07:41:36,,"BUG: `np.array()` assumes that bytestrings are ASCII, if mixed with strings",['unlabeled']
23895,open,Archern4r,"### Issue with current documentation:

Hello everyone, 

in the documentation https://numpy.org/doc/stable/reference/generated/numpy.char.rjust.html it is stated that numpy.char.rjust calls str.rjust element-wise.

Though, str.rjust returns the original string in case the length of the string is greater than the specified _width_ (https://docs.python.org/3/library/stdtypes.html#str.rjust) which is not the case for numpy.char.rjust, which cuts strings off if their length is greater than the given _width_. 

```python
import numpy as np
    
# numpy.char.rjust
arr = np.array(['Mississippi', 'Rome', 'Washington'])
result = np.char.rjust(arr, 7)
print(result)
# Output: ['Mississ' '   Rome' 'Washing']
```

### Idea or request for content:

This is hinted at by the explanation of the _width_-argument ""The_ length of the resulting strings"" but is different than the built-in function's behaviour and should be explained as such, in case this is even intentional.

To me, it seems more logical for the resulting strings not to be cut but instead only to be padded and to preserve the original's function's behaviour, but if that is not wanted for numpy, the documentation should reflect that. In that case, a short example or warning would be in order. ",2023-06-07 16:35:55,,"DOC: numpy.char.rjust cuts strings to size ""width"" instead of just padding like str.rjust does",['04 - Documentation']
23860,open,qipengh,"### Describe the issue:

According to the IEEE 754 standard, when there is inf or -inf in the input of a division operation, the following processing method will be used:

1. If the divisor is 0 and the dividend is infinite, the result is NaN (not a number).

2. If both the divisor and the dividend are positive or negative infinity, the result is NaN.

3. If the divisor is positive or negative infinity and the dividend is a finite number, the result is positive or negative infinity, depending on the sign bit.

4. If the divisor is a finite number and the dividend is positive or negative infinity, the result is 0 or negative 0, depending on the sign bit.

so， I think  that  the third point still applies to the floor_divide method.  Is that correct?

This case as follow, why the result is nan and -1? I think the right result is `[inf, -inf, -inf, inf, -0., -0., -0., -0.]`

### Reproduce the code example:

```python
print(""numpy_floor: "", np.floor_divide([np.inf, -np.inf, np.inf, -np.inf, -0.1000, -3.1406, 0.1000,  3.1406], [3.1406, 3.1406, -3.1406, -3.1406, np.inf, np.inf, -np.inf, -np.inf]))

# numpy_floor:  [nan nan nan nan -1. -1. -1. -1.]
```


### Error message:

_No response_

### Runtime information:

1.21.6
3.7.8 (default, Dec  7 2022, 18:47:01) 
[GCC 7.5.0]


### Context for the issue:

_No response_",2023-06-01 09:18:56,,BUG: floor_divide is not normal when  calculating the input containing inf ,"['00 - Bug', 'component: numpy.ufunc']"
23849,open,jnettels,"### Proposed new feature or change:

The [documentation ](https://numpy.org/doc/stable/reference/generated/numpy.histogram.html) is pretty clear about the egdes behaviour in np.histogram(): All right edges (except the last) are open, i.e. excluded from each bin.

I would like to request the ability to choose if the right edge is open or closed.

Pandas [cut()](https://pandas.pydata.org/docs/reference/api/pandas.cut.html) allows control over the edges with the arguments ``right`` and ``include_lowest``. Is it reasonable to ask if this logic could be copied to np.histogram()?

The following code should hopefully clarify my intentions:

```python

""""""Demonstrate how a hist plot with closed right edges could be made easier.""""""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df = pd.Series([0, 1, 2, 3, 4, 5, 6])
edges = [0, 2, 5, 7]

hist1 = pd.cut(df, bins=edges, right=True, include_lowest=True).value_counts(sort=False)
hist2 = pd.cut(df, bins=edges, right=False).value_counts(sort=False)
hist3, _ = np.histogram(df, bins=edges)

print(hist1)  # desired output
print(hist2)  # equals np.histogram output
print(hist3==hist2)  # is all True

# The default plot is not the desired one
df.hist(bins=edges, edgecolor='white', grid=False)
plt.title('Default hist() plot with open right edges')
plt.show()

# Creating the desired plot is possible, but not very straight forward
plt.hist(edges[:-1], bins=edges, weights=hist1, edgecolor='white')
plt.title('Desired plot with closed right edges')
plt.show()

# It would be nice if np.histogram() accepted the same parameters as pd.cut()
# df.hist(bins=edges, right=True, include_lowest=True, edgecolor='white', grid=False)
```

![grafik](https://github.com/numpy/numpy/assets/8526498/6c91e8a1-154a-433a-bb5b-73f1ef822ab2)
![grafik](https://github.com/numpy/numpy/assets/8526498/f435afdb-d8e9-419d-93fe-75f18bf7e142)

",2023-05-31 17:59:23,,ENH: Allow closed right edges in histogram(),['unlabeled']
23821,open,bsipocz,"One of the docs examples has a missing import, and correctly produces an error in the CI log, yet the overall status is a very misleading green pass.

https://app.circleci.com/pipelines/github/numpy/numpy/19500/workflows/f8df500a-7943-4420-a3e7-bcee6f1b4b92/jobs/31883

```
reading sources... [100%] user/basics.io.genfromtxt .. user/whatisnumpy

/home/circleci/repo/venv/lib/python3.9/site-packages/numpy/__init__.py:docstring of numpy.bartlett:78: WARNING: Exception occurred in plotting numpy-bartlett-1
 from /home/circleci/repo/doc/source/reference/generated/numpy.bartlett.rst:
Traceback (most recent call last):
  File ""/home/circleci/repo/venv/lib/python3.9/site-packages/matplotlib/sphinxext/plot_directive.py"", line 481, in _run_code
    exec(code, ns)
  File ""<string>"", line 3, in <module>
NameError: name 'plt' is not defined
/home/circleci/repo/venv/lib/python3.9/site-packages/numpy/__init__.py:docstring of numpy.blackman:70: WARNING: Exception occurred in plotting numpy-blackman-1
 from /home/circleci/repo/doc/source/reference/generated/numpy.blackman.rst:
Traceback (most recent call last):
  File ""/home/circleci/repo/venv/lib/python3.9/site-packages/matplotlib/sphinxext/plot_directive.py"", line 481, in _run_code
    exec(code, ns)
  File ""<string>"", line 3, in <module>
NameError: name 'plt' is not defined
/home/circleci/repo/doc/source/dev/depending_on_numpy.rst:: ERROR: Anonymous hyperlink mismatch: 1 references but 0 targets.
See ""backrefs"" attribute for IDs.
/home/circleci/repo/venv/lib/python3.9/site-packages/numpy/__init__.py:docstring of numpy.kaiser:100: WARNING: Exception occurred in plotting numpy-kaiser-1
 from /home/circleci/repo/doc/source/reference/generated/numpy.kaiser.rst:
Traceback (most recent call last):
  File ""/home/circleci/repo/venv/lib/python3.9/site-packages/matplotlib/sphinxext/plot_directive.py"", line 481, in _run_code
    exec(code, ns)
  File ""<string>"", line 3, in <module>
NameError: name 'plt' is not defined
looking for now-outdated files... none found
pickling environment... done
```",2023-05-27 00:02:08,,BUG: docs build CI job is not failing on error,"['00 - Bug', '05 - Testing']"
23816,open,chrish42,"### Proposed new feature or change:

Currently, when `np.char.decode()` is called on a `np.array([b'foo'], dtype=np.object_)`, the exception message generated is kinda confusing. It is also not 100% clear from the doc whether this should work or not. 

I'd propose either adding support for decoding np.arrays of byte strings, or clarifying the doc and exception message to say this is not supported. Happy to supply a pull request once a decision is made on which one it should be.",2023-05-26 17:13:16,,"ENH: add support for arrays of dtype np.object_ (of bytes) to np.char.decode, or clarify doc",['unlabeled']
23807,open,ilayn,"### Proposed new feature or change:

I was wondering why certain functions are only allowed with the macro version. Examples are `PyArray_ZEROS` but not `PyArray_Zeros`, `PyArray_EMPTY` but not `PyArray_Empty` and so on. 

In the file there is a warning that comes more than a decade ago, 

https://github.com/numpy/numpy/blob/maintenance/1.25.x/numpy/__init__.pxd#L506-L508

Since I am using these in Cythonized functions over SciPy, I'd like to educate myself as to why or is there anything needs to be done to reenable them. I asked this over Cython mailing list but redirected to here since NumPy started to override their own `pxd` files for NumPy. ",2023-05-25 07:27:21,,ENH: numpy/__init__.pxd does not expose all ArrayAPI functions to Cython,['unlabeled']
23805,open,charris,"I noticed that NEP-51 was in markdown. I would like to propose that we start writing release notes in markdown, the reasons being:

- We could post the release notes directly on the github page without translation. The current  translation isn't perfect and requires some work to fix up.
- People tend to use markdown anyway when writing release note fragments, in particular single backticks for code bits. This also requires some work to fix up, mostly remembering the correct vim incantation to fix them all (I should write it down). Markdown is also common in the commit titles which tends to produce links when translated.

I would guess that most folks on github are more familiar with markdown than rst, so using it shouldn't be a big deal as far as learning is concerned.",2023-05-25 04:21:34,,Write release notes in markdown,"['04 - Documentation', '03 - Maintenance']"
23781,open,jbrockmendel,"### Describe the issue:

I spoke the other day with @seberg about performance in indexing operations like `arr[:, mask]`.  He said that `arr[:, mask]` would in general be more performant than `arr.take(mask.nonzero()[0], axis=1)`.  This sent me down a rabbit hole and I ended up profiling a few variants that should be equivalent:

```
def get_times(nrows, ncols, frac):
    np.random.seed(3244634)
    arr = np.random.randn(nrows, ncols)

    mask = arr[:, 0] > frac
    indices = mask.nonzero()[0]

    results = {}

    if nrows >= 10**6:
        results[""mask""] = %timeit -o arr[mask]
        results[""take""] = %timeit -o arr.take(indices, axis=0)
        results[""getitem""] = %timeit -o arr[indices]
        results[""mask_T""] = %timeit -o arr.T[:, mask].T
        results[""take_T""] = %timeit -o arr.T.take(indices, axis=1).T
        results[""getitem_T""] = %timeit -o arr.T[:, indices].T
        results[""pd_take""] = %timeit -o pd.core.array_algos.take._take_nd_ndarray(arr, indices, 0, None, False)
    else:
        results[""mask""] = %timeit -o -n 100 arr[mask]
        results[""take""] = %timeit -o -n 100 arr.take(indices, axis=0)
        results[""getitem""] = %timeit -o -n 100 arr[indices]
        results[""mask_T""] = %timeit -o -n 100 arr.T[:, mask].T
        results[""take_T""] = %timeit -o -n 100 arr.T.take(indices, axis=1).T
        results[""getitem_T""] = %timeit -o -n 100 arr.T[:, indices].T
        results[""pd_take""] = %timeit -o -n 100 pd.core.array_algos.take._take_nd_ndarray(arr, indices, 0, None, False)

    times = {key: results[key].average for key in results}
    return times

all_times = {}
for nrows in [10**4, 10**5, 10**6, 10**7]:
    for ncols in [1, 2, 5, 10, 20]:
        for frac in [0.01, 0.1, 0.5, 0.9, 0.99]:
            key = (nrows, ncols, frac)
            print(key)
            times = get_times(nrows, ncols, frac)
            all_times[key] = times


df = pd.DataFrame(all_times).T
df.index.names = [""nrows"", ""ncols"", ""frac""]
```

Results:
```
                         mask      take   getitem    mask_T    take_T  getitem_T   pd_take
nrows    ncols frac                                                                       
10000    1     0.01  0.000077  0.000016  0.000011  0.000065  0.000016   0.000012  0.000007
               0.10  0.000062  0.000015  0.000011  0.000063  0.000015   0.000011  0.000008
               0.50  0.000046  0.000009  0.000008  0.000048  0.000010   0.000008  0.000006
               0.90  0.000033  0.000007  0.000005  0.000033  0.000009   0.000006  0.000004
               0.99  0.000031  0.000007  0.000005  0.000031  0.000006   0.000005  0.000005
         2     0.01  0.000112  0.000018  0.000058  0.000113  0.000041   0.000058  0.000020
               0.10  0.000106  0.000015  0.000055  0.000103  0.000039   0.000053  0.000018
               0.50  0.000076  0.000011  0.000036  0.000078  0.000029   0.000038  0.000013
               0.90  0.000049  0.000007  0.000023  0.000051  0.000023   0.000023  0.000009
               0.99  0.000045  0.000006  0.000020  0.000047  0.000020   0.000020  0.000008
         5     0.01  0.000113  0.000020  0.000062  0.000115  0.000112   0.000063  0.000027
               0.10  0.000107  0.000019  0.000056  0.000109  0.000107   0.000059  0.000023
               0.50  0.000076  0.000013  0.000040  0.000078  0.000089   0.000040  0.000018
               0.90  0.000055  0.000009  0.000026  0.000053  0.000069   0.000029  0.000014
               0.99  0.000050  0.000009  0.000023  0.000051  0.000067   0.000024  0.000013
         10    0.01  0.000116  0.000027  0.000065  0.000120  0.000266   0.000065  0.000030
               0.10  0.000110  0.000025  0.000062  0.000114  0.000259   0.000062  0.000027
               0.50  0.000080  0.000020  0.000042  0.000081  0.000207   0.000041  0.000025
               0.90  0.000058  0.000013  0.000028  0.000057  0.000179   0.000030  0.000017
               0.99  0.000052  0.000013  0.000025  0.000052  0.000173   0.000025  0.000016
         20    0.01  0.000127  0.000041  0.000072  0.000126  0.000590   0.000074  0.000045
               0.10  0.000116  0.000041  0.000070  0.000119  0.000581   0.000072  0.000048
               0.50  0.000091  0.000029  0.000049  0.000090  0.000479   0.000052  0.000034
               0.90  0.000062  0.000019  0.000033  0.000063  0.000425   0.000033  0.000022
               0.99  0.000058  0.000017  0.000029  0.000055  0.000383   0.000030  0.000020
100000   1     0.01  0.000615  0.000145  0.000097  0.000611  0.000148   0.000096  0.000040
               0.10  0.000586  0.000136  0.000092  0.000583  0.000136   0.000092  0.000039
               0.50  0.000441  0.000092  0.000062  0.000438  0.000092   0.000062  0.000030
               0.90  0.000308  0.000057  0.000039  0.000316  0.000057   0.000038  0.000023
               0.99  0.000288  0.000049  0.000034  0.000285  0.000061   0.000047  0.000025
         2     0.01  0.001057  0.000147  0.000544  0.001058  0.000380   0.000544  0.000151
               0.10  0.001004  0.000136  0.000510  0.001011  0.000370   0.000516  0.000139
               0.50  0.000743  0.000094  0.000346  0.000737  0.000274   0.000347  0.000098
               0.90  0.000483  0.000061  0.000214  0.000482  0.000204   0.000213  0.000061
               0.99  0.000437  0.000054  0.000186  0.000437  0.000192   0.000188  0.000056
         5     0.01  0.001110  0.000189  0.000607  0.001111  0.001161   0.000601  0.000223
               0.10  0.001106  0.000181  0.000586  0.001133  0.001522   0.000652  0.000256
               0.50  0.001214  0.000160  0.000452  0.000907  0.001049   0.000473  0.000154
               0.90  0.000614  0.000112  0.000308  0.000695  0.000953   0.000311  0.000120
               0.99  0.000535  0.000075  0.000214  0.000568  0.000656   0.000215  0.000085
         10    0.01  0.001496  0.000472  0.001007  0.001597  0.006103   0.000835  0.000374
               0.10  0.001505  0.000541  0.000852  0.001342  0.004994   0.000934  0.000464
               0.50  0.001103  0.000219  0.000570  0.001040  0.004436   0.000470  0.000200
               0.90  0.000566  0.000127  0.000270  0.000564  0.003215   0.000279  0.000128
               0.99  0.000500  0.000109  0.000238  0.000497  0.003131   0.000250  0.000117
         20    0.01  0.002713  0.001858  0.002482  0.003748  0.015310   0.002545  0.002491
               0.10  0.003524  0.002116  0.002632  0.003054  0.015110   0.002466  0.002005
               0.50  0.002123  0.001221  0.001536  0.002157  0.013702   0.001749  0.001330
               0.90  0.000777  0.000225  0.000420  0.000808  0.011379   0.000368  0.000230
               0.99  0.000582  0.000201  0.000334  0.000636  0.011296   0.000332  0.000163
1000000  1     0.01  0.006450  0.001673  0.001226  0.006999  0.001945   0.001264  0.000658
               0.10  0.006332  0.001648  0.001277  0.006851  0.001628   0.001126  0.000540
               0.50  0.004786  0.001044  0.000751  0.004799  0.001137   0.000740  0.000426
               0.90  0.003254  0.000669  0.000458  0.003273  0.000668   0.000430  0.000259
               0.99  0.003316  0.000533  0.000398  0.003231  0.000547   0.000401  0.000239
         2     0.01  0.012048  0.003198  0.007490  0.012737  0.010760   0.007934  0.003018
               0.10  0.012960  0.003020  0.006884  0.012664  0.010378   0.007215  0.002756
               0.50  0.009799  0.002221  0.005121  0.009651  0.008768   0.005023  0.002215
               0.90  0.006719  0.001807  0.003327  0.006596  0.007651   0.003242  0.001741
               0.99  0.006023  0.001619  0.002833  0.006001  0.006960   0.002937  0.001699
         5     0.01  0.016283  0.006817  0.010179  0.016183  0.032980   0.009955  0.006914
               0.10  0.015775  0.006343  0.009606  0.015628  0.032905   0.009766  0.006785
               0.50  0.012279  0.005227  0.007719  0.012313  0.029272   0.008127  0.005674
               0.90  0.010435  0.004113  0.007098  0.010487  0.026350   0.007374  0.004572
               0.99  0.010019  0.003792  0.006680  0.010030  0.025908   0.006767  0.004254
         10    0.01  0.021566  0.012923  0.016585  0.020971  0.100662   0.016741  0.013307
               0.10  0.021519  0.012595  0.016652  0.021303  0.102315   0.015738  0.012211
               0.50  0.018872  0.010349  0.013955  0.019079  0.097062   0.013947  0.010597
               0.90  0.013848  0.006708  0.010081  0.013791  0.092073   0.010157  0.006989
               0.99  0.012577  0.005956  0.009257  0.012645  0.092085   0.009183  0.006303
         20    0.01  0.034928  0.024172  0.028407  0.035009  0.339010   0.028514  0.024243
               0.10  0.034200  0.023771  0.027740  0.033919  0.341899   0.027568  0.023931
               0.50  0.028142  0.019522  0.023404  0.028250  0.329279   0.023414  0.019702
               0.90  0.018316  0.011894  0.015192  0.018337  0.317028   0.015234  0.011950
               0.99  0.016700  0.010726  0.013791  0.016736  0.314524   0.013483  0.010789
10000000 1     0.01  0.075609  0.023387  0.017521  0.077880  0.023426   0.016926  0.013651
               0.10  0.073974  0.021879  0.016341  0.073254  0.021880   0.016493  0.013057
               0.50  0.057912  0.015818  0.013160  0.058059  0.015887   0.013158  0.010421
               0.90  0.038654  0.009910  0.008896  0.038642  0.009897   0.008997  0.007295
               0.99  0.037461  0.009286  0.008410  0.037014  0.009149   0.008468  0.007257
         2     0.01  0.118148  0.028719  0.063149  0.117240  0.144011   0.063728  0.029231
               0.10  0.112227  0.027425  0.058993  0.113615  0.140042   0.060131  0.027359
               0.50  0.084482  0.022841  0.049230  0.084631  0.129548   0.048744  0.022950
               0.90  0.062597  0.018733  0.032421  0.062519  0.119284   0.031893  0.018181
               0.99  0.061172  0.017828  0.029592  0.058220  0.118524   0.029290  0.018449
         5     0.01  0.230349  0.129534  0.160529  0.212056  0.443435   0.161078  0.130775
               0.10  0.220766  0.123149  0.163095  0.216893  0.534799   0.161821  0.123609
               0.50  0.122455  0.052631  0.075124  0.121537  0.426743   0.077600  0.054333
               0.90  0.105465  0.043727  0.070794  0.104175  0.403974   0.070125  0.048561
               0.99  0.100057  0.037517  0.067227  0.099688  0.399199   0.066469  0.044731
         10    0.01  0.348990  0.246103  0.289579  0.347541  1.469184   0.282955  0.244499
               0.10  0.331451  0.228151  0.271604  0.328380  1.430803   0.272612  0.235470
               0.50  0.270255  0.182151  0.224866  0.255167  1.323136   0.224629  0.185771
               0.90  0.197208  0.118689  0.157459  0.191277  1.238476   0.159513  0.121140
               0.99  0.127579  0.059402  0.096368  0.127093  1.167290   0.096096  0.063261
         20    0.01  0.595607  0.473539  0.538422  0.597438  3.788825   0.520809  0.459594
               0.10  0.557066  0.442453  0.502142  0.554500  3.696630   0.501840  0.437989
               0.50  0.437317  0.332252  0.401684  0.442417  3.487455   0.397814  0.326209
               0.90  0.281199  0.209124  0.252269  0.283901  3.302511   0.250339  0.207368
               0.99  0.249273  0.181366  0.221780  0.248647  3.265589   0.219971  0.182562
```

The first thing that stands out is that the transposed `take` can be much slower than the regular indexing:

```
df2 = df[[""mask"", ""take"", ""getitem""]]
df3 = df[[""mask_T"", ""take_T"", ""getitem_T""]]
df3.columns = df2.columns

div = df3 / df2

>>> div.min()
mask       0.747117
take       0.857143
getitem    0.824561
dtype: float64
>>> div.max()
mask        1.381496
take       56.199005
getitem     1.382353
dtype: float64
```

(The only cases where `div[""take""] < 1` have `ncols=1`)

In the pandas _take_nd_ndarray there is a check for f-contiguous arrays that I suspect was implemented because of this difference.

The pandas version behaves slightly differently from the numpy version, checking for -1 entries which are then filled with a fill_value.  Because of that my expectation going in was that it would underperform regular ndarray.take.

Based on my conversation with seberg I expected the masking to be faster than take, but we have `(df[""mask""] > df[""take""]).all()`, by a factor of between 1.25 and 8.1.

Not sure if this is actionable, but wanted to post it somewhere.

### Reproduce the code example:

```python
NA
```


### Error message:

```shell
NA
```


### Runtime information:

NA

### Context for the issue:

NA",2023-05-19 01:14:26,,PERF: arr[mask] vs arr.take vs ...,['00 - Bug']
23759,open,SANTHOSH-MAMIDISETTI,"Dear NumPy Team,

I recently came across the NumPy project and I am thrilled to see the amazing work being done by the community. As someone with a strong background in C, C++, Java, and Python, I believe I can contribute effectively to the project and help take it to new heights.

Although I'm relatively new to the world of open source, I am extremely enthusiastic about becoming a part of it. I am eager to apply my skills and knowledge to a meaningful project like NumPy. However, I find myself unsure about where to get started and how my specific skills can fit into the project.

I would greatly appreciate your guidance and advice on how I can best contribute to NumPy. If you could provide some direction or suggest specific areas where my expertise can be of value, it would be immensely helpful. I am open to taking on programming tasks, code reviews, documentation, or any other responsibilities that align with my skill set.

Additionally, while exploring the NumPy repository on GitHub (https://github.com/numpy/numpy), I noticed that the environment is currently experiencing failures.
![image](https://github.com/numpy/numpy/assets/92091342/3a2dddc7-053e-4b01-afc0-3caa2dcc6107)

 I believe this issue might be related to the development and deployment processes(Devops) . I would be more than happy to dive into this matter and contribute towards its resolution. If you could provide more information about this environment failure and assign the issue to me, I will gladly work on it.
Your guidance and support would be invaluable in helping me understand the project's infrastructure and development processes. I am confident that by collaborating with the community, I can make a positive impact and help improve the NumPy project.

Thank you for your time and consideration. I look forward to your response and the opportunity to contribute to the NumPy project. @mattip @rgommers @charris @seberg @rossbar 

Best regards,
@SANTHOSH-MAMIDISETTI",2023-05-14 08:09:03,,Eager to Contribute to the NumPy Project and Assistance Request,['33 - Question']
23754,open,3rdcycle,"### Describe the issue:

I'm compiling numpy 1.24.2 with python 3.10.8 and gcc 12.2.0 on our old opteron based hpc. The compilation (through easybuild) seems to complete without errors. When running the tests (`numpy.test(verbose=2)`) I see multiple failures, most if not all of them in connection with dtype `g`/`float128` producing `nan` values. 

### Reproduce the code example:

```python
import numpy
numpy.test(verbose=2)
```


### Error message:

```shell
First error (will attach full test log in comment)

=================================== FAILURES ===================================
___________________________ TestCreation.test_zeros ____________________________

self = <numpy.core.tests.test_multiarray.TestCreation object at 0x7fa3a28b39d0>

    def test_zeros(self):
        types = np.typecodes['AllInteger'] + np.typecodes['AllFloat']
        for dt in types:
            d = np.zeros((13,), dtype=dt)
            assert_equal(np.count_nonzero(d), 0)
            # true for ieee floats
>           assert_equal(d.sum(), 0)
E           AssertionError: 
E           Items are not equal:
E            ACTUAL: nan
E            DESIRED: 0

d          = array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float128)
dt         = 'g'
self       = <numpy.core.tests.test_multiarray.TestCreation object at 0x7fa3a28b39d0>
types      = 'bBhHiIlLqQpPefdgFDG'

/tmp/eb-i46u0s0e/tmpzz3f8r08/lib/python3.10/site-packages/numpy/core/tests/test_multiarray.py:943: AssertionError
```


### Runtime information:

`1.24.2`

`3.10.8 (main, May  9 2023, 22:42:54) [GCC 12.2.0]`


```
[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['POPCNT'],
                      'not_found': ['SSSE3',
                                    'SSE41',
                                    'SSE42',
                                    'AVX',
                                    'F16C',
                                    'FMA3',
                                    'AVX2',
                                    'AVX512F',
                                    'AVX512CD',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'BARCELONA',
  'filepath': '/sw/easybuild/amd/k10/2022b/software/OpenBLAS/0.3.21-GCC-12.2.0/lib/libopenblas_barcelonap-r0.3.21.so',
  'internal_api': 'openblas',
  'num_threads': 16,
  'prefix': 'libopenblas',
  'threading_layer': 'openmp',
  'user_api': 'blas',
  'version': '0.3.21'},
 {'filepath': '/sw/easybuild/amd/k10/2022b/software/GCCcore/12.2.0/lib64/libgomp.so.1.0.0',
  'internal_api': 'openmp',
  'num_threads': 16,
  'prefix': 'libgomp',
  'user_api': 'openmp',
  'version': None}]
None
```

### Context for the issue:

_No response_",2023-05-12 12:23:27,,BUG: Various tests failing in connection with float128 on AMD Barcelona,['00 - Bug']
23745,open,mikeandike523,"### Proposed new feature or change:

Please update ArrayLike to include a strict shape or range of shapes, or `ndim`, but not both.

Rationale: I would like to be able to specify that custom type annotation RGBImage should be something akin to `TRGBImage=numpy.typing.NDArray[dtype=np.uint8, ndim=3]`

By similar reasoning, the following use cases would also be relevant

```
TMask = numpy.typing.NDArray[dtype=bool, ndim=2]
TClusterMap =  numpy.typing.NDArray[dtype=int, ndim=2]
THeatmap = numpy.typing.NDArray[dtype=float, ndim=2]
```",2023-05-10 19:08:22,,ENH: Update numpy.typing.ArrayLike to allow specification of both dtype and shape/ndim,"['01 - Enhancement', '57 - Close?', 'Static typing']"
23738,open,keithbriggs,"hypot currently doesn't work for complex inputs (and this is not documented).  I can't see any good reason for this to not be implemented.

```
>>> import numpy as np
>>> np.hypot(1,1j)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: ufunc 'hypot' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
```",2023-05-09 09:12:25,,"ENH: np.hypot(x1: complex, x2: complex) -> complex","['01 - Enhancement', '23 - Wish List', '57 - Close?']"
23737,open,JelleZijlstra,"### Describe the issue:

```
>>> import numpy
>>> numpy.__version__
'1.24.3'
>>> class X(numpy.flexible, numpy.ma.core.MaskedArray): pass
... 
>>> with memoryview(X()): pass
... 
zsh: segmentation fault  python
```

### Reproduce the code example:

```python
import numpy
class X(numpy.flexible, numpy.ma.core.MaskedArray): pass
with memoryview(X()): pass
```


### Error message:

```shell
(lldb) target create ""/Users/jelle/py/venvs/py311/bin/python""
Current executable set to '/Users/jelle/py/venvs/py311/bin/python' (arm64).
(lldb) r 
Process 62767 launched: '/Users/jelle/py/venvs/py311/bin/python' (arm64)
Python 3.11.1 (main, Dec 21 2022, 16:19:04) [Clang 14.0.0 (clang-1400.0.29.202)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy
>>> numpy.__version__
'1.24.3'
>>> class X(numpy.flexible, numpy.ma.core.MaskedArray):
...     pass
... 
>>> with memoryview(X()): pass
... 
Process 62767 stopped
* thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=2, address=0x16edfffc0)
    frame #0: 0x00000001a4226994 libsystem_malloc.dylib`nanov2_allocate_from_block + 8
libsystem_malloc.dylib`nanov2_allocate_from_block:
->  0x1a4226994 <+8>:  stp    x28, x27, [sp, #0x20]
    0x1a4226998 <+12>: stp    x26, x25, [sp, #0x30]
    0x1a422699c <+16>: stp    x24, x23, [sp, #0x40]
    0x1a42269a0 <+20>: stp    x22, x21, [sp, #0x50]
Target 0: (python) stopped.
(lldb) bt
* thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=2, address=0x16edfffc0)
  * frame #0: 0x00000001a4226994 libsystem_malloc.dylib`nanov2_allocate_from_block + 8
    frame #1: 0x00000001a42261e0 libsystem_malloc.dylib`nanov2_allocate + 128
    frame #2: 0x00000001a42260fc libsystem_malloc.dylib`nanov2_malloc + 64
    frame #3: 0x00000001a4243748 libsystem_malloc.dylib`_malloc_zone_malloc + 156
    frame #4: 0x00000001000a0998 python`_PyObject_Malloc + 64
    frame #5: 0x00000001001eedcc python`state_init + 164
    frame #6: 0x00000001001ecf1c python`_sre_SRE_Pattern_match + 224
    frame #7: 0x0000000100058a2c python`method_vectorcall_FASTCALL_KEYWORDS_METHOD + 144
    frame #8: 0x000000010004e02c python`PyObject_VectorcallMethod + 144
    frame #9: 0x0000000100104b30 python`check_matched + 108
    frame #10: 0x00000001001035ac python`warn_explicit + 1284
    frame #11: 0x000000010010487c python`do_warn + 828
    frame #12: 0x0000000100102eac python`PyErr_WarnEx + 72
    frame #13: 0x00000001036ada78 _multiarray_umath.cpython-311-darwin.so`PyArray_DescrFromTypeObject + 496
    frame #14: 0x00000001036acbb8 _multiarray_umath.cpython-311-darwin.so`PyArray_DescrFromScalar + 212
    frame #15: 0x000000010009ef28 python`_PyObject_GenericGetAttrWithDict + 200
    frame #16: 0x000000010009e7dc python`PyObject_GetAttr + 100
    frame #17: 0x000000010009e720 python`PyObject_GetAttrString + 72
    frame #18: 0x00000001036acccc _multiarray_umath.cpython-311-darwin.so`PyArray_DescrFromScalar + 488
    frame #19: 0x000000010009ef28 python`_PyObject_GenericGetAttrWithDict + 200
    frame #20: 0x000000010009e7dc python`PyObject_GetAttr + 100
    frame #21: 0x000000010009e720 python`PyObject_GetAttrString + 72
    frame #22: 0x00000001036acccc _multiarray_umath.cpython-311-darwin.so`PyArray_DescrFromScalar + 488
    frame #23: 0x000000010009ef28 python`_PyObject_GenericGetAttrWithDict + 200
    frame #24: 0x000000010009e7dc python`PyObject_GetAttr + 100
    frame #25: 0x000000010009e720 python`PyObject_GetAttrString + 72
(lots more of this, snipped)
    frame #279490: 0x00000001036acccc _multiarray_umath.cpython-311-darwin.so`PyArray_DescrFromScalar + 488
    frame #279491: 0x000000010009ef28 python`_PyObject_GenericGetAttrWithDict + 200
    frame #279492: 0x000000010009e7dc python`PyObject_GetAttr + 100
    frame #279493: 0x0000000100138d50 python`_PyEval_EvalFrameDefault + 17896
    frame #279494: 0x00000001001346c8 python`_PyEval_Vector + 200
    frame #279495: 0x000000010004e300 python`object_vacall + 256
    frame #279496: 0x000000010004e564 python`PyObject_CallFunctionObjArgs + 48
    frame #279497: 0x0000000103603278 _multiarray_umath.cpython-311-darwin.so`PyArray_NewFromDescr_int + 1268
    frame #279498: 0x00000001035fa958 _multiarray_umath.cpython-311-darwin.so`PyArray_View + 84
    frame #279499: 0x0000000103688110 _multiarray_umath.cpython-311-darwin.so`array_view + 324
    frame #279500: 0x000000010005867c python`method_vectorcall_FASTCALL_KEYWORDS + 136
    frame #279501: 0x000000010004ccd4 python`PyObject_Vectorcall + 80
    frame #279502: 0x000000010013cf1c python`_PyEval_EvalFrameDefault + 34740
    frame #279503: 0x00000001001346c8 python`_PyEval_Vector + 200
    frame #279504: 0x000000010004c64c python`_PyObject_FastCallDictTstate + 272
    frame #279505: 0x000000010004d394 python`_PyObject_Call_Prepend + 148
    frame #279506: 0x00000001000b6b68 python`slot_tp_new + 88
    frame #279507: 0x00000001000b3550 python`type_call + 84
    frame #279508: 0x000000010004c834 python`_PyObject_MakeTpCall + 344
    frame #279509: 0x000000010013cf1c python`_PyEval_EvalFrameDefault + 34740
    frame #279510: 0x0000000100134598 python`PyEval_EvalCode + 272
    frame #279511: 0x000000010018c5a8 python`PyRun_InteractiveOneObjectEx + 744
    frame #279512: 0x000000010018bad0 python`_PyRun_InteractiveLoopObject + 152
    frame #279513: 0x000000010018b97c python`_PyRun_AnyFileObject + 96
    frame #279514: 0x000000010018c234 python`PyRun_AnyFileExFlags + 68
    frame #279515: 0x00000001001ac1dc python`Py_RunMain + 2196
    frame #279516: 0x00000001001ac424 python`pymain_main + 324
    frame #279517: 0x00000001001ac4c4 python`Py_BytesMain + 40
    frame #279518: 0x000000010057508c dyld`start + 520
```


### Runtime information:

```
>>> import sys, numpy; print(numpy.__version__); print(sys.version)
1.24.3
3.11.1 (main, Dec 21 2022, 16:19:04) [Clang 14.0.0 (clang-1400.0.29.202)]
>>> print(numpy.show_runtime())
[{'simd_extensions': {'baseline': ['NEON', 'NEON_FP16', 'NEON_VFPV4', 'ASIMD'],
                      'found': ['ASIMDHP', 'ASIMDDP'],
                      'not_found': ['ASIMDFHM']}},
 {'architecture': 'armv8',
  'filepath': '/Users/jelle/py/venvs/py311/lib/python3.11/site-packages/numpy/.dylibs/libopenblas64_.0.dylib',
  'internal_api': 'openblas',
  'num_threads': 8,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.21'}]
None
```

### Context for the issue:

No concrete use case; I found this while trying to instantiate all pairs of buffer classes to look for real-world impact of python/cpython#104297.",2023-05-08 17:09:14,,BUG: Stack overflow on double inheritance from numpy.flexible and numpy.ma.core.MaskedArray,['00 - Bug']
23704,open,SmartDev-0205,"### Describe the issue:

I am trying to install numpy on python3.11.
I am using the UOS system.
But I get an error and cannot install it.

#error Unknown CPU, please report this to numpy maintainers with \
            ^~~~~
      error: Command ""gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -Ibuild/src.linux-sw_64-3.11/numpy/core/src/npymath -Inumpy/core/include -Ibuild/src.linux-sw_64-3.11/numpy/core/include/numpy -Ibuild/src.linux-sw_64-3.11/numpy/distutils/include -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/home/uos/OCR/venv/include -I/usr/local/include/python3.11 -Ibuild/src.linux-sw_64-3.11/numpy/core/src/common -Ibuild/src.linux-sw_64-3.11/numpy/core/src/npymath -c numpy/core/src/npymath/npy_math.c -o build/temp.linux-sw_64-3.11/numpy/core/src/npymath/npy_math.o -MMD -MF build/temp.linux-sw_64-3.11/numpy/core/src/npymath/npy_math.o.d"" failed with exit status 1

Please let me know what is the solution.

### Reproduce the code example:

```python
I cannot install
```


### Error message:

```shell
#error Unknown CPU, please report this to numpy maintainers with \
            ^~~~~
      error: Command ""gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -Ibuild/src.linux-sw_64-3.11/numpy/core/src/npymath -Inumpy/core/include -Ibuild/src.linux-sw_64-3.11/numpy/core/include/numpy -Ibuild/src.linux-sw_64-3.11/numpy/distutils/include -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/home/uos/OCR/venv/include -I/usr/local/include/python3.11 -Ibuild/src.linux-sw_64-3.11/numpy/core/src/common -Ibuild/src.linux-sw_64-3.11/numpy/core/src/npymath -c numpy/core/src/npymath/npy_math.c -o build/temp.linux-sw_64-3.11/numpy/core/src/npymath/npy_math.o -MMD -MF build/temp.linux-sw_64-3.11/numpy/core/src/npymath/npy_math.o.d"" failed with exit status 1
```


### Runtime information:

I cannot install

### Context for the issue:

I cannot install",2023-05-03 09:39:18,,BUG: Cannot install numpy on UOS system.,['00 - Bug']
23694,open,jamesbraza,"### Proposed new feature or change:

It would be great to support a `dtype` argument in [`random.Generator.normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.normal.html#numpy.random.Generator.normal).

`dtype` is already supported the similar function `random.Generator.standard_normal` for `float32` and `float64` as of `numpy==1.24.3`.  Perhaps this behavior can be easily ported over.",2023-05-02 05:38:14,,ENH: `dtype` argument in `random.Generator.normal`,"['01 - Enhancement', 'component: numpy.random']"
23693,open,Dr-Irv,"### Describe the issue:

See code below.

The revealed type is `bool`.  But the code will raise an exception.  The revealed type should be `NoReturn` .  `bool(x)` raises as well.

Note - there is discussion at https://github.com/microsoft/pyright/issues/5039 about having `pyright` recognize that the `if` statement is invalid, and I also started a discussion in the typing community here:  https://github.com/python/typing/discussions/1398


### Reproduce the code example:

```python
import numpy as np
x = np.array([1,2,3])
reveal_type(x.__bool__())
if x:
    print(""hey"")
else:
    print(""no"")
```


### Error message:

```shell
From pyright:

Type of ""x.__bool__()"" is ""bool""
```


### Runtime information:

>>> import sys, numpy; print(numpy.__version__); print(sys.version)
1.24.3
3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:28:38) [MSC v.1929 64 bit (AMD64)]
>>> print(numpy.show_runtime())
WARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information
[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}}]
None

### Context for the issue:

I'm trying to make the typing between pandas and numpy consistent.
",2023-05-01 15:24:00,,TYP: Typing of `np.ndarray.__bool__()` is incorrect,"['57 - Close?', 'Static typing']"
23679,open,Quacken8,"### Describe the issue:

VScode can't correctly hint the wrapped function's docstring, hinting instead about vectorize:

![Peek 2023-04-28 09-33](https://user-images.githubusercontent.com/78506690/235112237-469fa13c-9fec-4b02-9be2-1b3c1ab38c26.gif)

but the `__doc__` is passed on correctly.

[In this issue](https://github.com/microsoft/pylance-release/issues/4292) it was suggested this is becuase of annotations.

### Reproduce the code example:

```python
import numpy as np

def foo(x):
    """"""
    I am a foo function
    don't put zero into me
    """"""
    return 1/x



@np.vectorize
def vectorizedFoo(x):
    """"""
    I am a vectorized foo function
    don't put zero into me
    """"""
    return 1/x

print(vectorizedFoo.__doc__)
```


### Error message:

_No response_

### Runtime information:

numpy version: 1.23.5
sys version : 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]

### Context for the issue:

Without good hinting np vectorize is almost useless for complex functions, since the user always has to peek the definition to get the docstring",2023-04-28 09:39:08,,BUG: vectorize is unannotated which leads to doc strings being useless,['00 - Bug']
23671,open,mattip,"While reviewing #18053, @seberg mentioned
> if really would be nicer if sop would just have a return value IMO. Then we could avoid the whole PyErr_Occurred() check

This would involve refactoring the [declaration](https://github.com/numpy/numpy/blob/6d34e34e7bc0dad1722958f00f3767d707c1849f/numpy/core/src/multiarray/einsum_sumprod.h#L6) and many of the [functions definitions](https://github.com/numpy/numpy/blob/main/numpy/core/src/multiarray/einsum_sumprod.c.src), and then fixing the actual code that calls these.",2023-04-27 12:33:13,,ENH: update einsum som_of_product functions to return a value instead of `void`,['unlabeled']
23633,open,nbehrnd,"### Issue with current documentation:

Prior to filing a PR about the presentation of f2py [here](https://numpy.org/doc/stable/f2py/f2py.getting-started.html#the-quick-way), I would like to reach out for the maintainers of numpy for some discussion.

Limiting to the first example ""the quick way"", `fib1.f` currently presented is written in FORTRAN77 equally known as fixed style.  Modern Fortran (everything since and including Fortran 90, with a last revision 2018, and standard 2023 in preparation) follows set of modernized rules, the statement `implicit none` might be one known equally by those outside Fortran.  I rewrote the procedure as in snippet `fib1a.f90` which `f2py` by

``` shell
python -m numpy.f2py -c fib1a.f90 -m fib
```

accepts to wrap into an extension module.  Python can use it to replicate replicate the computation `a = np.zeros(8, 'd')` to yield `[0. 1. 1. 2. 3. 5. 8. 13.]`.

Observation 1:  Line 6 defines double precision as suitable for the gfortran compiler which is applied on the elements of array `a(n)`.  With the advent of Fortran 2003, `iso_fortran_env` ([ref](https://fortranwiki.org/fortran/show/iso_fortran_env)) offers a compiler independent definition of precision which can be used instead.  While fine for gfortran, and apparently fine for `f2py`, the subsequent use of the .so extension module by Python did not follow through with either version `fib2a.f90` (which loads from `iso_fortran_env` only what is relevant to for ""double precision"" of a floating number), or `fib2b.f90` (which does not impose such a constrain what other procedure definitions this Fortran intrinsic module might contain).  For either one of the two, their use in the minimal Python example terminates with the report

```
[0. 0. 0. 0. 0. 0. 0. 0.]
corrupted size vs. prev_size
Aborted
```

Question: Does the use of `iso_fortran_env` in the Fortran procedure require a different syntax in the Python script to use the wrapper module?  Does `d` in `a = np.zeros(8, 'd')` relate to double precision of reals (Fortran) / floating numbers (numpy)?

-----

Observation 2:  Especially Fortran subroutines are safer in use with an explicit definition if processed data shall
+ only enter a procedure without being altered by this procedure (`intent(in)`)
+ shall return to the main program, possibly altered (`intent(out)`), or
+ if the exchange between main program and routine shall be freely bidirectional, including the possibility that the procedure alters these values (`intent(in out)`, or functional equivalent, `intent(inout)`).  Hence I derived `fib3a.f90` from `fib1a.f90` which compiles well and yields the anticipated result `[ 0.  1.  1.  2.  3.  5.  8. 13.]`.  However the use of these keywords and the double precision provided by Fortran's intrinsic `iso_fortran_env` module now yields the result `[0. 0. 0. 0. 0. 0. 0. 0.]`

Question: Here, I do not know how to interpret the result for the second case.  Do you recommend a different input syntax on level of numpy?

-----

The observations refer to an instance of Linux Debian 12/bookworm with Python (3.11.2) with numpy (1.24.2), gcc and gfortran (12.2.0-14) as provided by this Linux distributions.  The Fortran modules were considered as ""good enough"" because their compilation to yield object files in a pattern of

``` shell
gfortran ./fib3a.f90 -c -std=f2018 -Wall -Wextra -Wpedantic
```

did not yield an error.  The two warnings e.g.

```
$ gfortran ./fib3a.f90 -c -std=f2018 -Wall -Wextra -Wpedantic
./fib3a.f90:16:15:

   10 |    do i = 1, n
      |              2 
......
   16 |       a(i) = a(i - 1) + a(i - 2)
      |               1
Warning: Array reference at (1) out of bounds (0 < 1) in loop beginning at (2) [-Wdo-subscript]
./fib3a.f90:16:26:

   10 |    do i = 1, n
      |              2            
......
   16 |       a(i) = a(i - 1) + a(i - 2)
      |                          1
Warning: Array reference at (1) out of bounds (-1 < 1) in loop beginning at (2) [-Wdo-subscript]
```

were noted, though not considered as relevant for the discussion.  All relevant source code is in the zip archive below.

[2023-04-21_f2py_discussion.zip](https://github.com/numpy/numpy/files/11299204/2023-04-21_f2py_discussion.zip)



### Idea or request for content:

_No response_",2023-04-21 20:56:45,,DOC: clarification about the interaction of numpy / f2py with Fortran procedures,"['04 - Documentation', 'component: numpy.f2py']"
23625,open,bashtage,"### Proposed new feature or change:

Pandas is moving toward copy-on-write.  When CoW is enabled, calling `np.asarray(df_with_cow)` returns a read-only version of an ndarray.  This breaks a lot of existing code.  This probably isn't precisely a numpy issue, since `asarray` is just doing what is promises.

Perhaps `np.asarray(x, writable=True)` so that  copy would be taken from anything that is read only.

As a small measure of this issue, enabling CoW leads to about 300 failures on statsmodels tests.


",2023-04-21 08:01:57,,ENH: np.asarray should have an option to ensure the array returned is writeable,['unlabeled']
23622,open,darrencl,"### Proposed new feature or change:

Hi all, I am writing a code to write a numpy 2D array in a csv file and then read them back at some point. The column that contains the 2D array is written similarly as the output of `np.array_str`, e.g. below:

```python
In [19]: np.array_str(np.ones((2,4)))                                                                                                                                                                              
Out[19]: '[[1. 1. 1. 1.]\n [1. 1. 1. 1.]]'
```

However, there is no converter/reader available to turn this string representation back to numpy array object. As an user, I would expect to have a convenient 'reader' of an output if there is a 'writer', just like `np.savetxt` and `np.loadtxt`. There is `np.fromstring`, but this function does not any option to read `np.array_str` output despite being in the same numpy ecosystem.

I know that it can be tackled by something like [this](https://stackoverflow.com/a/38886759/5023889) or the answer below that, but it seems like a 'hacky' approach and this functionality should be added to numpy instead.

I am happy to submit a PR to this small function if numpy team agreed to have this. Feel free to advice where is best to put this and suggest a suitable function name.

Thanks!",2023-04-21 02:44:42,,ENH: Reader/converter for np.array_str output,['unlabeled']
23588,open,merny93,"### Describe the issue:

The implementation of `matmul` does some basic checks to verify if the array can be passed to `gemm` and if it deems that this is not posssible it will fallback on a `noblas` routine which has no regard for memory cache resulting in ~100 slowdowns. Here is the offending bit of [source code](https://github.com/numpy/numpy/blob/6f55bbf049db3fd50994a97ec66665f5685ec5be/numpy/core/src/umath/matmul.c.src#L217).

`dot` on the other hand is a lot more flexible, attempting to make a copy before passing the array to the blas routine. For small arrays the difference is not significant but for large arrays this results in much better performance. This behavior is explicitly seen in the [source code](https://github.com/numpy/numpy/blob/669cd13c692cfe8476e24dad3d42bbbd94547727/numpy/core/src/common/cblasfuncs.c#L221) (line 234 and 243 are the bad-stride copies) and can be confirmed by profiling as shown by [hpaulj](https://stackoverflow.com/a/76010625/17638323).

Neither of the docs pages for `dot` or `matmul` reference this behavior. On the contrary the `dot` page states: _If both a and b are 2-D arrays, it is matrix multiplication, but using [matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html#numpy.matmul) or a @ b is preferred._

There are a handful of similar issues open:
 - #23123 
 - [complextype](https://stackoverflow.com/q/64914877/17638323)
 - [views](https://stackoverflow.com/q/76009612/17638323)
 - #23260 

I am not sure what the best fix is. Updating the docs would be a good start. Perhaps `matmul` should try to copy too? at least for large arrays...

### Reproduce the code example:

```python
import numpy as np
from timeit import timeit
N = 2600
xx = np.random.randn(N, N) 
yy = np.random.randn(N, N) 

x = xx[::2, ::2]
y = yy[::2, ::2]
assert np.shares_memory(x, xx)
assert np.shares_memory(y, yy)

dot = timeit('np.dot(x,y)', number = 10, globals = globals())
matmul = timeit('np.matmul(x,y)', number = 10, globals = globals())

print('time for np.matmul: ', matmul)
print('time for np.dot: ', dot)
```


### Error message:

```shell
time for np.matmul:  29.04214559996035
time for np.dot:  0.2680714999441989
```


### Runtime information:

```
> import sys, numpy; print(numpy.__version__); print(sys.version)
1.24.2
3.11.2 (tags/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)]
> print(numpy.show_runtime())
[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Prescott',
  'filepath': 'C:\\Users\\______\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll',
  'internal_api': 'openblas',
  'num_threads': 24,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.21'}]
None
```

### Context for the issue:

The scientific computing community regularly uses the `@` shorthand on large arrays. Developers expect that with Numpy they do not need to think about CPU cache and expect operations to run close to CPU limits rather than memory bandwidth limits.

At the very least a warning should be provided to users such that they know why their code is running orders of magnitude slower than expected.",2023-04-14 16:24:34,,BUG: matmul (@ overload) and dot significant performance differences for non-contiguous arrays (noblas) ,"['00 - Bug', 'Project']"
23586,open,John-Venti,"### Issue with current documentation:

The current description of np.tensordot() is
 ```
When `axes` is integer_like, the sequence for evaluation will be: first
the -Nth axis in `a` and 0th axis in `b`, and the -1th axis in `a` and
Nth axis in `b` last.
```
However the source code of it is
```
except Exception:
    axes_a = list(range(-axes, 0))
    axes_b = list(range(0, axes))
else:
    axes_a, axes_b = axes
```
There is a conflict. For example, when N=10 list(range(0,10)) and list(range(-10,0)) are the following, respectively:
```
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 
and
[-10, -9, -8, -7, -6, -5, -4, -3, -2, -1]
```
So I think it should be ""first 0 and N-1th axis in b last"" rather than ""first 0 and Nth axis in b last"".


### Idea or request for content:

Therefore, I modified the description to
```
When `axes` is integer_like, the sequence for evaluation will be: first
the -Nth axis in `a` and 0th axis in `b`, and the -1th axis in `a` and
(N-1)th axis in `b` last. When N-1 is smaller than 0, or when -N is larger
than -1, the element of 'a' and 'b' are defined as the 'axes'.
```
And I add an example to use the default parameter, as follows:
```
""traditional"" examples:
>>> # An example on integer_like
>>> a_0 = np.array([[1,2],[3,4]])
>>> b_0 = np.array([[5,6],[7,8]])
>>> c_0 = np.tensordot(a_0,b_0,0)
>>> c_0.shape
(2, 2, 2, 2)
>>> c_0
array([[[[ 5,  6],
         [ 7,  8]],
        [[10, 12],
         [14, 16]]],
       [[[15, 18],
         [21, 24]],
        [[20, 24],
         [28, 32]]]])
>>> # An example on array_like
```
I submit the PR here https://github.com/numpy/numpy/pull/23547
I hope it can be modified at an early date, because I found many people are confused for this description at stack overflow and other places.",2023-04-14 09:12:18,,"DOC: There is a typo in the tutorial documentation of ""np.tensordot()""",['04 - Documentation']
23582,open,h-vetinari,"The function `np.isnat` asks a very specific question, which is what it says on the tin ""is the argument the nan-marker for datetime/timedelta?""

There are situations -- especially in trying to write library code that survives a lot of user inputs (or tests for that) -- where it's necessary or at least desirable to handle NaT specifically. 

However the function unfortunately is way less useful than it could be, because it only works for datetime/timedelta:
```
TypeError: ufunc 'isnat' is only defined for np.datetime64 and np.timedelta64
```
so now, to use it somewhere (that's not in a hyperspecific chain of branches), I have to write
```
isinstance(x, (np.datetime64, np.timedelta64)) and np.isnat(x)
```
to achieve what `np.isnat(x)` could be doing in the first place.

### Why not make this function simply return False instead of TypeError in that case?",2023-04-13 23:53:24,,BUG: np.isnat fails on non-datetime,['unlabeled']
23576,open,pgp,"### Describe the issue:

As the subject says, see code example attached

### Reproduce the code example:

```python
import numpy as np

M = np.array([[1.23, 4.56, 7.89],[2.34,5.67, 8.91],[3.45, 6.78, 9.01]])  # np.ndarray of shape (3,3)

# Trying to slice some rows/columns from this matrix works both using non-empty and empty tuples:
M[:,(0,2)]  # get a submatrix with first and third column of the original one
M[(0,1),:]  # get a submatrix with first and second row of the original one
M[(),:]  # get an empty-matrix of shape (0,3)
M[:,()]  # get an empty-matrix of shape (3,0)

# However, if we use numpy 1D arrays for indexing instead of tuples...

# everything fine for non-empty 1D arrays...
assert np.all(M[:,np.array((0,2))] == M[:,(0,2)])  # OK
assert np.all(M[np.array((0,1)),:] == M[(0,1),:])  # OK

# but with empty np.arrays an error is thrown
M[:,np.array(())]
```


### Error message:

```shell
IndexError: arrays used as indices must be of integer (or boolean) type
```


### Runtime information:

1.23.5
3.11.0 | packaged by conda-forge | (main, Jan 14 2023, 12:27:40) [GCC 11.3.0]



### Context for the issue:

Don't know if this is intended behaviour, but I'm optimizing some code with numba (using numba 0.57.0rc1 with python 3.11) in nopython mode, and I'm replacing standard tuples with np.arrays where possible. There's a point in the code when I perform slicing/indexing and then doing an np.sum, and sometimes it happens to have empty index sets with zero sum - hence, empty tuples or empty np.arrays - I cannot use the latter ones due to this.",2023-04-13 10:13:49,,"BUG: empty-np.array indexing does not work, but empty-tuple indexing works instead",['00 - Bug']
23574,open,h-vetinari,"### Describe the issue:

`None` is clearly not an array, hence scalar. I couldn't find a previous issue, but I really doubt this is intentional - at least it makes no sense to me, and it makes it very hard to nicely generic code, which should survive a user having a `None` somewhere, and not suddenly jumping form the scalar into the array path.

### Reproduce the code example:

```python
>>> import numpy as np
>>> np.isscalar(None)
False
>>> np.__version__
'1.24.2'
```",2023-04-13 02:36:35,,"BUG: isscalar(None) should be True, is False",['00 - Bug']
23546,open,rohanjain101,"### Describe the issue:

When using a safe cast, numpy should raise if the value cannot be safely casted. In this case, the int64 18014398509481983 cannot be safely casted to a float64, so an error should be raised.

Similar issue discussed in https://github.com/apache/arrow/issues/34901

### Reproduce the code example:

```python
>>> import numpy as np
>>> np.__version__
'1.24.2'

# Bug: No safety error for initial int64 -> float64 conversion
>>> np.array([18014398509481983]).astype(""float64"", casting=""safe"").astype(str)
array(['1.8014398509481984e+16'], dtype='<U32')
```


### Error message:

_No response_

### Runtime information:

```
WARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information
[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2',
                                'AVX512F',
                                'AVX512CD',
                                'AVX512_SKX',
                                'AVX512_CLX'],
                      'not_found': ['AVX512_CNL', 'AVX512_ICL']}}]
None
>>>
>>> print(np.__version__)
1.24.2
>>> sys.version
'3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]'
>>>
```

### Context for the issue:

_No response_",2023-04-06 21:11:09,,"BUG: Error not raised for an unsafe cast when using casting=""safe""",['00 - Bug']
23540,open,Game4Move78,"### Proposed new feature or change:

My argument is that `where` makes sense unless `out` is default `None`, in which case it simply obscures the creation of a partially uninitialised array.

The casual numpy user accessing through the public API likely only wants garbage values if they have explicitly asked for them using `empty_like`. If they have decided not to provide `out`, it probably means they expect a partially transformed copy of the input array and have overlooked the note in the docstring.

Example:

User creates a function that takes log on values where log is defined .

```python
def safe_log(val, out):
    return np.log(val, where=np.greater(val, 0), out=out)
```

User suddenly decides that they don't want to specify `out` every time, making `safe_log` unsafe.

```python
def safe_log(val, out=None):
    return np.log(val, where=np.greater(val, 0), out=out)
```",2023-04-06 16:03:23,,ENH: ufunc should warn when argument `where` is specified and `out` is default None.,['unlabeled']
23531,open,stefmolin,"### Describe the issue:

I was under the assumption that the `Polynomial.degree()` method yielded the degree of the polynomial, but after looking at the code in main and the return value in the docs, it just subtracts one from the number of coefficients, which doesn't account for a 0 in the highest degree coefficient (possible if another process determines the coefficients that get passed in).

### Reproduce the code example:

```python
import numpy as np
np.polynomial.Polynomial([1, 7, 4, 0]).degree()  # returns 3, but I would expect 2
```


### Error message:

_No response_

### Runtime information:

1.23.1
3.8.11 (default, Jul 29 2021, 14:57:32)
[Clang 12.0.0 ]

### Context for the issue:

I'm happy to put in a PR if this is not by design.",2023-04-05 01:21:04,,BUG: Polynomial degree doesn't account for 0 coefficient,['00 - Bug']
23526,open,onitake,"### Issue with current documentation:

The [documentation for the bincount function](https://numpy.org/doc/stable/reference/generated/numpy.bincount.html) does not explain that the input must be of the native type used for array indexes.

This causes problems on 32-bit architectures, when a simple example like the following is executed with safety mode on:

```python
import numpy as np
data = np.random.default_rng().integers(low=0, high=100, dtype=np.int64, size=10)
np.bincount(data)
```

Result:

```
Traceback (most recent call last):
  File ""bincount.py"", line 6, in <module>
    np.bincount(data)
  File ""<__array_function__ internals>"", line 200, in bincount
TypeError: Cannot cast array data from dtype('int64') to dtype('int32') according to the rule 'safe'
```

With `dtype=int32` (fixed type) or `dtype.int_` or simply `int` (native int type), it works fine.

### Idea or request for content:

I'd suggest changing the example at https://github.com/numpy/numpy/blob/main/numpy/core/multiarray.py#L948-L955 from:

```
    The input array needs to be of integer dtype, otherwise a
    TypeError is raised:
    >>> np.bincount(np.arange(5, dtype=float))
    Traceback (most recent call last):
      ...
    TypeError: Cannot cast array data from dtype('float64') to dtype('int64')
    according to the rule 'safe'
```

to:

```
    The input array needs to be of the native integer index type, otherwise a
    TypeError is raised:
    >>> np.bincount(np.arange(5, dtype=float))
    Traceback (most recent call last):
      ...
    TypeError: Cannot cast array data from dtype('float64') to dtype('int64')
    according to the rule 'safe'

    The simplest way to do this is by always using the Python int type or dtype.int_:
    >>> np.bincount(np.arange(5, dtype=int))
```
",2023-04-04 11:18:54,,DOC: bincount documentation should mention native index type requirement,['04 - Documentation']
23523,open,asreimer,"### Describe the issue:

It appears that there is a small difference between functions executed using AVX-512 and not using AVX-512. Is this expected? My admittedly limited understanding is that computation with AVX-512 should produce results identical to other computations.

For example, `math.sin` and `np.sin` agree with AVX-512 disabled, but disagree by approximately`2**-53` (approximately double precision) depending on the input angle. `np.cos` is similar. I've also tested some of the other SIMD backed numpy functions, such as `np.exp` and `np.log` and they also return non-zero differences with the `math` functions unless AVX-512 is explicitly disabled or not available (such as on AMD or ARM).

### Reproduce the code example:

```python
(py39) [asreimer@fedora ~]$ python
Python 3.9.16 (main, Apr  3 2023, 12:24:01) 
[GCC 11.3.1 20220421 (Red Hat 11.3.1-3)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import math
>>> import numpy as np
>>> print('Difference of sines: {}'.format(np.sin(np.radians(45))-math.sin(np.radians(45))))
Difference of sines: 1.1102230246251565e-16
>>> print('Difference of exponentials: {}'.format(np.exp(12.312)-math.exp(12.312)))
Difference of exponentials: -2.9103830456733704e-11
>>> print('Difference of natural logorithms: {}'.format(np.log(1332.312)-math.log(1332.312)))
Difference of natural logorithms: -8.881784197001252e-16
>>> 
(py39) [asreimer@fedora ~]$ NPY_DISABLE_CPU_FEATURES=""AVX512F,AVX512CD,AVX512_SKX,AVX512_CLX,AVX512_CNL,AVX512_ICL"" python
Python 3.9.16 (main, Apr  3 2023, 12:24:01) 
[GCC 11.3.1 20220421 (Red Hat 11.3.1-3)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import math
>>> import numpy as np
>>> print('Difference of sines: {}'.format(np.sin(np.radians(45))-math.sin(np.radians(45))))
Difference of sines: 0.0
>>> print('Difference of exponentials: {}'.format(np.exp(12.312)-math.exp(12.312)))
Difference of exponentials: 0.0
>>> print('Difference of natural logorithms: {}'.format(np.log(1332.312)-math.log(1332.312)))
Difference of natural logorithms: 0.0
>>>
```


### Error message:

_No response_

### Runtime information:

Default:

    >>> import sys, numpy; print(numpy.__version__); print(sys.version); print(numpy.show_runtime())
    1.24.2
    3.9.16 (main, Apr  3 2023, 12:24:01) 
    [GCC 11.3.1 20220421 (Red Hat 11.3.1-3)]
    [{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                          'found': ['SSSE3',
                                    'SSE41',
                                    'POPCNT',
                                    'SSE42',
                                    'AVX',
                                    'F16C',
                                    'FMA3',
                                    'AVX2',
                                    'AVX512F',
                                    'AVX512CD',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL'],
                          'not_found': ['AVX512_KNL', 'AVX512_KNM']}},
     {'architecture': 'SkylakeX',
      'filepath': '/home/asreimer/venvs/py39/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so',
      'internal_api': 'openblas',
      'num_threads': 4,
      'prefix': 'libopenblas',
      'threading_layer': 'pthreads',
      'user_api': 'blas',
      'version': '0.3.21'}]

With AVX-512 disabled via `NPY_DISABLE_CPU_FEATURES`:

    >>> import sys, numpy; print(numpy.__version__); print(sys.version); print(numpy.show_runtime())
    1.24.2
    3.9.16 (main, Apr  3 2023, 12:24:01) 
    [GCC 11.3.1 20220421 (Red Hat 11.3.1-3)]
    [{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                          'found': ['SSSE3',
                                    'SSE41',
                                    'POPCNT',
                                    'SSE42',
                                    'AVX',
                                    'F16C',
                                    'FMA3',
                                    'AVX2'],
                          'not_found': ['AVX512F',
                                        'AVX512CD',
                                        'AVX512_KNL',
                                        'AVX512_KNM',
                                        'AVX512_SKX',
                                        'AVX512_CLX',
                                        'AVX512_CNL',
                                        'AVX512_ICL']}},
     {'architecture': 'SkylakeX',
      'filepath': '/home/asreimer/venvs/py39/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so',
      'internal_api': 'openblas',
      'num_threads': 4,
      'prefix': 'libopenblas',
      'threading_layer': 'pthreads',
      'user_api': 'blas',
      'version': '0.3.21'}]

### Context for the issue:

While the differences here are small, they are unexpected and can cause larger problems. For example, when performing vector rotations using rotation matrices, the small differences of 1e-16 can blow up to something closer to 1e-5. This is how I originally found this bug: finding differences in expected output while using rotation matrices on AMD and ARM based systems without AVX-512 compared to the results from an Intel system with AVX-512.",2023-04-04 06:45:17,,"BUG: AVX-512 results differ from non-AVX-512 for sin, cos, exp, log, etc.",['00 - Bug']
23518,open,vanc0uv3r,"### Issue with current documentation:

While working with float numbers using `np.float32` datatype I noticed that using rather big numbers (which does not have fraction) and printing them (or just representing as string) can mislead.

Example code below shows, how representing `np.float32` number as string can mislead. 
```python
import numpy as np

num = np.nextafter(np.float32(134217720), np.float32(np.inf))
print(num)  # 134217730
```
Number _134217720_ belongs to the interval $[2^{26}, 2^{27}]$, where precision (or the smallest difference between two nearest numbers) is _8_. So if we what to take the next after _134217720_ we should get _134217728_ (which can directly stored as float32 without conversion error unlike _134217730_). 

Even if _134217728_ casted to `np.float32` datatype and we want to represent it as a string, we will get _134217730_
```python3
num = np.float32(134217728)
print(num) # 134217730
```
Going deeper, I found that there is a `format_float_positional` [function](https://github.com/numpy/numpy/blob/v1.24.0/numpy/core/arrayprint.py#L1130-L1219) which is responsible for representing float as a string (I don't know how do `__str__` or `__repr__`  functions work for `np.float32` but looks like they have the same behavior). It has a _unique_ parament which is *True* by default. From [documentation](https://numpy.org/doc/stable/reference/generated/numpy.format_float_positional.html#numpy.format_float_positional):
> If True, use a digit-generation strategy which gives the shortest representation which uniquely identifies the floating-point number from other values of the same type, by judicious rounding

So it means that _134217730_ is a shortest unique representation for _134217728_ made by **judicious** rounding (Dragon4 algorithm), which looks weird. And as the number becomes bigger inaccuracy by rounding increases.
Of course this kind of representation is more convenient when you work with small numbers, but in this case (especially if you don't about float representation at all) it misleads.

### Idea or request for content:

So I don't know is it a bug, but may be it's a good idea to make a remark in docs describing that effect starting with large numbers.",2023-04-02 22:11:23,,DOC: Representing big float32 numbers as string,['04 - Documentation']
23500,open,ngoldbaum,"### Proposed new feature or change:

Currently the buffer protocol is supported only for a limited number of legacy dtypes (see this [switch statement](https://github.com/numpy/numpy/blob/3e77f904a9cc82e03d20824425e30edcd8088484/numpy/core/src/multiarray/buffer.c#L362)).

In principle it could be possible to encode dtype information *in* the format string and simply export the buffer as a `void*` buffer, leaving it up to consumers to parse the exported bytes using the format string. Consumers of the format strings (e.g. cython or any other downstream code working with numpy's buffers) would then need to add support for new dtypes, but at least they'd have a path forward.

See the comment from @eric-wieser https://github.com/numpy/numpy/issues/4983#issuecomment-573121067 for more details on how this might work. In principle this is complementary to fixing support for datetimes, so we could also close https://github.com/numpy/numpy/issues/4983.

See also the discussion about this in the context of legacy custom dtypes: https://github.com/numpy/numpy/issues/18442.

It would probably also be worth checking for any prior discussion about this in CPython's discussion channels or any relevant PEPs.",2023-03-30 17:18:34,,ENH: Support new-style custom dtypes in the buffer protocol,"['01 - Enhancement', 'component: numpy._core', 'component: numpy.dtype']"
23496,open,peland,"```np.ma.zeros_like(np.ma.array([0,1],mask=[0,1]))```
masked_array(data=[0, --],
             mask=[False,  True],
       fill_value=999999)

This behaviour (copying the mask) is counterintuitive, dangerous (it's caught me out twice so far) and seems to be contrary to https://numpy.org/doc/stable/reference/generated/numpy.ma.zeros_like.html
",2023-03-30 11:59:40,,BUG: ma.zeros_like sets mask,"['00 - Bug', 'component: numpy.ma']"
23488,open,r-devulap,"### Describe the issue:

Using `np.arange` to produce a float16 array with elements in reverse order returns an in correct output in certain cases. 

### Reproduce the code example:

```python
>>> np.arange(2048, -1, -1, dtype='e')
array([2.048e+03, 2.047e+03, 2.046e+03, ..., 2.000e+00, 1.000e+00,
       0.000e+00], dtype=float16)
>>> np.arange(2049, -1, -1, dtype='e')
array([2048., 2048., 2048., ..., 2048., 2048., 2048.], dtype=float16)
```


### Error message:

_No response_

### Runtime information:

```
>>> np.__version__
'1.24.0'

```

```
>>> print(np.show_runtime())
[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2',
                                'AVX512F',
                                'AVX512CD',
                                'AVX512_SKX'],
                      'not_found': ['AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'SkylakeX',
  'filepath': '/home/raghuveer/anaconda3/envs/np-py39/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so',
  'internal_api': 'openblas',
  'num_threads': 20,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.21'}]
None

```

### Context for the issue:

_No response_",2023-03-29 19:30:59,,BUG: np.arange produces wrong output for float16,['00 - Bug']
23483,open,Mark799,"### Proposed new feature or change:

I've written a code to compare two floats (or arrays of floats), and determine whether these are equal within a given tolerance. I've called this function 'float_equal'.

I believe that this function is an improvement over the currently existing 'isclose' and 'allclose' functions. Therefore I would like to share it with the Numpy community. There are two reasons why this might be an improvement: 1) it runs faster than the currently existing functions: about 2 to 3 times faster. 2) the definition for the tolerance is more intuitive. 

Perhaps the code is not perfect, but this could still be interesting for the Numpy developers. see the function below:

    def float_equal(A, B, size=1.0, digits=8):

        '''
        Returns True if two floats / arrays are element-wise equal within a tolerance.
        The tolerance is determined based on the 'size' and 'digits'. Size refers to the order of magnitude for A and B (should not be too big). This value is adjusted automatically if A>size or B>size. Digits refers to the number of digits used to compare the two numbers (in scientific notation). 

        Parameters:

        A, B: float or array_like
            Input floats/arrays to compare
        size: float
            The order of magnitude for A and B (see Notes).
        digits: int
            The number of digits to check (in scientific notation) (see Notes).

        Returns:

        float_equal : bool
            Returns True if the two floats / arrays are equal within the given tolerance; False otherwise.

        '''

        from numpy import array

        # Convert input to flat arrays
        A=array([A]).flatten()
        B=array([B]).flatten()

        # Adjust order of magnutide if needed
        size=max(max(A),max(B),size)

        # Determine value that is considered zero / neglectable
        small=size*10**-digits/2

        # Calculate difference between A and B
        diff=max(abs(A-B))

        return diff<small

To prove it runs faster for small arrays, I have run the following test:

    from numpy import arange,isclose,allclose
    import time
    from random import uniform

    # Generate floats

    A_float=3.659823649325679238739456
    B_float=3.659823659325679238739456

    n=10
    A_array=arange(n,dtype='float')
    B_array=arange(n,dtype='float')

    for i in range(n):
        A_array[i]+=uniform(0,2e-8)
        B_array[i]+=uniform(0,2e-8)

    # Comparing float_equal to isclose

    iterations=int(1e5)

    start = time.time()

    for i in range(iterations):
        isclose(A_float,B_float)

    end = time.time()
    print('isclose:    ',end - start)

    ###

    start = time.time()

    for i in range(iterations):
        float_equal(A_float,B_float)

    end = time.time()
    print('float_equal:',end - start)

    # Comparing float_equal to allclose

    iterations=int(1e5)

    start = time.time()

    for i in range(iterations):
        allclose(A_array,B_array)

    end = time.time()
    print('allclose:   ',end - start)

    ###

    start = time.time()

    for i in range(iterations):
        float_equal(A_array,B_array)

    end = time.time()
    print('float_equal:',end - start)

Giving me the following result:

    isclose:     2.5732486248016357
    float_equal: 0.8317923545837402

    allclose:    2.3336801528930664
    float_equal: 1.1338562965393066
",2023-03-29 14:29:31,,Possible improvement for isclose / allclose,['unlabeled']
23479,open,Strilanc,"### Describe the issue:

`np.from_file` only works on files that correspond to binary blobs on disk. It fails on pipes such as stdin. It also fails for named pipes on unix. The issue appears to be that `from_file` is attempting to compute a size to read, **even if the size was specified by the call**.

The correct behavior, when no file size is available and a count is provided, would be to allocate a buffer of the requested size and then fill it from the file. If the file ends before the buffer is full, raise an exception. If no count is provided and a filesize is not available, incur the cost of resizing the buffer as needed and copying to a perfectly sized buffer at the end.

### Reproduce the code example:

```python
import sys
import numpy as np
read_single_byte = np.fromfile(sys.stdin, dtype=np.uint8, count=1)
```


### Error message:

```shell
Traceback (most recent call last):
  File ""bug_repro.py"", line 3, in <module>
    read_single_byte = np.fromfile(sys.stdin, dtype=np.uint8, count=1)
OSError: obtaining file position failed
```


### Runtime information:

```
$ python -c ""import numpy; print(numpy.show_runtime())""

[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2',
                                'AVX512F',
                                'AVX512CD',
                                'AVX512_SKX'],
                      'not_found': ['AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'SkylakeX',
  'filepath': '{redacted}/lib/python3.10/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so',
  'internal_api': 'openblas',
  'num_threads': 12,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.21'}]
None
```

### Context for the issue:

I ran into this when attempting to modify code to read from either a given file name or from stdin. This bug is forcing me to jump through hoops conditional on where the data is being read from.",2023-03-28 21:28:55,,ENH: np.fromfile should be able to support when reading from sys.stdin,['00 - Bug']
23478,open,idokazma,"### Proposed new feature or change:

In the FFT algorithm, the first sample is considered to be at t=0.
This means that even that *ALL* the vectors that will go inside are asymmetric. Therefore, all the answers will be complex.

This is mathmatically true, but when you learn for example the FFT of a rect function (lets say, np.ones(N)), you ""expect"" to get a sinc function (which is Real only) but you get a complex answer. Again, this is because the FFT is done on signal that STARTS from zero, and is not symmetric around zero effectivly.

If you could insert a flag that can make you do an FFT as if the signal was symmetric around zero (t=-n/2 to t=n/2) and cancel this insertion of phase.

https://www.linkedin.com/posts/ido-kazma-596310174_signalprocessing-fft-numpy-activity-6922788016688103424-4PPe?utm_source=share&utm_medium=member_desktop

![image](https://user-images.githubusercontent.com/46976101/228369267-dccb0620-6f2b-45a0-a2c2-4b3643cc716f.png)
![image](https://user-images.githubusercontent.com/46976101/228369310-847487c1-6cdd-4d79-ad64-6ac2a3538abb.png)
",2023-03-28 21:19:47,,ENH: Perform FFT over a signal as if it was centered around zero,['unlabeled']
23442,open,cgobat,"### Describe the issue:

When a user creates a `vectorize`  object from a function that returns a string as its output, and specifies the function's output type(s) using the `otypes` argument, string typecode specifiers (e.g. `""U10""` for a 10-character string) of any length cause the returned strings to be truncated to 1 character (i.e. `np.dtype(""<U1"")`. The same things happens with bytes (typecode `""S""`, with any length specified). In order to make it work, one must either not specify any `otypes` (omit the argument), or use `""O""` to get a generic `object` dtype.

It seems this issue is possibly related to #2485 and/or [StackOverflow: How to explicitly specify the output's string length in numpy.vectorize](https://stackoverflow.com/questions/12137018/how-to-explicitly-specify-the-outputs-string-length-in-numpy-vectorize), but I can't say for sure. It seems odd that `otypes` ignores explicit  length declarations.

### Reproduce the code example:

```python
import numpy as np

def make_10char_str(n: int) -> str:
    """"""Returns a string version of the input integer, with spaces to the
       right to left-justify it and pad the string out to 10 characters""""""
    return f""{n:<10d}""

vector_str_func = np.vectorize(make_str_from_number, signature=""()->()"", otypes=[""<U10""]) # ""<U10"" should correspond to a 10-character unicode str

print(vector_str_func([1, 24, 365, 4096])) # expected output is array([['1         ', '24        ', '365       ', '4096      '], dtype='<U10')
```
Output:
```
array(['1', '2', '3', '4'], dtype='<U1')
```

### Runtime information:

`np.__version__` is 1.24.2. `sys.version` is
```
3.8.16 | packaged by conda-forge | (default, Feb  1 2023, 16:01:55) 
[GCC 11.3.0]
```

Output of `np.show_runtime()` is:
```
[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'AVX2'],
                      'not_found': ['F16C',
                                    'FMA3',
                                    'AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Haswell',
  'filepath': '/home/cgobat/miniconda3/envs/.../lib/libopenblasp-r0.3.21.so',
  'internal_api': 'openblas',
  'num_threads': 4,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.21'}]
```

### Context for the issue:

This issue can cause problems because if users want to specify their function's `otypes` explicitly, they are forced to use `""O""`, which other operations that expect to see string dtype outputs (rather than `np.object`) may not be able to handle without additional processing.",2023-03-23 20:53:27,,"BUG: `vectorize` truncates string outputs to 1 character, even with explicitly-specified `otypes`","['00 - Bug', 'component: numpy.lib']"
23434,open,mjclarke94,"### Proposed new feature or change:

`np.interp` takes a 1D list of scalars as an input `y`. When one wishes to interpolate multiple variables with the same basis/lookup points, `np.interp()` must be called repeatedly. The majority of the cost associated with this is the initial finding of what indexes should be looked at and the ratios of [n] and [n+1] required to get the interpolated value, so this is wasteful.

A toy example of the above:
```
In [1]: import numpy as np

In [2]: N = 10001
   ...: x = np.linspace(0, 1, N)
   ...: y1 = np.sort(np.random.normal(0, 1, N))
   ...: y2 = np.sort(np.random.uniform(0, 1, N))

In [3]: xr = np.random.random(1000000)

In [4]: %%timeit
   ...: y1i = np.interp(xr, x, y1)
   ...: y2i = np.interp(xr, x, y2)
   ...:
   ...:
131 ms ± 357 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

In [5]: %%timeit
   ...: ycomplex = y1 + (complex(imag=1) * y2)
   ...: ycomplexi = np.interp(xr, x, ycomplex)
   ...: y1ic = np.real(ycomplexi)
   ...: y2ic = np.imag(ycomplexi)
   ...:
   ...:
65.3 ms ± 190 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

By combining arrays y1 and y2 in to a complex number, interpolating _that_, then decomposing them, the same result is achieved in approximately half the time, indicating that the majority of the runtime arises from working out what indexes and ratios are required for each `xp`.

There are two issues with this:
1) It is limited to pairs of y vectors. In the event you have 1000 y vectors, there is still a ~500X speedup still on the table (assuming the limiting step as described above)
2) It is completely ridiculous, and anyone suggesting casting pairs of vectors to a complex number to speed up a production codebase will quite rightly be dismissed as having gone insane.

Both of these problems can be addressed by allowing the input for `y` to be an ndarray.",2023-03-22 15:46:26,,ENH: Interpolation should be able to interpolate vectorised inputs,['unlabeled']
23431,open,dobos,"### Proposed new feature or change:

The shape of a subarray can easily be determined by slicing/dicing/fancy indexing an existing array by actually performing the slicing and dicing then looking at .shape. What if I just want to calculate the shape of the results without allocating any memory. I suggest to add a dummy array object for the purposes of ""shape arithmetic"" only. One could do something like:

```
idx = # some valid index, slice, etc. into an array
dummy = np.dummy_array((12, 25), dtype=float) # No memory is allocated
print(dummy[idx].shape) # No actual processing or memory allocation, just shape calculation.
```

Probably a function for this purpose is better than an entire dummy array class. I also understand that in many cases the shape cannot be directly calculated but the loops need to be performed. Still, I think it can be done without allocating any memory.",2023-03-21 04:27:37,,ENH: Add routine to determine shape of subarray from shape of input array and index only.,['unlabeled']
23419,open,tom-pytel,"### Describe the issue:

When comparing np.void against a memoryview results in ""TypeError: Cannot compare structured with unstructured void arrays. (unreachable error, please report to NumPy devs.)"".

### Reproduce the code example:

```python
import numpy as np
memoryview(np.void(4)) == np.void(4)  # this works
np.void(4) == memoryview(np.void(4))  # this excepts
```


### Error message:

```shell
>>> np.void(4) == memoryview(np.void(4))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: Cannot compare structured with unstructured void arrays. (unreachable error, please report to NumPy devs.)
```


### Runtime information:

>>> import sys, numpy; print(numpy.__version__); print(sys.version)
1.24.2
3.10.7 (tags/v3.10.7-dirty:6cc6b13308, Sep 12 2022, 08:30:52) [GCC 9.4.0]

>>> print(numpy.show_runtime())
WARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information
[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'AVX2'],
                      'not_found': ['F16C',
                                    'FMA3',
                                    'AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}}]
None


### Context for the issue:

Not a priority, just doing my duty and reporting to NumPy devs as requested in the error message.",2023-03-17 13:59:41,,BUG: comparison of np.void with memoryview hits unreachable error,['00 - Bug']
23398,open,Beliavsky,"### Proposed new feature or change:

numpy.corrcoef returns an ndarray. I suggest adding a way to return a scalar for the common case where you want to compute the correlation between two 1-D arrays, instead of having every user write a function such as

```
import numpy as np

def correl(x,y):
    return np.corrcoef(x,y)[0,1]
```

",2023-03-15 19:19:25,,ENH: compute scalar correlation coefficient,['unlabeled']
23383,open,Gabriel-Kissin,"`np.spacing` and `np.nextafter` are designed for use with floats. They tell you the gap between adjacent values, and what the next value is.  

So for example, 
`np.nextafter(np.float16(1), np.float16(2)), np.spacing(np.float16(1))`
returns 
`(1.001, 0.000977)`.

However on integers this doesn't perform as expected. [Of course there is no real use for these functions with integers, where the spacing is always 1 (except for at max and min of dtype) and next adjacent value is always 1 greater (except for at max of dtype). However, it should nonetheless give correct results, so it can easily be used in a situation where either a float or an int may be passed.]

**Setup the problem:**  
```
import numpy as np
import pandas as pd

dtypes_ints = [  
    int,
    np.int8,	    # Byte (-128 to 127)
    np.int16,	    # Integer (-32768 to 32767)
    np.int32,	    # Integer (-2147483648 to 2147483647)
    np.int64,	    # Integer (-9223372036854775808 to 9223372036854775807)
    np.uint8,	    # Unsigned integer (0 to 255)
    np.uint16,	    # Unsigned integer (0 to 65535)
    np.uint32,	    # Unsigned integer (0 to 4294967295)
    np.uint64,	    # Unsigned integer (0 to 18446744073709551615)
    ]
dtypes_floats = [
    float,
    np.float16,	    # Half precision float: sign bit, 5 bits exponent, 10 bits mantissa
    np.float32,	    # Single precision float: sign bit, 8 bits exponent, 23 bits mantissa
    np.float64,	    # Double precision float: sign bit, 11 bits exponent, 52 bits mantissa
    ]

```


**And the problem:**  
```
spacing_info = [(dtype, 
                    np.nextafter(dtype(1)  , dtype(2)), 
                    dtype(np.nextafter(dtype(1)  , dtype(2))),
                    np.spacing(dtype(1)), )
                for dtype in  dtypes_floats + dtypes_ints ]

pd.set_option(""display.precision"", 18)

pd.DataFrame(spacing_info, 
    columns=['dtype', 
             'next after 1 (np)', 
             'next after 1 as this dtype',
             'np.spacing', 
             ])
```

**Gives this:**  
![image](https://user-images.githubusercontent.com/118690308/224964163-3cef43f8-7dfc-48d8-a95a-a2af0181da55.png)

As can be seen, the `next after 1` column for the rows of integers seems to give the value of one of the types of floats, a different type for each type of integer. When the answer should be `2` for all of them.   

And the same problem is reflected in the `np.spacing` column.  ",2023-03-14 10:01:10,,np.spacing and np.nextafter on integers,['unlabeled']
23379,open,0-wiz-0,"### Describe the issue:

numpy builds fine on NetBSD with Python 3.11.2. However, when running the self tests, I see an error.
```
/usr/pkg/bin/python3.11 -c ""import numpy; numpy.test()""
Traceback (most recent call last):
  File ""/scratch/wip/py-numpy/work/.destdir/usr/pkg/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>
    from . import multiarray
  File ""/scratch/wip/py-numpy/work/.destdir/usr/pkg/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>
    from . import overrides
  File ""/scratch/wip/py-numpy/work/.destdir/usr/pkg/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>
    from numpy.core._multiarray_umath import (
ImportError: /scratch/wip/py-numpy/work/.destdir/usr/pkg/lib/python3.11/site-packages/numpy/core/_multiarray_umath.so: Undefined PLT symbol ""log2l"" (symnum = 437)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/scratch/wip/py-numpy/work/.destdir/usr/pkg/lib/python3.11/site-packages/numpy/__init__.py"", line 141, in <module>
    from . import core
  File ""/scratch/wip/py-numpy/work/.destdir/usr/pkg/lib/python3.11/site-packages/numpy/core/__init__.py"", line 49, in <module>
    raise ImportError(msg)
ImportError: 

IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!

Importing the numpy C-extensions failed. This error can happen for
many reasons, often due to issues with your setup or how NumPy was
installed.

We have compiled some common reasons and troubleshooting tips at:

    https://numpy.org/devdocs/user/troubleshooting-importerror.html

Please note and check the following:

  * The Python version is: Python3.11 from ""/usr/pkg/bin/python3.11""
  * The NumPy version is: ""1.24.2""

and make sure that they are the versions you expect.
Please carefully study the documentation linked above for further help.

Original error was: /scratch/wip/py-numpy/work/.destdir/usr/pkg/lib/python3.11/site-packages/numpy/core/_multiarray_umath.so: Undefined PLT symbol ""log2l"" (symnum = 437)

```


### Reproduce the code example:

```python
Try running the self tests for numpy 1.24.2 on NetBSD.
```


### Error message:

```shell
See above.
```


### Runtime information:

Can't print that due to the import error shown above.
Python 3.11.2, numpy 1.24.2.

### Context for the issue:

This is obviously a bug in NetBSD (http://gnats.netbsd.org/cgi-bin/query-pr-single.pl?number=53234) but perhaps you can suggest a workaround.
This problem is new in the 1.24.x series of numpy, 1.23.5 builds and tests (mostly) fine in the same environment:
8 failed, 18561 passed, 782 skipped, 1306 deselected, 41 xfailed, 2 xpassed, 59 warnings in 199.07s (0:03:19)
",2023-03-13 17:33:28,,"BUG: NetBSD: fails due to missing log2l(), log1pl(), and expm1l()",['00 - Bug']
23366,open,pauljurczak,"### Issue with current documentation:

Page https://numpy.org/doc/stable/reference/arrays.dtypes.html recommends using `?` to declare Boolean dtype and `b` for (signed) byte. Page https://numpy.org/doc/stable/reference/arrays.interface.html#arrays-interface recommends `b for Boolean (integer type where all values are only True or False)`.

### Idea or request for content:

Perhaps `'?'   boolean` could be added to https://numpy.org/doc/stable/reference/arrays.interface.html#arrays-interface and meaning of `b` edited?",2023-03-09 22:37:57,,DOC: Confusion about Boolean dtype,['04 - Documentation']
23362,open,seberg,"Yesterday there was a bit of a discussion around adding an env variable to allow runtime selection (mainly opt out of lower precision SVML).  I need to think it a bit over, but overall, I would be fine with doing that.

There was also a discussion that a proper solution may be to allow selecting this maybe as a context (or similar).  I don't think that is a small project, but I am not sure it is huge either.

I am not planning on attacking it, but if e.g. @mattip is interested in it together with @seiko2plus it may be a well scoped project, so I thought I open an issue to see if things drop out.

What would it take?  Some internal reorganization for sure:
1. ~We probably need to fix `np.errstate` to be a context variable (that is needed anyway).  I think we could then expand it to tag on a ""float precision"" context?~ `np.errstate` is now a contextvar with gh-23936.  This makes it now easier to extend and absolutely no worry to fetch the internal `npy_extobj` once and pass it around to anywhere needed.  (It should be very fast to fetch now, so even multiple fetches are fine in practice.) 
2. We need to include that information in the `context` passed into ufuncs and `get_loop`.
3. We need to actually use it to dispatch :).

The first thing is well scoped, but a mid-sized project (not too hard really).  The second thing should be very easy afterwards, but may need refactoring of the `errstate` handling as a follow-up to the first.

The second point is easy enough.

For the third point, we end up here currently: https://github.com/numpy/numpy/blob/main/numpy/core/src/umath/legacy_array_method.c#L195
that function *can* be customized freely for each ufunc.  And the float context is even passed through to the inner-loop (but you need to customize that function to use the new-style inner-loop signature).

Now, we need to customize the `get_loop()`, which is a bit annoying in the current machinery.  But I am not sure it is *that* bad: Matti effectively did the same for the `ufunc.at` loop specialization already.

Note that I am happy to leave `ufunc->functions[i]` function pointer that we currently use/fill at `NULL`.  Nobody should be using that!  (This part might need very managable followups if you want to keep supporting `pnumpy` style things, but I suspect that is a bridge that can be crossed when someone asks for it.)

(I am happy to close this, if there is no explicit interest.)",2023-03-09 09:40:12,,DISCUSS: Runtime precision selection drive?,['15 - Discussion']
23353,open,hchau630,"### Proposed new feature or change:

Currently `numpy.ma.unique` only has the `return_index` and `return_inverse`, in contrast to `numpy.unique` that has `return_index`, `return_inverse`, `return_count`, `equal_nan`, and `axis`. I'm not sure whether or not `equal_nan` makes sense as a keyword argument for `numpy.ma.unique` (personally I always use `equal_nan=True`), but I think it would be great to have `return_count` and `axis` arguments in `numpy.ma.unique`. My guess is that this should be relatively easy to implement once the bug in #23286 is fixed.",2023-03-06 21:24:37,,ENH: Add missing keyword arguments to numpy.ma.unique,['unlabeled']
23350,open,maarten-vandenberg,"### Describe the issue:

Hi,

we are using numpy.fromstring to read data from a string. Usually, the string we pass to fromstring does not contain a trailing delimiter (`"",""`). However, in a recent application it did: the data in the string was followed by a trailing delimiter and whitespace. We find that in that case fromstring returns an additional -1.0 in the resulting array.
```python
import numpy as np

x = np.fromstring(""0, "", sep="","")
print(x)
# array([ 0., -1.])
```
I would expect fromstring to simply return `array([0.])`. Hopefully you can help me find what is going on here (and if it's an actual bug), many thanks.

### Reproduce the code example:

```python
import numpy as np

x = np.fromstring(""0, "", sep="","")
print(x)
# array([ 0., -1.])
```


### Error message:

```shell
There is no error message
```


### Runtime information:

sys.version:
3.11.0 | packaged by conda-forge | (main, Oct 25 2022, 06:12:32) [MSC v.1929 64 bit (AMD64)]
numpy.__version__:
1.24.2
numpy.show_runtime()
WARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information
[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2',
                                'AVX512F',
                                'AVX512CD',
                                'AVX512_SKX',
                                'AVX512_CLX'],
                      'not_found': ['AVX512_CNL', 'AVX512_ICL']}}]
None

### Context for the issue:

_No response_",2023-03-06 15:35:08,,BUG: numpy.fromstring returns additional -1. not present in the string when it contains a trailing delimiter ,['00 - Bug']
23349,open,shangke1988,"### Describe the issue:

np.ascontiguousarray don't process negative strides of single channle CWH data


### Reproduce the code example:

```python
import numpy as np
import torch
a=np.zeros((1,10,20))[::-1]
b=np.ascontiguousarray(a)
b.strides
c=torch.from_numpy(b)
```


### Error message:

```shell
>>> a=np.zeros((1,10,20))[::-1]
>>> b=np.ascontiguousarray(a)
>>> b.strides
(-1600, 160, 8)
>>> c=torch.from_numpy(b)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ValueError: At least one stride in the given numpy array is negative, and tensors with negative strides are not currently supported. (You can probably work around this by making a 
copy of your array  with array.copy().)
```


### Runtime information:

>>> import sys, numpy; print(numpy.__version__); print(sys.version)
1.23.5
3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]

### Context for the issue:

_No response_",2023-03-06 15:17:29,,ENH: `ascontiguousarray` leaves negative strides for length-1 dimensions,['01 - Enhancement']
23338,open,charris,"In `crackfortran.py` line 938.
```   
nameargspattern = re.compile(
    r'\s*(?P<name>\b[\w$]+\b)\s*(@\(@\s*(?P<args>[\w\s,]*)\s*@\)@|)\s*((result(\s*@\(@\s*(?P<result>\b[\w$]+\b)\s*@\)@|))|(bind\s*@\(@\s*(?P<bind>.*)\s*@\)@))*\s*\Z', re.I)

This part of the regular expression may cause exponential backtracking on strings containing many repetitions of '@)@bind@(@'.
CodeQL
```",2023-03-04 16:30:04,,Crackfortran regular expression could use improvement,['component: numpy.f2py']
23330,open,ngoldbaum,"### Describe the issue:

I was debugging some memory leaks in `stringdtype` and noticed that numpy has some minor memory leaks coming from the ufunc object creation. I ended up adding some debug prints to `PyUFunc_FromFuncAndDataAndSignatureAndIdentity` and `ufunc_dealloc`:

```diff
diff --git a/numpy/core/src/umath/ufunc_object.c b/numpy/core/src/umath/ufunc_object.c
index a159003de..1cbdab3ac 100644
--- a/numpy/core/src/umath/ufunc_object.c
+++ b/numpy/core/src/umath/ufunc_object.c
@@ -5239,8 +5239,8 @@ PyUFunc_FromFuncAndDataAndSignatureAndIdentity(PyUFuncGenericFunction *func, voi
     else {
         ufunc->name = name;
     }
+    printf(""alloc %s\n"", ufunc->name);
     ufunc->doc = doc;

     ufunc->op_flags = PyArray_malloc(sizeof(npy_uint32)*ufunc->nargs);
     if (ufunc->op_flags == NULL) {
         Py_DECREF(ufunc);
@@ -5682,6 +5682,7 @@ PyUFunc_RegisterLoopForType(PyUFuncObject *ufunc,
 static void
 ufunc_dealloc(PyUFuncObject *ufunc)
 {
+    printf(""dealloc %s\n"", ufunc->name);
     PyObject_GC_UnTrack((PyObject *)ufunc);
     PyArray_free(ufunc->core_num_dims);
     PyArray_free(ufunc->core_dim_ixs);
```

I then did `python -c ""import numpy"" > out.txt` and used the following script to process the output:

```python
with open('out.txt', 'r') as f:
    lines = f.readlines()

lines = list(map(str.strip, lines))

alloc = []
dealloc = []

for line in lines:
    if line.startswith(""alloc""):
        alloc.append(line.lstrip('alloc '))
    else:
        dealloc.append(line.lstrip('dealloc '))

no_dealloc = set(alloc) - set(dealloc)
```

`no_dealloc` has 54 entries:

<details>

```
{'_ones_like',
 'bitwise_and',
 'bitwise_or',
 'bitwise_xor',
 'brt',
 'bsolute',
 'dd',
 'deg2rad',
 'degrees',
 'det',
 'dexp',
 'divide',
 'divmod',
 'eft_shift',
 'eig',
 'eigh_lo',
 'eigh_up',
 'eigvals',
 'eigvalsh_lo',
 'eigvalsh_up',
 'eil',
 'equal',
 'ess',
 'ess_equal',
 'exp',
 'exp2',
 'expm1',
 'floor',
 'floor_divide',
 'gical_and',
 'gical_or',
 'greater',
 'greater_equal',
 'invert',
 'ip',
 'isfinite',
 'isnan',
 'matmul',
 'maximum',
 'minimum',
 'multiply',
 'negative',
 'njugate',
 'not_equal',
 'positive',
 'power',
 'reciprocal',
 'remainder',
 'right_shift',
 'rint',
 'sign',
 'sqrt',
 'square',
 'subtract'}
```

</details>

I don't immediately see what's special about these versus the other ufuncs that are properly deallocated. I figured I'd open this issue in case anyone else has a better idea and for future people wading through valgrind output.

### Error message:

Sample valgrind output from the memory leak:

```shell

==676897== 32 bytes in 1 blocks are possibly lost in loss record 13 of 548
==676897==    at 0x4848899: malloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==676897==    by 0x4A05C34: _PyMem_RawMalloc (obmalloc.c:99)
==676897==    by 0x4A08DFA: _PyMem_DebugRawAlloc (obmalloc.c:2471)
==676897==    by 0x4A08ECF: _PyMem_DebugRawMalloc (obmalloc.c:2504)
==676897==    by 0x4A06AB4: PyMem_RawMalloc (obmalloc.c:572)
==676897==    by 0x6B354D6: PyUFunc_FromFuncAndDataAndSignatureAndIdentity (ufunc_object.c:5244)
==676897==    by 0x6AF81EC: InitOperators (__umath_generated.c:1926)
==676897==    by 0x6B03172: initumath (umathmodule.c:263)
==676897==    by 0x6A9FE2F: PyInit__multiarray_umath (multiarraymodule.c:5196)
```


### Runtime information:
```
1.25.0.dev0+842.gdc6852934
3.10.9 (main, Mar  2 2023, 11:31:32) [GCC 11.3.0]
[{'numpy_version': '1.25.0.dev0+842.gdc6852934',
  'python': '3.10.9 (main, Mar  2 2023, 11:31:32) [GCC 11.3.0]',
  'uname': uname_result(system='Linux', node='rous', release='5.19.0-32-generic', version='#33~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Jan 30 17:03:34 UTC 2', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}}]

```",2023-03-03 18:19:01,,MAINT: Some ufuncs aren't freed at exit (NumPy has no module teardown yet),"['01 - Enhancement', '03 - Maintenance']"
23315,open,InessaPawson,"Develop a policy for not including copyright notices in the NumPy code base. Once it's approved, the policy should be added to https://numpy.org/devdocs/dev/index.html. 
Refer to the meeting notes from [2023-02-15](https://github.com/numpy/archive/blob/main/community_meetings/community-2023-02-15.md) and [2023-03-01](https://github.com/numpy/archive/blob/main/community_meetings/community-2023-03-01.md) for the related discussions.",2023-03-02 18:42:06,,DOC: Develop and add to the docs a policy for not including copyright notices in the code base,['04 - Documentation']
23313,open,nheinsdorf,"### Describe the issue:

Hi, 
I'm using bincount to sum up complex values. However, the method does not accept complex valued weights, however that would be a useful usecase for me. 

### Reproduce the code example:

```python
import numpy as np
w = 1j * array([0.3, 0.5, 0.2, 0.7, 1., -0.6]) # weights
x = array([0, 1, 1, 2, 2, 2])
np.bincount(x,  weights=w)
```


### Error message:

```shell
Traceback (most recent call last):
  File ""/Users/user/PycharmProjects/project/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3378, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-74-24012315fdf7>"", line 5, in <module>
    np.bincount(x,  weights=w)
  File ""<__array_function__ internals>"", line 180, in bincount
TypeError: Cannot cast array data from dtype('complex128') to dtype('float64') according to the rule 'safe'
```


### Runtime information:

1.23.3
3.9.6 (default, Oct 18 2022, 12:41:40) 
[Clang 14.0.0 (clang-1400.0.29.202)]


### Context for the issue:

Happy to make a pull request myself (if I can do it), but I actually wasn't able to find the definition of the bincount method in the repo. ",2023-03-02 06:22:07,,bincount does not accept complex valued weights,['00 - Bug']
23305,open,knwng,"### Describe the issue:

Hi, I encountered a seg fault using ndarray with SharedMemory as its buffer, as discussed in https://discuss.python.org/t/should-sharedmemory-close-be-called-explicitly-or-we-should-just-rely-on-del/24281/1

### Reproduce the code example:

```python
import numpy as np
from multiprocessing.shared_memory import SharedMemory

class Block(object):
    def __init__(self, nbytes, shape, dtype):
        self._nbytes = nbytes
        self._shape = shape
        self._dtype = dtype

    def get_ndarray(self):
        shm = SharedMemory(name=self._shm_name)
        return np.ndarray(self._shape, dtype=self._dtype, buffer=shm.buf)

    def set_ndarray(self, arr):
        shm = SharedMemory(create=True, size=self._nbytes)
        shm_arr = np.ndarray(self._shape, dtype=self._dtype, buffer=shm.buf)
        shm_arr[:] = arr[:]
        self._shm_name = shm.name

arr = np.random.rand(1, 100)
block = Block(arr.nbytes, arr.shape, arr.dtype)
block.set_ndarray(arr)
arr_on_shm = block.get_ndarray()
print(f'arr_on_shm: {arr_on_shm}')
```


### Error message:

```shell
[1]    13006 segmentation fault  python3 test_shm_numpy.py
```


### Runtime information:

>>> import sys, numpy; print(numpy.__version__); print(sys.version)
1.24.2
3.11.1 (main, Dec 23 2022, 09:39:26) [Clang 14.0.0 (clang-1400.0.29.202)]
>>> print(numpy.show_runtime())
WARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information
[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}}]
None

### Context for the issue:

_No response_",2023-03-01 10:58:18,,BUG: Got Segmentation Fault when use-after-free a ndarray using SharedMemory as its buffer,['00 - Bug']
23304,open,magicse,"### Describe the issue:
Arch armv7l Alpine Docker
 pip install --no-cache numpy on RAMv7 i get libs for ARMv8 arch

```bash
bash-5.1# pip install --no-cache numpy
Collecting numpy
  Downloading numpy-1.24.2.tar.gz (10.9 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.9/10.9 MB 1.0 MB/s eta 0:00:00
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Building wheels for collected packages: numpy
  Building wheel for numpy (pyproject.toml) ... done
  Created wheel for numpy: filename=numpy-1.24.2-cp310-cp310-linux_armv7l.whl size=6850133 sha256=fb8de3244e8c7987ae7a895e8f4dc64370848ceaf868363d110c3c1a94392620
  Stored in directory: /tmp/pip-ephem-wheel-cache-i62n9ysv/wheels/31/42/8e/88540c3411ed4734c7fd06056942e82136135724593ecec35a
Successfully built numpy
Installing collected packages: numpy
Successfully installed numpy-1.24.2
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
```

```bash
readelf -A /usr/local/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-arm-linux-gnueabihf.so
```

Attribute Section: aeabi
File Attributes with cpu 8-A and arch v8 
```bash
  Tag_CPU_name: ""8-A""
  Tag_CPU_arch: v8
  Tag_CPU_arch_profile: Application
  Tag_ARM_ISA_use: Yes
  Tag_THUMB_ISA_use: Thumb-2
  Tag_FP_arch: FP for ARMv8
  Tag_Advanced_SIMD_arch: NEON for ARMv8
  Tag_ABI_PCS_wchar_t: 4
  Tag_ABI_FP_denormal: Needed
  Tag_ABI_FP_exceptions: Needed
  Tag_ABI_FP_number_model: IEEE 754
  Tag_ABI_align_needed: 8-byte
  Tag_ABI_enum_size: int
  Tag_ABI_VFP_args: VFP registers
  Tag_CPU_unaligned_access: v6
  Tag_ABI_FP_16bit_format: IEEE 754
  Tag_MPextension_use: Allowed
  Tag_Virtualization_use: TrustZone and Virtualization Extensions
```
For example Python give normal attributes for armv7
readelf -A /usr/local/bin/python3
```bash
Attribute Section: aeabi
File Attributes
  Tag_CPU_name: ""7-A""
  Tag_CPU_arch: v7
  Tag_CPU_arch_profile: Application
  Tag_ARM_ISA_use: Yes
  Tag_THUMB_ISA_use: Thumb-2
  Tag_FP_arch: VFPv3-D16
  Tag_ABI_PCS_wchar_t: 4
  Tag_ABI_FP_rounding: Needed
  Tag_ABI_FP_denormal: Needed
  Tag_ABI_FP_exceptions: Needed
  Tag_ABI_FP_number_model: IEEE 754
  Tag_ABI_align_needed: 8-byte
  Tag_ABI_enum_size: int
  Tag_ABI_VFP_args: VFP registers
  Tag_ABI_optimization_goals: Aggressive Size
  Tag_CPU_unaligned_access: v6
```
```bash
  Tag_CPU_name: ""8-A""
/usr/local/lib/python3.10/site-packages/numpy/core/_simd.cpython-310-arm-linux-gnueabihf.so
  Tag_CPU_name: ""7-A""
/usr/local/lib/python3.10/site-packages/numpy/core/_rational_tests.cpython-310-arm-linux-gnueabihf.so
  Tag_CPU_name: ""8-A""
/usr/local/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-arm-linux-gnueabihf.so
  Tag_CPU_name: ""7-A""
/usr/local/lib/python3.10/site-packages/numpy/core/_operand_flag_tests.cpython-310-arm-linux-gnueabihf.so
  Tag_CPU_name: ""7-A""
/usr/local/lib/python3.10/site-packages/numpy/core/_struct_ufunc_tests.cpython-310-arm-linux-gnueabihf.so
  Tag_CPU_name: ""7-A""
/usr/local/lib/python3.10/site-packages/numpy/core/_multiarray_tests.cpython-310-arm-linux-gnueabihf.so
  Tag_CPU_name: ""8.2-A""
/usr/local/lib/python3.10/site-packages/numpy/core/_umath_tests.cpython-310-arm-linux-gnueabihf.so
```

### Reproduce the code example:

```python
bash-5.1# python3 
Python 3.10.7 (main, Nov 24 2022, 13:02:43) [GCC 11.2.1 20220219] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy
Segmentation fault
bash-5.1#
```


### Error message:

```shell
Segmentation fault 
```


### Runtime information:

Also debug of import numpy I see problem with lib _multiarrary

```
Starting program: /usr/local/bin/python 
Python 3.10.7 (main, Nov 24 2022, 13:02:43) [GCC 11.2.1 20220219] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy

Program received signal SIGSEGV, Segmentation fault.
sysv_lookup (s=s@entry=0x75fe97f7 ""__libc_start_main"", h=h@entry=24641422, dso=dso@entry=0x7dffbdc8) at ldso/dynlink.c:249
249             char *strings = dso->strings;
(gdb) bt
#0  sysv_lookup (s=s@entry=0x75fe97f7 ""__libc_start_main"", h=h@entry=24641422, dso=dso@entry=0x7dffbdc8) at ldso/dynlink.c:249
#1  0x75fb87e4 in find_sym2 (use_deps=0, need_def=1, s=0x75fe97f7 ""__libc_start_main"", dso=0x7dffbdc8) at ldso/dynlink.c:313
#2  find_sym (dso=dso@entry=0x7dffbdc8, s=0x75fe97f7 ""__libc_start_main"", need_def=need_def@entry=1) at ldso/dynlink.c:334
#3  0x75fa6f7c in load_library (name=name@entry=0x759348a0 ""/usr/local/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-arm-linux-gnueabihf.so"", 
    needed_by=<optimized out>) at ldso/dynlink.c:1128
#4  0x75fa7f34 in dlopen (file=0x759348a0 ""/usr/local/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-arm-linux-gnueabihf.so"", mode=2)
    at ldso/dynlink.c:2089
#5  0x75e07c54 in ?? () from /usr/local/lib/libpython3.10.so.1.0
Backtrace stopped: previous frame identical to this frame (corrupt stack?)
```

### Context for the issue:

Here fragment of log file of strace before SIGFAULT for future check.
bash-5.1# strace python -c ""import numpy""

```
open(""/usr/local/lib/python3.10/site-packages/numpy/core/__pycache__/multiarray.cpython-310.pyc"", O_RDONLY|O_LARGEFILE|O_CLOEXEC) = 3
fcntl64(3, F_SETFD, FD_CLOEXEC)         = 0
statx(3, """", AT_STATX_SYNC_AS_STAT|AT_EMPTY_PATH, STATX_BASIC_STATS, 0x7dfed658) = -1 ENOSYS (Function not implemented)
fstat64(3, {st_mode=S_IFREG|0644, st_size=53810, ...}) = 0
ioctl(3, TIOCGWINSZ, 0x7dfed8c0)        = -1 ENOTTY (Not a tty)
_llseek(3, 0, [0], SEEK_CUR)            = 0
_llseek(3, 0, [0], SEEK_CUR)            = 0
statx(3, """", AT_STATX_SYNC_AS_STAT|AT_EMPTY_PATH, STATX_BASIC_STATS, 0x7dfed828) = -1 ENOSYS (Function not implemented)
fstat64(3, {st_mode=S_IFREG|0644, st_size=53810, ...}) = 0
mmap2(NULL, 65536, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x75698000
read(3, ""o\r\r\n\0\0\0\0\306L\355c\212\330\0\0\343\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0""..., 53811) = 53810
read(3, """", 1)                          = 0
close(3)                                = 0
mmap2(NULL, 32768, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x75778000
mmap2(NULL, 32768, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x75690000
munmap(0x75698000, 65536)               = 0
statx(AT_FDCWD, ""/usr/local/lib/python3.10/site-packages/numpy/core"", AT_STATX_SYNC_AS_STAT, STATX_BASIC_STATS, 0x7dfecae8) = -1 ENOSYS (Function not implemented)
stat64(""/usr/local/lib/python3.10/site-packages/numpy/core"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0
statx(AT_FDCWD, ""/usr/local/lib/python3.10/site-packages/numpy/core/overrides.py"", AT_STATX_SYNC_AS_STAT, STATX_BASIC_STATS, 0x7dfec8c8) = -1 ENOSYS (Function not implemented)
stat64(""/usr/local/lib/python3.10/site-packages/numpy/core/overrides.py"", {st_mode=S_IFREG|0644, st_size=7297, ...}) = 0
statx(AT_FDCWD, ""/usr/local/lib/python3.10/site-packages/numpy/core/overrides.py"", AT_STATX_SYNC_AS_STAT, STATX_BASIC_STATS, 0x7dfecb78) = -1 ENOSYS (Function not implemented)
stat64(""/usr/local/lib/python3.10/site-packages/numpy/core/overrides.py"", {st_mode=S_IFREG|0644, st_size=7297, ...}) = 0
open(""/usr/local/lib/python3.10/site-packages/numpy/core/__pycache__/overrides.cpython-310.pyc"", O_RDONLY|O_LARGEFILE|O_CLOEXEC) = 3
fcntl64(3, F_SETFD, FD_CLOEXEC)         = 0
statx(3, """", AT_STATX_SYNC_AS_STAT|AT_EMPTY_PATH, STATX_BASIC_STATS, 0x7dfeca88) = -1 ENOSYS (Function not implemented)
fstat64(3, {st_mode=S_IFREG|0644, st_size=6716, ...}) = 0
ioctl(3, TIOCGWINSZ, 0x7dfeccf0)        = -1 ENOTTY (Not a tty)
_llseek(3, 0, [0], SEEK_CUR)            = 0
_llseek(3, 0, [0], SEEK_CUR)            = 0
statx(3, """", AT_STATX_SYNC_AS_STAT|AT_EMPTY_PATH, STATX_BASIC_STATS, 0x7dfecc58) = -1 ENOSYS (Function not implemented)
fstat64(3, {st_mode=S_IFREG|0644, st_size=6716, ...}) = 0
mmap2(NULL, 32768, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x756a0000
read(3, ""o\r\r\n\0\0\0\0\306L\355c\201\34\0\0\343\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0""..., 6717) = 6716
read(3, """", 1)                          = 0
close(3)                                = 0
munmap(0x756a0000, 32768)               = 0
statx(AT_FDCWD, ""/usr/local/lib/python3.10/site-packages/numpy/core"", AT_STATX_SYNC_AS_STAT, STATX_BASIC_STATS, 0x7dfec2d0) = -1 ENOSYS (Function not implemented)
stat64(""/usr/local/lib/python3.10/site-packages/numpy/core"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0
statx(AT_FDCWD, ""/usr/local/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-arm-linux-gnueabihf.so"", AT_STATX_SYNC_AS_STAT, STATX_BASIC_STATS, 0x7dfec0b0) = -1 ENOSYS (Function not implemented)
stat64(""/usr/local/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-arm-linux-gnueabihf.so"", {st_mode=S_IFREG|0755, st_size=2374876, ...}) = 0
open(""/usr/local/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-arm-linux-gnueabihf.so"", O_RDONLY|O_LARGEFILE|O_CLOEXEC) = 3
fcntl64(3, F_SETFD, FD_CLOEXEC)         = 0
statx(3, """", AT_STATX_SYNC_AS_STAT|AT_EMPTY_PATH, STATX_BASIC_STATS, 0x7dfebc50) = -1 ENOSYS (Function not implemented)
fstat64(3, {st_mode=S_IFREG|0755, st_size=2374876, ...}) = 0
read(3, ""\177ELF\1\1\1\0\0\0\0\0\0\0\0\0\3\0(\0\1\0\0\0\0\0\0\0004\0\0\0""..., 936) = 936
mmap2(NULL, 2555904, PROT_READ|PROT_EXEC, MAP_PRIVATE, 3, 0) = 0x75420000
mmap2(0x75650000, 229376, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED, 3, 0x220000) = 0x75650000
mmap2(0x75670000, 98304, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = 0x75670000
mmap2(0x75680000, 32768, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED, 3, 0x238000) = 0x75680000
mmap2(0x75680000, 65536, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED, 3, 0x240000) = 0x75680000
close(3)                                = 0
--- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_MAPERR, si_addr=NULL} ---
+++ killed by SIGSEGV +++
Segmentation fault
```",2023-03-01 08:34:43,, pip install --no-cache numpy on RAMv7 i get libs for ARMv8 arch,['00 - Bug']
23286,open,hchau630,"### Describe the issue:

np.unique fails to remove duplicate NaNs even though equal_nan=True when axis is not None. In the code example below, the output is
```
[ 0.  1.  2. nan nan]
```
but one should expect
```
[ 0.  1.  2. nan]
```
This bug does not occur when axis is None.

### Reproduce the code example:

```python
import numpy as np

a = np.array([0., np.nan, 2., np.nan, 2., 1., 0., 1., 2., 0.])
print(np.unique(a, equal_nan=True, axis=0))
```


### Error message:

_No response_

### Runtime information:

```
1.24.2
3.9.13 (main, Oct 13 2022, 16:12:19) 
[Clang 12.0.0 ]
WARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information
[{'simd_extensions': {'baseline': ['NEON', 'NEON_FP16', 'NEON_VFPV4', 'ASIMD'],
                      'found': ['ASIMDHP', 'ASIMDDP'],
                      'not_found': ['ASIMDFHM']}}]
None
```


### Context for the issue:

_No response_",2023-02-26 22:16:22,,BUG: np.unique incorrect output given input with NaNs when equal_nan=True and axis is not None,['00 - Bug']
23273,open,Lorenzo-ITA,"### Describe the issue:

I want to convert a variable from numpy.datetima64 to datetime.date.
I used numpy astype function

```
import numpy as np
from datetime import datetime, date

original = datetime(2022,1,2)
dt64 = np.datetime64(original)
dt64.astype(date)

```
output is: _datetime.datetime(2022, 1, 2, 0, 0)_

expected output is _datetime.date(2022, 1, 2)_

### Reproduce the code example:

```python
import numpy as np
from datetime import datetime, date

original = datetime(2022,1,2)
dt64 = np.datetime64(original)
dt64.astype(date)
```


### Error message:

_No response_

### Runtime information:

output is: _datetime.datetime(2022, 1, 2, 0, 0)_

expected output is _datetime.date(2022, 1, 2)_

### Context for the issue:

_No response_",2023-02-24 09:53:48,,BUG: np.datetime64.astype(date) return datetime,['00 - Bug']
23271,open,bernd-wechner,"### Proposed new feature or change:

This is documented here in source:

https://github.com/numpy/numpy/blob/d92cc2d1c7c7153525e03c4d10377714d85cfde6/numpy/core/numeric.py#L738

and here live:

https://numpy.org/doc/stable/reference/generated/numpy.convolve.html

# Context
There are three modes documented, each of which produces a different length of output array. 

In practical applications, this operation is often tied to other axes. The implicit one of course is simply the index, as this general convolution, rightly, makes no assumptions about the application.

And yet, because real applications abound (not uncommonly in signal processing) in which there is a tied axis (often a time measure, or a frequency) measure to the signal that is being provided to this function as `a` and the array `v` represents a filter of some sort. 

In many (perhaps most, by some significant margin) such applications, the `a` provides the reference axis implicitly (is the signal which has an associated time - or frequency - for each sample). 

# The Problem (missing documentation)

In such applications it's important to the choice of mode here, to adjust (modify) any associated axis data and to do so in a 'correct' (or plausible, or useful) manner.

We can do some guess work here. `same` is the simplest, in which each element of the result maps one to one with each element of the longer of `a` and `v`. If we conjecture (and that is all it is at present) that many others in this application sphere and elsewhere are likely to use this method with `len(a)>len(v)` (quite probably much greater than), then for `full` we are forced into conjecture (or code analysis inside of numpy - which is why this is a request to document). 

`full` documents an output of (N+M-1,) and which is up front which suggests possibly (or, we might imagine that) the array `a` and `v` overlap by one sample being `a[0]` and `v[0]` with `a[-M:0]` set to 0 and then ends with and overlap of `a[N]` with `v[0]`. The last N samples of the output then map to the samples in `a` with the first M-1 a boundary effect.  That is what the first ""Example"" suggests, but I'd feel far more comfortable if it were stated explicitly (not least because it is very different to ending the process with `a[-1]` overlapping `v[0]` which would add another M-1 samples to the output or other scenarios.

`valid` similarly can be inferred as commencing with `a[0]` overlapping `v[-1]` and ending with `a[-1]` overlapping `v[0]` which, if `a` and `v` are close in length, produces a very short result. 

# The Solution

The Examples are lovely, and useful, and the sliding window imagery very commonly found in signal processing applications too. So I'd like to see some modest documentation that speaks to the mapping of the result's indices to `a`'s indices. If my inferences here are true, I am more than willing to PR a modest change to:

https://github.com/numpy/numpy/blob/d92cc2d1c7c7153525e03c4d10377714d85cfde6/numpy/core/numeric.py#L738

documenting that as time permits.  For now, I take these conclusions as evidenced by the examples ;-).

**Aside:** I came across this issue and need, indeed by looking at how a particular filter changes over a series of filter and decimation steps in a signal processing application, and so decimated the time array as well, but this convolution introduced changes to the length of the signal which I need to replicate in the time array to keep matplotlib happy ;-).




",2023-02-24 04:17:37,,"DOC: numpy.correlate, document the resultant index mapping ",['unlabeled']
23260,open,bogdan-pechounov,"### Describe the issue:

I was comparing the performance of numpy and numba for matrix multiplication. I was using integers so that numpy doesn't use BLAS and the comparison would be more fair (both numpy and numba would only be using vectorization). 
Based on my [discussion ](https://stackoverflow.com/a/75505787/20898396) with another user, it would seem that the loop order being used by [@TYPE@_matmul_inner_noblas](https://github.com/numpy/numpy/blob/main/numpy/core/src/umath/matmul.c.src#L217) isn't the one that is most cache friendly.

### Reproduce the code example:

```python
import numpy as np
from numba import njit, prange

@njit
def matrix_multiplication(A, B):
  m, n = A.shape
  _, p = B.shape
  C = np.zeros((m, p))
  for i in range(m):
    for j in range(n):
      for k in range(p):
        C[i, k] += A[i, j] * B[j, k]
  return C

@njit
def matrix_multiplication2(A, B): # loop order equivalent to numpy
  m, n = A.shape
  _, p = B.shape
  C = np.zeros((m, p))
  for i in prange(m):
    for k in range(p):
      for j in range(n):
        C[i, k] += A[i, j] * B[j, k]
  return C

m = 1000
n = 1000
p = 1000
A = np.random.randint(1, 100, size=(m, n))
B = np.random.randint(1, 100, size=(n, p))

# compile function
matrix_multiplication(A, B)
matrix_multiplication2(A, B)


%timeit matrix_multiplication(A, B)
%timeit matrix_multiplication2(A, B)
%timeit A @ B 
# numpy is a little slower than matrix_multiplication but faster than matrix_multiplication2
# 1.62 s ± 167 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
# 2.48 s ± 157 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
# 2 s ± 37.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```


### Error message:

_No response_

### Runtime information:

1.23.5
3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]

### Context for the issue:

_No response_",2023-02-22 17:52:14,,BUG: Matrix multiplication with integers not using the cache friendly loop order,['01 - Enhancement']
23222,open,FirefoxMetzger,"### Describe the issue:

Computing `np.linalg.norm(...)` vs `np.linalg.norm(..., axis=-1)` produces the same result on an array of shape `(3,)` most of the time (as it should), but may produce differing results depending on the CPU it runs on.

I know that there are differences between processors and how they handle floating point math, but I would expect the behavior to be consistent within the same processor, i.e., it shouldn't matter if I specify `axis=-1` on if I omit it.

For the processors I have available, the behavior of the snippet below is as follows:

```
# run on Windows 21H2 Build 22000.1574
# numpy==1.24.0
Works: Python 3.8.8,  AMD Ryzen 7 5800X 8-Core Processor
Works: Python 3.9.8,  AMD Ryzen 7 5800X 8-Core Processor
Works: Python 3.10.0, AMD Ryzen 7 5800X 8-Core Processor
Works: Python 3.11.1, AMD Ryzen 7 5800X 8-Core Processor

# From GH Actions
# run on Ubuntu 22.04.1 LTS
# numpy==1.24.0
FAILS: Python 3.8.16, Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz
Works: Python 3.10.9, Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz
FAILS: Python 3.9.16, Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz
Works: Python 3.8.16, Intel(R) Xeon(R) CPU E5-2673 v4 @ 2.30GHz
Works: Python 3.10.9, Intel(R) Xeon(R) CPU E5-2673 v4 @ 2.30GHz
```



### Reproduce the code example:

```python
import numpy as np
import numpy.testing as npt

vector = np.array(
    [11107866628593.0, 11107866628593.0, 11107866628593.0], dtype=np.float64
)
result = np.linalg.norm(vector, axis=-1)
expected = np.linalg.norm(vector)

npt.assert_almost_equal(result, expected)
```


### Error message:

```shell
Arrays are not almost equal to 7 decimals
 ACTUAL: 19239389364421.89
 DESIRED: 19239389364421.887
```


### Runtime information:

1.24.0

### Context for the issue:

_No response_",2023-02-16 10:54:51,,BUG: np.linalg.norm produces different results on 1D inputs when using axis kwarg on some processors,"['00 - Bug', '57 - Close?']"
23217,open,FactorizeD,"### Proposed new feature or change:

Hi all,

Consider the following code (numpy v1.23.0, mypy v0.982; I checked and the signature of the function hasn't changed in numpy v.1.24.0):

```
df = pd.DataFrame({'some_column': [0, 1, 2, 3]})
for batch in np.array_split(
    df,
    2,
):
    print(type(batch))

# <class 'pandas.core.frame.DataFrame'>
# <class 'pandas.core.frame.DataFrame'>
```

However, when mypy is run on this code with `reveal_type(batch)` instead of `print(...)`, we get: `Revealed type is ""numpy.ndarray[Any, numpy.dtype[Any]]""`.

Has this been thought of in the past? I guess the best solution would be to introduce `TypeVar` somewhere, but it might be not so obvious?",2023-02-15 15:21:03,,ENH: Annotate the first array_split argument with TypeVar,"['01 - Enhancement', 'Static typing']"
23203,open,csbrown,"There is a [specific implementation for binary search](https://numpy.org/doc/stable/reference/generated/numpy.searchsorted.html).  Can there also be one for linear search?  There are use cases where the first element where [condition] might frequently be very close to the front of the array, but existing methods for executing a linear search in numpy require operations on the entire array.  There seems to be [some demand for this](https://stackoverflow.com/questions/53020764/efficiently-return-the-index-of-the-first-value-satisfying-condition-in-array) (see also linked questions).  I can be convinced to work on this, but I've never contributed, and would appreciate some tips on where to start with this.

Related:  https://github.com/numpy/numpy/issues/1835",2023-02-13 01:01:26,,Linear Search ,"['01 - Enhancement', '54 - Needs decision']"
23187,open,simonykq,"### Describe the issue:

When you input a big integers type ('int64') as input to `numpy.linspace()` function, the result seem to have the last three digits chopped/altered, even if I explicitly pass the `dtype='int64'` argument as documented. See the following example.

The last three digits from the input, `089` are replaced with `352`.

Chances are that in the implementation of the `np.linspace()`, there is a conversion from `int` to `float` at some point, and then back to `int`:

```
int(float(8680131335536356089))
Out[24]: 8680131335536356352
```


### Reproduce the code example:

```python
import numpy as np

np.linspace(8680131335536356089, 8680131335536356089, 2, dtype='int64')
Out[24]: array([8680131335536356352, 8680131335536356352])
```


### Error message:

_No response_

### Runtime information:

1.21.5
3.8.10 | packaged by conda-forge | (default, Sep 13 2021, 21:14:52) 
[Clang 11.1.0 ]

### Context for the issue:

_No response_",2023-02-09 15:50:02,,BUG: The `numpy.linspace()` does not seem to work properly for big integers data types,['00 - Bug']
23180,open,ngoldbaum,"### Proposed new feature or change:

Currently user-defined dtypes cannot be included in numpy's dtype type hierarchy. One consequence is that it's currently not straightforward to programmatically determine if an arbitrary user-defined dtype represents numeric values or not. This is a problem for pandas supporting custom numpy dtypes, as pandas has a concept of numeric vs non-numeric dtypes.

It would be nice if one could do one of:

* `isinstance(dtype, numpy.numeric_dtype)` where `numpy.numeric_dtype` is a publicly available abstract dtype that all numeric dtypes are subtypes of. 
* check `dtype.is_numeric`, where `is_numeric` is a property that is set for all dtypes and is overridable by dtype authors writing dtypes that represent numbers.

The numeric dtype is just an example, there are likely a number of nodes in the dtype type hierarchy where it would be nice to be able to do these sorts of checks.

If we added publicly accessible abstract dtypes, we would also need to add a way for dtype authors to register themselves with the appropriate abstract dtypes they are supposed to be subtypes of or require them to be implemented as subtypes of the abstract dtypes. If we wanted to have flags on the dtype, we could also use a similar registration system so that dtype authors don't need to manually set any flags.

Currently numpy already has three abstract dtypes representing floating point numbers, integers, and complex numbers. These dtypes are not publicly accessible and there is no way for dtype authors to register themselves.",2023-02-08 21:09:00,,ENH: Fleshing out the dtype type hierarchy,['unlabeled']
23176,open,mattip,"PR #23136 and #22889 created fast paths through `ufunc.at` for 1d aligned arrays with no casting. I closed issues #11156, #7998, #5922, and #8495, as mentioned in [this comment](https://github.com/numpy/numpy/pull/23136#issuecomment-1412201236) and opened this to continue the discussion around additional speedups and concerns:
- 2d and more contiguous arrays could be sped up by calling the indexed loop on the fastest dimension. Currently the indexed loop will only be called for 1d arrays
- Add documentation on how user-defined loops can utilize this. The PR added a test that uses the experimental DType API to create a ufunc type with both a strided loop and an indexed loop. Is that the desired API?
- https://github.com/ml31415/numpy-groupies has a number of specific ufunc-at loops. Is there more we could integrate from there? @nschloe thoughts?
- Are there still cases where `bincount` is faster than `ufunc.add.at`?
- #9397 asks for an axis argument to `bincount`. `ufunc.at` does not support the `axis` kwarg, so I left that issue open.",2023-02-08 08:48:17,,ENH: make ufunc.at faster for more cases,['unlabeled']
23170,open,SeppMe,"### Describe the issue:

### Problem
If numpy is imported _before_ a Posix signal is blocked by setting a signal mask, the signal will not actually be blocked. If, however, numpy is imported _after_ the signal is blocked, it works as intended.

### Steps to reproduce
```python
# Code demonstrating the issue
import numpy
import signal
import time

signal.pthread_sigmask(signal.SIG_BLOCK, {signal.SIGUSR1})
for i in range(200):
    print(os.getpid(), signal.sigpending())
    time.sleep(2)
```
1. Run the script
2. While the script is still running, send SIGUSR1 to your Python process (`kill -s SIGUSR1 {the shown pid}`)

Expectation: The output should change from ""7654321 set()"" to ""7654321  {<Signals.SIGUSR1: 10>}"" (if the pid is 7654321) because the signal is blocked but pending
Actual result: The Python process is terminated with the message ""User defined signal 1"", because the default signal handler is triggered (which terminates the Python process in case of an uncaught SIGUSR1)

The problem does not arise if the numpy import occurs after the signal block:
```python
# Code without the issue
import signal
signal.pthread_sigmask(signal.SIG_BLOCK, {signal.SIGUSR1})
import numpy
import time

for i in range(200):
    print(os.getpid(), signal.sigpending())
    time.sleep(2)
```
Steps: See above
Expectation: See above
Result: The expectation is met!

### More observations

* The issue was reproduced on multiple machines using multiple different numpy versions. Oldest tested version was 1.16.4, newest was 1.24.1
* The same behavior also occurs if a custom signal handler is installed. The custom handler will execute _immediately_ despite the signal being blocked.
* This behavior can be reproduced for other blockable signals as well.
* Interaction with multiprocessing: If the process is forked after the numpy import, the problem does not occur, for _neither_ process.
```python
# Multiprocessing demo. Code does not show the issue despite the early numpy import
import numpy
import signal
import time
import os

pid = os.fork()
if pid > 0:
    signal.pthread_sigmask(signal.SIG_BLOCK, {signal.SIGUSR1})

for i in range(200):
    print(os.getpid(), signal.sigpending())
    time.sleep(2)
```
Send SIGUSR1 to either process, the signal will correctly show as pending.


### Reproduce the code example:

```python
import numpy
import signal
import time

signal.pthread_sigmask(signal.SIG_BLOCK, {signal.SIGUSR1})
for i in range(200):
    print(os.getpid(), signal.sigpending())
    time.sleep(2)

# 1. Run the script
# 2. While the script is still running, send SIGUSR1 to your Python process (`kill -s SIGUSR1 {the shown pid}`)
```


### Error message:

_No response_

### Runtime information:

Newest tested system
```text
1.24.1
3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]

[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Haswell',
  'filepath': '/var/SP/anaconda3/envs/env_3.10.8_e41a7bcc_e69a1b96/lib/libopenblasp-r0.3.21.so',
  'internal_api': 'openblas',
  'num_threads': 20,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.21'}]
```

The problem could also be reproduced using older numpy versions on another computers, all showing the same behavior. Oldest tested version was:
```
1.16.4
3.7.3 | packaged by conda-forge | (default, Jul  1 2019, 21:52:21)
[GCC 7.3.0]
```


### Context for the issue:

The issue prevents applications from using advanced inter-process signaling techniques, unless special care is taken with regard to import order.",2023-02-07 16:09:46,,BUG: Importing numpy interferes with signal masking,['00 - Bug']
23157,open,jpivarski,"### Describe the issue:

I wouldn't expect numbers with units, like datetime64 and timedelta64, to be comparable with numbers without units, but comparing datetime64 with datetime64 and timedelta64 with timedelta64 could be meaningful if the `atol` has units. And maybe `rtol` doesn't have a meaning for datetime64, since that is an interval measure in Stevens's classification, rather than a ratio measure.

Anyway, even if the chosen behavior is that all time-related types should fail in `np.isclose`, the error messages don't say that—they just reach a step in the calculation and fail. See the code examples below.

### Reproduce the code example:

```python
First, with datetime64:


np.allclose(np.array([1, 2, 3, 4], ""M8""), np.array([1, 2, 3, 4], ""M8""))


Now with timedelta64:

```python
np.allclose(np.array([1, 2, 3, 4], ""m8""), np.array([1, 2, 3, 4], ""m8""))
```
```


### Error message:

```shell
The error message with datetime64:


Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ValueError: Converting an integer to a NumPy datetime requires a specified unit
```

The error message with timedelta64:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<__array_function__ internals>"", line 180, in allclose
  File ""/home/jpivarski/mambaforge/lib/python3.9/site-packages/numpy/core/numeric.py"", line 2265, in allclose
    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
  File ""<__array_function__ internals>"", line 180, in isclose
  File ""/home/jpivarski/mambaforge/lib/python3.9/site-packages/numpy/core/numeric.py"", line 2375, in isclose
    return within_tol(x, y, atol, rtol)
  File ""/home/jpivarski/mambaforge/lib/python3.9/site-packages/numpy/core/numeric.py"", line 2356, in within_tol
    return less_equal(abs(x-y), atol + rtol * abs(y))
numpy.core._exceptions._UFuncBinaryResolutionError: ufunc 'add' cannot use operands with types dtype('float64') and dtype('<m8')
```
```


### Runtime information:

```python
>>> import sys, numpy; print(numpy.__version__); print(sys.version)
1.23.5
3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:45:29) 
[GCC 10.4.0]
```

```python
>>> print(numpy.show_runtime())
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/jpivarski/mambaforge/lib/python3.9/site-packages/numpy/__init__.py"", line 311, in __getattr__
    raise AttributeError(""module {!r} has no attribute ""
AttributeError: module 'numpy' has no attribute 'show_runtime'
```

(NumPy is 1.23.)

### Context for the issue:

I was examining the limits of `np.isclose` so that Awkward Array can formally extend it, in https://github.com/scikit-hep/awkward/pull/2198. As I said above, what `np.isclose` can do with time-unit types is somewhat limited; an error message saying that it isn't supported would be fine. But then Awkward Array would raise the same exception under the same circumstances.",2023-02-03 17:57:10,,BUG: np.isclose with datetime64 and timedelta64 types fail with error messages that reveal implementation details,['00 - Bug']
23156,open,ngoldbaum,"### Issue with current documentation:

I google searched for docs on `NpyIter` which landed me on this page:

https://docs.scipy.org/doc/numpy-1.9.2/reference/c-api.iterator.html

There is a header at the top of the page that purports to link to the equivalent page in the latest version of the numpy docs, but instead links to https://docs.scipy.org/doc/scipy/search.html?q=Array+Iterator+API.

### Idea or request for content:

It would be nice if old doc builds correctly linked to the numpy.org doc pages for the latest stable release. That would likely help SEO too (with e.g. a rel=canonical tag).",2023-02-03 17:25:00,,DOC: Links to latest numpy docs on old scipy.org doc builds redirect to scipy docs,['04 - Documentation']
23146,open,ngoldbaum,"### Proposed new feature or change:

Currently dtypes that have embedded references can't influence numpy or python's facilities for determining the amount of memory used by an array.

For example, the sizes reported for object dtypes only include the space allocated in the array buffer and do not include the sizes of the objects stored in the array:

```
import numpy as np
import sys

really_long_string = 'a'*100_000

arr = np.array([really_long_string]*5, dtype=object)

print(sys.getsizeof(arr))
print(arr.nbytes)
```

This script prints `152` and `40` on my 64 bit linux machine.

Pandas has a workaround for this for object arrays:

https://github.com/pandas-dev/pandas/blob/dbed1316d10ee699ad6d5c8205a3f4e0b1377e5d/pandas/core/base.py#L1132-L1136

So long as object dtypes are the only real-world usage of dtypes that use embedded references this isn't a big deal, but with the new dtype API this will likely happen more often and downstream users of these dtypes will need to learn more complicated workarounds. In particular, I'm writing [stringdtype](https://github.com/numpy/numpy-user-dtypes/tree/main/stringdtype) for variable-length strings which has the same issue as the object dtype example above.

This information could be stored on the array object itself, but that will likely require a large number of code changes inside numpy since the places where allocations happen (inside cast loops and inside the dtype's `setitem` implementation, maybe other places I'm not aware) don't have access to the array object, so the allocation tracking information would need to be threaded through numpy's internal API. 

It would be more straightforward with the way dtypes are currently set up to store allocation tracking metadata on the dtype object as allocations happen, since the routines that do allocations will have access to a dtype instance, and then the array can read that information from the dtype if it exists when calculating the array size. However, that will require making sure arrays don't share dtype instances, or perhaps adding a registry to the dtype to map array instances to allocation tracking information.

For now, for my purposes, I can add a helper inside my dtype implementation that calculates the ""real"" memory usage for the array, so this isn't a blocker for me.",2023-02-02 17:48:24,,ENH: add a way for dtypes with embedded references to influence memory usage tracking,['unlabeled']
23132,open,lixi-zhou,"### Describe the issue:

If a rec array contains object dtype, after converting it to bytes and saving to disk, the data cannot be loaded back by another script but the code works if the saving and loading are within the same file. 

Please see the attached reproduce code for example. First, run a.py, the code works without issue. Later, run b.py it will throw a Segmentation fault error.

### Reproduce the code example:

```python
# =======================a.py======================
import numpy as np
arr_dtype = [('keys', 'O'), ('data', '<i8')]
a = np.rec.array([('abcd', 0), ('abbe', 1), ('abbe', 2), ('ded', 3), ('ads', 4)], 
      dtype=arr_dtype)
with open('test.data', 'wb') as f:
  f.write(a.tobytes())
with open('test.data', 'rb') as f:
  a_bytes = f.read()
b = np.rec.array(a_bytes, dtype=arr_dtype)
print(b)

# =======================b.py======================
import numpy as np
arr_dtype = [('keys', 'O'), ('data', '<i8')]
with open('test.data', 'rb') as f:
  a_bytes = f.read()
b = np.rec.array(a_bytes, dtype=arr_dtype)
print(b)
```


### Error message:

```shell
# after run a.py, successfully load the data without issue. The script outputs:
[('abcd', 0) ('abbe', 1) ('abbe', 2) ('ded', 3) ('ads', 4)]

# after run b.py. The script outputs:
Segmentation fault
```


### Runtime information:

1.24.1
3.8.13 (default, Mar 28 2022, 11:38:47) 
[GCC 7.5.0]
[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2',
                                'AVX512F',
                                'AVX512CD',
                                'AVX512_SKX',
                                'AVX512_CLX'],
                      'not_found': ['AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'SkylakeX',
  'filepath': '/home/xxx/miniconda3/envs/py38/lib/python3.8/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so',
  'internal_api': 'openblas',
  'num_threads': 8,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.21'}]

### Context for the issue:

_No response_",2023-01-30 22:08:16,,ENH: rec.array load from bytes should refuse object dtype,['00 - Bug']
23126,open,clime,"### Proposed new feature or change:

I would like some numpy global flag to exist that would globally disable broadcasting.

I don't love the implicit nature of broadcasting that can bring unexpected results. E.g.:

```
>>> np.array([[0.3, 0.7], [0.2, 0.8]]) * np.array([0.1, 0.2])
array([[0.03, 0.14],
       [0.02, 0.16]])
```

In that example, I would like an error to be raised because what I really needed was:

```
>>> np.array([[0.3, 0.7], [0.2, 0.8]]) * np.array([0.1, 0.2])[:, np.newaxis]
array([[0.03, 0.07],
       [0.04, 0.16]])
```

I know it is my responsibility to make the program correct but such implicit features like broadcasting make it more difficult for me. And it doesn't feel safe to me.

I would welcome it if there was an explicit operator, instead, allowing one to specify on which axis you want to broadcast. 

E.g.
```
np.array([[0.3, 0.7], [0.2, 0.8]]) * np.array([0.1, 0.2])[np.newaxis|np.broadcast, :] # adds new dimension that is also broadcastable

np.array([[0.3, 0.7], [0.2, 0.8]]) * np.array([[0.1, 0.2]])[np.broadcast, :] # if the dimension already exists
```

This probably can't work just like that for whatever reason but I just wanted to try to convey the basic idea.",2023-01-29 22:59:39,,Make it possible to disable broadcasting library-wise,['57 - Close?']
23123,open,davidsj,"### Describe the issue:

A 500 x 4096 by 4096 x 4096 float64 matrix multiplication takes ~90 seconds when the second matrix is the real part of a complex matrix. However, if I copy the real part first, it takes ~200 milliseconds.

### Reproduce the code example:

```python
Python 3.8.13 (default, Mar 28 2022, 06:16:26) 
Type 'copyright', 'credits' or 'license' for more information
IPython 8.7.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import numpy as np; import sys; print(np.__version__); print(sys.version)
1.21.2
3.8.13 (default, Mar 28 2022, 06:16:26) 
[Clang 12.0.0 ]

In [2]: A = np.random.rand(500, 4096).astype(np.float64)

In [3]: B = np.random.rand(4096, 4096).astype(np.complex128)

In [4]: %time A @ B.real.copy()
CPU times: user 661 ms, sys: 32.5 ms, total: 693 ms
Wall time: 208 ms
Out[4]: 
array([[1031.28559148, 1016.17905869, 1034.24549966, ..., 1056.87465704,
        1026.28741105, 1038.26833557],
       [1025.92445058, 1013.48585125, 1017.4986476 , ..., 1047.55891348,
        1023.70195356, 1031.99613791],
       [1065.46563622, 1031.90604975, 1050.00822611, ..., 1078.80268065,
        1054.78029417, 1060.26089953],
       ...,
       [1025.73960694, 1014.40748249, 1025.57724684, ..., 1046.70235119,
        1024.04754168, 1029.73316398],
       [1018.27542455, 1011.34196549, 1019.5848226 , ..., 1047.61990492,
        1010.1769123 , 1008.49849528],
       [1022.48335183, 1005.82987121, 1022.86583977, ..., 1042.67415337,
        1018.51901983, 1024.17000994]])

In [5]: %time A @ B.real
CPU times: user 1min 27s, sys: 732 ms, total: 1min 28s
Wall time: 1min 29s
Out[5]: 
array([[1031.28559148, 1016.17905869, 1034.24549966, ..., 1056.87465704,
        1026.28741105, 1038.26833557],
       [1025.92445058, 1013.48585125, 1017.4986476 , ..., 1047.55891348,
        1023.70195356, 1031.99613791],
       [1065.46563622, 1031.90604975, 1050.00822611, ..., 1078.80268065,
        1054.78029417, 1060.26089953],
       ...,
       [1025.73960694, 1014.40748249, 1025.57724684, ..., 1046.70235119,
        1024.04754168, 1029.73316398],
       [1018.27542455, 1011.34196549, 1019.5848226 , ..., 1047.61990492,
        1010.1769123 , 1008.49849528],
       [1022.48335183, 1005.82987121, 1022.86583977, ..., 1042.67415337,
        1018.51901983, 1024.17000994]])
```


### Error message:

_No response_

### Runtime information:

1.21.2
3.8.13 (default, Mar 28 2022, 06:16:26) 
[Clang 12.0.0 ]

### Context for the issue:

_No response_",2023-01-29 07:53:58,,BUG: Multiplication by real part of complex matrix very slow,['00 - Bug']
23107,open,clime,"### Describe the issue:

E.g.:

```
>>> list(np.sum(map(lambda x: x, [[1, 2], [1,2]]), axis=0))
[[1, 2], [1, 2]]
```
Incorrect/unexpected result is returned and no error is raised. I would expect `np.sum` to do a type check and error out in this case.

NOTE: There is something similar for generators already (although it is just a warning):
```
>>> np.sum(i for i in range(5))
<stdin>:1: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.
```

### Reproduce the code example:

```python
list(np.sum(map(lambda x: x, [[1, 2], [1,2]]), axis=0))
```


### Error message:

```shell
No error
```


### Runtime information:

1.23.5
3.9.16 (main, Dec  7 2022, 01:12:08) 
[GCC 11.3.0]

### Context for the issue:

_No response_",2023-01-26 18:37:03,,BUG: application of np.sum on a python iterator should raise an error,"['54 - Needs decision', '07 - Deprecation']"
23104,open,raulfrk,"### Describe the issue:

After upgrading to any Numpy version >= 1.23, importing setup_common, which can be used to get the current C_API_VERSION, does not work anymore. This was tested on both python 3.7.16 and 3.8.16 and in both cases it does not work.

### Reproduce the code example:

```python
import numpy.core.setup_common
```


### Error message:

```shell
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/user01/v38/lib/python3.8/site-packages/numpy/core/setup_common.py"", line 116, in <module>
    with open(pathlib.Path(__file__).parent / file) as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/user01/v38/lib/python3.8/site-packages/numpy/core/feature_detection_locale.h'
```


### Runtime information:

>>> import sys, numpy; print(numpy.__version__); print(sys.version)
1.24.1
3.8.13 (default, Apr 19 2022, 00:53:22) 
[GCC 7.5.0]
>>> print(numpy.show_runtime())
[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2',
                                'AVX512F',
                                'AVX512CD',
                                'AVX512_SKX'],
                      'not_found': ['AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'SkylakeX',
  'filepath': '/home/user01/v38/lib/python3.8/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so',
  'internal_api': 'openblas',
  'num_threads': 56,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.21'}]
None

### Context for the issue:

Not being able to import setup_common anymore prevents users from getting the value of the `C_API_VERSION` constant found inside of it. `C_API_VERSION` is useful to check if there is a C_API mismatch in C modules before the ""import_array()"" function is called so that preventive action can be taken in case of C api version mismatch.

Here is an example of how C_API_VERSION was used before in C:

``` C
PyObject *module = PyImport_ImportModule(""numpy.core.setup_common"")
PyObject *obj_c_api_version = PyObject_GetAttrString(""C_API_VERSION"")
const char* c_api_version  = PyString_AsString(obj_c_api_version);
```
`c_api_version` would be later used to compare it with the NPY_FEATURE_VERSION (build-time C api version) to see if there is a mismatch and act on it before calling `import_array()` which performs similar checks but crashes due to the mismatch.

As an alternative, it would also be nice if a proper way to get the runtime C_API_VERSION was implemented in the C source code. I am aware that a similar function already exists and it is called `PyArray_GetNDArrayCFeatureVersion()`, but this function cannot be called before the `import_array()` function is called which is a bit problematic if API version checks need to be performed before that.",2023-01-26 15:04:48,,BUG: Impossible to import setup_common (needed to get C_API_VERSION) on Numpy >= 1.23,['00 - Bug']
23075,open,jsaladich,"### Describe the issue:

Hi,

I have a numpy array of shape (4350, 110, 214) with values between the range 232.9 and 312.2 and `dtype` float16. The array contains no NaNs values. When I calculate the `np.nanmean`  in `axis = 0` of this array I get an overflow warning and the operation returns an array of `inf` values.

If I calculate the `np.mean` reducer in the same `axis=0` returns an expected array with all the mean values. Furthermore, the operation `np.nanmean` in `axis=None` returns an expected float16 value averaging all values. 

The unexpected result is for `np.nanmean` in `axis=0`  that seems to overflow

The warning:
""""""
/home/XXXX/YYYYY/python3.10/site-packages/numpy/core/fromnumeric.py:(line) 86: RuntimeWarning:overflow encountered in reduce
""""""
If you are willing to get the array in an npy file, let me know please

Thanks a lot!

### Reproduce the code example:

```python
import numpy as np

arr = np.load('./xxxxx.npy').astype('float16')
# unexpected behaviour
np.nanmean(arr ,axis=0)

>> array([[inf, inf, inf, ..., inf, inf, inf],
       [inf, inf, inf, ..., inf, inf, inf],
       [inf, inf, inf, ..., inf, inf, inf],
       ...,
       [inf, inf, inf, ..., inf, inf, inf],
       [inf, inf, inf, ..., inf, inf, inf],
       [inf, inf, inf, ..., inf, inf, inf]], dtype=float16)

# But check this
np.nanmean(y_v_corr.astype('float32'),axis=0)
>>array([[279.41907, 279.4087 , 279.40546, ..., 272.96454, 272.99265,
        273.00772],
       [279.59586, 279.5931 , 279.585  , ..., 272.96997, 273.01962,
        273.08087],
       [279.76947, 279.76224, 279.7534 , ..., 272.9398 , 273.0746 ,
        273.19916],
       ...,
       [289.4564 , 289.47028, 289.48328, ..., 291.8542 , 291.95828,
        292.0766 ],
       [289.55225, 289.57397, 289.58206, ..., 292.00092, 292.2027 ,
        292.36783],
       [289.63907, 289.65765, 289.6707 , ..., 292.20093, 292.40012,
        292.62448]], dtype=float32)
```


### Error message:

_No response_

### Runtime information:

Numpy version: 1.23.4
Python version: 3.10.7 (main, Sep  7 2022, 15:22:19) [GCC 9.4.0]

### Context for the issue:

_No response_",2023-01-23 15:52:03,,"BUG: Unexpected behaviour with `nanmean` compared to `mean`, overflow encountered in reduce",['33 - Question']
23068,open,yamadafuyuka,"Functions such as sin and log use libm except for `AVX512_SKX`, and at least in my environment SIMD instruction were not used.
Therefore, I added implementation to use SIMD library SLEEF ( https://sleef.org/ ) and measured the calculation time of some functions.
My branch: ( https://github.com/yamadafuyuka/numpy/tree/add_SLEEF )

I graphed the results. We also confirmed that using  SVE intrinsics as in ( [PR-22265](https://github.com/numpy/numpy/pull/22265) ) further speeds up (the log10 function is about 4 times faster).
I would like to add SLEEF support, but I am not sure which part of NumPy is the best place to implement it. Could you please advise?

<img src=""https://user-images.githubusercontent.com/121528995/213957906-7c8ec01b-b4ac-408f-97a1-ae29bb517cc5.png"" width=""500"">",2023-01-23 02:50:41,,ENH: Add support SLEEF for transcendental functions,['component: SIMD']
23062,open,HaoZeke,"### Describe the issue:

`complex` cannot be converted to `float` is reasonable behavior, but the `0d` conversions work for `complex256` which seems strange. 

### Reproduce the code example:

```python
import numpy as np

CMP_TYPES = [np.complex64, np.complex128, np.complex256]

for cmptype in CMP_TYPES:
  xarg = np.array(3, dtype = cmptype)
  try:
    print(xarg.__float__())
  except:
    print(f""{cmptype} can't be converted"")
```


### Error message:

```shell
<class 'numpy.complex64'> can't be converted
<class 'numpy.complex128'> can't be converted
<ipython-input-10-7296ced4eee5>:8: ComplexWarning: Casting complex values to real discards the imaginary part
  print(xarg.__float__())
3.0
```


### Runtime information:

```python
WARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information
[{'numpy_version': '1.24.0.dev0+1549.gb21c9f846',
  'python': '3.9.15 | packaged by conda-forge | (main, Nov 22 2022, '
            '08:55:37) \n'
            '[Clang 14.0.6 ]',
  'uname': uname_result(system='Darwin', node='MacBook-Pro-3.local', release='22.2.0', version='Darwin Kernel Version 22.2.0: Fri Nov 11 02:08:47 PST 2022; root:xnu-8792.61.2~4/RELEASE_X86_64', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}}]
None
```

### Context for the issue:

Came up while writing benchmarks. Not really much of a priority.

### Suggested resolution:
Would prefer to have `complex256` fail like `complex64` and `complex128`.",2023-01-22 07:38:05,,BUG: Inconsistent `complex`-->`float` casting failures,['00 - Bug']
23056,open,EngrStudent,"### Proposed new feature or change:

(Scope) This suggestion is to modify ""numpy.linalg.svd"" and should give, for matrices where one axis is much larger than the other no less than a 20x speedup, and up to a 10,000x speedup.  

If you have a matrix that is say 200 rows by 8 million columns, then when you multiply it by its transpose the result is 200x200 matrix.  If you perform SVD on that you can determine both the left-singular vectors and the squares of the eigenvalues.  It is symmetric and real, so according to [this ](https://mathworld.wolfram.com/HermitianMatrix.html) it is likely Hermitian.  

(Versions) I am using numpy version 1.19.2, on python 3.8.12 in the following.

Here is example code to show the performance difference (using JuPyTeR):
```
import numpy as np
print(np.__version__)
---
A = np.random.rand(int(1e2), int(1e4))
AA = np.matmul(A,np.transpose(A))
---
%%timeit -n 3 -r 3
u1, s1, vt1 = np.linalg.svd(A)
---
u1, s1, vt1 = np.linalg.svd(A)
---
%%timeit -n 3 -r 3
u2, s2, vt2 = np.linalg.svd(AA)

s2b = np.sqrt(s2)                    #get eigens
rot_mat = np.matmul(u2, u1.T)        #get rot for u2 --> u1
u2_in_u1 = np.matmul(rot_mat, u2)    #perform rotation
B = np.matmul(np.diag(s2b),u2_in_u1) #get u1*s
B_inv = np.linalg.pinv(B)            #find pinv 
vt2b = np.matmul(B_inv,A)            #isolate vt in u1 coords
```

the horizontal '---' indicate cell breaks

Here is a screenshot of the timing difference:
![image](https://user-images.githubusercontent.com/10841261/213777375-2a954816-0619-48e9-b8c1-b03f6e949b0b.png)


The image shows something like an order of 3-decade speedup for the operation.  All the commented code in the SVD for AA, except for the square root, is extra.  It maps V2 into V1.  This is a strong over-estimate of how long it takes.  

here is the code that shows the eigenvalues are compatible:
```
#are eigenvalues from 1 sqrt of eigenvalues from 2?
print(np.linalg.norm(s1-np.sqrt(s2)))
```

Here is the output:
![image](https://user-images.githubusercontent.com/10841261/213770402-11284593-79a9-4e2b-b1c3-6800811d2d29.png)


Here is the code that shows the left singular vectors are compatible:
```
#align the eigenvectors because they aren't in same order
rot_mat = np.matmul(u1, u2.T)  #find mapping
u1_in_u2 = np.matmul(rot_mat, u1) #map it
np.linalg.norm(u2 - u1_in_u2,ord='fro') #look at norm of difference
```

Here is the image of the output
![image](https://user-images.githubusercontent.com/10841261/213771148-5ae483d4-5b7e-405c-ad16-cb74be093528.png)

So what about the reduced dim options?

When I look for reduced dims, it gives this:
![image](https://user-images.githubusercontent.com/10841261/213771516-bc2d8f4f-e5c6-4293-b61d-4ac5cd3a1566.png)

This takes 55.6/7.43 = 7.48 times longer to compute.  I should be able to get the left singular matrix for the first decomposition, the rotation, using the results of the simpler matrix in less time.  

If the matrix is scaled the other way, then some algebra and transposes should still yield similar speedups.  

If the matrix is specified as Hermitian, (as symmetric and real should be) then the compute is done even more quickly.  
![image](https://user-images.githubusercontent.com/10841261/213780027-ed6a4349-6c14-4b6a-8662-56559b8cd44e.png)

I don't think every rectangular matrix multiplied by its transpose is Hermitian.  This form of the real-valued A*A.T here can be contrived as a projection matrix, so it might be Hermitian.

(Disclaimer) I'm not sure if this is currently being done, but I think it isn't because of the decent multiple differences in timing (shown below).  It has been some time since I took linear algebra, and so perhaps I am not as fresh as I could be there.

",2023-01-20 18:26:28,,ENH: possible 3000x speedup for high aspect-ratio matrices by SVD on `A*A.T`,"['01 - Enhancement', 'component: numpy.linalg']"
23040,open,seberg,"```
>>> np.diff([1], n=0, prepend=[2], append=[3])
[1]
```
should probably include prepend/append (considering that we don't have special logic in other paths).  This seems like an oversight when adding append/prepend.

The implementation for `np.ma.diff` sharese this proprty.

I suspect we could get away with just changing it, but maybe a FutureWarning is just as well...

Ping @markopacak since you thought you may look into it.",2023-01-18 23:41:00,,"BUG: `np.diff` has a fast path for `n=0`, but unlike `n=1` it discards `append`/`prepand`","['component: numpy.lib', 'component: numpy.ma', '60 - Major release']"
23014,open,seberg,"Ping @greglucas, @shoyer, and @mhvk, since I think you may be interested or have an opinion.  I am not quite sure if I may just be missing something here.

The implementation of `ndarray.__array_function__` seems very subtly wrong to me (see below for the failure mode).  It should be:
```
class ndarray:

    def __array_function__(self, types, ...):
        for type in types:
            if not isinstance(self, type):
                return NotImplemented
        # do stuff
```
or actuall better/cleaner (and more broadly usable!):
```
           if type.__array_function__ is not np.ndarray.__array_function__:
               return NotImplemented
```
(Our binops use this pattern too, for a reason!)

This clears up the failure mode of:
```
class myarr_naive(np.ndarray):
    pass

class myarr_bossy(np.ndarray):
    def __array_function__(self, func, types, *args, **kwargs):
         # knows how to deal with myarr_naive!
         print(func, types, args, kwargs)
         return ""huhu""

arr1 = np.array([1]).view(myarr_naive)
arr2 = np.array([1]).view(myarr_bossy)

# should use arr2.__array_function__, but does not:
np.concatenate((arr1, arr2))  # not ""huhu"" :(
```

The other imporant part here may be to suggest the first pattern in the NEP, if we deem it correct.",2023-01-15 20:54:42,,BUG: Subclass behavior of `__array_function__` seem subtly flawed?,['unlabeled']
23007,open,ischurov,"### Describe the issue:

I want to make sure that some variable is an instance of `np.uint64`. To this end, I use the following:

```
assert isinstance(idx, np.uint64)
```
However, in that case I have the following error on that line during type-checking with `pyright`:

```
Second argument to ""isinstance"" must be a class or tuple of classes
  TypeVar or generic type with type arguments not allowed
```

As the developers of pyright [explained](https://github.com/microsoft/pylance-release/issues/3830#issuecomment-1380949145), this is due to fact that type stubs model `np.uint64` as a specialized version of the generic class `unsignedinteger` (`uint64 = unsignedinteger[_64Bit]`), while in fact it is a proper subclass of `unsignedinteger`. As specialized versions of a class (like `list[int]`) are not allowed in `isinstance`, the type-checker reports an error, making it impossible to use this `assert` at all.

Are there any reasons to model these types (like `np.uint64`, `np.uint32`, etc) in this way (as a specialized versions) instead of making separate types?

### Reproduce the code example:

```python
import numpy as np
import pandas as pd

def foo(x: np.uint64):
    pass
series = pd.Series([1, 2, 3], index=np.array([10, 20, 30], dtype='uint64'))
idx = series.index[0]
assert isinstance(idx, np.uint64)
foo(idx)
```


### Error message:

```shell
(type alias) uint64: Type[unsignedinteger[_64Bit]]
Second argument to ""isinstance"" must be a class or tuple of classes
  TypeVar or generic type with type arguments not allowed
```


### Runtime information:

1.24.1
3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]
[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Haswell',
  'filepath': '/vol/tcm10/ischurov/.conda/envs/latsym2/lib/libopenblasp-r0.3.21.so',
  'internal_api': 'openblas',
  'num_threads': 1,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.21'}]
None

### Context for the issue:

Initially, I had a code that looked like the following:

```
def foo(x: np.uint64):
    pass
series = pd.Series([1, 2, 3], index=np.array([10, 20, 30], dtype='uint64'))
idx = series.index[0]
foo(idx)
```

On the type-checking, I have an error: `Argument of type ""Scalar | Unknown"" cannot be assigned to parameter ""x"" of type ""uint64"" in function ""foo""`. That's because the type-checker cannot infer that index of `pd.Series` has dtype `uint64`. Due to [open issue](https://github.com/pandas-dev/pandas/issues/34248) in pandas, it is not yet possible to specify `pd.Series.index`'s dtype using type hints. To give the type-checker information that `idx` has type `np.uint64`, I want to add the following assert before running `foo`:

```
assert isinstance(idx, np.uint64)
```

And that leads to type-checker error, as explained above.",2023-01-13 14:41:30,,BUG: modelling various integer types as a specialized version of the generic class breaks type-checking in pyright,"['00 - Bug', 'Static typing']"
23005,open,dasistkeintest,"### Describe the issue:

The annotation for the return type of `np.split` is `list[NDArray[Any]]`, but when a `pd.DataFrame` is passed, the return type is actually `list[pd.DataFrame]`.

The code example is valid, but `mypy` flags an error.

### Reproduce the code example:

```python
import numpy as np
import pandas as pd

df = np.split(pd.DataFrame(), ())[0]
pd.melt(df)
```


### Error message:

```shell
example.py:5: error: Argument 1 to ""melt"" has incompatible type ""ndarray[Any, dtype[Any]]""; expected ""DataFrame""  [arg-type]
```


### Runtime information:
<details>
1.24.1
3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]
[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2',
                                'AVX512F',
                                'AVX512CD',
                                'AVX512_SKX',
                                'AVX512_CLX',
                                'AVX512_CNL',
                                'AVX512_ICL'],
                      'not_found': []}},
 {'architecture': 'SkylakeX',
  'filepath': 'C:\\Users\\username\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll',
  'internal_api': 'openblas',
  'num_threads': 8,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.21'}]
None
</details>",2023-01-13 01:58:38,,BUG: wrong return type when applying np.split to pd.DataFrame,"['00 - Bug', 'Static typing']"
22995,open,hannorein,"### Proposed new feature or change:

Numpy supports various ways to convert an instance of a class to a numpy array. For example, using the `__array__` syntax:

```python
import numpy as np
class Vec3d:
    x = 1
    y = 2
    z = 3
    def __array__(self, dtype):
        return np.array([self.x, self.y, self.z])

v = Vec3d()
n = np.zeros((2,3))
n[0] = v  # v can be used in assignments like this
```

However, when the class supports PEP 3118, then this overwrites all other ways. For example, the following `Vec3dS` class is a subclass of ctypes' `Structure` (which implements PEP 3118):

```python
from ctypes import Structure, c_double
class Vec3dS(Structure):
    _fields_ = [(""x"", c_double), (""y"", c_double), (""z"", c_double)]
    def __array__(self, dtype):
        return np.array([self.x, self.y, self.z])
```

When converting an instance of `Vec3dS` to a numpy array, the resulting array has  `dtype([('x', '<f8'), ('y', '<f8'), ('z', '<f8')])`. The `__array__` function (as well as `__array_interface__`) is ignored. This prevents any subclass of `ctypes.Structure` to be used in assignments like this:

```python
vs = Vec3dS(1,2,3)
n[1] = vs # this will fail
```

Of course, there are several other ways to create a numpy array from a ctypes Structure. But unless I'm missing something, none of them allow a user to use the simple `n[1] = vs` syntax in the above example. 

It would be nice to have a way to explicitly tell numpy how to convert an instance of a class to a numpy array rather than always prioritizing PEP 3118. Correct me if I'm wrong, but I don't think I can overwrite the PEP 3118 interface from the python side.

",2023-01-11 16:35:10,,ENH: Provide a way to overwrite the standard PEP 3118 way on how objects are converted to numpy arrays,['unlabeled']
22993,open,kc611,"### Describe the issue:

While generation of random numbers using NumPy `Generators`, we ran into the following problem:

Due to instruction selection differences across compilers, there are discrepancies within NumPy outputs across different architectures. (In the order of 1000s of ULPs on 32-bit architectures as well as linux-aarch64 and linux-ppc64le platforms). This is suspected to be happening due to Fused-Multiply-Add (FMA) optimisations at compiler level.

Linked comment: https://github.com/numba/numba/pull/8038#issuecomment-1165571368

cc @stuartarchibald 

### Reproduce the code example:

```python
import numpy as np

rng_a = np.random.default_rng(1)
rng_b = np.random.default_rng(1)

scale = np.float64(3.)
loc = np.float64(1.5)
size = (100,)

# The following will undergo fma optimisations on fma enabled NumPy builds/architectures
ra = rng_a.normal(loc=loc, scale=scale, size=size)

# The following won't undergo fma optimizations
rb = loc + scale * rng_b.standard_normal(size=size, dtype=np.float64)

first = True

for (x, y) in zip(ra.flat, rb.flat):
    print('---')
    print(x)
    print(y)
    if first:
        first = False
        # check that the first result is the expected value
        np.testing.assert_array_max_ulp(x, 2.536752576194358, 1)
    np.testing.assert_array_max_ulp(x, y, 1)
```


### Error message:

```shell
N.A.
```


### Runtime information:

N.A.

### Context for the issue:

_No response_",2023-01-11 10:14:47,,BUG: FMA contractions/optimisations lead to inconsistent numerical results across different architectures,['00 - Bug']
22977,open,LukasNickel,"### Describe the issue:

The `can_cast` function fails with a rather non-descriptive error if `from_` is a string (Or 'None'-descriptive? SCNR).

Since strings are scalars in numpy, I would expect this to return False if comparing to a non-string dtype.
Although on the other hand:
```python
x = np.float32(""42.0"")
type(x)
>> numpy.float32
```
works just fine, so maybe one would expect it to return `True` in some cases as well.



### Reproduce the code example:

```python
import numpy as np
np.can_cast(""42"", np.float32)
```


### Error message:

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[112], line 1
----> 1 np.can_cast(""42"", np.float32)

File <__array_function__ internals>:180, in can_cast(*args, **kwargs)

TypeError: did not understand one of the types; 'None' not accepted
```


### Runtime information:

sys.version:
'3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 15:55:03) \n[GCC 10.4.0]'

np.__version__:
'1.23.5'

### Context for the issue:

I want to convert inputs to the expected dtypes.
My plan was to use `np.can_cast(input_value, expected_dtype, casting=""safe"")` to test if the values are valid and can be casted without loss of precision.

I could of course compare the dtypes directly as well and only cast to strictly larger types.
The advantage in this solution for me was that it would check whether the python integers and floats fit into the dtype:
`np.can_cast(42.0, np.float32)` is `True`, whereas `np.can_cast(type(42.0), np.float32)` returns `False`, because python floats are 64-bit and in general the conversion will be lossy.",2023-01-09 14:53:45,,BUG: np.can_cast fails with string input,['00 - Bug']
22956,open,cesaryuan,"### Describe the issue:

For a large size float32 array with second dimension larger than 1, `np.mean(axis=0)` return incorrect value as shown follow.
```ipython
Python 3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]
Type 'copyright', 'credits' or 'license' for more information
IPython 7.22.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import numpy as np

In [2]: np.__version__
Out[2]: '1.24.1'

In [3]: np.ones((20000000, 1), dtype=np.float32).mean(axis=0)
Out[3]: array([1.], dtype=float32)

In [4]: np.ones((20000000, 2), dtype=np.float32).mean(axis=0)
Out[4]: array([0.8388608, 0.8388608], dtype=float32)

In [5]: np.ones((20000000, 2), dtype=np.float32).mean()
Out[5]: 1.0
```

### Reproduce the code example:

```python
import numpy as np
# result: [1.]
print(np.ones((20000000, 1), dtype=np.float32).mean(axis=0))
# result: [0.8388608, 0.8388608]
print(np.ones((20000000, 2), dtype=np.float32).mean(axis=0))
# result: 1.0
print(np.ones((20000000, 2), dtype=np.float32).mean())
```


### Error message:

_No response_

### Runtime information:

1.24.1
3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]

### Context for the issue:

_No response_",2023-01-06 14:10:22,,BUG: np.mean(axis=0) return incorrect value for a large size float32 array,['33 - Question']
22928,open,JavierGOrdonnez,"### Describe the issue:

Good afternoon,

I was multiplying a 2D NumPy array of dimensions (t,n,n) by a 1D NumPy vector (n). At some point I needed to change the n dimension to approximately triple (66 to 176) and I saw a very substantial time overhead, from 0.4ms to 56.4ms. I would have expected a 3^2 ~ 10x increase in computational time, but this was higher than 100x.

I narrowed this down to there being a precise value (n from 95 to 96) where somehow the behavior switches and the computational time increases drastically on a very small n change.
![image](https://user-images.githubusercontent.com/56032114/210561536-c254511d-8877-462a-9e23-bed4fae8f4f1.png)
Casting to smaller data types helped (pushed the boundary to somewhere around n~150), but produced an overhead of ~10ms for all n values, so it is not a good solution.

PS1 I have checked that the computational time is linear on the ""t"" dimension, and the ""crossing point"" between 95 and 96 is still there for bigger and smaller t values (t=101 and t=1.001).
PS2 I have found that np.tensordot (axes=(2,0)) does not have this problem, its computational time scales approximately with n^2 (slightly higer), as expected. Perhaps comparing the implementation of np.tensordot and np.matmul can help clarify the issue.

PS3 When ignoring the t-dimension (simply multiplying a (n,n) matrix by a (n,) vector), then tensordot also gets a very substantial overhead at the step from n=95 to 96: 
![image](https://user-images.githubusercontent.com/56032114/210567170-13c8c222-0e1c-4201-90f8-488c00b19fda.png)




### Reproduce the code example:

```python
from time import time
import numpy as np

def measure_time_n_reps(func, n=10, *args, **kwargs):
    """"""Measure the execution time (in ms) of a function, executed n times.""""""
    start = int(round(time() * 1000))
    for i in range(n):
        func(*args, **kwargs)
    end_ = int(round(time() * 1000)) - start
    print(f""Average execution time of {func.__name__} over {n} repetitions: {end_/n if end_ > 0 else 0} ms\n"")

t, n = 101, 95
measure_time_n_reps(np.matmul, 100, np.random.randn(t,n,n), np.random.randn(n,))
measure_time_n_reps(np.tensordot, 100, np.random.randn(t,n,n), np.random.randn(n,), axes=(2,0))
t, n = 101, 96
measure_time_n_reps(np.matmul, 100, np.random.randn(t,n,n), np.random.randn(n,))
measure_time_n_reps(np.tensordot, 100, np.random.randn(t,n,n), np.random.randn(n,), axes=(2,0))
```


### Error message:

_No response_

### Runtime information:

print(np.__version__)
1.24.1

print(sys.version)
3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]

print(np.show_runtime())
[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Zen',
  'filepath': 'C:\\Program '
              'Files\\Sim4Life_7.2.1.11125\\Python\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll',
  'internal_api': 'openblas',
  'num_threads': 24,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.21'},
 {'architecture': 'Zen',
  'filepath': 'C:\\Program '
              'Files\\Sim4Life_7.2.1.11125\\Python\\Lib\\site-packages\\scipy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll',
  'internal_api': 'openblas',
  'num_threads': 24,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.17'},
 {'filepath': 'C:\\Windows\\System32\\vcomp140.dll',
  'internal_api': 'openmp',
  'num_threads': 24,
  'prefix': 'vcomp',
  'user_api': 'openmp',
  'version': None},
 {'filepath': 'C:\\Program Files\\Sim4Life_7.2.1.11125\\libiomp5md.dll',
  'internal_api': 'openmp',
  'num_threads': 24,
  'prefix': 'libiomp',
  'user_api': 'openmp',
  'version': None}]


### Context for the issue:

As explained above, np.tensordot seems to solve the issue. However, np.matmul is widely used by the community, and a multiplication of a 100x100 matrix by a 100-element vector is a normal (or even small) data dimensionality. Therefore, this issue might be causing severe computational overhead for a broad base of users.",2023-01-04 13:38:47,,"BUG: OpenBLAS switches to use many threads at once, causing a slowdown as matrix size grows past the threshold","['00 - Bug', '57 - Close?']"
22904,open,xkszltl,"### Describe the issue:

We found, since v1.21, the result of log has changed and misaligned with typical C code using math.h or python's own impl.

The issue is consistently repro on Intel servers, with Skylake or above chips.
But at the same time we cannot repro on Intel desktop, including older chips and 13-gen chips with their AVX512 removed.
So this one is possibly related:
- https://github.com/numpy/numpy/issues/17551

### Reproduce the code example:

```bash
# One-liner repro in docker across numpy versions.
# Remove that --index-url outside China, that is just my geo preference.
sudo docker run --rm -it ubuntu:20.04 sh -c 'apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y python3-pip && python3 --version && python3 -c ""import math; print(math.log(1e4))"" && for ver in $(seq 20 25); do python3 -m pip install --index-url ""https://mirrors.ustc.edu.cn/pypi/web/simple"" -U ""numpy<1.$ver"" >/dev/null 2>&1; python3 -m pip list | grep numpy && python3 -c ""import numpy as np; print(np.log(1e4))""; done; python3 -c ""import numpy as np; print(np.show_runtime())""'

# Replace 1e4 with 1e6, to show changes on v1.23
sudo docker run --rm -it ubuntu:20.04 sh -c 'apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y python3-pip && python3 --version && python3 -c ""import math; print(math.log(6e4))"" && for ver in $(seq 20 25); do python3 -m pip install --index-url ""https://mirrors.ustc.edu.cn/pypi/web/simple"" -U ""numpy<1.$ver"" >/dev/null 2>&1; python3 -m pip list | grep numpy && python3 -c ""import numpy as np; print(np.log(6e4))""; done; python3 -c ""import numpy as np; print(np.show_runtime())""'
```


### Error message:

```bash
# For log(1e4)
...
9.210340371976184
Python 3.8.10
numpy      1.19.5 
9.210340371976184
numpy      1.20.3 
9.210340371976184
numpy      1.21.6 
9.210340371976182
numpy      1.22.4 
9.210340371976182
numpy      1.23.5 
9.210340371976182
numpy      1.24.1 
9.210340371976182
WARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information
[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],
                      'found': ['SSSE3',
                                'SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2',
                                'AVX512F',
                                'AVX512CD',
                                'AVX512_SKX'],
                      'not_found': ['AVX512_KNL',
                                    'AVX512_KNM',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}}]
None
...
```

```bash
# For log(6e4).
...
11.002099841204238
Python 3.8.10
numpy      1.19.5 
11.002099841204238
numpy      1.20.3 
11.002099841204238
numpy      1.21.6 
11.002099841204236
numpy      1.22.4 
11.002099841204236
numpy      1.23.5 
11.002099841204238
numpy      1.24.1 
11.002099841204238
...
```",2022-12-30 12:26:55,,Numeric mismatch of np.log(),"['00 - Bug', '04 - Documentation']"
22898,open,divideconcept,"### Proposed new feature or change:

I find numpy.save and numpy.load quite slow, in particular with small arrays, which can become a bottleneck when dealing with large datasets in machine learning.

I came up with faster save/load routines for simple arrays (standard dtypes, no Fortran ordering or nested/labelled data) - which probably represents the vast majority of use cases: https://github.com/divideconcept/fastnumpyio

```
numpy.save: 0:00:00.813492
fastnumpyio.save: 0:00:00.334398
numpy.load: 0:00:07.910804
fastnumpyio.load: 0:00:00.306737
```

Maybe there's a way to optimize numpy.save()/numpy.load() npy header parsing for simple arrays ?",2022-12-28 08:16:07,,ENH: Faster numpy.save and numpy.load,['unlabeled']
22896,open,jhaggle,"### Describe the issue:

I have a C++ script that I run in Visual studio 2022 on windows 10 and where I include the python module. In the code I import numpy in a for-loop. The code executes fine the first round, but in the second round and exception is thrown as soon as the code tries to execute the import statement. 



### Reproduce the code example:

```python
#include <C:\Program Files\Python39\include\Python.h>
#include <iostream>

using namespace std;

int main() {

	for (int i = 0; i < 5; i++) {
		cout << i << endl;

		Py_Initialize();
		PyRun_SimpleString(""import numpy as np""); 
		PyRun_SimpleString(""arr = np.array([1,2,3])"");
		PyRun_SimpleString(""print(arr)"");
		Py_Finalize();

	}
}

```
```


### Error message:

```shell
This is the output from the console when running the code:


0
[1 2 3]
1
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
```

Here is the error message in visual studio:

```
Exception thrown at 0x00007FFCB9461319 (_multiarray_umath.cp39-win_amd64.pyd) in App4.exe: 0xC0000005: Access violation writing location 0x0000000000000008.
```
```


### Runtime information:

1.23.5
3.9.12 (tags/v3.9.12:b28265d, Mar 23 2022, 23:39:15) [MSC v.1929 32 bit (Intel)]

### Context for the issue:

_No response_",2022-12-27 20:54:53,,ENH: support re-initialization,"['01 - Enhancement', '23 - Wish List', '50 - Duplicate']"
22881,open,jsaladich,"### Describe the issue:

Hi!
While using the amazing numpy module, I found myself in the need of replace ""fancy"" indexing for `numpy take`.

I love working with boolean arrays and fancy indexing but I found this combination does not works when using concretely `numpy take`.  Perhaps this is an expected beahviour, but in [Numpy's docs says](https://numpy.org/doc/stable/reference/generated/numpy.take.html): ""When axis is not None, this function does the same thing as “fancy” indexing (indexing arrays using arrays)""

### Reproduce the code example:

```python
import numpy as np
# Dummy array
a = np.random.random(size=(3,5,8)) 
# Dummy boolean
b = np.asarray([False, True, False]) 

np.take(a,b, axis=0).shape == a[b,...].shape # returns False --> I would expect a True
```
```


### Error message:

_No response_

### Runtime information:

Numpy version: 1.23.4
Python version: 3.10.7 (main, Sep  7 2022, 15:22:19) [GCC 9.4.0]

### Context for the issue:

Perhaps am I wrong, but shouldn't ""fancy"" indexing and `np.take`behave the same with boolean arrays?

Thanks a lot for your work and your time!",2022-12-24 10:58:38,,"Numpy take does not the same thing as ""fancy"" indexing (or I am wrongly interpreting the docs?)",['04 - Documentation']
22870,open,Astlaan,"### Describe the issue:

I ran the following code:

```python
a = np.random.randint(1, 100, (10**8, 2))
%timeit a.sum(axis=1)
%timeit a[:,0]+a[:,1]
```
and this was the output:

```
1.01 s ± 16.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
163 ms ± 10.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

I.e, the .sum() method was about 6-7X slower than manual sum. If instead I do a slightly less narrow array, say (10**6, 2), .sum() is still 6-7X slower, so this doesn't appear to be just about numpy C-function call overhead.

For more ""square"" arrays, we actually see the expected improvement:
```python
b = np.random.randint(1, 100, (1000000, 200))
```


```python
%%timeit 
b.sum(axis=1)
```

```
105 ms ± 7.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

```python
%%timeit 
c = b[:,0]
for i in range(1, b.shape[1]):
    c+=b[:,i]
```

```
4 s ± 21.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

Are is .sum() needlessly slow for narrow arrays?

### Reproduce the code example:

```python
a = np.random.randint(1, 100, (100000000, 2))
%timeit a.sum(axis=1)
%timeit a[:,0]+a[:,1]
```


### Error message:

_No response_

### Runtime information:

1.21.5
3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]

### Context for the issue:

_No response_",2022-12-22 15:42:58,,BUG: ndarray.sum() slow compared to + operation for narrow array,['00 - Bug']
22860,open,qingjiaoguodong,"### Proposed new feature or change:

## Proposed new feature or change:

In the subsripts of `einsum`, allow ellipsis replaced by a named star notation like '`(*i)`', which can be explicitly referenced and expanded after '`->`'.

### Example

```python
import numpy as np

a = np.zeros((2, 3, 4))

assert np.einsum('(*i)k,(*j)k->(*i)(*j)', a, a).shape == (2, 3, 2, 3)
assert np.einsum('(*i)k,(*i)k->(*i)', a, a).shape == (2, 3)
```

## Alternative
Omit parentheses

```python
import numpy as np

a = np.zeros((2, 3))

assert np.einsum('*ik,*jk->*i*j', a, a).shape == (2, 3, 2, 3)
assert np.einsum('*ik,*ik->*i', a, a).shape == (2, 3)
```",2022-12-21 19:01:36,,ENH: Named ellipsis in einsum,['01 - Enhancement']
22859,open,dsm-72,"### Proposed new feature or change:

Just copy paste any of these solutions from [here](https://stackoverflow.com/questions/12200580/numpy-function-for-simultaneous-max-and-min) into your repository so we can just go

```python
import numpy as np

# ...
_min, _max = np.extrema(arr)
```

rather than

```
_min = np.min(arr)
_max = np.maxarr)
```",2022-12-21 17:38:27,,ENH: the well known missing extrema function to simultaneously get min / max of an array,['01 - Enhancement']
22852,open,FirefoxMetzger,"### Describe the issue:

When running the example test case below the assertion works fine on Windows, but fails on Linux with the message below.

This is a cross-post from: https://stackoverflow.com/questions/74872407/why-does-numpy-handle-overflows-inconsistently

Related PRs: https://github.com/numpy/numpy/pull/21437 , https://github.com/numpy/numpy/pull/21123

### Reproduce the code example:

```python
import numpy as np

print(f""Numpy Version: {np.__version__}"")

# version A
versionA = np.eye(2)
versionA[0, 0] = 2**32 + 1
versionA = versionA.astype(np.uint32)

# version B
versionB = np.eye(2, dtype=np.uint32)
versionB[0, 0] = np.asarray(2**32 + 1)

np.testing.assert_array_equal(versionA, versionB)
```


### Error message:

```shell
Numpy Version: 1.24.0
foo.py:8: RuntimeWarning: invalid value encountered in cast
  versionA = versionA.astype(np.uint32)
Traceback (most recent call last):
  File ""foo.py"", line 14, in <module>
    np.testing.assert_array_equal(versionA, versionB)
  File ""/home/firefoxmetzger/.local/lib/python3.8/site-packages/numpy/testing/_private/utils.py"", line 983, in assert_array_equal
    assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
  File ""/usr/lib/python3.8/contextlib.py"", line 75, in inner
    return func(*args, **kwds)
  File ""/home/firefoxmetzger/.local/lib/python3.8/site-packages/numpy/testing/_private/utils.py"", line 862, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Arrays are not equal

Mismatched elements: 1 / 4 (25%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0],
       [0, 1]], dtype=uint32)
 y: array([[1, 0],
       [0, 1]], dtype=uint32)
```


### NumPy/Python version information:

1.24.0 3.8.10 (default, Jun  2 2021, 10:49:15)
[GCC 9.4.0]

### Context for the issue:

I've tried to reduce the example further, but any further modifications change the behavior to make it work as expected. For example, changing from `np.eye(2)` to `np.empty((1, 1))` or `np.eye(1)` changes the behavior on linux and the test passes.

",2022-12-21 10:19:52,,DOC: (Undefined) behavior of float to integer casts only really explained in release notes,['04 - Documentation']
22821,open,Niwotian60,"### Describe the issue:

Running np.convolve with the default pip numpy install (libopenblas) runs a simple test 45x slower on Win11 than the identical  code on the same machine in a WSL2 Debian instance or on the same machine with a different installation of numpy compiled with the intel MKL libraries. I first encountered this in numpy 1.23.4 (it probably existed prior to this) and still manifests itself in 1.24.0.

The timing for the simple code (below) is ~25s/loop on Win 11 vs 546ms/loop on linux (same machine running as a WSL2 debian virtual machine) and 433ms/loop on Win 11 with the MKL library build.

Importing ThreadpoolController reports the following:
```
In [3]: threadpool_info() 
Out[3]: [{'user_api': 'blas',
          'internal_api': 'openblas',
          'prefix': 'libopenblas',
          'filepath': 'C:\python\Lib\site-packages\numpy\.libs\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll',
          'version': '0.3.20',
          'threading_layer': 'pthreads',
          'architecture': 'Haswell',
          'num_threads': 20,
        }]
In [4]: tc = ThreadpoolController() 
In [5]: a1,a2=np.random.random(100000), np.random.random(100000)
In [6]: %timeit np.convolve(a1,a2) 
25.2 s ± 160 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) 
In [7] tc.info() 
Out[7]: [{'user_api': 'blas',
          'internal_api': 'openblas',
          'prefix': 'libopenblas',
          'filepath': 'C:\python\Lib\site-packages\numpy\.libs\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll',
          'version': '0.3.20',
          'threading_layer': 'pthreads',
          'architecture': 'Haswell',
          'num_threads': 20,
        }]
```
The reported num_threads matches my cpu (10/20 core). Watching the task manager performance monitor during the program run showed a very strong affinity to a single CPU core (~100%) while the others were at noise level (0-10% utilization).

Just as a reference point, running the same code with the same install but using `scipy.signal.fftconvolve` gives:
`8.54 ms ± 42.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)`

System Info:
```
Processor	Intel(R) Core(TM) i9-10900KF CPU @ 3.70GHz   3.70 GHz
Installed RAM	64.0 GB (61.8 GB usable)
GPU                 Nvidia GeForce RTX 3090 (24GB VRAM)
```

### Reproduce the code example:

```python
>>> import numpy as np
>>> sample_size = 100_000
>>> a1, a2 = np.random.random(sample_size), np.random.random(sample_size)
>>> %timeit np.convolve(a1,a2)
25.1 s ± 76.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```


### Error message:

```shell
No errors, just major (45x) performance issue
```


### NumPy/Python version information:

Python 3.11.1 installed from Python home page
Numpy 1.24.0 (and 1.23.5 and 1.23.4, probably earlier versions as well)

### Context for the issue:

No direct impact to me due to multiple alternatives. Not sure how pervasive the issue is, though.

Edit: (mattip) formatting",2022-12-18 21:46:24,,BUG: Major performance issue (45x) with np.convolve on win 11 vs linux or MKL numpy,['00 - Bug']
22809,open,ngoldbaum,"### Proposed new feature or change:

Currently NEP 42 dtypes do not have a way to determine the output dtype of an operation based on the values of the inputs.

I hit this issue trying to implement string multiplication for [`ASCIIDType`](https://github.com/numpy/numpy-user-dtypes/tree/main/asciidtype):

```python
import numpy as np
from asciidtype import ASCIIDType

dtype = ASCIIDType(5)

arr = np.array([""data""], dtype=dtype)

arr * 2
```

In this example, the output dtype should be `ASCIIDType(10)` (10 character wide ASCII), following what currently happens with `np.char` for string or byte dtypes:



```python
In [8]: arr = np.array([""hello"", ""world""], dtype=""U5"")

In [9]: np.char.multiply(arr, [2, 3])
Out[9]: array(['hellohello', 'worldworldworld'], dtype='<U15')

In [10]: np.char.multiply(arr, 2)
Out[10]: array(['hellohello', 'worldworld'], dtype='<U10')
```

Another example is quantity exponentiation:

```python
import numpy as np
from unitdtype import UnitDType

arr = np.array([1, 2, 3], dtype=UnitDType('meter'))
arr ** 2
```

The result should have units of `meter**2`, but we can't know that without inspecting the value of the second operand to the power ufunc. Note that unlike the string multiplication example, unit exponentiation only makes sense when exponentiating by a scalar.

NEP 42 defines the `resolve_descriptors` function to determine the output dtype for ufuncs, but that function only has access to the input dtypes, not the input values.

@seberg tells me that it was an explicit design decision to not allow looking at the input data in `resolve_descriptors` since it can enable confusing runtime behaviors. Since some use cases *need* this to work, a nice compromise might be to define an alternative to `NPY_METH_resolve_descriptors` that allows implementors to write a dtype resolution function that has pointers to the input arrays. That way dtype implementations will need to explicitly opt in to this behavior.",2022-12-15 21:05:25,,ENH: Add a way for ufuncs to do value-based promotion with NEP 42 dtypes,"['01 - Enhancement', '15 - Discussion', 'component: numpy.dtype']"
22782,open,alanhdu,"### Describe the issue:

Perhaps this is a false assumption on my part, but I assumed that given a NumPy array that `arr.tobytes()` and `bytes(arr)` would alwyas return the same thing, but this does not seem to be true for 0-dimensional arrays. In this case, `bytes(arr)` returns an empty bytestring, while `arr.tobytes()` returns the element casted as a byte. I personally find the latter behavior more intuitive, but I think they should probably be consistent.

### Reproduce the code example:

```python
arr = np.array(0)
assert arr.tobytes() == bytes(arr)
```


### Error message:

```python
AssertionError
```

### NumPy/Python version information:

```
NumPy: 1.23.5 
Python: 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]
```

### Context for the issue:

_No response_",2022-12-12 19:58:25,,BUG: bytes(arr) != arr.tobytes() for 0-d arrays,['00 - Bug']
22766,open,aschaffer,"### Describe the issue:

When `out is not None` and it is of a different type than `float` or different from the type the quantile() call would return as a return parameter, `quantile()` fails (see error message below).

The documented `numpy.quantile` functionality specifies that casting should be done if necessary: 
```
out ndarray, optional
Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type (of the output) will be cast if necessary.
```


### Reproduce the code example:

```python
import numpy as np
qs_arr = np.array([0.0, 0.5, 1.0])
q_out = np.zeros(3, dtype=int)
np.quantile(np.arange(4, dtype=float), qs_arr, out=q_out) # fails with error message below

q_outf = np.zeros(3, dtype=float)
np.quantile(np.arange(4, dtype=float), qs_arr, out=q_outf) # works fine
array([0. , 1.5, 3. ])
```


### Error message:

```shell
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<__array_function__ internals>"", line 180, in quantile
  File ""miniconda3/envs/dev_22_12_11_4_3_8/lib/python3.8/site-packages/numpy/lib/function_base.py"", line 4412, in quantile
    return _quantile_unchecked(
  File ""miniconda3/envs/dev_22_12_11_4_3_8/lib/python3.8/site-packages/numpy/lib/function_base.py"", line 4424, in _quantile_unchecked
    r, k = _ureduce(a,
  File ""miniconda3/envs/dev_22_12_11_4_3_8/lib/python3.8/site-packages/numpy/lib/function_base.py"", line 3725, in _ureduce
    r = func(a, **kwargs)
  File ""miniconda3/envs/dev_22_12_11_4_3_8/lib/python3.8/site-packages/numpy/lib/function_base.py"", line 4593, in _quantile_ureduce_func
    result = _quantile(arr,
  File ""miniconda3/envs/dev_22_12_11_4_3_8/lib/python3.8/site-packages/numpy/lib/function_base.py"", line 4710, in _quantile
    result = _lerp(previous,
  File ""miniconda3/envs/dev_22_12_11_4_3_8/lib/python3.8/site-packages/numpy/lib/function_base.py"", line 4529, in _lerp
    lerp_interpolation = asanyarray(add(a, diff_b_a * t, out=out))
numpy.core._exceptions.UFuncTypeError: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'
```


### NumPy/Python version information:

1.23.3 3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10) 
[GCC 10.3.0]

### Context for the issue:

_No response_",2022-12-09 18:35:05,,BUG: quantile fails with out=output when casting is necessary,['00 - Bug']
22764,open,ustcfdm,"### Proposed new feature or change:

For the function of `nansum`, when all elements are nan, it returns 0. However, in some cases I want it return 'nan'. Is it possible to add an option to let user determine which value (0 or nan) to return? For example:

```python
a = np.array([[np.nan, np.nan], [1,np.nan]])

np.nansum(a, axis=1)    # returns array([0, 1])

# This is what I want
np.nansum(a, axis=1, keep_nan=True)    # return array([nan, 1])
```

The reason that I need this is because when I process image data, I need to distinguish pixels with value 0 and value nan. ""Zero"" means the pixel has a meaningful value, which is 0; ""nan"" means the pixel has a meaningless value.

I noticed that there are alrealy some disscussions about this (#6549), i.e. whether should return 0 or nan. Since it is controversial, I suggest given users an option to determine which one they want.


",2022-12-09 08:19:38,,ENH: nansum for all nan arrays,['unlabeled']
22755,open,herrlich10,"### Describe the issue:

np.rec.fromarrays(...) may fail if a field contains arrays of the same shape[0]. The error will not occur if the arrays are of different shape[0], e.g., `[np.zeros(2), np.ones(3)]`. The error will not occur if written as `np.array([(np.zeros(2), 'a'), (np.ones(2), 'b')], dtype=[('field1', object), ('field2', '<U1')])`.

### Reproduce the code example:

```python
import numpy as np
np.rec.fromarrays([[np.zeros(2), np.ones(2)], ['a', 'b']], names=['field1', 'field2'])
```


### Error message:

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1-eecf8753473e> in <module>
      1 import numpy as np
----> 2 np.rec.fromarrays([[np.zeros(2), np.ones(2)], ['a', 'b']], names=['field1', 'field2'])

~/PythonPlus/anaconda3/lib/python3.9/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder)
    679         testshape = obj.shape[:obj.ndim - nn]
    680         if testshape != shape:
--> 681             raise ValueError(""array-shape mismatch in array %d"" % k)
    682 
    683     _array = recarray(shape, descr)

ValueError: array-shape mismatch in array 1
```


### NumPy/Python version information:

1.20.3 3.9.7 (default, Sep 16 2021, 08:50:36) 
[Clang 10.0.0 ]

### Context for the issue:

_No response_",2022-12-07 15:36:18,,BUG: np.rec.fromarrays(...) may fail if a field contains arrays of the same shape[0],['01 - Enhancement']
22742,open,SV-97,"### Describe the issue:

When using `np.polynomial.Polynomial.fit` to fit a polynomial to some data, the `dtype` of the `window` of the resulting `np.polynomial.Polynomial` is different than the one of an instance constructed directly via `np.polynomial.Polynomial`.

This leads to polynomials effectively having the same window but still being incompatible because of the windows equality comparison evaluating to `False`.

I locally tested modifying the dtype of `polydomain` [here](https://github.com/numpy/numpy/blob/54c52f13713f3d21795926ca4dbb27e16fada171/numpy/polynomial/polynomial.py#L97) by setting `polydomain = np.array([-1., 1.])` which fixed the issue for me - however I have no idea if this raises other problems in other places.

### Reproduce the code example:

```python
import numpy as np

xs1 = np.array([0.0]) 
ys1 = np.array([1.0])

# fit a constant polynomial to (xs1, ys1)
p1 = np.polynomial.Polynomial(np.mean(ys1))

xs2 = np.array([0.0, 1.0])
ys2 = np.array([0.0, 2.0])

# fit a linear polynomial to (xs2, ys2)
p2 = np.polynomial.Polynomial.fit(xs2, ys2, deg=1)

print(p1.window.dtype, p2.window.dtype)
assert(p1.window.dtype != p2.window.dtype) # this assertion should Error but it doesn't
```


### Error message:

_No response_

### NumPy/Python version information:

1.23.5 3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:36:39) [GCC 10.4.0]

### Context for the issue:

Since `np.polynomial.Polynomial` can't fit constant polynomials, a wrapping fitting function has to construct some polynomials via the default constructor and others via `.fit`. My work requires finding `(poly_1 - poly_2)**2` which fails because of incompatible windows.

This can be worked around by manually setting the windows on either instance.",2022-12-06 10:36:31,,BUG: `Polynomial.fit(...).window.dtype` inconsistent with `Polynomial(...).dtype`,['00 - Bug']
22739,open,HaoZeke,"```
[100.00%] ··· bench_ma.MAMethodGetItem.time_methods_getitem
ok
[100.00%] ··· ========= ========== ============
              --                 msize
              --------- -----------------------
                margs     small        big
              ========= ========== ============
                  0      21.8±2μs    20.2±2μs
                (0, 0)   4.95±1μs   4.05±0.4μs
               [0, -1]   44.6±4μs    43.0±3μs
              ========= ========== ============
```

_Originally posted by @mattip in https://github.com/numpy/numpy/issues/22731#issuecomment-1338088738_
      

The reproducer for this is:

```python
xs = np.random.uniform(-1, 1, 6).reshape(2, 3)
m1 = [[True, False, False], [False, False, True]]
xl = np.random.uniform(-1, 1, 100*100).reshape(100, 100)
maskx = xl > 0.8
nmxs = np.ma.array(xs, mask=m1)
nmxl = np.ma.array(xl, mask=maskx)
getattr(nmxl, '__getitem__')((0,0))
getattr(nmxs, '__getitem__')([0,-1])
```

Or by running the ASV benchmark: `python runtests.py --bench bench_ma.MAMethodGetItem`",2022-12-06 02:02:11,,BUG: MaskedArray indexing is slow (except with tuples),['component: numpy.ma']
22734,open,seberg,"This is just an early thought, but I am wondering whether we should change the ""context"" used in the ufunc inner-loop.  We currently have a context:
```
Context:
    * caller (the ufunc for ufuncs)
    * ""self""
    * dtypes (descriptors)
```
and a separately passed user allocated `NpyAuxData *` (a replacement for which also may be nice).  The `caller` is not super interesting for the inner-loop function, but nice for `resolve_dtypes` and `get_loop`.
OTOH, I am thinking that e.g. the interpreter state may be something we should eventually be added (or maybe even a way to ""pass"" kwargs?!).

There are two reasons why I am thinking about it:
1. I was looking into `decref`/`clear` functionality which really is a very simple loop and the only thing it needs is the `dtype` (and maybe whatever we add in the future). 
2. If you look at structured dtypes, the ""caller"" is the same always, while self and dtypes are not.  So if that was split up, one could be passed on unchanged and the other not.

Anyway, just fishing for opinions and writing things down in case it helps me...",2022-12-05 16:15:08,,"DISCUSS: Change the `context` setup used in ""ArrayMethod"" ufunc loops?",['unlabeled']
22710,open,seiko2plus,"### Adopting a C++ style guide:

The current approach to using C++ is somewhat unclear. While trying to review #22315, I faced several obstacles, and I couldn't determine which style/rules I should follow; C++ is a monster of features compared to the C language. We urgently need to impose a specific code style to avoid chaos.

I suggest adopting the latest [Google C++ Style guide](https://google.github.io/styleguide/cppguide.html), with minor modifications limited to the following:

- C++ Version: code should target C++14 , or maybe keep it as-is C++17. Would it be possible? However, keep targeting C++11 in 2023 is too rigorous.

- Spaces vs. Tabs: indent four spaces at a time. 

- Function Declarations and Definitions: similar to our C style, except the return type goes to the same line next to the function name.
",2022-12-02 03:36:15,,ENH: Adopting a C++ style guide,['C++']
22688,open,hadriman,"### Describe the issue:

nanmedian will sometimes return inf instead of a proper value for numbers that are close the maximum value of floats data type.
This behaviour seems to depend on the usage of the ""axis"" keyword parameter.

For example np.nanmedian(np.array([[1,4e4]],dtype=""float16""),axis=0)
returns [ 1., inf] instead of [1.,4e4]

This is true at least for float16, float32 and float64 with values that are close to the respective max of these data types.

np.median doesn't seem have any issues with these values. 

### Reproduce the code example:

```python
import numpy as np

# I'll start with a simple array
# max value for float16 should be 6.55e4
a = np.array([1,1e4,4e4],dtype=""float16"")
print(a)

# then try to compute the median along a given axis
# this is the line that produces faulty output
b = np.nanmedian([a,a],axis=0)
print(b)
# we get [1.e+00 1.e+04    inf] instead of [1.e+00 1.e+04 4.e+04]


# the regular median shows the expected behaviour
c = np.median([a,a],axis=0)
print(c)


# the strange behaviour is not always present
# for example nanmedian works ok in this situation
d = np.nanmedian(np.array([[4e4,4e4]],dtype=""float16""))
print(d)
```


### Error message:

```shell
No error message but a runtime warning :
C:\Users\***\Anaconda3\lib\site-packages\numpy\core\_methods.py:48: RuntimeWarning: overflow encountered in reduce
  return umr_sum(a, axis, dtype, out, keepdims, initial, where)
```


### NumPy/Python version information:

1.21.5 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]

### Context for the issue:

This issue causes valid values that are not inf to be treated as inf (and eventually as NaNs after further operations) unless a larger data type is used which isn't always feasible when treating very large data sets.",2022-11-29 12:49:36,,BUG: nanmedian has issues with very large float values,"['00 - Bug', 'component: numpy.lib']"
22686,open,bluescarni,"### Describe the issue:

Hello!

I am trying to use the new configurable memory routines from NEP 49, described here:

https://numpy.org/devdocs/reference/c-api/data_memory.html#configurable-memory-routines-in-numpy-nep-49

My use case is that I want to store non-trivial C++ objects as elements of NumPy arrays. In order to correctly manage the lifetime of my C++ objects (including correct construction and destruction without memory leaking), I override the default ``malloc``/``calloc``/``realloc``/``free`` implementations so that metadata is associated to every array data segment allocated by NumPy. This metadata keeps track of which C++ objects have been constructed in the data segment. Then, in the basic NumPy primitives for my C++ dtype (e.g., ``get/setitem``, ``nonzero``, ``copyswap``, etc.), I query the metadata of the data segment and construct C++ object in-place as needed. The ``free`` function override takes care of calling the destructors of the constructed C++ objects before de-allocating the data segment.

For reference, here is the implementation of the overriding memory functions + metadata handling:

https://github.com/bluescarni/heyoka.py/blob/b661040030ff4d05c0dc2e03c8c58217d9ca0d71/heyoka/numpy_memory.cpp

And here is the code for exposing my C++ type (called ``mppp::real``) to Python and NumPy:

https://github.com/bluescarni/heyoka.py/blob/b661040030ff4d05c0dc2e03c8c58217d9ca0d71/heyoka/expose_real.cpp

So far, most of NumPy's basic functionality seems to work fine: I can create new arrays with my custom C++ dtype, access elements, etc. See here for a WIP testing suite I am working on:

https://github.com/bluescarni/heyoka.py/blob/b661040030ff4d05c0dc2e03c8c58217d9ca0d71/heyoka/test.py#L5399

I am running however in an issue when I attempt to cast arrays of my custom dtype to/from other NumPy dtypes.

What happens is that the casting functions seem to receive ``from``/``to`` arrays in which the data segment has **NOT** been allocated via the configurable memory routines from NEP 49. This of course is a big issue, because there is no metadata associated to the data segments, and my whole scheme for managing the lifetimes of my C++ objects comes to a crashing halt.

I have spent a few hours with a debug build of NumPy and a debugger. Although I haven't fully understood the intricacies of the casting logic, I think I have managed to pinpoint where the data segments passed to the casting functions originate from, and it seems to be here:

https://github.com/numpy/numpy/blob/main/numpy/core/src/multiarray/dtype_transfer.c#L2865

I.e., from a direct ``PyMem_Malloc()`` call (rather than leveraging the configurable memory routines from NEP 49). The ``to``/``from`` buffers that are passed to the casting functions are defined a bit below:

https://github.com/numpy/numpy/blob/main/numpy/core/src/multiarray/dtype_transfer.c#L2876

(``from_buffer`` and ``to_buffer``).

Am I wrong in expecting that these two buffers should be allocated via the NEP 49 functions?



### Reproduce the code example:

```python
No easy reproducer, sorry - but I included links in the issue description.
```


### Error message:

_No response_

### NumPy/Python version information:

1.25.0.dev0+20.gd4b2d4f80 3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:36:39) [GCC 10.4.0]

### Context for the issue:

_No response_",2022-11-28 22:14:26,,BUG: casting not using configurable memory functions from NEP 49?,['00 - Bug']
22682,open,yasirroni,"### Issue with current documentation:

From [structured array doc](https://numpy.org/doc/stable/user/basics.rec.html), there is no a clear and concise tutorial to get `key` names or even the `key`-`value` pairs.

Currentyly, list of `key` tutorial only tell it implicitly:

```python
d = np.dtype([('x', 'i8'), ('y', 'f4')])
d.names
# ('x', 'y')
```

### Idea or request for content:

Tell user that the `keys` are under `dtype`, and to get `key`-`value` pairs, user need to do the following:

```python
for key in x.dtype.names:
    value = x[key]
```

Also might be related, please

1. support `dict()` conversion:

    ```python
    dict_ = dict(x)

    # equal to
    dict_ = {}
    for key in x.dtype.names:
        dict_[key] = x[key]
    ```

1. support `.items()` method to iterate over `key`-`value` pairs:

    ```python
    for key, value in x.items():
        key, value

    # equal to
    for key in x.dtype.names:
        value = x[key]
    ```

I might be able to submit PR to implement that functionality.",2022-11-28 03:45:56,,Improve structured array doc regarding extracting `key`-`value` pairs,['04 - Documentation']
22680,open,nieder,"### Describe the issue:

On macOS 12.5.1 (native x86_64), these two tests fail (tried python3.8 - 3.10):
```
FAILED numpy/core/tests/test_mem_policy.py::test_new_policy - AssertionError: assert False
FAILED numpy/core/tests/test_ufunc.py::TestUfuncGenericLoops::test_unary_PyUFunc_O_O_method_full[reciprocal] - AssertionError: FloatingPointError not raised
```
The same build settings passes all tests on macOS 14.5.6.

#20637 shows the same error in test_unary_PyUFunc_O_O_method_full, but that was supposedly fixed after 1.21.5, and this is with 1.23.4.
Maybe #22416 relates to test_new_policy but it's not clear.

### Reproduce the code example:

```python
Run tests with `/opt/sw/bin/python3.9 -Bm pytest numpy`
```


### Error message:

```shell
______________________________________________________________________________ test_new_policy ______________________________________________________________________________

get_module = <module 'mem_policy' from '/private/tmp/pytest-of-fink-bld/pytest-0/test_set_policy0/mem_policy/mem_policy.cpython-39-darwin.so'>

    @pytest.mark.slow
    def test_new_policy(get_module):
        a = np.arange(10)
        orig_policy_name = np.core.multiarray.get_handler_name(a)
    
        orig_policy = get_module.set_secret_data_policy()
    
        b = np.arange(10)
        assert np.core.multiarray.get_handler_name(b) == 'secret_data_allocator'
    
        # test array manipulation. This is slow
        if orig_policy_name == 'default_allocator':
            # when the np.core.test tests recurse into this test, the
            # policy will be set so this ""if"" will be false, preventing
            # infinite recursion
            #
            # if needed, debug this by
            # - running tests with -- -s (to not capture stdout/stderr
            # - setting extra_argv=['-vv'] here
>           assert np.core.test('full', verbose=2, extra_argv=['-vv'])
E           AssertionError: assert False
E            +  where False = <numpy._pytesttester.PytestTester object at 0x10270b0d0>('full', verbose=2, extra_argv=['-vv'])
E            +    where <numpy._pytesttester.PytestTester object at 0x10270b0d0> = <module 'numpy.core' from '/opt/sw/build.build/root-numpy-py39-1.23.4-1/opt/sw/lib/python3.9/site-packages/numpy/core/__init__.py'>.test
E            +      where <module 'numpy.core' from '/opt/sw/build.build/root-numpy-py39-1.23.4-1/opt/sw/lib/python3.9/site-packages/numpy/core/__init__.py'> = np.core

numpy/core/tests/test_mem_policy.py:378: AssertionError
```

```
___________________________________________________ TestUfuncGenericLoops.test_unary_PyUFunc_O_O_method_full[reciprocal] ____________________________________________________

self = <numpy.core.tests.test_ufunc.TestUfuncGenericLoops object at 0x1214632e0>, ufunc = <ufunc 'reciprocal'>

    @pytest.mark.parametrize(""ufunc"", UNARY_OBJECT_UFUNCS)
    def test_unary_PyUFunc_O_O_method_full(self, ufunc):
        """"""Compare the result of the object loop with non-object one""""""
        val = np.float64(np.pi/4)
    
        class MyFloat(np.float64):
            def __getattr__(self, attr):
                try:
                    return super().__getattr__(attr)
                except AttributeError:
                    return lambda: getattr(np.core.umath, attr)(val)
    
        # Use 0-D arrays, to ensure the same element call
        num_arr = np.array(val, dtype=np.float64)
        obj_arr = np.array(MyFloat(val), dtype=""O"")
    
        with np.errstate(all=""raise""):
            try:
>               res_num = ufunc(num_arr)
E               FloatingPointError: invalid value encountered in reciprocal

numpy/core/tests/test_ufunc.py:174: FloatingPointError

During handling of the above exception, another exception occurred:

self = <numpy.core.tests.test_ufunc.TestUfuncGenericLoops object at 0x1214632e0>, ufunc = <ufunc 'reciprocal'>

    @pytest.mark.parametrize(""ufunc"", UNARY_OBJECT_UFUNCS)
    def test_unary_PyUFunc_O_O_method_full(self, ufunc):
        """"""Compare the result of the object loop with non-object one""""""
        val = np.float64(np.pi/4)
    
        class MyFloat(np.float64):
            def __getattr__(self, attr):
                try:
                    return super().__getattr__(attr)
                except AttributeError:
                    return lambda: getattr(np.core.umath, attr)(val)
    
        # Use 0-D arrays, to ensure the same element call
        num_arr = np.array(val, dtype=np.float64)
        obj_arr = np.array(MyFloat(val), dtype=""O"")
    
        with np.errstate(all=""raise""):
            try:
                res_num = ufunc(num_arr)
            except Exception as exc:
                with assert_raises(type(exc)):
>                   ufunc(obj_arr)

numpy/core/tests/test_ufunc.py:177: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/sw/lib/python3.9/unittest/case.py:226: in __exit__
    self._raiseFailure(""{} not raised"".format(exc_name))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.case._AssertRaisesContext object at 0x121463700>, standardMsg = 'FloatingPointError not raised'

    def _raiseFailure(self, standardMsg):
        msg = self.test_case._formatMessage(self.msg, standardMsg)
>       raise self.test_case.failureException(msg)
E       AssertionError: FloatingPointError not raised

/opt/sw/lib/python3.9/unittest/case.py:163: AssertionError
```
```


### NumPy/Python version information:

1.23.4 3.9.12 (main, Apr 29 2022, 21:10:17) 
[Clang 13.1.6 (clang-1316.0.21.2)]


### Context for the issue:

_No response_",2022-11-27 13:34:54,,BUG: 2 tests failing on macOS 12.5.1 (x86_64) (numpy-1.23.4),['00 - Bug']
22651,open,s-banach,"### Describe the issue:

I assume this is a documentation issue rather than a real bug.

`power()` is very slow.
I suppose it must be slow in order to achieve accuracy?
Maybe the documentation can explain how much accuracy is lost by using the faster approach `exp(p*log(x))`, so that users can decide between the two approaches.



### Reproduce the code example:

```python
import numpy as np

def power(x: np.ndarray, p: float):
    temp = np.empty_like(x)
    np.log(x, out=temp)
    np.multiply(p, temp, out=temp)
    np.exp(temp, out=temp)
    return temp

x = np.logspace(-4, 4, 50_000_000)

%%timeit
power(x, 2.1)
""477 ms ± 34.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)""

%%timeit
np.power(x, 2.1)
""2.71 s ± 32.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)""

np.allclose(power(x, 2.1), np.power(x, 2.1))
""True""
```


### Error message:

_No response_

### NumPy/Python version information:

1.22.4 3.9.7 (default, Sep 16 2021, 13:09:58) 
[GCC 7.5.0]

### Context for the issue:

_No response_",2022-11-22 20:27:57,,"BUG? DOC? Why is exp(p*log(x)) faster than power(x, p)?","['00 - Bug', '33 - Question']"
22635,open,InessaPawson,"Remove mentions of `python2` (without space). 
Please note that the files with release notes should not be modified.
For context, refer to https://github.com/numpy/numpy/issues/22400.
      ",2022-11-21 00:49:54,,DOC: Remove info specific to Python 2 from the latest docs version,['sprintable']
22631,open,hughperkins,"### Describe the issue:

When using numpy.typing, multiplying an array with dtype `uint8` by any integer converts the typing dtype to `signedinteger`

### Reproduce the code example:

```python
import numpy as np
from numpy.typing import NDArray


def get_uint8() -> NDArray[np.uint8]:
    return np.random.rand(3).astype(np.uint8)


a = get_uint8()  # a is correctly NDArray[np.uint8]
b = a * 1        # b is NDArray[signedinteger]
```


### Error message:

```shell
`b` in the code shows as `NDArray[signedinteger]`, instead of `NDArray[np.uint8]`
```


### NumPy/Python version information:

```
$ python -c 'import sys, numpy; print(numpy.__version__, sys.version)'
1.23.4 3.10.7 (main, Nov 14 2022, 18:57:04) [Clang 14.0.0 (clang-1400.0.29.202)]
```

### Context for the issue:

I'm using numpy.typing to differentiate between some arrays of type `uint8` and some arrays of type `float32`. However, I have to use a `cast` every time I multiply an array by a number, which compromises the effectiveness of type checking.",2022-11-20 15:01:17,,numpy.typing uint8 becomes signedinteger on multiplication,"['00 - Bug', 'Static typing']"
22610,open,mattip,I still have not worked out what is failing but the MacPython/openblaslib build of 0.3.21 times out on ppc64le when building the 64-bit interface library. The 32-bit interface builds without timing out. The test CI matrix was changed to use the 32-bit interface in #22525. This should be reverted once there are 64-bit interface builds.,2022-11-17 16:05:03,,BUILD: restore the use of 64-bit OpenBLAS interfaces on ppc64le,['unlabeled']
22609,open,mfkasim1,"### Describe the issue:

The reason for `log1p` existence is to provide a more numerically stable function for small input, compared to `log(1 + x)`.
If the input is real valued, `np.log1p` can serve its purpose.
However, when the input is complex and small, `np.log1p` loses its accuracy.

### Reproduce the code example:

```python
import numpy as np
print(np.log1p(1e-18 + 1e-18j))  # wrongly produces 1e-18j, should be 1e-18 + 1e-18j
```


### Error message:

_No response_

### NumPy/Python version information:

`1.23.4 3.9.15 (main, Nov  4 2022, 16:13:54) 
[GCC 11.2.0]`

### Context for the issue:

_No response_",2022-11-17 14:50:23,,BUG: Inaccurate `log1p` for small complex input,['00 - Bug']
22604,open,mattbarrett98,"### Describe the issue:

`np.einsum` is ~20x slower than other libraries. 

### Reproduce the code example:

```python
import numpy as np
x = np.random.uniform(size=(1000, 1, 500))
y = np.random.uniform(size=(1000, 1, 500))
%timeit np.einsum('thd,Thd->thT', x, y)
363 ms ± 2.21 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)


import jax.numpy as jnp
%timeit jnp.einsum('thd,Thd->thT', x, y)
21.2 ms ± 1.84 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)


import torch
x = torch.rand(size=(1000, 1, 500))
y = torch.rand(size=(1000, 1, 500))
%timeit torch.einsum('thd,Thd->thT', x, y)
14 ms ± 87.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)


import tensorflow as tf
x = tf.random.uniform(shape=(1000, 1, 500))
y = tf.random.uniform(shape=(1000, 1, 500))
%timeit tf.einsum('thd,Thd->thT', x, y)
18.7 ms ± 109 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```


### Error message:

_No response_

### NumPy/Python version information:

1.21.6 3.7.15 (default, Oct 12 2022, 19:14:55) 
[GCC 7.5.0]

### Context for the issue:

_No response_",2022-11-16 16:55:54,,how is numpy's `einsum` so much slower than other libraries?,['33 - Question']
22590,open,cube2022,"### Describe the issue:

For large data matrices the method np.linalg.eigh and scipy.linalg.eigh produce different results regarding the eigenvalues.
I realize this behaviour only for matrices which have more than 40000 columns (or rows depending which one is used in the method). 
Example (see code example): I use a random matrix with 40000 rows and 2 columns, calculate the covariance-matrix regarding the rows and want to determine the eigenvalues.

Is there a reason for this behaviour or is there something wrong on my side?

### Reproduce the code example:

```python
import numpy as np
rand=np.random.rand(40000,2)
eigvals_np,_=np.linalg.eigh(np.cov(rand))

from scipy import linalg
eigvals_scipy,_=linalg.eigh(np.cov(rand))
```


### Error message:

_No response_

### NumPy/Python version information:

numpy      1.23.1

### Context for the issue:

It would be interesting to know if one of the methods (numpy vs. scipy) is preferable or useless when the matrix is large. ",2022-11-15 09:54:20,,BUG: Different eigenvalue results between np.linalg.eigh and scipy.linalg.eigh for large matrices,['00 - Bug']
22569,open,robertbrauer1988,"### Describe the issue:

np.testing.assert_equal() only tests for lists and tuples via isinstance. Therefore, collections.abc.Sequence objects cannot be compared.

### Reproduce the code example:

```python
import numpy as np
import collections

class Seq(collections.abc.Sequence):
    def __init__(self, lst):
        self._list = lst
        
    def __getitem__(self, index):
        return self._list[index]
    
    def __len__(self):
        return len(self._list)
    
sequence1 = Seq([1, 2, 3])
sequence2 = Seq([1, 2, 3])

np.testing.assert_equal(sequence1, sequence2)
```


### Error message:

```shell
AssertionError: 
Items are not equal:
 ACTUAL: <__main__.Seq object at 0x7f0460255cd0>
 DESIRED: <__main__.Seq object at 0x7f04602752e0>
```


### NumPy/Python version information:

1.21.5 3.9.13 (main, Aug 25 2022, 23:26:10) 
[GCC 11.2.0]

### Context for the issue:

I cannot write a proxy class for a numpy array to add additional functionality to it.",2022-11-11 07:48:56,,BUG: np.testing.assert_equal() does not support collections.abc.Sequence objects.,"['00 - Bug', '57 - Close?']"
22537,open,NickleDave,"### Issue with current documentation:

Hi numpy maintainers,

Very minor issue but I noticed a link on this page is wrong, under this heading:  
https://numpy.org/doc/stable/dev/releasing.html#numpy-docs

Some how the .rst extension got a .txt extension added after it, which gives a 404:  
https://github.com/numpy/numpy/blob/main/doc/HOWTO_RELEASE.rst.txt

The page is here in source
https://github.com/numpy/numpy/blob/main/doc/source/dev/releasing.rst
that then does a literal include for this
https://github.com/numpy/numpy/blob/main/doc/HOWTO_RELEASE.rst
where the link is clearly correct (without the .txt extension)
https://github.com/numpy/numpy/blob/2081f2997b3ff192604415587761ce1806dc20a8/doc/HOWTO_RELEASE.rst?plain=1#L16

This must be a special thing sphinx is set up to do post-hoc?  
I searched for issues but didn't find anything super relevant

Again I know this is not a major issue since presumably you don't usually need a reminder for how to release, just trying to be a good open source citizen.

### Idea or request for content:

_No response_",2022-11-06 00:11:46,,"DOC: Incorrect link to HOWTORELEASE in rendered ""releasing"" page",['04 - Documentation']
22535,open,oscarbenjamin,"### Describe the issue:

Here `numpy.float64.__lt__` is doing something weird that seems to take the result of `x.__gt__` and call `bool` on it. It should just return `NotImplemented` to let `x.__gt__` handle the operation.
```python
In [1]: import sympy

In [2]: import numpy

In [3]: x = sympy.symbols('x')

In [4]: xx = numpy.float64(0.1)

In [5]: x > xx
Out[5]: x > 0.1

In [6]: xx < x
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-6-af13ae84b73d> in <module>
----> 1 xx < x

~/current/sympy/sympy.git/sympy/core/relational.py in __bool__(self)
    509 
    510     def __bool__(self):
--> 511         raise TypeError(""cannot determine truth value of Relational"")
    512 
    513     def _eval_as_set(self):

```
Note that an ordinary `float` works:
```python
In [7]: float(xx) < x
Out[7]: x > 0.1

In [8]: float(xx).__lt__(x)
Out[8]: NotImplemented
```

### Reproduce the code example:

```python
import numpy as np
import sympy
x = sympy.symbols('x')
xx = np.float64(0.1)
xx < x
```


### Error message:

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/current/sympy/sympy.git/t.py in <module>
      3 x = sympy.symbols('x')
      4 xx = np.float64(0.1)
----> 5 xx < x

~/current/sympy/sympy.git/sympy/core/relational.py in __bool__(self)
    509 
    510     def __bool__(self):
--> 511         raise TypeError(""cannot determine truth value of Relational"")
    512 
    513     def _eval_as_set(self):

TypeError: cannot determine truth value of Relational
```


### NumPy/Python version information:

In [9]: import sys, numpy; print(numpy.__version__, sys.version)
1.23.4 3.8.9 (default, Apr  3 2021, 01:02:10) 
[GCC 5.4.0 20160609]

### Context for the issue:

This is coming from SO:
https://stackoverflow.com/questions/74326008/type-error-in-sympy-piecwise-python-cannot-determine-truth-value-of-relational/74327769#74327769",2022-11-05 12:48:42,,BUG: float64.__lt__ should return NotImplemented,"['04 - Documentation', '57 - Close?']"
22522,open,oscargus,"### Proposed new feature or change:

Currently, the `around`-function supports rounding to a given number of decimal digits. It is often quite convenient to be able to round to a given number of binary digits to mimic fixed-point representations. Currently this can be readily achieved using e.g.

``` python
fractional_bits = 5
scale = 2**fractional_bits
x_round = np.around(x*scale)/scale
```

However, it would be more convenient (and probably faster) to provide dedicated support for it.

I see a few different ways to obtain this:
1. Provide a separate function (`binaryround`?)
2. Provide a `base` argument to `around` which defaults to `10`.
3. Provide a `quant(ization)` function where the argument is the step-size. (For completeness, one may think of having multiple quantization modes, not just rounding)",2022-11-03 10:37:21,,ENH: binary (arbitrary base) rounding,['01 - Enhancement']
22516,open,Renmusxd,"### Describe the issue:

My entire computer crashes when computing eigenvalues on the pip precompiled numpy but not the source/compiled version. 
The version of LAPACK/BLAS changes and the pip version was multithreaded but the locally compiled version was not. 
The matrices were not large enough to cause memory pressure on this machine. I have used the same numpy version on different machines without any issue so I am including CPU information as well:
broken on: Intel(R) Core(TM) i9-7900X CPU @ 3.30GHz
It worked on a Ryzen 5900x however.




### Reproduce the code example:

```python
import numpy as np
np.linalg.eigvals(np.random.random((1000,1000)))
```


### Error message:

```shell
N/A - computer crashes and no logs are available afaik.
Help getting relevant logs appreciated.
```


### NumPy/Python version information:

Broken

```
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.23.4 3.10.6 (main, Aug 10 2022, 11:40:04) [GCC 11.3.0]

>>> numpy.show_config()
openblas64__info:
    libraries = ['openblas64_', 'openblas64_']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None)]
    runtime_library_dirs = ['/usr/local/lib']
blas_ilp64_opt_info:
    libraries = ['openblas64_', 'openblas64_']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None)]
    runtime_library_dirs = ['/usr/local/lib']
openblas64__lapack_info:
    libraries = ['openblas64_', 'openblas64_']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None), ('HAVE_LAPACKE', None)]
    runtime_library_dirs = ['/usr/local/lib']
lapack_ilp64_opt_info:
    libraries = ['openblas64_', 'openblas64_']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None), ('HAVE_LAPACKE', None)]
    runtime_library_dirs = ['/usr/local/lib']
Supported SIMD extensions in this NumPy install:
    baseline = SSE,SSE2,SSE3
    found = SSSE3,SSE41,POPCNT,SSE42,AVX,F16C,FMA3,AVX2,AVX512F,AVX512CD,AVX512_SKX
    not found = AVX512_KNL,AVX512_KNM,AVX512_CLX,AVX512_CNL,AVX512_ICL
```

Working version on same machine
```
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.23.4 3.10.6 (main, Aug 10 2022, 11:40:04) [GCC 11.3.0]
>>> numpy.show_config()
blas_armpl_info:
  NOT AVAILABLE
blas_mkl_info:
  NOT AVAILABLE
blis_info:
  NOT AVAILABLE
openblas_info:
  NOT AVAILABLE
accelerate_info:
  NOT AVAILABLE
atlas_3_10_blas_threads_info:
  NOT AVAILABLE
atlas_3_10_blas_info:
  NOT AVAILABLE
atlas_blas_threads_info:
  NOT AVAILABLE
atlas_blas_info:
    language = c
    define_macros = [('HAVE_CBLAS', None), ('NO_ATLAS_INFO', -1)]
    libraries = ['f77blas', 'cblas', 'atlas', 'f77blas', 'cblas']
    library_dirs = ['/usr/lib/x86_64-linux-gnu']
blas_opt_info:
    language = c
    define_macros = [('HAVE_CBLAS', None), ('NO_ATLAS_INFO', -1)]
    libraries = ['f77blas', 'cblas', 'atlas', 'f77blas', 'cblas']
    library_dirs = ['/usr/lib/x86_64-linux-gnu']
lapack_armpl_info:
  NOT AVAILABLE
lapack_mkl_info:
  NOT AVAILABLE
openblas_lapack_info:
  NOT AVAILABLE
openblas_clapack_info:
  NOT AVAILABLE
flame_info:
  NOT AVAILABLE
atlas_3_10_threads_info:
  NOT AVAILABLE
atlas_3_10_info:
  NOT AVAILABLE
atlas_threads_info:
  NOT AVAILABLE
atlas_info:
    language = f77
    libraries = ['lapack', 'f77blas', 'cblas', 'atlas', 'f77blas', 'cblas']
    library_dirs = ['/usr/lib/x86_64-linux-gnu']
    define_macros = [('NO_ATLAS_INFO', -1)]
lapack_opt_info:
    language = f77
    libraries = ['lapack', 'f77blas', 'cblas', 'atlas', 'f77blas', 'cblas']
    library_dirs = ['/usr/lib/x86_64-linux-gnu']
    define_macros = [('NO_ATLAS_INFO', -1)]
Supported SIMD extensions in this NumPy install:
    baseline = SSE,SSE2,SSE3
    found = SSSE3,SSE41,POPCNT,SSE42,AVX,F16C,FMA3,AVX2,AVX512F,AVX512CD,AVX512_SKX
    not found = AVX512_KNL,AVX512_KNM,AVX512_CLX,AVX512_CNL,AVX512_ICL
```


### Context for the issue:

A lot of my work involves getting eigenvalues of matrices, but I do have a fallback which works.",2022-11-02 18:53:56,,BUG: Computer crash with numpy 1.23.4 and eigvals / eigvalsh on large matrices,['00 - Bug']
22511,open,hawkinsp,"### Describe the issue:

Under upstream LLVM `flang`, `f2py` creates NumPy arrays with data pointing to stack temporary variables, which leads to use-after-scope problems.


### Reproduce the code example:

```python
Consider the following Fortran module:


module f2py_test_module

  integer, public, parameter :: my_const = 1234
  public :: cross

contains

  ! Simple function to calculate vector cross product for 3-D vectors.
  SUBROUTINE cross(a, b, c)

    REAL, INTENT(IN) :: a(3)
    REAL, INTENT(IN) :: b(3)
    REAL, INTENT(OUT) :: c(3)

    c(1) = a(2) * b(3) - a(3) * b(2)
    c(2) = a(3) * b(1) - a(1) * b(3)
    c(3) = a(1) * b(2) - a(2) * b(1)
    RETURN
  END

end module f2py_test_module
```

If we compile this example using upstream LLVM flang and print the value of `my_const` from Python using the f2py bindings, we get garbage. The same example works fine when compiled with `gfortran`.

The root cause can be shown as follows. First, run f2py to generate the bindings:
```
python -m numpy.f2py  f2py_test_module.f95  -m f2py_test_module
```

When compiled with gfortran 12.2.0, we get the following

```
$ gfortran -c f2py_test_module.f95 -fPIC
$ gfortran -S f2py_test_module-f2pywrappers2.f90 -fPIC
$ cat f2py_test_module-f2pywrappers2.s
        .file   ""f2py_test_module-f2pywrappers2.f90""
        .text
        .section        .rodata
        .align 4
.LC0:
        .long   1234
        .text
        .globl  f2pyinitf2py_test_module_
        .type   f2pyinitf2py_test_module_, @function
f2pyinitf2py_test_module_:
.LFB0:
        .cfi_startproc
        pushq   %rbp
        .cfi_def_cfa_offset 16
        .cfi_offset 6, -16
        movq    %rsp, %rbp
        .cfi_def_cfa_register 6
        subq    $16, %rsp
        movq    %rdi, -8(%rbp)
        movq    -8(%rbp), %rdx
        movq    __f2py_test_module_MOD_cross@GOTPCREL(%rip), %rax
        movq    %rax, %rsi
        leaq    .LC0(%rip), %rax
        movq    %rax, %rdi
        movl    $0, %eax
        call    *%rdx
        nop
        leave
        .cfi_def_cfa 7, 8
        ret
        .cfi_endproc
.LFE0:
        .size   f2pyinitf2py_test_module_, .-f2pyinitf2py_test_module_
        .ident  ""GCC: (Debian 12.2.0-3) 12.2.0""
        .section        .note.GNU-stack,"""",@progbits
```

The value passed in `%rdi` is the offset of the global constant 1234 in constant memory.

When the same module is built using upstream LLVM `flang` (I used a build from LLVM commit 748922b31f7f1f48af76efc66a7af0674b1c4c06), we get the following assembly:
```
$ ~/p/llvm-project/build/bin/flang-new -c f2py_test_module.f95 -fPIC
$ ~/p/llvm-project/build/bin/flang-new -S f2py_test_module-f2pywrappers2.f90 -fPIC
$ cat f2py_test_module-f2pywrappers2.s
        .text
        .file   ""FIRModule""
        .globl  f2pyinitf2py_test_module_
        .p2align        4, 0x90
        .type   f2pyinitf2py_test_module_,@function
f2pyinitf2py_test_module_:
        .cfi_startproc
        pushq   %rax
        .cfi_def_cfa_offset 16
        movq    %rdi, %rax
        movl    $1234, 4(%rsp)
        leaq    4(%rsp), %rdi
        movq    _QMf2py_test_modulePcross@GOTPCREL(%rip), %rsi
        callq   *%rax
        popq    %rax
        .cfi_def_cfa_offset 8
        retq
.Lfunc_end0:
        .size   f2pyinitf2py_test_module_, .Lfunc_end0-f2pyinitf2py_test_module_
        .cfi_endproc

        .section        "".note.GNU-stack"","""",@progbits
```

Here note the the constant 1234 is passed via a stack location `4(%rsp)`.

The function to which the Fortran function provides this address is an f2py-generated C function:
```
static void f2py_setup_f2py_test_module(char *my_const,char *cross) {
  int i_f2py=0;
  f2py_f2py_test_module_def[i_f2py++].data = my_const;
  f2py_f2py_test_module_def[i_f2py++].data = cross;
}
```

i.e. we save the value of `my_const` into a global data structure, even though when built with `flang` it points to stack-allocated data. For this to work, we would need to copy the data.
```


### Error message:

See above.

### NumPy/Python version information:

```
1.23.4 3.10.4 (main, Apr 13 2022, 09:37:33) [GCC 11.2.0]
```

Also relevant:

```
$ flang-new --version
$ ~/p/llvm-project/build/bin/flang-new --version
flang-new version 16.0.0 (https://github.com/llvm/llvm-project.git 748922b31f7f1f48af76efc66a7af0674b1c4c06)
Target: x86_64-unknown-linux-gnu
Thread model: posix
...
```

### Context for the issue:

_No response_",2022-11-01 21:50:17,,BUG: f2py creates dangling stack reference for constants of modules built with LLVM `flang`.,"['00 - Bug', 'component: numpy.f2py']"
22506,open,kwsp,"### Issue with current documentation:

Currently it isn't obvious how one should type `ndarray`: should one type them like `ndarray[shape, dtype]` or `ndarray[dtype, shape]`? 

In the few places in the docs where I think this is relevant, no information is given as to how one should type `ndarray`:

* [numpy.ndarray.__class_getitem__](https://numpy.org/devdocs/reference/generated/numpy.ndarray.__class_getitem__.html#numpy.ndarray.__class_getitem__)
* [numpy.ndarray](https://numpy.org/devdocs/reference/generated/numpy.ndarray.html#numpy.ndarray)

Both pages link to [numpy.typing.NDArray](https://numpy.org/devdocs/reference/typing.html#numpy.typing.NDArray), it appears that one should follow the convention of `ndarray[shape, dtype]`

> `numpy.typing.NDArray = numpy.ndarray[typing.Any, numpy.dtype[+ScalarType]]`

### Idea or request for content:

If that is the case, should we make this explicit in the two doc pages linked above? 

I came across this issue recently when I was writing some custom serialization/deserialization functions that take into account the shape and type of `ndarray`s from the type annotations, and found the typing convention isn't clearly documented. Having a convention for annotation `ndarray` could help make the type annotations more interoperable.",2022-10-31 21:16:51,,DOC: document the typing conventions for generic ndarray,"['04 - Documentation', 'Static typing']"
22497,open,ev-br,"### Describe the issue:

`np.max` of a zero-sized array throws a ValueError

### Reproduce the code example:

```python
In [1]: import numpy as np

In [2]: a = np.zeros([])

In [3]: np.max(a)        # works
Out[3]: 0.0

In [4]: b = np.zeros((5, 0, 3))

In [5]: np.max(b)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Input In [5], in <cell line: 1>()
----> 1 np.max(b)

File <__array_function__ internals>:5, in amax(*args, **kwargs)

File ~/.virtualenvs/scipy-dev/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2754, in amax(a, axis, out, keepdims, initial, where)
   2638 @array_function_dispatch(_amax_dispatcher)
   2639 def amax(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,
   2640          where=np._NoValue):
   2641     """"""
   2642     Return the maximum of an array or maximum along an axis.
   2643 
   (...)
   2752     5
   2753     """"""
-> 2754     return _wrapreduction(a, np.maximum, 'max', axis, None, out,
   2755                           keepdims=keepdims, initial=initial, where=where)

File ~/.virtualenvs/scipy-dev/lib/python3.8/site-packages/numpy/core/fromnumeric.py:86, in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)
     83         else:
     84             return reduction(axis=axis, out=out, **passkwargs)
---> 86 return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

ValueError: zero-size array to reduction operation maximum which has no identity
```


### Error message:

```shell
ValueError                                Traceback (most recent call last)
Input In [5], in <cell line: 1>()
----> 1 np.max(b)

File <__array_function__ internals>:5, in amax(*args, **kwargs)

File ~/.virtualenvs/scipy-dev/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2754, in amax(a, axis, out, keepdims, initial, where)
   2638 @array_function_dispatch(_amax_dispatcher)
   2639 def amax(a, axis=None, out=None, keepdims=np._NoValue, initial=np._NoValue,
   2640          where=np._NoValue):
   2641     """"""
   2642     Return the maximum of an array or maximum along an axis.
   2643 
   (...)
   2752     5
   2753     """"""
-> 2754     return _wrapreduction(a, np.maximum, 'max', axis, None, out,
   2755                           keepdims=keepdims, initial=initial, where=where)

File ~/.virtualenvs/scipy-dev/lib/python3.8/site-packages/numpy/core/fromnumeric.py:86, in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)
     83         else:
     84             return reduction(axis=axis, out=out, **passkwargs)
---> 86 return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

ValueError: zero-size array to reduction operation maximum which has no identity
```


### NumPy/Python version information:

In [8]: np.__version__
Out[8]: '1.21.1'


### Context for the issue:

Seen as a part of https://github.com/scipy/scipy/issues/17241#issuecomment-1281518885. Can be worked around of course, am just reporting that it needs a workaround.  
",2022-10-29 05:03:47,,"ENH,DOC: np.max of a zero-sized array throws a ValueError ","['01 - Enhancement', '04 - Documentation']"
22472,open,IamSanjid,"It seems Melissa O'Neill's `PCG64DXSM` implementation advances the initial state with a 64-bit multiplier, but the current implementation of numpy's `PCG64DXSM` appears to be using the default 128-bit multiplier. When `PCG64DXSM` gets initialized for the first time it calls `pcg64_set_seed` and which calls `pcg_setseq_128_srandom_r` and at the end `pcg_setseq_128_step_r` gets called to advance the initial state which uses the default 128-bit multiplier.

As was mentioned here https://numpy.org/doc/stable/reference/random/upgrading-pcg64.html that the current 
`PCG64DXSM BitGenerator` will eventually become the default `BitGenerator` implementation used by `default_rng` in future releases, I thought it was worth mentioning the issue.",2022-10-23 18:08:01,,PCG64DXSM initial state doesn't get advanced with a 64-bit “cheap multiplier” in the LCG,"['00 - Bug', 'component: numpy.random']"
22471,open,ntessore,"### Proposed new feature or change:

I would like to propose adding the output array as an optional parameter to `bincount`.  This makes `bincount` very useful when iteratively tallying large amounts of data with large indices.

After a bit of discussion on the mailing list, I think that a good interface for this might be a `bins` parameter, name debatable, that can either be an integer, which fixes the number of bins, or an array, to which the bin counts are then added. If there are indices larger than allowed by the `bins` specification, the function should probably raise a `ValueError`.

To see where this can be very useful, consider this example of making a high-resolution map from large batches of data:

```py
high_res_map = np.zeros(10000**2)
for indices, quantities in next_big_chunk_of_data():
    high_res_map += np.bincount(indices, quantities, 10000**2)  # slow: repeatedly adding large arrays
```

This would be trivially sped up:

```py
high_res_map = np.zeros(10000**2)
for indices, quantities in next_big_chunk_of_data():
    np.bincount(indices, quantities, bins=high_res_map)  # fast: plain sum-loop in C
```

Loops like the above can be found a lot in e.g. astronomy.

As far as I know, there is no equivalent numpy functionality, and there isn't any fast alternative outside of writing the simplest of loops in C/Cython/numba/..., which is what some packages, including my own, have done many times over. But this exact loop is what `bincount` does under the hood, and the change still fits both the purpose and description of `bincount` nicely, without changing existing behaviour.

The only moral issue is of course what happens when both `minlength` and `bins` are given. I think `minlength` should then be silently ignored.  (I actually cannot imagine a situation where you actually want to specify `minlength` and not `bins`, but that's beside the point.)",2022-10-23 11:15:02,,ENH: pass counting array to bincount,['unlabeled']
22467,open,emezh,"### Issue with current documentation:

From https://numpy.org/doc/stable/user/basics.subclassing.html (as of the date of filing):
### Issue 1
The example in ""`__array_wrap__` for ufuncs"" (https://numpy.org/doc/stable/user/basics.subclassing.html#array-wrap-for-ufuncs-and-other-functions) shows that value of ret is 
```
>>> ret
MySubClass([1, 3, 5, 7, 9])
```
but in reality it is 
`MySubClass([0, 1, 2, 3, 4])`

Some more details:
Here is the array_wrap example with additional prints of object IDs. I've tried both numpy 1.23 and 1.21 - same result.
```
import numpy as np

class MySubClass(np.ndarray):
    def __new__(cls, input_array, info=None):
        obj = np.asarray(input_array).view(cls)
        print('In __new__ ')
        print(f'   obj is {repr(obj)} at {id(obj)}')
        obj.info = info
        return obj
    
    def __array_finalize__(self, obj):
        print('In __array_finalize__:')
        print(f'   self is {repr(self)} at {id(self)}')
        print(f'   obj  is {repr(obj)} at {id(obj)}')
        if obj is None: return
        self.info = getattr(obj, 'info', None)

    def __array_wrap__(self, out_arr, context=None):
        print('In __array_wrap__:')
        print(f'   self is {repr(self)} at {id(obj)}')
        print(f'   arr is {repr(out_arr)} at {id(out_arr)}')
        # then just call the parent
        final = super().__array_wrap__(self, out_arr, context) 
        return final
    
obj = MySubClass(np.arange(5), info='spam')
print('obj created')
arr2 = np.arange(5)+1
ret = np.add(arr2, obj)
print(f'{repr(ret)} at {id(ret)}')
```
If you run the above code you get:
```
In __array_finalize__:
   self is MySubClass([0, 1, 2, 3, 4]) at 140046557987664
   obj  is array([0, 1, 2, 3, 4]) at 140046558433168
In __new__ 
   obj is MySubClass([0, 1, 2, 3, 4]) at 140046557987664
obj created
In __array_finalize__:
   self is MySubClass([0, 1, 2, 3, 4]) at 140046557990128
   obj  is MySubClass([0, 1, 2, 3, 4]) at 140046557987664
In __array_wrap__:
   self is MySubClass([0, 1, 2, 3, 4]) at 140046557987664
   arr is MySubClass([1, 3, 5, 7, 9]) at 140046557990128
MySubClass([0, 1, 2, 3, 4]) at 140046557987664
```
Observations:
- See bug #17986  (array_wrap is called after finalize)
- `final = super().__array_wrap__(self, out_arr, context) ` in `__array_wrap__` doesn't call `super()` method because it doesn't exist.
- based on the printed object IDs, `__array_wrap__` need to return `out_arr` and not `final`.

### Issue 2
The example with `__array_wrap__` doesn't include `__array_ufunc__`.
If ufunc method from https://numpy.org/doc/stable/user/basics.subclassing.html#array-ufunc-for-ufuncs is added to array_wrap example, then `__array_wrap__` is no longer called at all.
```
import numpy as np

class MySubClass(np.ndarray):
    def __new__(cls, input_array, info=None):
        obj = np.asarray(input_array).view(cls)
        print('In __new__ ')
        print(f'   obj is {repr(obj)} at {id(obj)}')
        obj.info = info
        return obj
    
    def __array_ufunc__(self, ufunc, method, *inputs, out=None, **kwargs):
        print('In __array_ufunc__:')
        print(f'   self is {repr(self)} at {id(self)}')
        d = {repr(obj):id(obj) for obj in inputs}
        print(f'   inputs  are {d}')
        if out:
            print(f'   out is {repr(out)} at {id(out)}')
    
        args = []
        in_no = []
        for i, input_ in enumerate(inputs):
            if isinstance(input_, MySubClass):
                in_no.append(i)
                args.append(input_.view(np.ndarray))
            else:
                args.append(input_)
    
        outputs = out
        out_no = []
        if outputs:
            out_args = []
            for j, output in enumerate(outputs):
                if isinstance(output, MySubClass):
                    out_no.append(j)
                    out_args.append(output.view(np.ndarray))
                else:
                    out_args.append(output)
            kwargs['out'] = tuple(out_args)
        else:
            outputs = (None,) * ufunc.nout
    
        info = {}
        if in_no:
            info['inputs'] = in_no
        if out_no:
            info['outputs'] = out_no
    
        results = super().__array_ufunc__(ufunc, method, *args, **kwargs)
        if results is NotImplemented:
            return NotImplemented
    
        if method == 'at':
            if isinstance(inputs[0], MySubClass):
                inputs[0].info = info
            return
    
        if ufunc.nout == 1:
            results = (results,)
    
        results = tuple((np.asarray(result).view(MySubClass)
                         if output is None else output)
                        for result, output in zip(results, outputs))
        if results and isinstance(results[0], MySubClass):
            results[0].info = info
    
        d = {repr(obj):id(obj) for obj in results}
        print(f'   results  are {d}')
        return results[0] if len(results) == 1 else results
    
    def __array_finalize__(self, obj):
        print('In __array_finalize__:')
        print(f'   self is {repr(self)} at {id(self)}')
        print(f'   obj  is {repr(obj)} at {id(obj)}')
        if obj is None: return
        self.info = getattr(obj, 'info', None)

    def __array_wrap__(self, out_arr, context=None):
        print('In __array_wrap__:')
        print(f'   self is {repr(self)} at {id(obj)}')
        print(f'   arr is {repr(out_arr)} at {id(out_arr)}')
        # then just call the parent
        final = super().__array_wrap__(self, out_arr, context) 
        return final
    
obj = MySubClass(np.arange(5), info='spam')
print('obj created')
arr2 = np.arange(5)+1
ret = np.add(arr2, obj)
print(f'{repr(ret)} at {id(ret)}')
```
that produces:
```
In __array_finalize__:
   self is MySubClass([0, 1, 2, 3, 4]) at 140580386788400
   obj  is array([0, 1, 2, 3, 4]) at 140580387237200
In __new__ 
   obj is MySubClass([0, 1, 2, 3, 4]) at 140580386788400
obj created
In __array_ufunc__:
   self is MySubClass([0, 1, 2, 3, 4]) at 140580386788400
   inputs  are {'array([1, 2, 3, 4, 5])': 140580386830928, 'MySubClass([0, 1, 2, 3, 4])': 140580386788400}
In __array_finalize__:
   self is MySubClass([1, 3, 5, 7, 9]) at 140580386790976
   obj  is array([1, 3, 5, 7, 9]) at 140580386830832
   results  are {'MySubClass([1, 3, 5, 7, 9])': 140580386790976}
MySubClass([1, 3, 5, 7, 9]) at 140580386790976
```
Although the `ret` value is correct, `__array_wrap__` has NOT been called.
Pls update documentation to explain how array_wrap and array_ufunc need to be used together.




### Idea or request for content:

_No response_",2022-10-21 14:52:48,,DOC: <example with __array_wrap__ on basics.subclassing doesn't match actual behavior>,['04 - Documentation']
22459,open,jsaladich,"### Issue with current documentation:

Hi!

I was checking how object np.newaxis behaves when I use a MaskedArray (like ```np.ma.is_masked()``` returns ```True```).
The official documentation example in [here](https://numpy.org/doc/stable/reference/generated/numpy.ma.expand_dims.html) is showing ```numpy.ma.expand_dims``` with normal ndarrays. While the old one [here](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ma.expand_dims.html) describes the example with MaskedArrays, solving my doubt. Shouldn't be the official documentation updated?

Just to make sure, if I use ```np.newaxis``` I assume that the mask of my array will be always respected, am I correct?

Thanks a lot for your help and the amazing job with Numpy!

### Idea or request for content:

_No response_",2022-10-20 08:59:20,,DOC: numpy.ma.expand_dims examples are not properly documented,['04 - Documentation']
22451,open,Photonnnn,"### Describe the issue:

I'm using ```numpy.f2py``` to build a module from fortran to python. However, if I pass a function argument to another function(in fortran code), f2py can't deal with it in the right way.

What I want to do is to use the following code in python.
```python
import test
def myfun(x):
    return x
test.fun2(1,myfun)
```

Below is my fortran code. It fails when I use ```f2py -c -m test test.f90```.

### Reproduce the code example:

```python
function fun1(myfun1)
    implicit none
    real(8) :: fun1
    real(8),external :: myfun1
    fun1 = myfun1(1)
    return
end function fun1

function fun2(b,myfun2)
    implicit none
    real(8):: b,fun2
    real(8),external:: myfun2,fun1
    b = fun1(myfun2)
    fun2 = b
    return
end function fun2
```


### Error message:

```shell
running build
running config_cc
INFO: unifing config_cc, config, build_clib, build_ext, build commands --compiler options
running config_fc
INFO: unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options
running build_src
INFO: build_src
INFO: building extension ""test"" sources
INFO: f2py options: []
INFO: f2py:> C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c
creating C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10
Reading fortran codes...
        Reading file 'test.f90' (format:free)
Post-processing...
        Block: test
                        Block: fun1
                                        Block: myfun1
                        Block: fun2
Post-processing (stage 2)...
Building modules...
    Constructing call-back function ""cb_myfun1_in_fun1__user__routines""
append_needs: unknown need 'int'
      def myfun1(e_1_err): return fun1
    Building module ""test""...
                Creating wrapper for Fortran function ""fun1""(""fun1"")...
        Constructing wrapper function ""fun1""...
          fun1 = fun1(myfun1,[myfun1_extra_args])
                Creating wrapper for Fortran function ""fun2""(""fun2"")...
        Constructing wrapper function ""fun2""...
routsign2map: Confused: function fun2 has externals ['myfun2'] but no ""use"" statement.
sign2map: Confused: external myfun2 is not in lcb_map[].
append_needs: unknown need 'myfun2'
append_needs: unknown need 'myfun2'
          fun2 = fun2(b,myfun2,[myfun2_extra_args])
    Wrote C/API module ""test"" to file ""C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c""
    Fortran 77 wrappers are saved to ""C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\test-f2pywrappers.f""
INFO:   adding 'C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\fortranobject.c' to sources.
INFO:   adding 'C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10' to include_dirs.
copying C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\numpy\f2py\src\fortranobject.c -> C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10
copying C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\numpy\f2py\src\fortranobject.h -> C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10
INFO:   adding 'C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\test-f2pywrappers.f' to sources.
INFO: build_src: building npy-pkg config files
running build_ext
INFO: No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils
INFO: customize MSVCCompiler
INFO: customize MSVCCompiler using build_ext
INFO: get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'
INFO: customize GnuFCompiler
WARN: Could not locate executable g77
WARN: Could not locate executable f77
INFO: customize IntelVisualFCompiler
INFO: Found executable D:\Program Files (x86)\Intel\oneAPI\compiler\2022.0.3\windows\bin\intel64\ifort.exe
WARN: Could not locate executable D:\Program
INFO: customize AbsoftFCompiler
WARN: Could not locate executable f90
INFO: customize CompaqVisualFCompiler
WARN: Could not locate executable DF
INFO: customize IntelItaniumVisualFCompiler
WARN: Could not locate executable efl
INFO: customize Gnu95FCompiler
INFO: Found executable D:\mingw64\bin\gfortran.exe
Using built-in specs.
COLLECT_GCC=D:\mingw64\bin\gfortran.exe
COLLECT_LTO_WRAPPER=D:/mingw64/bin/../libexec/gcc/x86_64-w64-mingw32/8.1.0/lto-wrapper.exe
Target: x86_64-w64-mingw32
Configured with: ../../../src/gcc-8.1.0/configure --host=x86_64-w64-mingw32 --build=x86_64-w64-mingw32 --target=x86_64-w64-mingw32 --prefix=/mingw64 --with-sysroot=/c/mingw810/x86_64-810-win32-seh-rt_v6-rev0/mingw64 --enable-shared --enable-static --disable-multilib --enable-languages=c,c++,fortran,lto --enable-libstdcxx-time=yes --enable-threads=win32 --enable-libgomp --enable-libatomic --enable-lto --enable-graphite --enable-checking=release --enable-fully-dynamic-string --enable-version-specific-runtime-libs --disable-libstdcxx-pch --disable-libstdcxx-debug --enable-bootstrap --disable-rpath --disable-win32-registry --disable-nls --disable-werror --disable-symvers --with-gnu-as --with-gnu-ld --with-arch=nocona --with-tune=core2 --with-libiconv --with-system-zlib --with-gmp=/c/mingw810/prerequisites/x86_64-w64-mingw32-static --with-mpfr=/c/mingw810/prerequisites/x86_64-w64-mingw32-static --with-mpc=/c/mingw810/prerequisites/x86_64-w64-mingw32-static --with-isl=/c/mingw810/prerequisites/x86_64-w64-mingw32-static --with-pkgversion='x86_64-win32-seh-rev0, Built by MinGW-W64 project' --with-bugurl=https://sourceforge.net/projects/mingw-w64 CFLAGS='-O2 -pipe -fno-ident -I/c/mingw810/x86_64-810-win32-seh-rt_v6-rev0/mingw64/opt/include -I/c/mingw810/prerequisites/x86_64-zlib-static/include -I/c/mingw810/prerequisites/x86_64-w64-mingw32-static/include' CXXFLAGS='-O2 -pipe -fno-ident -I/c/mingw810/x86_64-810-win32-seh-rt_v6-rev0/mingw64/opt/include -I/c/mingw810/prerequisites/x86_64-zlib-static/include -I/c/mingw810/prerequisites/x86_64-w64-mingw32-static/include' CPPFLAGS=' -I/c/mingw810/x86_64-810-win32-seh-rt_v6-rev0/mingw64/opt/include -I/c/mingw810/prerequisites/x86_64-zlib-static/include -I/c/mingw810/prerequisites/x86_64-w64-mingw32-static/include' LDFLAGS='-pipe -fno-ident -L/c/mingw810/x86_64-810-win32-seh-rt_v6-rev0/mingw64/opt/lib -L/c/mingw810/prerequisites/x86_64-zlib-static/lib -L/c/mingw810/prerequisites/x86_64-w64-mingw32-static/lib '
Thread model: win32
gcc version 8.1.0 (x86_64-win32-seh-rev0, Built by MinGW-W64 project)
INFO: customize Gnu95FCompiler
Using built-in specs.
COLLECT_GCC=D:\mingw64\bin\gfortran.exe
COLLECT_LTO_WRAPPER=D:/mingw64/bin/../libexec/gcc/x86_64-w64-mingw32/8.1.0/lto-wrapper.exe
Target: x86_64-w64-mingw32
Configured with: ../../../src/gcc-8.1.0/configure --host=x86_64-w64-mingw32 --build=x86_64-w64-mingw32 --target=x86_64-w64-mingw32 --prefix=/mingw64 --with-sysroot=/c/mingw810/x86_64-810-win32-seh-rt_v6-rev0/mingw64 --enable-shared --enable-static --disable-multilib --enable-languages=c,c++,fortran,lto --enable-libstdcxx-time=yes --enable-threads=win32 --enable-libgomp --enable-libatomic --enable-lto --enable-graphite --enable-checking=release --enable-fully-dynamic-string --enable-version-specific-runtime-libs --disable-libstdcxx-pch --disable-libstdcxx-debug --enable-bootstrap --disable-rpath --disable-win32-registry --disable-nls --disable-werror --disable-symvers --with-gnu-as --with-gnu-ld --with-arch=nocona --with-tune=core2 --with-libiconv --with-system-zlib --with-gmp=/c/mingw810/prerequisites/x86_64-w64-mingw32-static --with-mpfr=/c/mingw810/prerequisites/x86_64-w64-mingw32-static --with-mpc=/c/mingw810/prerequisites/x86_64-w64-mingw32-static --with-isl=/c/mingw810/prerequisites/x86_64-w64-mingw32-static --with-pkgversion='x86_64-win32-seh-rev0, Built by MinGW-W64 project' --with-bugurl=https://sourceforge.net/projects/mingw-w64 CFLAGS='-O2 -pipe -fno-ident -I/c/mingw810/x86_64-810-win32-seh-rt_v6-rev0/mingw64/opt/include -I/c/mingw810/prerequisites/x86_64-zlib-static/include -I/c/mingw810/prerequisites/x86_64-w64-mingw32-static/include' CXXFLAGS='-O2 -pipe -fno-ident -I/c/mingw810/x86_64-810-win32-seh-rt_v6-rev0/mingw64/opt/include -I/c/mingw810/prerequisites/x86_64-zlib-static/include -I/c/mingw810/prerequisites/x86_64-w64-mingw32-static/include' CPPFLAGS=' -I/c/mingw810/x86_64-810-win32-seh-rt_v6-rev0/mingw64/opt/include -I/c/mingw810/prerequisites/x86_64-zlib-static/include -I/c/mingw810/prerequisites/x86_64-w64-mingw32-static/include' LDFLAGS='-pipe -fno-ident -L/c/mingw810/x86_64-810-win32-seh-rt_v6-rev0/mingw64/opt/lib -L/c/mingw810/prerequisites/x86_64-zlib-static/lib -L/c/mingw810/prerequisites/x86_64-w64-mingw32-static/lib '
Thread model: win32
gcc version 8.1.0 (x86_64-win32-seh-rev0, Built by MinGW-W64 project)
INFO: customize Gnu95FCompiler using build_ext
INFO: building 'test' extension
INFO: compiling C sources
creating C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\Release\Users
creating C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\Release\Users\ADMINI~1
creating C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\Release\Users\ADMINI~1\AppData
creating C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\Release\Users\ADMINI~1\AppData\Local
creating C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\Release\Users\ADMINI~1\AppData\Local\Temp
creating C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\Release\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned
creating C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\Release\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10
INFO: D:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.33.31629\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -DNPY_DISABLE_OPTIMIZATION=1 -IC:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10 -IC:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\numpy\core\include -IC:\Users\Administrator\AppData\Local\Programs\Python\Python310\include -IC:\Users\Administrator\AppData\Local\Programs\Python\Python310\Include -ID:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.33.31629\include -ID:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.33.31629\ATLMFC\include -ID:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include -ID:\Windows Kits\10\include\10.0.19041.0\ucrt -ID:\Windows Kits\10\\include\10.0.19041.0\\um -ID:\Windows Kits\10\\include\10.0.19041.0\\shared -ID:\Windows Kits\10\\include\10.0.19041.0\\winrt -ID:\Windows Kits\10\\include\10.0.19041.0\\cppwinrt -IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um /TcC:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c /FoC:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\Release\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.obj
error: Command ""D:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.33.31629\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -DNPY_DISABLE_OPTIMIZATION=1 -IC:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10 -IC:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\numpy\core\include -IC:\Users\Administrator\AppData\Local\Programs\Python\Python310\include -IC:\Users\Administrator\AppData\Local\Programs\Python\Python310\Include -ID:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.33.31629\include -ID:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.33.31629\ATLMFC\include -ID:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include -ID:\Windows Kits\10\include\10.0.19041.0\ucrt -ID:\Windows Kits\10\\include\10.0.19041.0\\um -ID:\Windows Kits\10\\include\10.0.19041.0\\shared -ID:\Windows Kits\10\\include\10.0.19041.0\\winrt -ID:\Windows Kits\10\\include\10.0.19041.0\\cppwinrt -IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um /TcC:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c /FoC:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\Release\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.obj"" failed with exit status 2
testmodule.c
C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\numpy\core\include\numpy\npy_1_7_deprecated_api.h(14) : Warning Msg: Using deprecated NumPy API, disable it with #define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(294): warning C4244: ��=��: �ӡ�Py_ssize_t��ת������int�������ܶ�ʧ����
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(490): warning C4244: ��=��: �ӡ�Py_ssize_t��ת������int�������ܶ�ʧ����
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(639): error C2065: ��myfun2_t��: δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(639): error C2146: �﷨����: ȱ�١�;��(�ڱ�ʶ����myfun2_cb����ǰ��)
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(639): error C2065: ��myfun2_cb��:  δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(639): error C2059: �﷨����:��{� �
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(640): error C2065: ��myfun2_t��: δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(640): error C2065: ��myfun2_cb_ptr� �: δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(640): error C2065: ��myfun2_cb��:  δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(640): warning C4047: ��=��:��int ���롰int *���ļ�Ӽ���ͬ
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(640): error C2106: ��=��: ���� �������Ϊ��ֵ
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(642): error C2065: ��myfun2_typedef��: δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(642): error C2146: �﷨����: ȱ�١�;��(�ڱ�ʶ����myfun2_cptr����ǰ��)
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(642): error C2065: ��myfun2_cptr��: δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(651): error C2065: ��myfun2_cb��:  δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(651): error C2224: ��.capi����� �������нṹ/��������
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(659): error C2065: ��myfun2_cb��:  δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(659): error C2224: ��.capi����� �������нṹ/��������
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(659): error C2198: ��F2PyCapsule_Check��: ���ڵ��õĲ���̫��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(660): error C2065: ��myfun2_cptr��: δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(660): error C2065: ��myfun2_cb��:  δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(660): error C2224: ��.capi����� �������нṹ/��������
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(660): error C2198: ��F2PyCapsule_AsVoidPtr��: ���ڵ��õĲ���̫��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(660): warning C4047: ��=��:��int ���롰void *���ļ�Ӽ���ͬ
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(662): error C2065: ��myfun2_cptr��: δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(662): error C2065: ��myfun2��: δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(665): error C2065: ��myfun2_cb��:  δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(665): error C2224: ��.capi����� �������нṹ/��������
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(665): warning C4133: ��������: �ӡ�PyTupleObject *������PyObject *�������Ͳ�����
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(665): error C2014: Ԥ��������� ������Ϊ��һ���ǿհ׿ռ����
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(667): error C2059: �﷨����:��;� �
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(668): error C2065: ��myfun2_cb_ptr� �: δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(668): warning C4013: ��swap_active_myfun2��δ���壻�����ⲿ���� int
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(674): error C2065: ��myfun2_cb��:  δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(674): error C2224: ��.jmpbuf���� ��������нṹ/��������
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(674): error C2168: ��_setjmp��: �ڲ�������ʵ��̫��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(677): error C2065: ��myfun2_cptr��: δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(677): warning C4047: ��������: ��double *���롰int���ļ�Ӽ���ͬ
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(677): warning C4024: ��f2py_func��: �βκ�ʵ�� 3 �����Ͳ�ͬ
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(695): error C2065: ��myfun2_cb_ptr� �: δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(696): error C2065: ��myfun2_cb��:  δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(696): error C2224: ��.args_capi������������нṹ/��������
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(696): error C2198: ��_Py_DECREF��: ���ڵ��õĲ���̫��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(703): error C2059: �﷨����:��if��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(705): error C2059: �﷨����:��else��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(713): error C2059: �﷨����:��return��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(714): error C2059: �﷨����:��}� �
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(783): error C2065: ��f2py_routine_defs��: δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(783): error C2109: �±�Ҫ������ �ָ������
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(784): error C2065: ��f2py_routine_defs��: δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(784): error C2109: �±�Ҫ������ �ָ������
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(784): error C2198: ��PyFortranObject_NewAsAttr��: ���ڵ��õĲ���̫��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(785): error C2065: ��f2py_routine_defs��: δ�����ı�ʶ��
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(785): error C2109: �±�Ҫ������ �ָ������
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(785): warning C4133: ��������: �ӡ�PyObject *������const char *�������Ͳ�����
C:\Users\ADMINI~1\AppData\Local\Temp\tmp08peyned\src.win-amd64-3.10\testmodule.c(785): error C2198: ��PyDict_SetItemString��: ���ڵ��õĲ���̫��
```


### NumPy/Python version information:

1.22.1 3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]

### Context for the issue:

_No response_",2022-10-19 08:46:47,,BUG: f2py fails if a function argument passed to another function,"['00 - Bug', 'component: numpy.f2py']"
22435,open,rem3-1415926,"### Describe the issue:

Accessing the object returned by np.load() on a .npz file results in a BadZipFile exception if a .npz file of the same name is written in the mean time. The error descritpion is either CRC (see example below) if the array names match, or ""File name in directory [...] differ"" if they don't.

Interestingly enough, this does **not** happen if the first element of the loaded object is accessed before the np.savez_compressed() that breaks it afterwards. (line commented out in example code). Accessing another element doesn't have this effect, but it doesn't hurt either.

### Reproduce the code example:

```python
import numpy as np

fn = ""test.npz""
np.savez_compressed(fn, d1= np.array([1,2]), d2= np.array([11,22]))

backup = np.load(fn)

# backup[backup.files[0]]

np.savez_compressed(fn, d1= np.array([33,33]), d2= np.array([3,3]))

backup[backup.files[0]]
```


### Error message:

```shell
File ""/home/▓▓▓▓/py_venvs/default/lib/python3.8/site-packages/numpy/lib/npyio.py"", line 241, in __getitem__
    magic = bytes.read(len(format.MAGIC_PREFIX))
  File ""/usr/lib/python3.8/zipfile.py"", line 940, in read
    data = self._read1(n)
  File ""/usr/lib/python3.8/zipfile.py"", line 1030, in _read1
    self._update_crc(data)
  File ""/usr/lib/python3.8/zipfile.py"", line 958, in _update_crc
    raise BadZipFile(""Bad CRC-32 for file %r"" % self.name)
zipfile.BadZipFile: Bad CRC-32 for file 'd1.npy'

# OR, with different array names on the second write:
  File ""/home/▓▓▓▓/py_venvs/default/lib/python3.8/site-packages/numpy/lib/npyio.py"", line 240, in __getitem__
    bytes = self.zip.open(key)
  File ""/usr/lib/python3.8/zipfile.py"", line 1556, in open
    raise BadZipFile(
zipfile.BadZipFile: File name in directory 'd1.npy' and header b'd3.npy' differ.
```


### NumPy/Python version information:

Python 3.8.10 / Numpy 1.23.4

### Context for the issue:

Happens when trying to backup previous data to restore it at a later point when the new data was already written. There are numerous ways around it, such as the above mentioned hack, or immediately reading the arrays into a dict and then closing the loaded file object properly (as, if I understand correctly, you should do anyway)",2022-10-13 15:42:29,,DOC: NPZ files could mention more explicitly that reading is lazy/delayed,"['04 - Documentation', 'sprintable']"
22434,open,dhingrachirag,"### Describe the issue:

I am getting below error every-time 

Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.9/bin/robotmetrics"", line 5, in <module>
    from robotframework_metrics.runner import main
  File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/robotframework_metrics/runner.py"", line 3, in <module>
    from .robotmetrics import generate_report
  File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/robotframework_metrics/robotmetrics.py"", line 10, in <module>
    from .keyword_times import KeywordTimes
  File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/robotframework_metrics/keyword_times.py"", line 1, in <module>
    import pandas as pd
  File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/__init__.py"", line 16, in <module>
    raise ImportError(
ImportError: Unable to import required dependencies:
numpy: 

IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!

Importing the numpy C-extensions failed. This error can happen for
many reasons, often due to issues with your setup or how NumPy was
installed.

We have compiled some common reasons and troubleshooting tips at:

    https://numpy.org/devdocs/user/troubleshooting-importerror.html

Please note and check the following:

  * The Python version is: Python3.9 from ""/Library/Frameworks/Python.framework/Versions/3.9/bin/python3""
  * The NumPy version is: ""1.23.3""

and make sure that they are the versions you expect.
Please carefully study the documentation linked above for further help.

> Original error was:
> dlopen(/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so,
> 0x0002): tried:
> '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so'
> (mach-o file, but is an incompatible architecture (have (arm64), need
> (x86_64)))


### Reproduce the code example:

```python
File ""/Library/Frameworks/Python.framework/Versions/3.9/bin/robotmetrics"", line 5, in <module>
    from robotframework_metrics.runner import main
  File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/robotframework_metrics/runner.py"", line 3, in <module>
    from .robotmetrics import generate_report
  File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/robotframework_metrics/robotmetrics.py"", line 10, in <module>
    from .keyword_times import KeywordTimes
  File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/robotframework_metrics/keyword_times.py"", line 1, in <module>
    import pandas as pd
  File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/__init__.py"", line 16, in <module>
    raise ImportError(
ImportError: Unable to import required dependencies:
numpy:
```


### Error message:

_No response_

### NumPy/Python version information:

- I am using macbook pro system with Apple M1 Pro chip inside
- Running robot metrics command from Pycharm PyCharm 2022.1 (Community Edition)
- The Python version is: Python3.9 
- The NumPy version is: ""1.23.3""

### Context for the issue:

Requesting you to please help me in solving this issue as many of the peers are also facing the same issue with Macbook pro having M1 pro chip inside",2022-10-13 13:38:42,,BUG: ImportError while running robotmetrics command from PyCharm,['00 - Bug']
22427,open,albertvaka,"### Describe the issue:

Calling `np.append` on a float32 array will return a float64 array unless we append explicitly a float32.

It seems confusing (and a potential performance issue) that the entire array gets casted up just because I appended something.

I think the type of the original array should be preserved, or `append` be given an optional `dtype` argument like other methods have.

### Reproduce the code example:

```python
import numpy as np
a = np.array([1, 2], dtype=np.float32)
print(a.dtype) # prints float32
b = np.append(a, 42)
print(b.dtype) # prints float64
```

The workaround is to explicitly append a float32 by doing `np.float32(42)` first, but it's quite hard to find out this cast is happening in the first place.

### NumPy/Python version information:

1.23.3 3.10.6 (main, Aug 30 2022, 04:58:14) [Clang 13.1.6 (clang-1316.0.21.2.5)]

### Context for the issue:

* Apple's mps only supports float32.
* numpy defaults to float64 everywhere if you are not careful.
* This causes a crash on Macs (eg: when using numpy arrays from pytorch), but it could be a hidden performance issue on other places because of the casts (which IMO would be worse than the crash).",2022-10-11 21:35:32,,BUG: array's append should respect the type of the array you are appending to,['00 - Bug']
22380,open,hawkinsp,"### Describe the issue:

See the code example.

The cause is probably that `np.linalg.lstsq`, which is called internally by `polyfit`, can return empty residuals depending on the matrix rank.

### Reproduce the code example:

```python
In [1]: import numpy as np

In [3]: x = np.array([-4.2073038 , -3.01299361,  0.17435259, -0.45885279, -2.18740973,
   ...:        -1.65722718])
   ...: y = np.array([ 21.47761358,  -3.06717015, -12.85000342, -10.2560546 ,
   ...:          0.03992826,   3.36907162])
   ...: w = np.array([4.73816559, 0.79306992, 4.84662284, 2.68313602, 1.2997438 ,
   ...:        1.09350907])

In [4]: np.polyfit(x, y, deg=3, rcond=0.01, full=False, w=w, cov=True)
/usr/local/google/home/phawkins/.pyenv/versions/3.10.0/envs/py310/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3444: RankWarning: Polyfit may be poorly conditioned
  exec(code_obj, self.user_global_ns, self.user_ns)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-4-4e7b76afa78f> in <module>
----> 1 np.polyfit(x, y, deg=3, rcond=0.01, full=False, w=w, cov=True)

~/.pyenv/versions/3.10.0/envs/py310/lib/python3.10/site-packages/numpy/core/overrides.py in polyfit(*args, **kwargs)

~/.pyenv/versions/3.10.0/envs/py310/lib/python3.10/site-packages/numpy/lib/polynomial.py in polyfit(x, y, deg, rcond, full, w, cov)
    691             fac = resids / (len(x) - order)
    692         if y.ndim == 1:
--> 693             return c, Vbase * fac
    694         else:
    695             return c, Vbase[:,:, NX.newaxis] * fac

ValueError: operands could not be broadcast together with shapes (4,4) (0,)
```


### Error message:

```shell
See above.
```


### NumPy/Python version information:

1.23.3 3.10.0 (default, Nov 19 2021, 17:04:42) [GCC 10.3.0]

### Context for the issue:

Low priority, but noticed in passing.",2022-10-05 13:27:27,,BUG: np.polyfit produces a broadcasting error internally with `full=False`,['00 - Bug']
22355,open,EwoutH,"### Proposed new feature or change:

Add CPU feature detection for Intel's [Advanced Matrix Extensions](https://en.wikipedia.org/wiki/Advanced_Matrix_Extensions) (AMX). On wide CPU cores the Advanced Matrix Extensions have the potential to increase performance manyfold compared to AVX for certain operations.

### Feature sets and Detection

Intel AMX consists of 3 feature sets. The `AMX-TILE` is the base instruction set. For both INT8 and BF16 there are Tile Matrix Multiply (TMUL) units with each an additional instruction set: `AMX-INT8` and `AMX-BF16`.

|  CPU ID Input  | CPU ID Output | Instruction Set |
|:--------------:|:-------------:|:---------------:|
| EAX=07H, ECX=0 | EDX[bit 22]   | AMX-BF16        |
| EAX=07H, ECX=0 | EDX[bit 24]   | AMX-TILE        |
| EAX=07H, ECX=0 | EDX[bit 25]   | AMX-INT8        |

See https://en.wikichip.org/wiki/x86/amx#Instructions

### Development history

Intel AMX was merged into Linux in [October 2021](https://www.phoronix.com/news/Intel-AMX-For-Linux-5.16) and included in the 5.16 kernel. Support was backported to Ubuntu 22.04.1 LTS in [August 2022](https://www.phoronix.com/news/Ubuntu-22.04.1-LTS).

### Similar effords

- https://github.com/google/cpu_features/commit/33bd72c1bcf080e8d9cf48403e0ebcbb8bf5ab5a
- https://reviews.llvm.org/D82705
- https://lore.kernel.org/lkml/20210825155413.19673-20-chang.seok.bae@intel.com/
- https://patchwork.kernel.org/project/kvm/patch/20220107185512.25321-17-pbonzini@redhat.com/
- https://sourceware.org/git/?p=glibc.git;a=commit;h=4fdd4d41a17dda26c854ed935658154a17d4b906

### Resources

- [Intel AMX support in 5.16](https://lwn.net/Articles/874846/)
- [x86: Support Intel Advanced Matrix Extensions](https://lwn.net/Articles/864700/)

This enhancement might be similar to https://github.com/numpy/numpy/pull/20821, https://github.com/numpy/numpy/pull/20552 and https://github.com/numpy/numpy/pull/22265.",2022-09-29 19:51:36,,ENH: Add CPU feature detection for Intel AMX (Advanced Matrix Extensions),"['01 - Enhancement', 'component: SIMD']"
22347,open,davzaman,"### Describe the issue:

I am testing my data pipeline that uses `SimpleImputer` from sklearn. Their code uses masked arrays to compute the mean for example. However, when the arrays have large float64's (and therefore their means are also large, but not infinite/larger than the max float64), np.ma.mean produces the value (which is not nan or inf) but considers it masked.

My best guess is that `np.ma.mean` is somehow going by float32  even though the array is type float64, under which those values would be considered infinite.

### Reproduce the code example:

```python
""""""This code is based on SimpleImputer from sklearn since this is what raised this bug for me.""""""
# %%
from sklearn.utils._mask import _get_mask
from numpy import array, nan, mean as npmean
import numpy.ma as ma

# %%
# The last column does not contain any nans
X = array(
    [
        [nan, 2.00000000e000, nan, 2.00000000e000, 6.10351562e-005],
        [
            1.00000000e000,
            2.00000000e000,
            -3.40282347e038,
            1.00000000e000,
            1.79769313e308,
        ],
    ]
)
X.dtype
""""""
>>> dtype('float64')
""""""

# %%
missing_mask = _get_mask(X, nan)
""""""
The last column is not flagged to have nans.
>>> array([[ True, False,  True, False, False],
       [False, False, False, False, False]])
""""""

# %%
masked_X = ma.masked_array(X, mask=missing_mask)
""""""
The last column is not flagged to be invalid.
>>> masked_array(
  data=[[--, 2.0, --, 2.0, 6.10351562e-05],
        [1.0, 2.0, -3.40282347e+38, 1.0, 1.79769313e+308]],
  mask=[[ True, False,  True, False, False],
        [False, False, False, False, False]],
  fill_value=1e+20)
""""""

# %%
mean_masked = ma.mean(masked_X, axis=0)
""""""
The last column is shown to have an invalid mean
>>> masked_array(data=[1.0, 2.0, -3.40282347e+38, 1.5, --],
             mask=[False, False, False, False,  True],
       fill_value=1e+20)
""""""

# %%
mean = npmean(X, axis=0)
""""""
The last column does not have a nan mean.
>>> array([            nan, 2.00000000e+000,             nan, 1.50000000e+000,
       8.98846565e+307])
""""""

# %%
print(mean.dtype)
print(ma.getdata(mean_masked).dtype)
""""""
>>> dtype('float64')
>>> dtype('float64')
""""""

# %%
ma.getdata(mean_masked)
""""""
>>> array([ 1.00000000e+000,  2.00000000e+000, -3.40282347e+038,
        1.50000000e+000,  1.79769313e+308])
""""""

# %%
ma.getdata(mean_masked).astype(float32)
""""""
>>> array([ 1.0000000e+00,  2.0000000e+00, -3.4028235e+38,  1.5000000e+00,
                  inf], dtype=float32)
""""""

# %%
1.79769313e308 < finfo(float64).max
""""""
>>> True

""""""
# %%
8.98846565e+307 < np.finfo(np.float64).max
""""""
Interestingly the values don't match either.
>>> True
""""""
```



### Error message:

```shell
There is no immediate error, the behavior is not what I expect and I end up with fewer columns than I expect downstream (shapes do not match).
```


### NumPy/Python version information:

1.22.4
3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) \n[GCC 10.3.0]

### Context for the issue:

In the data I work with it may not be unreasonable to see large values and taking the mean should work as expected even when masking other nans and with large float64 values, unless I am missing something. I was unable to find anything documented to explain this behavior.",2022-09-28 22:52:52,,BUG: Masked division considers large float64 values as inf ,"['00 - Bug', 'component: numpy.ma']"
22346,open,jbrockmendel,"### Describe the issue:

I'm used to getting silent overflows when converting to higher-resolution units.  This one is new to me.

### Reproduce the code example:

```python
import numpy as np
dt = np.datetime64(np.iinfo(np.int64).min + 1, ""s"")
expected = np.datetime64('-292277022657-01-27T08:29')
dt.astype(""M8[m]"")

>>> dt.astype(""M8[m]"")
numpy.datetime64('292277026596-12-04T15:29')
```


### Error message:

_No response_

### NumPy/Python version information:

numpy 1.23.1
py3.9.13 Mac

### Context for the issue:

_No response_",2022-09-28 19:14:41,,"BUG: dt64[s].astype(""M8[m]"") overflow",['00 - Bug']
22338,open,peland,"### Describe the issue:

if I append a masked array and something else using np.append, it returns a masked array but with the original mask replaced with False. I wouldn't mind if it returned a non-masked array (my fault!), but the current behaviour seems positively misleading! If it returns a masked array, it should surely preserve the mask.

### Reproduce the code example:

```python
import numpy as np
np.append(np.ma.masked_all(2),np.arange(2))
```


### Error message:

_No response_

### NumPy/Python version information:

1.20.3

### Context for the issue:

No longer important for me (I corrected to np.ma.append), but could be misleading for others.",2022-09-26 13:37:15,,"ENH: Masked array `__array_function__` could ""fix"" e.g. `np.append` MA usage","['01 - Enhancement', 'component: numpy.ma', 'Project']"
22337,open,RossBoylan,"### Issue with current documentation:

In the discussion of the state of the random generators, most specifically https://numpy.org/doc/stable/reference/random/bit_generators/generated/numpy.random.PCG64.state.html#numpy.random.PCG64.state the semantics of the returned state are unclear.  Suppose something like this:
```python
state0 = myBitGenerator.state
# do stuff, including generating random numbers that use the generator
myBitGenerator.state = state0
```
If the `.state` accessor returns a view, or a direct reference to the internal state, `myBitGenerator.state = state0` is a no-op and does not reset the state to what it was.  And what the code should have done was to make a `copy` (deep? shallow?) of it when it first got it.

### Idea or request for content:

Say whether one needs to make a copy of the state, and what kind of copy, to reset it in the same session.  And/or say whether the thing you retrieve from `myBitGenerator.state` can change after you've retrieved it.

More generally, at the moment the story for state might differ for each type of `BitGenerator` and so there is no way to discuss them in general.  It would be helpful to be able to discuss them in general, and for them to have a common API.

I suppose another issue is whether state for the legacy `RandomState` could also be discussed at the same time.",2022-09-26 01:39:36,,DOC: clarify copy semantics of random state,['04 - Documentation']
22333,open,oscarbenjamin,"### Issue with current documentation:

This is coming from a SymPy issue. I'm not sure if this should be considered a documentation problem for NumPy or a bug:
https://github.com/sympy/sympy/issues/24071

The docs say that `np.finfo.nmant` represents:
```
The number of bits in the mantissa.
```
https://numpy.org/doc/stable/reference/generated/numpy.finfo.html

So if I try this with double I get:
```python
>>> np.finfo(np.double).nmant
52
```
Now in IEEE 754 64 bit floating point there is a 53 bit mantissa but only 52 of those bits are stored explicitly within the 64 bit data type with the leading `1` being implicit (for normal nonzero values).

If I try this with longdouble I get:
```python
>>> np.finfo(np.longdouble).nmant
63
```
My understanding is that on this x86-64 bit Linux system `long double` means 80 bit extended precision (as in x87). In that format there is a 64 bit mantissa and all 64 bits of it are stored explicitly.

From here I can see that `nmant` does not represent the number of bits actually stored for the mantissa but it also does not represent the number of bits of effective precision either. Rather the relationship seems to be:
```
precision = nmant + 1
```
That particular assumption is used by SymPy when converting numpy datatypes to SymPy's arbitrary precision Float format.

In the SymPy issue what is reported is that on PowerPC `long double` uses the `double-double` format and NumPy's `nmant` apparently gives 105:
```python
>>> np.finfo(np.longdouble).nmant # on PowerPC
105
```
Here the number of bits of mantissa explicitly stored would be `2*52 = 104`. With both implicit ones it would be `2*53 = 106`. However the effective precision is apparently 107 (I'm not sure how that works):
https://en.wikipedia.org/wiki/Quadruple-precision_floating-point_format#Double-double_arithmetic
Likewise on that hardware:
```python
>>> (np.longdouble(2)/3).as_integer_ratio()[0].bit_length() # PowerPC
107
```
Maybe I'm just misunderstanding what `nmant` is supposed to represent but at least the docs should clarify what it is supposed to mean because I really can't make sense of what `nmant = 105` is supposed to indicate here.

Alternatively is this a bug and should nmant actually report as 106 on that particular PowerPC system?

### Idea or request for content:

Clarify what nmant means or possibly change its value on PowerPC systems.",2022-09-24 19:27:21,,DOC: What does `np.finfo.nmant` mean?,"['01 - Enhancement', '04 - Documentation']"
22322,open,WarrenWeckesser,"### Issue with current documentation:

See, for example, the [online doc for `cdouble`](https://numpy.org/doc/stable/reference/arrays.scalars.html?highlight=cdouble#numpy.cdouble), or look at the result of `np.complex128?` in ipython:
```
In [7]: np.complex128?
Init signature: np.complex128(real=0, imag=0)
Docstring:     
Complex number type composed of two double-precision floating-point
numbers, compatible with Python `complex`.

:Character code: ``'D'``
:Canonical name: `numpy.cdouble`
:Alias: `numpy.cfloat`
:Alias: `numpy.complex_`
:Alias on this platform (Linux x86_64): `numpy.complex128`: Complex number type composed of 2 64-bit-precision floating-point numbers.
File:           ~/py3.10.1/lib/python3.10/site-packages/numpy/__init__.py
Type:           type
Subclasses: 
```
Problems:

* The signature of `complex128` is *not* `complex128(real=0, imag=0)`.  The constructor allows at most one argument, and it does not accept keywords `real` or `imag`.
* The function is not compatible with the built-in `complex`.  With the built-in `complex`, one can write `complex(real=1, imag=2)`.

",2022-09-21 14:58:38,,DOC: docstring and signature of `complex128` / `cdouble` are not correct,['04 - Documentation']
22318,open,ax3l,"### Proposed new feature or change:

On many leading supercomputers in the US, PowerPC is used. For example, Summit at OLCF and Sierra/Lassen at LLNL.

Installing numpy on those with pip currently needs to build from source.

### Describe the solution you'd like.

I saw you already publish arm64 wheels for Linux on pypi. Could you potentially also publish them for ppc64le?

In my projects, I use the free TravisCI PowerPC runners ([no limit/credits because they are part of the Partner Queue](https://docs.travis-ci.com/user/billing-overview/#partner-queue-solution)) for these deployments via cibuildwheel.

### Describe alternatives you've considered.

We currently build ourselves from source. Sometimes, these builds are so long that they often killed by runtime limits on shared/login nodes of HPC systems. Running on exclusive compute nodes for these systems is not always possible without work-arounds, since they might not have internet access for downloads.

Using other package managers such as conda and spack has their own issues, some fundamental and some can be overcome in the future, documented [here](https://github.com/scipy/scipy/issues/17054#issuecomment-1252695669) and [here](https://github.com/scipy/scipy/issues/17054#issuecomment-1252707872), but they work in a subset of workflows.

Update: I will likely focus on Spack and @rgommers's contributions: https://github.com/scipy/scipy/issues/17054#issuecomment-1252721615

### Additional context (e.g. screenshots, GIFs)

https://github.com/scipy/scipy/issues/17054#issuecomment-1251788198",2022-09-20 17:57:35,,Releasing PowerPC (ppc64le) wheels?,"['01 - Enhancement', '14 - Release']"
22286,open,nstarman,"### Proposed new feature or change:

It would be very useful to be able to type annotate that a ``dtype`` is structured.

A runtime-checkable ``Protocol`` is insufficient to detect `names` and `fields` are not `None` in a structured dtype  (https://docs.python.org/3/library/typing.html#typing.runtime_checkable).

```python
@runtime_checkable
class StructuredDType(Protocol):
    @property
    def names(self) -> tuple: ...

    @property
    def fields(self) -> Mapping[str, tuple]: ...
```",2022-09-16 19:34:58,,ENH: Add Protocol for checking if a dtype is structured,['Static typing']
22248,open,kalvdans,"### Proposed new feature or change:

Now `np.average` doesn't broadcast `a` to match the shape of `weights`. That's all fine and according to documentation, but it would be an enhancement to allow broadcast. See the session below when it first fails but then succeed when I do manual broadcasting of `a` to the shape of `weights`.

```
>>> import numpy as np
>>> np.average([[1, 2, 3]], weights=[[4,5,6], [7,8,9]], axis=1)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<__array_function__ internals>"", line 5, in average
  File ""/home/chn/env/lib/python3.10/site-packages/numpy/lib/function_base.py"", line 397, in average
    raise TypeError(
TypeError: 1D weights expected when shapes of a and weights differ.
>>> np.average([[1, 2, 3], [1, 2, 3]], weights=[[4,5,6], [7,8,9]], axis=1)
array([2.13333333, 2.08333333])
```",2022-09-12 11:57:48,,ENH: np.average to broadcast a and weights,['01 - Enhancement']
22244,open,h-vetinari,"In https://github.com/numpy/numpy/pull/20102, https://github.com/numpy/numpy/commit/fdda3ca1c4425da3b73ae2afb5e09538ec146ef1 landed, and that line has since been untouched.

Is this intended to be a permanent state of affairs? If not, here's an issue to track updating this. 🙃 

CC @thomasjpfan ",2022-09-11 08:03:58,,TEST: status of cffi & python 3.10,['unlabeled']
22237,open,seberg,"### Describe the issue:

Choose (I think) always overwrites all values in the output.  It is thus unnecessary to cast a given `out` to the resulting dtype (if a cast is necesssary).

This can lead to spurious warnings as given in the below example

### Reproduce the code example:

```python
import numpy as np

a = np.arange(3)
b = np.ones((3, 3), dtype=np.int64)

out = np.array([np.nan, np.nan, np.nan])

np.choose(a, b, out=out)
```


### Error message:

```shell
/home/sebastian/forks/numpy/build/testenv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57: RuntimeWarning: invalid value encountered in cast
  return bound(*args, **kwds)
```


### NumPy/Python version information:

All versions up to 1.24-dev will do the unnecessary cast, but only the current dev gives the spurious warning.
(We have a typing test that could run into this randomly, although I am changing it in a different PR)

### Context for the issue:

_No response_",2022-09-09 11:24:16,,BUG: `choose` does an unnecessary cast of the initial `out` values,['00 - Bug']
22217,open,coldwaterq,"### Proposed new feature or change:

After reading [issue 13983](https://github.com/numpy/numpy/issues/13983) I understand why making `savez` and `savez_compressed` suported `allow_pickle` isn't possible. However I would like an option that supports `allow_pickle` so that if an object works it's way into something being saved it will fail instead of creating an object using pickles.

The main use case being that if I screw up and save a model instead of a state_dict, I want it to fail in the very beginning instead of after finishing a multi-day training session.

Since `savez` and `savez_compressed` can't be changed, could `_savez` be re-named to something like `savez_complex`, `savez_controlled`, or whatever makes the most sense, and be added to the `__all__` on https://github.com/numpy/numpy/blob/main/numpy/lib/npyio.py. This would allow security concerned developers to easily discover the function, and allow developers to rely on the fact that it won't fundamentally change in a way that would break it in the future.

Thank you
",2022-09-06 19:11:11,,ENH: exposed savez version that doesn't fall back to pickles,['unlabeled']
22197,open,skaae,"### Describe the issue:

converting integers to np.datetime64 fails for numpy np.int64 integers. 

```python
np.datetime64(np.int64(123), 'ns')  # ValueError: Could not convert object to NumPy datetime
np.datetime64(123, 'ns')  # ok
```

### Reproduce the code example:

```python
np.datetime64(np.int64(123), 'ns')  # ValueError: Could not convert object to NumPy datetime
np.datetime64(123, 'ns')  # ok
```


### Error message:

```shell
np.datetime64(np.int64(123), 'ns')
*** ValueError: Could not convert object to NumPy datetime
```


### NumPy/Python version information:

1.23.2 3.8.10 (default, Jun 22 2022, 20:18:18) 
[GCC 9.4.0]

### Context for the issue:

_No response_",2022-09-02 09:24:44,,"BUG: datetime64(np.int64(123), 'ns') fails but datetime64(123, 'ns') is ok",['00 - Bug']
22188,open,daskol,"### Proposed new feature or change:

At the moment, print options are initialized with default options and can be changed with either `numpy.set_printoptions` function or `numpy.printoptions` context manager. Sometimes it is not very convenient to modify a code with mentioned above function to display arrays in more readable way.

It would be great to initialize print options from environment variables prefixed with `NUMPY_PRINTOPTION_` (e.g. `precision` print option should be initialized from `NUMPY_PRINTOPTION_PRECISION` envvar).
```shell
NUMPY_PRINTOPTION_LINEWIDTH=120 python my-fance-solver.py
```",2022-08-31 12:35:34,,ENH: Initilize Print Options from Environment Variables,['unlabeled']
22182,open,ankitkariryaa,"### Describe the issue:

np.unique produces incorrect output when used with masked uint8 arrays. In the following case, np.unique repeats 255 and nodata values multiple times. In this case, the number of unique values is higher than the range of uint8 (i.e. 255). The bug is easy to reproduce with the following code. 

### Reproduce the code example:

```python
import numpy as np
t1 = (np.random.rand(100,100) * 255).astype(np.uint8)
t1[55:59, 45:76]= 255

print(t1.dtype)
# uint8
unq = np.unique(t1)

tmpm = np.ma.masked_array(t1, mask=(t1 == 0))
unqm = np.unique(tmpm)
print(len(unq), len(unqm) )
print(unq)
print(unqm)

# 256 305
# [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
#   18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
#   36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
#   54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
#   72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
#   90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
#  108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
#  126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
#  144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
#  162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
#  180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
#  198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
#  216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
#  234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
#  252 253 254 255]
# [1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27
#  28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51
#  52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75
#  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99
#  100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117
#  118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135
#  136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153
#  154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
#  172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189
#  190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207
#  208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225
#  226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243
#  244 245 246 247 248 249 250 251 252 253 254 255 -- 255 -- 255 -- 255 --
#  255 -- 255 -- 255 -- 255 -- 255 -- 255 -- 255 -- 255 -- 255 -- 255 -- 255
#  -- 255 -- 255 -- 255 -- 255 -- 255 -- 255 -- 255 -- 255 -- 255 -- 255 --
#  255]
#
```


### Error message:

_No response_

### NumPy/Python version information:

1.21.2 3.9.7 (default, Sep 16 2021, 13:09:58) 
[GCC 7.5.0]


### Context for the issue:

_No response_",2022-08-30 09:32:58,,BUG: np.unique produces incorrect output with masked uint8 arrays,"['00 - Bug', 'component: numpy.ma']"
22174,open,danielhrisca,"### Describe the issue:

it is not possible to split using `\x00` as separator

### Reproduce the code example:

```python
from numpy.core.defchararray import rsplit
import numpy as np


x = np.frombuffer(b'___\0++++' * 3, dtype='S8')

print(x[0].rsplit(b'\0'))
rsplit(x, b'\0')
```


### Error message:

```shell
[b'___', b'++++']

Traceback (most recent call last):
  File ""D:\user\02__PythonWorkspace\__venvs\daxil\lib\site-packages\IPython\core\interactiveshell.py"", line 3553, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-6c8058c0020b>"", line 1, in <cell line: 1>
    runfile('C:/Users/user/AppData/Roaming/JetBrains/PyCharmCE2022.2/scratches/scratch.py', wdir='C:/Users/user/AppData/Roaming/JetBrains/PyCharmCE2022.2/scratches')
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2022.2\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_umd.py"", line 198, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2022.2\plugins\python-ce\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""C:/Users/user/AppData/Roaming/JetBrains/PyCharmCE2022.2/scratches/scratch.py"", line 8, in <module>
    rsplit(x, b'\0')
  File ""<__array_function__ internals>"", line 180, in rsplit
  File ""D:\user\02__PythonWorkspace\__venvs\daxil\lib\site-packages\numpy\core\defchararray.py"", line 1348, in rsplit
    return _vec_string(
ValueError: empty separator
```


### NumPy/Python version information:

1.23.1 3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]

### Context for the issue:

_No response_",2022-08-25 09:55:31,,BUG: numpy.char.rsplit does not accept null byte as separator,['00 - Bug']
22141,open,mwtoews,"### Describe the issue:

There are two probably related issues that involve a masked array 32-bit floats:

1. It was expected that `fill_value` should have the same dtype as the parent array. It depends (shown in example).
2. The `dtype` of `fill_value` depends on the order of getting or setting the `fill_value` attribute, which is also not expected.

### Reproduce the code example:

```python
import numpy as np

print(np.__version__)

# Example 1: the dtype of fill_value is set to float64
ar1 = np.ma.arange(6, dtype=np.float32)
getattr(ar1, ""fill_value"")
ar1.fill_value = -999.
print(""Example 1: {0}, {1}"".format(ar1.dtype, ar1.fill_value.dtype))

# Example 1: the dtype of fill_value is set to float32
ar2 = np.ma.arange(6, dtype=np.float32)
ar2.fill_value = -999.
print(""Example 2: {0}, {1}"".format(ar2.dtype, ar2.fill_value.dtype))
```
output:
```
1.23.2
Example 1: float32, float64
Example 2: float32, float32
```

### NumPy/Python version information:

```
1.23.2 3.8.10 (default, Jun 22 2022, 20:18:18) 
[GCC 9.4.0]
```
Note that versions as far back as NumPy 1.16.5 (included with Ubuntu) also have the same behavior, so it's not new.",2022-08-16 23:48:53,,BUG: masked array fill_value inconsistencies with float32,"['00 - Bug', 'component: numpy.ma']"
22134,open,theblazehen,"### Describe the issue:

On some of our hardware, running `inv()` on a matrix causes a floating point exception

### Reproduce the code example:

```python
from numpy.linalg.linalg import inv
import numpy
a = numpy.matrix([[100, 0, 0], [0, 100, 0], [0, 0, 1]])
inv(a)
```


### Error message:

```shell
Program received signal SIGFPE, Arithmetic exception.
0x00007ffff513733c in gemm_thread_n () from /home/ubuntu/venv/deepalert/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so
(gdb) bt
#0  0x00007ffff513733c in gemm_thread_n () from /home/ubuntu/venv/deepalert/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so
#1  0x00007ffff514dd63 in dgetrs_N_parallel () from /home/ubuntu/venv/deepalert/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so
#2  0x00007ffff4f2f5ae in dgesv_64_ () from /home/ubuntu/venv/deepalert/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so
#3  0x00007ffff43603d8 in void inv<double>(char**, long const*, long const*, void*) () from /home/ubuntu/venv/deepalert/lib/python3.8/site-packages/numpy/linalg/_umath_linalg.cpython-38-x86_64-linux-gnu.so
#4  0x00007ffff71b3d55 in generic_wrapped_legacy_loop () from /home/ubuntu/venv/deepalert/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so
#5  0x00007ffff71bfd8a in ufunc_generic_fastcall () from /home/ubuntu/venv/deepalert/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so
#6  0x000000000056cc1f in _PyEval_EvalFrameDefault ()
#7  0x00000000005f6cd6 in _PyFunction_Vectorcall ()
#8  0x00000000005f6082 in PyObject_Call ()
#9  0x00007ffff6f87f24 in array_implement_array_function () from /home/ubuntu/venv/deepalert/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so
#10 0x00000000005f69ca in PyCFunction_Call ()
#11 0x00000000005f74f6 in _PyObject_MakeTpCall ()
#12 0x0000000000570d55 in _PyEval_EvalFrameDefault ()
#13 0x0000000000569dba in _PyEval_EvalCodeWithName ()
#14 0x00000000005f6eb3 in _PyFunction_Vectorcall ()
#15 0x000000000056bacd in _PyEval_EvalFrameDefault ()
#16 0x0000000000569dba in _PyEval_EvalCodeWithName ()
#17 0x00000000006902a7 in PyEval_EvalCode ()
#18 0x000000000067f951 in ?? ()
#19 0x000000000067f9cf in ?? ()
#20 0x000000000067fa71 in ?? ()
#21 0x0000000000681b97 in PyRun_SimpleFileExFlags ()
#22 0x00000000006b9d32 in Py_RunMain ()
#23 0x00000000006ba0bd in Py_BytesMain ()
#24 0x00007ffff7de7083 in __libc_start_main (main=0x4efd60 <main>, argc=2, argv=0x7fffffffbfa8, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffbf98) at ../csu/libc-start.c:308
#25 0x00000000005fc5fe in _start ()
```



### NumPy/Python version information:

```
1.23.2 3.8.10 (default, Jun 22 2022, 20:18:18)
[GCC 9.4.0]
```

### Context for the issue:

Issue is occurring when generating graphs from matplotlib",2022-08-15 16:39:38,,BUG: Floating point exception when running inv on a matrix,['00 - Bug']
22133,open,mattip,"### Issue with current documentation:

In PR #19550 it is proposed to add a `ndarray.__format__` dunder method which would use format strings. In the documentation meeting we discussed the lack of documentation around displaying ndarrays.  We have a few ways currently to control representation:
- directly using `array2string`
  - use a `Formatter` class
  - use the `kwargs` in the function
- use global state in `set_printoptions` and then use str() or repr()
- (in the PR) use `__format__` with a format string.

It would be nice to have a [howto](https://numpy.org/devdocs/user/howtos_index.html) on array formatting to explain how to use the myriad options.

### Idea or request for content:

_No response_",2022-08-15 15:01:30,,DOC: Write a formatting howto,['04 - Documentation']
22129,open,bbuzz31,"### Describe the issue:

`np.ma.polyfit` fails if there is  a whole column of missing values. I think some broadcasting is incorrectly handled under the hood. See MWE for clearer description. Note that regular `np.polyfit` works correctly.


### Reproduce the code example:

```python
import numpy as np

x = np.arange(5)
y = np.random.rand(5, 4)

# put a few nans consistently along the x dimension
# even without these, the same error is raised.
y[3, :] = np.nan
y[1, :] = np.nan

# put all nans in one column; without this, the polyfit works.
y[:, -1] = np.nan

ym = np.ma.masked_invalid(y)

np.ma.polyfit(x, ym, 1)

# np.polyfit(x, ym, 1) # works correctly
```


### Error message:

```shell
TypeError: expected non-empty vector for x
> .../site-packages/numpy/lib/polynomial.py(630)polyfit()
    628         raise TypeError(""expected 1D vector for x"")
    629     if x.size == 0:
--> 630         raise TypeError(""expected non-empty vector for x"")
    631     if y.ndim < 1 or y.ndim > 2:
    632         raise TypeError(""expected 1D or 2D array for y"")
```


### NumPy/Python version information:

numpy: '1.21.6'
python: 3.9.13

### Context for the issue:

consider a geophysical datacube of time/lat/lon. Often, land is masked out during ocean studies, resulting in many lat/lon grid cells with all-nan time series. Then, there may also be missing data within the timeseries for the good lat/lon cells.",2022-08-14 18:29:48,,BUG: Broadcasting issue in masked polyfitting,"['00 - Bug', 'component: numpy.ma']"
22098,open,gpshead,"### Proposed new feature or change:

In https://github.com/numpy/numpy/issues/9968 code was added so that `longdouble(int)` would be more precise (good feature from a numpy perspective!) without transiting through a 64-bit double. But it chose a poorly performing algorithm when the long is large:

It requires transit through a base 10 string internally.  https://github.com/numpy/numpy/blob/maintenance/1.23.x/numpy/core/src/common/npy_longdouble.c#L130 in `npy_longdouble_from_PyLong`. That performs in the sub-quadratic to quadratic range, so it is slow on huge numbers. Ironic given the goal of this API is to only care about at _most_ the most significant 128-bits of the PyLong.

Instead of doing that, there are better options: If you still want the ease of going through a string, at the very least have it use hexadecimal instead. the equivalent of `b""%x"" % long_obj`. That is an O(num_digits) conversion.  But you can skip even that and use `long_obj.to_bytes()` to get a raw binary string from which you can even more directly determine the log2 for your exponent and directly grab the relevant mantissa value bits directly into your floating point representation.

You _could_ probably be even more pedantic and tie yourself to the CPython PyLong internals of its 30 or 15 bit digits to avoid an intermediate linear from_binary or hex allocation at all, but that'd make for more fragile specific CPython PyLong implementation dependant code even if it would be constant time rather than O(n).

A hack to get constant time best of both worlds might be something logically akin to:

```python
# ... mumble do something with the sign mumble ...
log2 = long_obj.bit_length()
if log2 > 128:
    long128 = long_obj >> (log2 - 128)
elif log2 < 128:
    long128 = long_obj << (128 - log2)
else:
   long128 = long_obj
assert long128.bit_length() == 128
bits128 = long128.to_bytes()
assert len(bits128) == 128//8
# construct your float from log2 and bits128
# (obviously don't need 128 bits as you've got an exponent to store)
```

That avoids doing an O(unbounded-N) to_bytes or hex conversion and only does it on the few bits you need. The shifts should be constant time as Python's internal PyLong bignum uses binary.",2022-08-08 21:17:14,,ENH: longdouble(int) performs poorly due to an unnecessary base 10 transit,['component: numpy._core']
22089,open,InessaPawson,"The goal of this tracking issue is to gather user stories for the official NumPy documentation and educational materials. User stories are short descriptions of a need written from the perspective of a user. User stories must have acceptance criteria and a definition of “done.” See an example of a good user story here: https://github.com/numpy/numpy.org/issues/42#issuecomment-552997858

To submit a user story, add a comment below. 

Before posting a new story, please check if there is already a similar story.",2022-08-07 19:16:20,,DOC: NumPy documentation user stories - tracking issue,['04 - Documentation']
22064,open,hhoppe,"### Describe the issue:

I'm trying to use typed `np.ndarray` with mypy.
I find many cases where computations involving `np.ndarray` eventually become type `Any`.
I am not certain if the fault lies in `numpy` or in `mypy`.

One obvious example is that `a.__add__(b)` behaves differently from `np.add(a, b)`, as in the following code example.

In the final line, even when starting with a typed array `b` (`numpy.ndarray[Any, numpy.dtype[numpy.floating[numpy.typing._64Bit]]]`), the computation `np.add(b, b) + np.add(b, b)` produces `Any`.

### Reproduce the code example:

```python
import numpy as np
from typing import Any

def function(a: np.ndarray[Any, Any]) -> None:
  b = np.ones(10)

  reveal_type(a)  # numpy.ndarray[Any, Any]
  d1 = a + a
  d1b = a.__add__(a)
  d2 = np.add(a, a)
  reveal_type(d1)  # Any
  reveal_type(d1b)  # Any
  reveal_type(d2)  # numpy.ndarray[Any, numpy.dtype[Any]]

  reveal_type(b)  # numpy.ndarray[Any, numpy.dtype[numpy.floating[numpy.typing._64Bit]]]
  d3 = b + b
  d4 = np.add(b, b)
  reveal_type(d3)  # numpy.ndarray[Any, numpy.dtype[numpy.floating[Any]]]
  reveal_type(d4)  # numpy.ndarray[Any, numpy.dtype[Any]]

  reveal_type(d3)  # numpy.ndarray[Any, numpy.dtype[numpy.floating[Any]]]
  d5 = d3 + d3
  d6 = np.add(d3, d3)
  reveal_type(d5)  # numpy.ndarray[Any, Any]
  reveal_type(d6)  # numpy.ndarray[Any, numpy.dtype[Any]]

  reveal_type(d4)  # numpy.ndarray[Any, numpy.dtype[Any]]
  d7 = d4 + d4
  d8 = np.add(d4, d4)
  reveal_type(d7)  # Any
  reveal_type(d8)  # numpy.ndarray[Any, numpy.dtype[Any]]

  reveal_type(np.add(b, b) + np.add(b, b))  # Any !
```


### Error message:

_No response_

### NumPy/Python version information:

1.22.4 3.10.4 (main, Jun 29 2022, 12:14:53) [GCC 11.2.0]

mypy==0.971
mypy-extensions==0.4.3
either numpy==1.22.4 or numpy==1.23.1
",2022-07-29 21:33:07,,BUG: mypy often infers type `Any` for overloaded operators (example: __add__ vs np.add),"['00 - Bug', 'Static typing']"
22059,open,magsol,"### Issue with current documentation:

This environment variable, which allows users to opt out of certain CPU features, is currently undocumented ([defined here](https://github.com/numpy/numpy/blob/d4732a26d2d044eee59bf6f889666a26ead5db92/numpy/core/src/common/npy_cpu_features.h#L100)).

There are a couple of open issues around the SIMD documentation that mention this flag (so maybe this should be a meta-issue, or one should be created):

 - https://github.com/numpy/numpy/issues/18901
 - https://github.com/numpy/numpy/issues/18926

In concert with the suggestion for adding the `NPY_ENABLE_CPU_FEATURES` flag (#22058), this could also include adding documentation for that flag as well.

### Idea or request for content:

_No response_",2022-07-29 15:47:10,,DOC: Add documentation for `NPY_DISABLE_CPU_FEATURES`,"['04 - Documentation', 'sprintable']"
22049,open,paolomattioli1989,"There seems to be an issue when trying to unmask a value in a structured array based on some predicate.
I would have expected this operation to successfully unmask the entry, but it does not - see the example below.

I'm running Ubuntu 20.04.3 and python 3.8.13
This behaviour has been observed on numpy versions 1.23.1 and 1.22.4 (the two ones i've tried).
The expected behaviour (so no AssertionError raised by the below snippet) is instead observed in 1.21.5.

```
out = np.ma.array(np.empty(3, dtype=[('a_string', '<U30'), ('a_float', '<f8')]), mask=True)
assert not out.hardmask
out['a_string'] = ['Aa', 'Bb', 'Cc']
pred = out['a_string'] == 'Aa'
assert pred.sum() == 1
out['a_float'][pred] = 1.23

assert not out['a_float'][pred].mask.item()  # <-- fails
```",2022-07-27 09:45:50,,Cannot unmask structured arrays values based on predicate,['component: numpy.ma']
22042,open,Jakobhenningjensen,"### Describe the issue:

The array `X = np.array([np.nan,np.nan,""foo"",np.nan,np.nan])` converts the `np.nan` to `'nan'` i.e a string instead of keeping it as `np.nan`. This can really easy introduce bugs, since stuff like `pd.issnull()` now evaluates the `'nan'` to `False` instead of `True`.

This behaviour/conversion might be intentional (since `type(np.nan)==float`) but if it is I think it should either raise a warning since the conversion happens behind the scenes, or should be controlled by e.g a `conversion` argument which does not allow to convert the elements to other types, or add it more clearly in the documentation.

Another way is to have convert it to an empty string `""""` or introduce a  ""string-nan"" `NaS` just like we have `NaN` and `NaT`.

Eitherway I think it's a very dangerous behaviour as it is right now, since it easily can/does introduce bugs/unwanted behaviour

### Reproduce the code example:

```python
import numpy as np
import pan
X = [np.nan,np.nan,""foo"",np.nan,np.nan]
idx_to_keep =pd.notnull(X) # [False,False,True,False,False]
.
.
X = np.array(X)
idx_to_keep =pd.notnull(X) # [True,True,True,True,True]
```


### Error message:

_No response_

### NumPy/Python version information:

1.22.4 3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]",2022-07-25 09:18:11,,"BUG: np.array(x) converts np.nan to string, if an element in x is a string",['00 - Bug']
22029,open,attack68,"### Issue with current documentation:

There are two issues I think worth expanding the docs with.

### 1) Diagonalising vectors in tensors

One can diagonalise a simple vector in variety of ways;

```python
>>> a = np.array([1, 2, 3])
>>> a * np.eye(3).             # broadcasting
>>> np.multiply(a, np.eye(3)). # method broadcasting
>>> np.diag(a).                # targeted method
>>> np.einsum('i, ij -> ij', a, np.eye(3)). # explicit einsum
```

I think it is useful to document `einsum` can do this since it can then be used for larger tensors where broadcasting does not.
My use case was to convert a set of 1d samples to a set of 2d-diagonal samples,

```python
>>> 1d_samples = np.array([1, 2, 3], [10, 20, 30]])
>>> 2d_samples = np.einsum(""ki, ij -> kij"", 1d_samples, np.eye(3))
>>> 2d_samples
array([[[ 1.,  0.,  0.],
        [ 0.,  2.,  0.],
        [ 0.,  0.,  3.]],

       [[10.,  0.,  0.],
        [ 0., 20.,  0.],
        [ 0.,  0., 30.]]])
``` 

### 2) Writable views

In the docs it is written that, for example as of NumPy 1.10

> np.einsum('ii->i', a) will return a writeable view of the diagonal of a 2D array.

I tried to use this recently and couldn't see how it was writeable to the original array. And thus, if the `return` is just a new ndarray isn't it expected that that array should be writeable as per any other array? I find this a confusing part of the docs if the meaning is just to say an array is returned.

### Idea or request for content:

as above",2022-07-23 13:35:00,,DOC: einsum examples of diagonalising and writeable view,['04 - Documentation']
22025,open,matteoacrossi,"### Describe the issue:
`np.linalg.det` on a 4x4 real matrix returns `nan`, while it should be 0.

If the same code example is run on an intel64 linux machine, the determinant is zero for both matrices.

This happens both with conda-forge and pypi numpy, on both python 3.9 and 3.10.

### Reproduce the code example:

```python
import numpy as np
np.linalg.det(np.array([[1.0000000000000000e+000, 0.0000000000000000e+000,
         0.0000000000000000e+000, 0.0000000000000000e+000],
        [0.0000000000000000e+000, 1.0000000000000000e+000,
         3.8307904270117927e-146, 1.4674955295685193e-291],
        [0.0000000000000000e+000, 3.8307904270117927e-146,
         1.4674955295685193e-291, 0.0000000000000000e+000],
        [0.0000000000000000e+000, 1.4674955295685193e-291,
         0.0000000000000000e+000, 0.0000000000000000e+000]]))
/opt/homebrew/Caskroom/miniforge/base/envs/numpy-test/lib/python3.10/site-packages/numpy/linalg/linalg.py:2154: RuntimeWarning: overflow encountered in det
  r = _umath_linalg.det(a, signature=signature)
/opt/homebrew/Caskroom/miniforge/base/envs/numpy-test/lib/python3.10/site-packages/numpy/linalg/linalg.py:2154: RuntimeWarning: invalid value encountered in det
  r = _umath_linalg.det(a, signature=signature)
nan
```

This one instead works fine:

```python
np.linalg.det(np.array([[1.0000000000000000e+000, 0.0000000000000000e+000,
         0.0000000000000000e+000, 0.0000000000000000e+000],
        [0.0000000000000000e+000, 1.0000000000000000e+000,
         3.8307904315347111e-146, 1.4674955330337903e-291],
        [0.0000000000000000e+000, 3.8307904315347111e-146,
         1.4674955330337899e-291, 0.0000000000000000e+000],
        [0.0000000000000000e+000, 1.4674955330337903e-291,
         0.0000000000000000e+000, 0.0000000000000000e+000]]))
```
I'm also attaching a .zip file with the matrices saved as `.npy` files in case there are numerical precision problems: 
[numpy_nan.zip](https://github.com/numpy/numpy/files/9168608/numpy_nan.zip).

`test_nan.npy` gives the `nan` while for `test_ok.npy` the determinant is correct.

### Error message:

_No response_

### NumPy/Python version information:

1.23.1 3.10.0 | packaged by conda-forge | (default, Nov 20 2021, 02:27:15) [Clang 11.1.0 ]

Output of `np.show_config()`:

```
>>> np.show_config()
openblas64__info:
    libraries = ['openblas64_', 'openblas64_']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None)]
    runtime_library_dirs = ['/usr/local/lib']
blas_ilp64_opt_info:
    libraries = ['openblas64_', 'openblas64_']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None)]
    runtime_library_dirs = ['/usr/local/lib']
openblas64__lapack_info:
    libraries = ['openblas64_', 'openblas64_']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None), ('HAVE_LAPACKE', None)]
    runtime_library_dirs = ['/usr/local/lib']
lapack_ilp64_opt_info:
    libraries = ['openblas64_', 'openblas64_']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None), ('HAVE_LAPACKE', None)]
    runtime_library_dirs = ['/usr/local/lib']
Supported SIMD extensions in this NumPy install:
    baseline = NEON,NEON_FP16,NEON_VFPV4,ASIMD
    found = ASIMDHP
    not found = ASIMDDP,ASIMDFHM
```",2022-07-22 11:56:26,,BUG: nan returned by np.linalg.det while it should be 0 on arm64 mac,['00 - Bug']
22018,open,benjidial,"### Describe the issue:

Many functions such as np.ones allow you to specify whether the data is stored by row and then column, or column and then row. Seemingly, the default option is always chosen when there is only one column of data, regardless of what is specified.

### Reproduce the code example:

```python
import numpy as np

A = np.ones((4, 2))
A.resize((4, 3))
print(A)

B = np.ones((4, 2), order='F')
B.resize((4, 3))
print(B)

C = np.ones((4, 1))
C.resize((4, 2))
print(C)

#does not behave as expected
D = np.ones((4, 1), order='F')
D.resize((4, 2))
print(D)
```


### Error message:

_No response_

### NumPy/Python version information:

1.23.1 3.10.5 (main, Jun  7 2022, 21:08:59) [GCC 11.3.0]",2022-07-20 17:37:47,,"ENH: resize could have `order=""A""` kwarg for better F-order support",['00 - Bug']
21990,open,Gattocrucco,"### Describe the issue:

As mentioned in #14104, multi-field views of structured arrays with object fields are forbidden because they could leave objects hidden in the padding. `recfunctions.structured_to_unstructured` does not have this problem because it always uses all the fields, yet it's forbidden from operating on arrays with object fields, even when doing a copy.

### Reproduce the code example:

```python
from numpy.lib import recfunctions
import numpy as np

recfunctions.structured_to_unstructured(np.empty(0, 'O,O'), copy=True)
```


### Error message:

```shell
TypeError                                 Traceback (most recent call last)
Input In [367], in <cell line: 4>()
      1 from numpy.lib import recfunctions
      2 import numpy as np
----> 4 recfunctions.structured_to_unstructured(np.empty(0, 'O,O'), copy=True)

File <__array_function__ internals>:180, in structured_to_unstructured(*args, **kwargs)

File ~/Documents/Scuola/lsqfitgp/Repository/lsqfitgp/pythonvenv/lib/python3.10/site-packages/numpy/lib/recfunctions.py:981, in structured_to_unstructured(arr, dtype, copy, casting)
    979 with suppress_warnings() as sup:  # until 1.16 (gh-12447)
    980     sup.filter(FutureWarning, ""Numpy has detected"")
--> 981     arr = arr.view(flattened_fields)
    983 # next cast to a packed format with all fields converted to new dtype
    984 packed_fields = np.dtype({'names': names,
    985                           'formats': [(out_dtype, dt.shape) for dt in dts]})

File ~/Documents/Scuola/lsqfitgp/Repository/lsqfitgp/pythonvenv/lib/python3.10/site-packages/numpy/core/_internal.py:494, in _view_is_safe(oldtype, newtype)
    491     return
    493 if newtype.hasobject or oldtype.hasobject:
--> 494     raise TypeError(""Cannot change data-type for object array."")
    495 return

TypeError: Cannot change data-type for object array.
```


### NumPy/Python version information:

1.22.4 3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]",2022-07-15 11:08:22,,BUG: `recfunctions.structured_to_unstructured` does not work with object fields,['00 - Bug']
21989,open,TobiasJacob,"### Proposed new feature or change:

Pandas supports pivot tables, but is limited to two dimensional dataframes. I suggest adding 2 new functions to numpy that can create and decompose 3 or more dimensional pivot tables. I'm also willing to contribue a solution if I receive some guidance.

Consider the following [Google Colab Demonstration](https://colab.research.google.com/drive/1UMbiX6h8viyRtePlf0Dqrg6U4Rtiylgh?usp=sharing):


```python
import numpy as np

# Generate random Array
A = np.random.rand(10, 20, 30)
A.shape

# Unpivot array into narrow table
ind = np.meshgrid(*[np.arange(s) for s in A.shape])

values = A[ind[0], ind[1], ind[2]]

unpivot = np.stack([
                    ind[0].reshape(-1),
                    ind[1].reshape(-1),
                    ind[2].reshape(-1),
                    values.reshape(-1),
])

# Pivot the narrow table back to the data cube
pivot = np.zeros_like(A)

pivot[unpivot[0].astype(int), unpivot[1].astype(int), unpivot[2].astype(int)] = unpivot[3]

# Check equality
assert np.sum(np.abs(pivot - A)) < 1e-5
```

It would be nice if numpy provided a pivot and unpivot function for this. Especially for pivot, different aggregation methods would be interesting, e. g. sum, mean, min, max, first, last, ... Also, my implementation is probably less efficient, and does not scale to arbitrary dimensions.

The reason for pivoting in our use case is that it is easier to store our datacube as a narrow table in a SQL database. A client can then filter/slice based on SQL WHERE statements, download the part of the datacube that is needed, and use pivot to generate the needed slice of the datacube.

Open Design Questions:
- Does it make more sense to treat the index values as absolute or collect all unique index values and assign everyone a slice. E. g. If the index values are [0, 2, 3, ...], should the resulting slice look like this [x[0], 0, x[2], x[3]], or like this [x[0], x[2], x[3]]? It might make sense to implement both versions and in the second case return the unique axis values as additional value.
- How to handle multiple aggregations or target values at once?
- A float array is probably needed to save the values, but the indices should be saved or casted to integers. This could cause issues with floating point conversion. Another problem, in case of filtering unique values, is that we need a small èpsilon that decides if two floats are equal. Or does it make sense to split indices and values into two seperate arguments using different dtypes, which allows the user to properly keep the integer datatype?
",2022-07-15 08:36:25,,ENH: Multidimensional Pivot and Unpivot,"['23 - Wish List', '57 - Close?']"
21969,open,Vishnu213,"This is the command in Julia with the scale option:
D,V=eigen(K; permute=true ,scale=false,sortby=nothing).

Also in Matlab, we can carry out the similar operation using: eig(K, 'nobalance') command.

Thank you in advance!",2022-07-11 15:34:00,,Is there any command in numpy to find eigen value and eigen vectors similar to Julia's Eigen without scaling the matrix? ,['unlabeled']
21961,open,david-cortes,"### Describe the issue:

Internally, the `datetime64` dtype is represented as an integer. One can do operations on it such as adding or substracting, which are well defined.

Nevertheless, `np.random.Generator.uniform` will not let me use this dtype to specify the highs and lows. I would want to do something like this:
```python
import numpy as np
low = np.datetime64(""2020-01-01 00:00:00"")
high = np.datetime64(""2020-01-31 23:59:59"")
np.random.default_rng().uniform(
    low,
    high,
    size=10
)
```

But it errors out so I have to add boilerplate code:
```python
low + np.random.default_rng().uniform(
    high=(high - low).astype(int),
    size=10
).astype(int)
```

One could argue that in theory `uniform` is not the most appropriate choice since it generates floating points which then have to be rounded to integers, but `np.random.Generator.integers` also errors out:
```python
np.random.default_rng().integers(low, high, size=10)
```
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-32-8d188f6ec7e3> in <module>
----> 1 np.random.default_rng().integers(low, high, size=10)

_generator.pyx in numpy.random._generator.Generator.integers()

_bounded_integers.pyx in numpy.random._bounded_integers._rand_int64()

TypeError: int() argument must be a string, a bytes-like object or a number, not 'datetime.datetime'
```

### Reproduce the code example:

```python
import numpy as np
low = np.datetime64(""2020-01-01 00:00:00"")
high = np.datetime64(""2020-01-31 23:59:59"")
np.random.default_rng().uniform(
    low,
    high,
    size=10
)
```


### Error message:

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-19-3abdd025d9c8> in <module>
      2 low = np.datetime64(""2020-01-01 00:00:00"")
      3 high = np.datetime64(""2020-01-31 23:59:59"")
----> 4 np.random.default_rng().uniform(
      5     low,
      6     high,

_generator.pyx in numpy.random._generator.Generator.uniform()

TypeError: float() argument must be a string or a number, not 'datetime.datetime'
```


### NumPy/Python version information:

1.21.5 3.9.12 (main, Apr  5 2022, 06:56:58) 
[GCC 7.5.0]",2022-07-10 08:42:02,,ENH: create a way to take random uniform datetimes,['00 - Bug']
21958,open,positr0nium,"### Proposed new feature or change:

Currently this is not available due to the line:
a, b = asarray(a), asarray(b)
This could simply be resolved by replacing this by
a, b = asanyarray(a), asanyarray(b)",2022-07-09 15:37:45,,ENH: Make tensordot available for subtypes of ndarray,['01 - Enhancement']
21947,open,zklaus,"### Proposed new feature or change:

For masked arrays, `np.ma.core` has both `set_fill_value` and `get_fill_value`, which are useful when one does not know if an incoming array is a masked array or not. While `set_fill_value` is listed in `np.ma.core.__all__` and consequently is available as `np.ma.set_fill_value` and appears in the documentation, `get_fill_value` is not. I'd like to propose to add `get_fill_value` in `np.ma.core.__all__` as well.",2022-07-08 10:07:45,,ENH: Add `get_fill_value` to `np.ma.core.__all__`,['unlabeled']
21944,open,parched,"### Describe the issue:

Comparing a 0-D MaskedArray when an operand is masked, the resulting `dtype` is `float64` but I expected it to be `bool_`

### Reproduce the code example:

```python
import numpy as np

result = np.ma.MaskedArray(3) < np.ma.MaskedArray(5, True)

assert result.dtype != np.float64
assert result.dtype == np.bool_
```


### Error message:

_No response_

### NumPy/Python version information:

1.23.0 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]
",2022-07-07 21:31:11,,BUG: 0-D MaskedArray comparison has wrong dtype if an operand is masked,"['00 - Bug', 'component: numpy.ma']"
21941,open,M-Wieler,"```
np.float16(np.random.normal(size=int(1e5))).var()
C:\Users\WIM3SI\.conda\envs\msa\lib\site-packages\numpy\core\_methods.py:247: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
Out[3]: inf

np.float16(np.random.normal(size=int(1e4))).var()
Out[4]: 1.005

np.float32(np.random.normal(size=int(1e5))).var()
Out[5]: 0.9998826
```

Numpy version: 1.23.0
Python version: 3.9.11
Windows 10",2022-07-07 07:34:17,,BUG: overflow encountered in var() for large float16 arrays,['unlabeled']
21933,open,phinate,"A short example that runs without error:

```python3
arr = np.array([
    [1000,  110],
    [ 190,   50],
    [ 500,  110],
    [ 750,  400],
    [ 205,   30],
    [1000,  500],
    [ 600,   70],
    [ 180,   50]
])

assert [110] in arr
assert [1000, 1] in arr
assert np.array([200, 50]) in arr
```

Clearly numpy is satisfied if any of the individual elements in a collection resides in the array used to compare with. I don't find this particularly intuitive, as I want to check the equality on an element-wise basis, i.e. mirroring the same statement if `arr` was a list of lists. I'm not sure if this requires changing, but certainly a warning of some kind to indicate the type of comparison numpy will do here seems warranted.

Hope this is helpful!",2022-07-06 16:17:12,,Usage of `in` statement with arrays produces ambiguous behaviour to the user,['unlabeled']
21932,open,Markus28,"### Describe the issue:

Even when an array contains `np.inf`, there are cases where certain percentiles are clearly defined. E.g. 
- `np.percentile(np.array([0, np.inf, np.inf]), 50)` should be the median, which would be `np.inf`
- `np.percentile(np.array([0, np.inf, np.inf, np.inf]), 50)` should (even more so) be `np.inf`

This is not the case. In both cases, we get `np.nan`.

### Reproduce the code example:

```python
import numpy as np
np.percentile(np.array([0, np.inf, np.inf]), 50) # Produces RuntimeWarning and returns np.nan
np.percentile(np.array([0, np.inf, np.inf, np.inf]), 50) # No warning but returns np.nan
```


### Error message:

```shell
/home/markus/.local/lib/python3.10/site-packages/numpy/lib/function_base.py:4527: RuntimeWarning: invalid value encountered in subtract
  diff_b_a = subtract(b, a)
```


### NumPy/Python version information:

1.23.0 3.10.4 (main, Mar 25 2022, 00:00:00) [GCC 11.2.1 20220127 (Red Hat 11.2.1-9)]",2022-07-06 16:03:31,,BUG: `np.percentile` gives unreasonable results when array contains `np.inf`,['00 - Bug']
21918,open,takagi,"### Describe the issue:

Since NumPy 1.23, the strides of empty (0 size) arrays are set to zero as changed in #21477. However, once such arrays are reshaped, the results have non-zero strides computed in a C-contiguous manner.

```py
>>> import numpy as np
>>> a = np.ones((2, 0, 3))
>>> a.strides
(0, 0, 0)
>>> b = a.reshape((3, 0, 2))
>>> b.strides
(16, 16, 8)  # also should be (0, 0, 0)
```

### NumPy/Python version information:

1.23.0 3.10.0 (default, Oct 15 2021, 11:40:42) [GCC 7.5.0]",2022-07-04 12:16:53,,Reshaped 0-size arrays have non-zero strides,['00 - Bug']
21916,open,InessaPawson,"### Issue with current documentation:

Many, even more advanced, NumPy users are unsure about usage of the following functions: `stack`, `concatenate`, and `block`.

### Idea or request for content:

Creating a how-to guide would address these concerns.",2022-07-04 03:15:52,,"DOC: write a how-to guide about using 'stack', 'concatenate', 'block'",['04 - Documentation']
21915,open,rmccampbell,"### Proposed new feature or change:

Currently there are lots of ways to compute dot products (dot, vdot, inner, tensordot, einsum...), but none of them are really convenient for the case of arrays of vectors, where one dimension (usually the last or the first) is the vector dimension. The simplest way to do this currently is `np.sum(a * b, axis=axis)`, but this makes vector algebra less readable without a wrapper function, and it's probably not optimized as much as matrix products. Another way to do it is by adding appropriate dimensions and using matmul, but that's arguably less readable and not obvious to do generically for arbitrary axes. I think either np.dot or np.vdot could easily be extended with an `axis` parameter that would convert it into a bulk vector operation, with the same semantics as `np.sum(a * b, axis=axis)`. It should also maybe have a `keep_dims` parameter, which is useful for preserving broadcasting.",2022-07-04 00:03:48,,ENH: dot product along arbitrary axes,['01 - Enhancement']
21907,open,0xjc,"### Describe the issue:

When passing in an array of known dtype, some of the numpy functions discard some or all of the dtype information in the return type. In the example below, `np.sum` and `np.sort` properly preserve the dtype. `np.mean`, `np.median`, `np.linalg.norm` preserve that the dtype was `floating` but not the specific dtype. `np.log` and `np.sin` discard the entire dtype.

Since the discarded types are replaced by `Any`, this means that further code that uses the result of these functions may lose useful type checks resulting in false negatives, unless the user manually narrows the result back to the known dtype.

### Reproduce the code example:

```python
from typing import TYPE_CHECKING
import numpy as np


x = np.array([1.0, 2.0], dtype=np.float64)

y1 = np.sum(x)
y2 = np.mean(x)
y3 = np.median(x)
y4 = np.linalg.norm(x)

z1 = np.sort(x)
z2 = np.log(x)
z3 = np.sin(x)


if TYPE_CHECKING:
    reveal_locals()
    """"""
    Type of ""x"" is ""ndarray[Any, dtype[floating[_64Bit]]]""
    Type of ""y1"" is ""floating[_64Bit]""
    Type of ""y2"" is ""floating[Any]""
    Type of ""y3"" is ""floating[Any]""
    Type of ""y4"" is ""floating[Any]""
    Type of ""z1"" is ""ndarray[Any, dtype[floating[_64Bit]]]""
    Type of ""z2"" is ""ndarray[Any, dtype[Any]]""
    Type of ""z3"" is ""ndarray[Any, dtype[Any]]""
    """"""

# Actual types at runtime
print(
    f""""""
               x  dtype={x.dtype}
        np.sum(x)  type={type(y1).__name__}
       np.mean(x)  type={type(y2).__name__}
     np.median(x)  type={type(y3).__name__}
np.linalg.norm(x)  type={type(y4).__name__}
       np.sort(x) dtype={z1.dtype}
        np.log(x) dtype={z2.dtype}
        np.sin(x) dtype={z3.dtype}
""""""
)
""""""
               x  dtype=float64
        np.sum(x)  type=float64
       np.mean(x)  type=float64
     np.median(x)  type=float64
np.linalg.norm(x)  type=float64
       np.sort(x) dtype=float64
        np.log(x) dtype=float64
        np.sin(x) dtype=float64
""""""
```


### NumPy/Python version information:

Python 3.8.10
numpy 1.23.0",2022-07-02 19:15:45,,BUG: Type annotations for ufuncs discard dtype information in the return type,"['00 - Bug', 'Static typing']"
21903,open,ganesh-k13,"### Proposed new feature or change:

In reference to #20083:

### Gap in current tests
Currently, the following tests are only implemented for `float64` type as a direct copy from CPython's test in https://github.com/python/cpython/blob/main/Lib/test/test_float.py:

- [ ] [1]() moving the point around (pi).
- [ ] [2]() Make it go towards NMANT (in ref to [1])
- [ ] [3]() results that should overflow...
- [ ] [4]() check round-half-even is working correctly near MIN
- [ ] [5]() check round-half-even is working correctly near 1.0

*Note* This needs prior knowledge on floats and IEEE_754.

Start after #20083 is merged. I'll populate the links above with permanent links to reach them better,",2022-07-02 07:21:47,,TST: `ftype` agnostic checks for `hex` and `fromhex`,"['05 - Testing', 'component: npy_math', '62 - Python API']"
21864,open,jwuttke,"### Issue with current documentation:

Doc page https://numpy.org/devdocs/reference/c-api/array.html describes a C API, but does not tell us which header (`*.h`) files we need to `#include`.

This is absolutely essential information. Look at an arbitrary manual page from the C standard library, for instance `man sin`. It tells us in the very beginning of the synopsis that we need `#include <math.h>` before we can call any of the functions `sin`, `sinf`, `sinl`.

The Array API doc page in question gets it right under the heading ""Array structure and data access"": In the beginning of that section, we are informed that macros and structure members are defined in `ndarraytypes.h`. Analogous information is missing, however, for other sections like ""Creating arrays"".

### Idea or request for content:

Is `ndarraytypes.h` all we need? Then move that information out of the first section; it rather belongs in front.

Or are other header files required? Then provide the missing information in each section.",2022-06-28 17:17:07,,DOC: Which header files for Array API?,['04 - Documentation']
21859,open,bwestover,"### Describe the issue:

In version 1.22.4 `np.log(np.e)` produces 1.0 as expected. In `1.23.0` I see `0.9999999999999999`.

1.22.4 output:
```
3.9.13 (main, Jun 23 2022, 11:19:48) 
[GCC 10.2.1 20210110]
1.22.4
1.0
```

1.23.0 output:
```
3.9.13 (main, Jun 23 2022, 11:19:48) 
[GCC 10.2.1 20210110]
1.23.0
0.9999999999999999
```

### Reproduce the code example:

```python
import numpy as np
import sys

print(sys.version)
print(np.__version__)
print(np.log(np.e))
```


### Error message:

_No response_

### NumPy/Python version information:

3.9.13 (main, Jun 23 2022, 11:19:48) [GCC 10.2.1 20210110]
",2022-06-27 17:15:47,,BUG: np.log decimal precision / rounding behavior change,['00 - Bug']
21858,open,jwuttke,"### Issue with current documentation:

Some doc chapters start with one or two mottos, like https://numpy.org/doc/stable/user/c-info.python-as-glue.html.

This is distracting. The more so as the connection between contents and motto is distant at best.

### Idea or request for content:

Just remove them. Leave such intellectual bragging to books with distinct authorship. It's inappropriate in collectively written technical documentation.

",2022-06-27 17:08:27,,DOC: Please remove mottos,['04 - Documentation']
21826,open,steff456,"### Describe the issue:

Writing the spec for complex number support in the Array API we found that NumPy currently fails in 2 special cases. The behavior is inconsistent with the 2014 version C99.

The full list of special cases and specification is detailed in https://github.com/data-apis/array-api/pull/458

cc @kgryte

### Reproduce the code example:

```python
import numpy as np
import math

def is_equal_float(x, y):
    """"""Test whether two floating-point numbers are equal with special consideration for zeros and NaNs.

    Parameters
    ----------
    x : float
        First input number.
    y : float
        Second input number.

    Returns
    -------
    bool
        Boolean indicating whether two floating-point numbers are equal.

    Examples
    --------
    >>> is_equal_float(0.0, -0.0)
    False
    >>> is_equal_float(-0.0, -0.0)
    True
    """"""
    # Handle +-0:
    if x == 0.0 and y == 0.0:
        return math.copysign(1.0, x) == math.copysign(1.0, y)

    # Handle NaNs:
    if x != x:
        return y != y

    # Everything else, including infinities:
    return x == y


def is_equal(x, y):
    """"""Test whether two complex numbers are equal with special consideration for zeros and NaNs.

    Parameters
    ----------
    x : complex
        First input number.
    y : complex
        Second input number.

    Returns
    -------
    bool
        Boolean indicating whether two complex numbers are equal.

    Examples
    --------
    >>> import numpy as np
    >>> is_equal(complex(np.nan, np.nan), complex(np.nan, np.nan))
    True
    """"""
    return is_equal_float(x.real, y.real) and is_equal_float(x.imag, y.imag)

def compare(v, e):
    actual = np.tanh(v) 
    print('Value: {value}'.format(value=str(v)))
    print('Actual: {actual}'.format(actual=str(actual)))
    print('Expected: {expected}'.format(expected=str(e)))
    print('Equal: {is_equal}'.format(is_equal=str(is_equal(actual, e))))
    print('\n')

# Case 1
v = complex(0.0, np.inf)
e = complex(0.0, np.nan)
compare(v, e) # returns `NaN + NaN j`; however, this does match old C99 behavior (see https://www.open-std.org/jtc1/sc22/wg14/www/docs/n1892.htm#dr_471)

# Case 2
v = complex(0.0, np.nan)
e = complex(0.0, np.nan)
compare(v, e) # returns `NaN + NaN j`; however, this does match old C99 behavior (see https://www.open-std.org/jtc1/sc22/wg14/www/docs/n1892.htm#dr_471)
```


### Error message:

```shell
Value: infj
Actual: (nan+nanj)
Expected: nanj
Equal: False

Value: nanj
Actual: (nan+nanj)
Expected: nanj
Equal: False
```


### NumPy/Python version information:

v1.22.4",2022-06-22 17:51:33,,BUG: tanh returns unexpected values in special cases with complex numbers,['00 - Bug']
21804,open,phgz,"### Describe the issue:

I need to perform a set difference between 2 numpy arrays with `dtype=str`, so I'm using `np.setdiff1d`. However, it takes much more time than the `np.intersect1d` followed by `np.delete` function for some reason.

For ~340 000 texts in each array, it takes 48 minutes for `np.setdiff1d`, while it takes not even a second for `np.intersect1d` on my machine.

The dataset I'm using can be downloaded from _Kaggle_ at https://www.kaggle.com/datasets/rtatman/blog-authorship-corpus/download.

### Reproduce the code example:

```python
from time import time

import numpy as np
import pandas as pd

df = pd.read_csv(""blogtext.csv"")
arr1 = df[:df.shape[0]//2]['text'].to_numpy().flatten()
arr2 = df[df.shape[0]//2:]['text'].to_numpy().flatten()
arr1=np.unique(arr1)

# Method 1
start = time()
updated1 = np.setdiff1d(arr1, arr2)
end = time()
print(end-start)

# Method 2
start = time()
_, indices, _ = np.intersect1d(arr1, arr2, return_indices=True)
updated2 = np.delete(arr1, indices)
end = time()
print(end-start)

assert (updated1 == update2).all()
```


### Error message:

_No response_

### NumPy/Python version information:

1.22.4 3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0]
Running on Ubuntu 20.04.

Output of `lscpu`:
```
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   48 bits physical, 48 bits virtual
...
Thread(s) per core:              1
...
CPU MHz:                         1497.531
CPU max MHz:                     2650.0000
CPU min MHz:                     1500.0000
BogoMIPS:                        5299.71
...
L1d cache:                       1.5 MiB
L1i cache:                       1.5 MiB
L2 cache:                        24 MiB
L3 cache:                        256 MiB
...
```",2022-06-21 04:26:31,,BUG: Speed issue with `setdiff1d`,['00 - Bug']
21799,open,galbramc,"### Describe the issue:

I am embedding CPython and executing within threads. We are getting a ""bus error"" when importing numpy within a pthread function, but importing numpy within the ""main thread"" works. I've observed this both on intel and M1 macOS machines. I'm using numpy 1.22.4 installed with pip. 

I compiled CPython 3.9.13 with the address sanitizer and found  a stack overflow in dgetrf_parallel. Unfortunately I was not able to compile numpy my self with OpenBLAS to get the complete back trace. Compiling numpy without OpenBLAS runs without any errors. I have not compiled with the address sanitizer on Linux to see if I get the same error there, but I can if that would help.

I did look at the OpenBLAS source and there is this comment in lapack/getrf/getrf_parallel.c:
```
//In this case, the recursive getrf_parallel may overflow the stack.
//Instead, use malloc to alloc job_t.
#if MAX_CPU_NUMBER > GETRF_MEM_ALLOC_THRESHOLD
#define USE_ALLOC_HEAP
#endif
```
MAX_CPU_NUMBER is determined by the Makefiles based on the computer where OpenBLAS is compiled, and GETRF_MEM_ALLOC_THRESHOLD is defined as 80 on macOS in common.h. Maybe the solution here is to compile OpenBLAS so it always uses heap allocation in this routine?

Right now I am resorting to trying to import numpy in the main thread to avoid this issue. I've attached c code which reproduces the issue and the address sanitizer back trace information. Any advice on how to fix this would be greatly appreciated. Please let me know if there is any further information I can provide as well.

### Reproduce the code example:

```python
#include <Python.h>
#include <pthread.h>

void *thread_function(void *arg)
{
  PyThreadState *mainThreadState = (PyThreadState *)arg;
  PyThreadState *myThreadState = PyThreadState_New(mainThreadState->interp);

  PyEval_RestoreThread(myThreadState);
  
  printf(""Thread import numpy\n"");
  PyRun_SimpleString(""import sys, numpy; print(numpy.__version__, sys.version)\n"");
  printf(""Thread import numpy done!\n"");

  PyThreadState_Clear(myThreadState);
  PyThreadState_DeleteCurrent();
  
  return NULL;
}

int main(int argc, char** argv)
{
  int            stat;
  pthread_t      thread;
  pthread_attr_t attr;
  PyThreadState *mainThreadState = NULL;

  Py_InitializeEx(0);
  
  // Uncomment to avoid stack-overflow error
  //PyRun_SimpleString(""try: import numpy\nexcept ImportError: pass\n"");
  
  mainThreadState = PyEval_SaveThread();

  pthread_attr_init(&attr);
  stat = pthread_create(&thread, &attr, thread_function, mainThreadState);
  if (stat != 0) {
    printf("" Threading ERROR: %d (pthread_create)\n"", stat);
    return 1;
  }
  stat = pthread_join(thread, NULL);
  if (stat != 0) {
    printf("" Threading ERROR: %d (pthread_join)\n"", stat);
    return 1;
  }

  PyEval_RestoreThread(mainThreadState);
  Py_Finalize();

  return 0;
}
```


### Error message:

```shell
Thread import numpy
AddressSanitizer:DEADLYSIGNAL
=================================================================
==83128==ERROR: AddressSanitizer: stack-overflow on address 0x700006499620 (pc 0x00010a42bf18 bp 0x70000651dc40 sp 0x700006499600 T1)
    #0 0x10a42bf18 in dgetrf_parallel+0x18 (libopenblas64_.0.dylib:x86_64+0x335f18)
    #1 0x10a11bf0b in dgesv_64_+0x19b (libopenblas64_.0.dylib:x86_64+0x25f0b)
    #2 0x105d9fd8e in DOUBLE_inv+0x3ae (_umath_linalg.cpython-39-darwin.so:x86_64+0x9d8e)
    #3 0x105783c0c in generic_wrapped_legacy_loop+0x1c (_multiarray_umath.cpython-39-darwin.so:x86_64+0x33dc0c)
    #4 0x10578aeb6 in ufunc_generic_fastcall+0x50e6 (_multiarray_umath.cpython-39-darwin.so:x86_64+0x344eb6)
    #5 0x10349a0fa in _PyObject_VectorcallTstate abstract.h:118
    #6 0x103496a89 in PyObject_Vectorcall abstract.h:127
    #7 0x103496bc7 in call_function ceval.c:5077
    #8 0x103491ab6 in _PyEval_EvalFrameDefault ceval.c:3537
    #9 0x1032e92ee in _PyEval_EvalFrame pycore_ceval.h:40
    #10 0x1032e74b8 in function_code_fastcall call.c:330
    #11 0x1032e6ed4 in _PyFunction_Vectorcall call.c:367
    #12 0x1032e68b7 in PyVectorcall_Call call.c:231
    #13 0x1032e6ae8 in _PyObject_Call call.c:266
    #14 0x1032e6be1 in PyObject_Call call.c:293
    #15 0x1054c78db in array_implement_array_function+0xdb (_multiarray_umath.cpython-39-darwin.so:x86_64+0x818db)
    #16 0x10335cacc in cfunction_call methodobject.c:552
    #17 0x1032e5f38 in _PyObject_MakeTpCall call.c:191
    #18 0x10349a0db in _PyObject_VectorcallTstate abstract.h:116
    #19 0x103496a89 in PyObject_Vectorcall abstract.h:127
    #20 0x103496bc7 in call_function ceval.c:5077
    #21 0x1034917aa in _PyEval_EvalFrameDefault ceval.c:3520
    #22 0x1034824ee in _PyEval_EvalFrame pycore_ceval.h:40
    #23 0x103498175 in _PyEval_EvalCode ceval.c:4329
    #24 0x1032e7333 in _PyFunction_Vectorcall call.c:396
    #25 0x10349a0fa in _PyObject_VectorcallTstate abstract.h:118
    #26 0x103496a89 in PyObject_Vectorcall abstract.h:127
    #27 0x103496bc7 in call_function ceval.c:5077
    #28 0x1034917aa in _PyEval_EvalFrameDefault ceval.c:3520
    #29 0x1034824ee in _PyEval_EvalFrame pycore_ceval.h:40
    #30 0x103498175 in _PyEval_EvalCode ceval.c:4329
    #31 0x1032e7333 in _PyFunction_Vectorcall call.c:396
    #32 0x1032e6970 in PyVectorcall_Call call.c:243
    #33 0x1032e6ae8 in _PyObject_Call call.c:266
    #34 0x1032e6be1 in PyObject_Call call.c:293
    #35 0x1054c78db in array_implement_array_function+0xdb (_multiarray_umath.cpython-39-darwin.so:x86_64+0x818db)
    #36 0x10335cacc in cfunction_call methodobject.c:552
    #37 0x1032e5f38 in _PyObject_MakeTpCall call.c:191
    #38 0x10349a0db in _PyObject_VectorcallTstate abstract.h:116
    #39 0x103496a89 in PyObject_Vectorcall abstract.h:127
    #40 0x103496bc7 in call_function ceval.c:5077
    #41 0x1034917aa in _PyEval_EvalFrameDefault ceval.c:3520
    #42 0x1034824ee in _PyEval_EvalFrame pycore_ceval.h:40
    #43 0x103498175 in _PyEval_EvalCode ceval.c:4329
    #44 0x1032e7333 in _PyFunction_Vectorcall call.c:396
    #45 0x10349a0fa in _PyObject_VectorcallTstate abstract.h:118
    #46 0x103496a89 in PyObject_Vectorcall abstract.h:127
    #47 0x103496bc7 in call_function ceval.c:5077
    #48 0x103491ab6 in _PyEval_EvalFrameDefault ceval.c:3537
    #49 0x1032e92ee in _PyEval_EvalFrame pycore_ceval.h:40
    #50 0x1032e74b8 in function_code_fastcall call.c:330
    #51 0x1032e6ed4 in _PyFunction_Vectorcall call.c:367
    #52 0x10349a0fa in _PyObject_VectorcallTstate abstract.h:118
    #53 0x103496a89 in PyObject_Vectorcall abstract.h:127
    #54 0x103496bc7 in call_function ceval.c:5077
    #55 0x1034917aa in _PyEval_EvalFrameDefault ceval.c:3520
    #56 0x1034824ee in _PyEval_EvalFrame pycore_ceval.h:40
    #57 0x103498175 in _PyEval_EvalCode ceval.c:4329
    #58 0x103498c65 in _PyEval_EvalCodeWithName ceval.c:4361
    #59 0x10348247a in PyEval_EvalCodeEx ceval.c:4377
    #60 0x103482369 in PyEval_EvalCode ceval.c:828
    #61 0x10347ecbb in builtin_exec_impl bltinmodule.c:1026
    #62 0x10347bbcb in builtin_exec bltinmodule.c.h:396
    #63 0x10335bdb5 in cfunction_vectorcall_FASTCALL methodobject.c:430
    #64 0x1032e68b7 in PyVectorcall_Call call.c:231
    #65 0x1032e6ae8 in _PyObject_Call call.c:266
    #66 0x1032e6be1 in PyObject_Call call.c:293
    #67 0x103496f0c in do_call_core ceval.c:5097
    #68 0x103491f8c in _PyEval_EvalFrameDefault ceval.c:3582
    #69 0x1034824ee in _PyEval_EvalFrame pycore_ceval.h:40
    #70 0x103498175 in _PyEval_EvalCode ceval.c:4329
    #71 0x1032e7333 in _PyFunction_Vectorcall call.c:396
    #72 0x10349a0fa in _PyObject_VectorcallTstate abstract.h:118
    #73 0x103496a89 in PyObject_Vectorcall abstract.h:127
    #74 0x103496bc7 in call_function ceval.c:5077
    #75 0x103491566 in _PyEval_EvalFrameDefault ceval.c:3489
    #76 0x1032e92ee in _PyEval_EvalFrame pycore_ceval.h:40
    #77 0x1032e74b8 in function_code_fastcall call.c:330
    #78 0x1032e6ed4 in _PyFunction_Vectorcall call.c:367
    #79 0x10349a0fa in _PyObject_VectorcallTstate abstract.h:118
    #80 0x103496a89 in PyObject_Vectorcall abstract.h:127
    #81 0x103496bc7 in call_function ceval.c:5077
    #82 0x1034915e5 in _PyEval_EvalFrameDefault ceval.c:3506
    #83 0x1032e92ee in _PyEval_EvalFrame pycore_ceval.h:40
    #84 0x1032e74b8 in function_code_fastcall call.c:330
    #85 0x1032e6ed4 in _PyFunction_Vectorcall call.c:367
    #86 0x10349a0fa in _PyObject_VectorcallTstate abstract.h:118
    #87 0x103496a89 in PyObject_Vectorcall abstract.h:127
    #88 0x103496bc7 in call_function ceval.c:5077
    #89 0x1034917aa in _PyEval_EvalFrameDefault ceval.c:3520
    #90 0x1032e92ee in _PyEval_EvalFrame pycore_ceval.h:40
    #91 0x1032e74b8 in function_code_fastcall call.c:330
    #92 0x1032e6ed4 in _PyFunction_Vectorcall call.c:367
    #93 0x10349a0fa in _PyObject_VectorcallTstate abstract.h:118
    #94 0x103496a89 in PyObject_Vectorcall abstract.h:127
    #95 0x103496bc7 in call_function ceval.c:5077
    #96 0x1034917aa in _PyEval_EvalFrameDefault ceval.c:3520
    #97 0x1032e92ee in _PyEval_EvalFrame pycore_ceval.h:40
    #98 0x1032e74b8 in function_code_fastcall call.c:330
    #99 0x1032e6ed4 in _PyFunction_Vectorcall call.c:367
    #100 0x1032e8aba in _PyObject_VectorcallTstate abstract.h:118
    #101 0x1032e8f00 in object_vacall call.c:792
    #102 0x1032e90dc in _PyObject_CallMethodIdObjArgs call.c:883
    #103 0x1034d9f8d in import_find_and_load import.c:1776
    #104 0x1034d912e in PyImport_ImportModuleLevelObject import.c:1877
    #105 0x103495a8f in import_name ceval.c:5198
    #106 0x10348eb10 in _PyEval_EvalFrameDefault ceval.c:3099
    #107 0x1034824ee in _PyEval_EvalFrame pycore_ceval.h:40
    #108 0x103498175 in _PyEval_EvalCode ceval.c:4329
    #109 0x103498c65 in _PyEval_EvalCodeWithName ceval.c:4361
    #110 0x10348247a in PyEval_EvalCodeEx ceval.c:4377
    #111 0x103482369 in PyEval_EvalCode ceval.c:828
    #112 0x1035058f4 in run_eval_code_obj pythonrun.c:1221
    #113 0x103503c5b in run_mod pythonrun.c:1242
    #114 0x103502f28 in PyRun_StringFlags pythonrun.c:1108
    #115 0x103502ded in PyRun_SimpleStringFlags pythonrun.c:497
    #116 0x102b6195b in thread_function test.c:13
    #117 0x7ff8016284e0 in _pthread_start+0x7c (libsystem_pthread.dylib:x86_64+0x64e0)
    #118 0x7ff801623f6a in thread_start+0xe (libsystem_pthread.dylib:x86_64+0x1f6a)

SUMMARY: AddressSanitizer: stack-overflow (libopenblas64_.0.dylib:x86_64+0x335f18) in dgetrf_parallel+0x18
Thread T1 created by T0 here:
    #0 0x10386f67c in wrap_pthread_create+0x5c (libclang_rt.asan_osx_dynamic.dylib:x86_64h+0x4267c)
    #1 0x102b61aba in main test.c:37
    #2 0x10610b51d in start+0x1cd (dyld:x86_64+0x551d)

==83128==ABORTING
Abort trap: 6
```


### NumPy/Python version information:

1.22.4 3.9.13 (main, Jun 14 2022, 22:13:49) ",2022-06-19 23:06:32,,BUG: stack-overflow in OpenBLAS64 on macOS with embedding and importing numpy with pthreads,['00 - Bug']
21791,open,MichaelCurrie,"### Describe the issue:

numpy.concatenate casts a list of integers to floats if concatenated with the empty list

### Reproduce the code example:

```python
>>> import numpy as np
>>> np.__version__
'1.20.3'
>>> np.concatenate([[1,2],[1]])   # Good result
array([1, 2, 1])
>>> np.concatenate([[1,2],[]])    # Bad result (should not cast to float!)
array([1., 2.])
```


### Error message:

_No response_

### NumPy/Python version information:

1.20.3 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]",2022-06-17 20:44:37,,BUG: numpy.concatenate casts a list of integers to floats if concatenated with the empty list (it shouldn't do this),['00 - Bug']
21787,open,randolf-scholz,"### Proposed new feature or change:

Not sure why this is not a thing already, but for example 

```python
np.einsum(""i -> ii"", np.arange(5))
```

raises `ValueError: einstein sum subscripts string includes output subscript 'i' multiple times`

But that is unnecessary. There is only a single logical option for what the output should be: the same as `np.diag(np.arange(5))`. This translates to arbitrarily complicated cases.

this feature would be nice because it would allow to very easily perform diagonalization operations that are otherwise hard to express, for example:

- `np.einsum(""...i -> ...ii"", x)` would diagonalize a batch
- `np.einsum(""...i -> ...iii"", x)` would perform a higher order diagonalization on a batch
- `np.einsum(""...i, ji -> ...jj"", x, A)` would perform a fused matrix multiplication and diagonalization
- `np.einsum(""...chw -> ...cchw"", x)` would diagonalize the channels `c` of a (generic) batch of `h×w` images.

-----

Implementation-wise, if an index is located multiple times in the output, then I suppose one first performs a shape inference for the output, creates a zero-matrix of that shape and then fill the respective entries.





",2022-06-17 09:19:45,,ENH: `einsum` allow subscript multiple times in the output,['unlabeled']
21780,open,axil,"### Proposed new feature or change:

The `str()` method of Polynomials uses Unicode symbols like ², ³ for conciseness.

Currently it falls back to ascii on Windows platform because of poor Unicode support on Windows.

It is worth mentioning that on Windows 10 console does support Unicode:

```
>>> p = poly.Polynomial([1/7, 1/3,1/2, 1])
>>> str(p)
'0.14285714 + 0.33333333*x + 0.5*x**2 + 1.0*x**3'
>>> p._use_unicode = True
>>> str(p)
'0.14285714 + 0.33333333·x + 0.5·x² + 1.0·x³'
```

But for backwards compatibility it might be a good idea to keep ascii by default.

What I suggest is to use Unicode on Windows inside Jupyter notebooks for they definitely support Unicode since the initial release.",2022-06-16 18:01:38,,ENH: Allow Unicode characters in polynomials in Jupyter notebooks on Windows,['unlabeled']
21767,open,lucianopaz,"### Describe the issue:

This might be more of a question than a defect itself, and I originally posted it on the scipy repo (scipy/scipy#16412). Over at pymc, we use some of scipy's linalg methods through the aesara package. To parallelize some tasks, we serialize a compiled function to send it to several worker processes. We realized that some of the compiled functions had references to scipy.linalg.lapack methods, which are Fortran objects and these cannot be pickled (or cloudpickled since we are using the latter for serialization).

I'm not sure if this is intended or not, but since the lapack functions are at the top level of the lapack module, it would be great if these could also be safely pickled.

### Reproduce the code example:

```python
from scipy.linalg import lapack
import cloudpickle


cloudpickle.dumps(lapack.cpotrf)
```


### Error message:

```shell
TypeError                                 Traceback (most recent call last)
Input In [39], in <cell line: 5>()
      1 from scipy.linalg import lapack
      2 import cloudpickle
----> 5 cloudpickle.dumps(lapack.cpotrf)

File ~/anaconda3/envs/test_env/lib/python3.10/site-packages/cloudpickle/cloudpickle_fast.py:73, in dumps(obj, protocol, buffer_callback)
     69 with io.BytesIO() as file:
     70     cp = CloudPickler(
     71         file, protocol=protocol, buffer_callback=buffer_callback
     72     )
---> 73     cp.dump(obj)
     74     return file.getvalue()

File ~/anaconda3/envs/test_env/lib/python3.10/site-packages/cloudpickle/cloudpickle_fast.py:633, in CloudPickler.dump(self, obj)
    631 def dump(self, obj):
    632     try:
--> 633         return Pickler.dump(self, obj)
    634     except RuntimeError as e:
    635         if ""recursion"" in e.args[0]:

TypeError: cannot pickle 'fortran' object
```


### NumPy/Python version information:

1.20.3 sys.version_info(major=3, minor=9, micro=7, releaselevel='final', serial=0)",2022-06-15 14:39:11,,BUG: Cannot pickle Fortran objects ,"['00 - Bug', 'component: numpy.f2py']"
21758,open,bsipocz,"### Proposed new feature or change:

A few downstream libraries experience test regression with the numpy 1.23.0rc3 release. Some of these packages (e.g. astropy) have a CI job using the ""nightly"" wheel, but didn't notice the issue as it's not yet updated to have PR included that triggers the failures with the rc3. A library one layer down noticed this though as it has a cronjob using RC releases of dependencies.

So it was a bit of a puzzle to figure out why we don't see the issue with the default branch (==nightly), but see it with the RC, maybe because something didn't get backported, or warnings behave differently, but now we converge on the fact that the wheel was build earlier than the RC tag.

So, the proposed policy change would be to add a step to the release process to trigger the nightly wheel builds, even if it's not yet due from the cron.

",2022-06-14 19:13:28,,ENH: policy change: trigger nightly wheel builds along with tagged releases,['14 - Release']
21746,open,steff456,"### Describe the issue:

Writing the spec for complex number support in the Array API we found that NumPy currently fails in 4 special cases. The behavior is inconsistent with `np.exp(z)-1` and C99 as shown in the code example.

The full list of special cases and specification is detailed in https://github.com/data-apis/array-api/pull/452

cc @kgryte

### Reproduce the code example:

```python
import numpy as np

def is_equal(x, y):
    """"""Test whether two complex numbers are equal with special consideration for NaNs.

    Parameters
    ----------
    x : complex
        First input number.
    y : complex
        Second input number.

    Returns
    -------
    bool
        Boolean indicating whether two complex numbers are equal.

    Examples
    --------
    >>> import numpy as np
    >>> is_equal(complex(np.nan, np.nan), complex(np.nan, np.nan))
    True
    """"""
    re1 = y.real
    im1 = y.imag

    re2 = x.real
    im2 = x.imag
    if re1 == re1:
        if im1 == im1:
            # Second value has non-NaN real and imaginary components, so test for component equality:
            return (re1 == re2) and (im1 == im2)
        
        if im2 == im2:
            # Second value has a NaN imaginary component, but first value does not:
            return False
        
        # Both values have NaN imaginary components:
        return True

    if im1 == im1:
        # Second value has a NaN real component, but a non-NaN imaginary component...
        if re2 == re2:
            # First value has a non-NaN real component:
            return False
        
        # Both values have NaN real components, so test for imaginary component equality:
        return (im1 == im2)

    if re2 == re2 or im2 == im2:
        # Second value has real and imaginary components which are NaN, but first value does not:
        return False

    # Both values have real and imaginary components which are NaN:
    return True

def compare(v, e):
    actual = np.expm1(v) 
    print('Value: {value}'.format(value=str(v)))
    print('Actual: {actual}'.format(actual=str(actual)))
    print('Expected: {expected}'.format(expected=str(e)))
    print('Equal: {is_equal}'.format(is_equal=str(is_equal(actual, e))))
    print('\n')

# Case 1
v = complex(np.inf, 0.0)
e = complex(np.inf, 0.0)
compare(v, e) # np.exmp1 returns (inf+nanj), vs np.exp(complex(np.inf, 0.0))-1.0 == (inf+0j)

# Case 2
v = complex(-np.inf, np.inf)
e = complex(-1.0, 0.0)
compare(v, e) # np.exmp1 returns (nan+nanj), vs np.exp(complex(-np.inf, np.inf))-1.0 == (-1+0j)

# Case 3
v = complex(-np.inf, np.nan)
e = complex(-1.0, 0.0)
compare(v, e) # np.exmp1 returns (nan+nanj), vs np.exp(complex(-np.inf, np.nan))-1.0 == (-1+0j)

# Case 4
v = complex(np.nan, 0.0)
e = complex(np.nan, 0.0)
compare(v, e) # np.exmp1 returns (nan+nanj), vs np.exp(complex(np.nan, 0.0))-1.0 == (nan+0j)
```


### Error message:

```shell
Value: (inf+0j)
Actual: (inf+nanj)
Expected: (inf+0j)
Equal: False

Value: (-inf+infj)
Actual: (nan+nanj)
Expected: (-1+0j)
Equal: False

Value: (-inf+nanj)
Actual: (nan+nanj)
Expected: (-1+0j)
Equal: False

Value: (nan+0j)
Actual: (nan+nanj)
Expected: (nan+0j)
Equal: False
```


### NumPy/Python version information:

v1.22.4",2022-06-13 21:43:34,,BUG: np.expm1 returns unexpected values in special cases with complex numbers,['00 - Bug']
21744,open,cantonios,"### Describe the issue:

The [docs](https://numpy.org/doc/stable/user/basics.indexing.html#assigning-values-to-indexed-arrays) for slice assignment only say

> The value being assigned to the indexed array must be shape consistent (the same shape or broadcastable to the shape the index produces).

But the following works:
```
x = np.zeros([3, 3])
y = np.ones([1, 1, 1, 1, 1, 1, 1, 1, 3])
x[:, :] = y  # Works, despite squeezing to smaller number of dimensions
```
In this case, `y` is neither the same nor ""broadcastable to"" the target shape, unless ""broadcastable to"" means something special in this case that doesn't seem to be defined in the docs.  I doubt this though, because
```
z = np.broadcast_to(y, x.shape)  # Fails
```
fails, so it would seem `y` is in fact _not_ ""broadcastable to"" the shape of `x`.

Is this intended behavior?  Is it a bug, feature, or documentation issue?

This is also related to #11309 in which a slice assignment fails silently when assigning to a 0-D slice.  The dimension mismatch should probably trigger *something*, since this seems to fail the ""broadcastable to"" precondition.

### Reproduce the code example:

```python
import numpy as np

x = np.zeros([3, 3])
y = np.ones([1, 1, 1, 1, 1, 1, 1, 1, 3])

x[:, :] = y                        # Works, despite squeezing to smaller dimensions
z = np.broadcast_to(y, x.shape)    # Fails
```


### Error message:

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-3-b6a49c116e53> in <module>()
      8 
      9 x[:, :] = y                        # Works, despite squeezing to smaller dimensions
---> 10 z = np.broadcast_to(y, x.shape)    # Fails
     11 
     12 print(numpy.__version__, sys.version)

<__array_function__ internals> in broadcast_to(*args, **kwargs)

1 frames
/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py in broadcast_to(array, shape, subok)
    409            [1, 2, 3]])
    410     """"""
--> 411     return _broadcast_to(array, shape, subok=subok, readonly=True)
    412 
    413 

/usr/local/lib/python3.7/dist-packages/numpy/lib/stride_tricks.py in _broadcast_to(array, shape, subok, readonly)
    348     it = np.nditer(
    349         (array,), flags=['multi_index', 'refs_ok', 'zerosize_ok'] + extras,
--> 350         op_flags=['readonly'], itershape=shape, order='C')
    351     with it:
    352         # never really has writebackifcopy semantics

ValueError: input operand has more dimensions than allowed by the axis remapping
```


### NumPy/Python version information:

1.21.6 3.7.13 (default, Apr 24 2022, 01:04:09) 
[GCC 7.5.0]",2022-06-13 20:31:36,,"BUG: (or DOC:?) Slice assignment allows broadcasting to fewer dimensions, broadcast_to doesn't.",['00 - Bug']
21737,open,headtr1ck,"### Proposed new feature or change:

It would be nice to have static typing support for custom array containers like pandas Series, xarrays DataArray (https://github.com/pydata/xarray/issues/6524) etc.
Such that you can do:
```python
import pandas as pd
import numpy as np

x = pd.Series([1, 2, 3])
y = np.exp(x)
reveal_type(y)  # Should be pd.Series but is np.ndarray
```

I assume this should be possible using Protocols and using something like this:
```python
from typing import Protocol, TypeVar

class HasArrayUFunc(Protocol):
    def __array_ufunc__(ufunc, method, *inputs, **kwargs):
        pass

ArrayOrHasArrayUFunc = TypeVar(""ArrayOrHasArrayUFunc"", ndarray, HasArrayUFunc)

def exp(x: ArrayOrHasArrayUFunc) -> ArrayOrHasArrayUFunc: ...
```",2022-06-12 18:37:25,,ENH: Static typing support for custom array containers,"['01 - Enhancement', 'Static typing']"
21730,open,lorenz-gorini,"### Describe the issue:

When I call `numpy.moveaxis` function and I use a `torch.Tensor` argument, the wrong message is shown.

**Expected behaviour**:  
Usually torch.Tensor can also be used as array, so I would expect a more explanatory description of the issue. 

**Workaround**:
Transform `torch.Tensor` to `numpy.ndarray` before passing it as argument:
```python
import numpy as np
import torch

np.moveaxis(torch.tensor([[3,2],[3,4]]).numpy(), 0, -1)
```

PyTorch version: `1.10.0`
OS: Ubuntu 20.04


### Reproduce the code example:

```python
import numpy as np
import torch

np.moveaxis(torch.tensor([[3,2],[3,4]]), 0, -1)
```


### Error message:

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Input In [1], in <cell line: 4>()
      1 import numpy as np
      2 import torch
----> 4 np.moveaxis(torch.tensor([[3,2],[3,4]]), 0, -1)

File <__array_function__ internals>:180, in moveaxis(*args, **kwargs)

File /opt/conda/lib/python3.8/site-packages/numpy/core/numeric.py:1471, in moveaxis(a, source, destination)
   1468 for dest, src in sorted(zip(destination, source)):
   1469     order.insert(dest, src)
-> 1471 result = transpose(order)
   1472 return result

TypeError: transpose() received an invalid combination of arguments - got (list), but expected one of:
 * (int dim0, int dim1)
 * (name dim0, name dim1)
```


### NumPy/Python version information:

1.22.3 3.8.10 | packaged by conda-forge | (default, May 11 2021, 07:01:05) 
[GCC 9.3.0]",2022-06-10 22:09:06,,BUG: Wrong message when calling `moveaxis` with `torch.Tensor` argument,['00 - Bug']
21725,open,mashatarasevich,"### Describe the issue:

When doing _MaskedBinaryOperation on two masked arrays the actual operation is done on underlying `data` arrays ignoring mask. The relevant code is https://github.com/numpy/numpy/blob/main/numpy/ma/core.py#L1008-L1013
```python
        # Get the data, as ndarray
        (da, db) = (getdata(a), getdata(b))
        # Get the result
        with np.errstate():
            np.seterr(divide='ignore', invalid='ignore')
            result = self.f(da, db, *args, **kwargs)
```
Division by zero and invalid operations are ignored for this operation, **but not over- or underflow**. Also this code silently swallows errors occurred for nonmasked elements.

### Reproduce the code example:

```python
import numpy as np

np.seterr(all='raise')

x = np.array([[1, 1e20], [1e20, 2]], dtype=np.float32)
y = np.array([[3, 1e20], [1e20, 4]], dtype=np.float32)

undef = np.float32(1e20)

z = np.ma.masked_equal(x, undef)
w = np.ma.masked_equal(y, undef)

z*w
```


### Error message:

```shell
Traceback (most recent call last):
  File ""test.py"", line 13, in <module>
    z*w
  File ""<snip>/miniconda3/lib/python3.7/site-packages/numpy/ma/core.py"", line 4169, in __mul__
    return multiply(self, other)
  File ""<snip>/miniconda3/lib/python3.7/site-packages/numpy/ma/core.py"", line 1021, in __call__
    result = self.f(da, db, *args, **kwargs)
FloatingPointError: overflow encountered in multiply
```


### NumPy/Python version information:

1.19.2 3.7.5 (default, Oct 25 2019, 15:51:11) 
[GCC 7.3.0]",2022-06-10 17:10:30,,BUG: overflow encountered in multiply when multiplying masked arrays of float32,"['00 - Bug', 'component: numpy.ma']"
21713,open,stuartarchibald,"### Proposed new feature or change:

This is about Numba's interaction with NumPy as part of its software dependency chain.

1. In approximately the last six months a number of CVEs have been issued against NumPy (this issue is invariant of the contents of the CVEs!).
2. Anecdotally, based on issues reported by Numba users, ensuring software dependency chains are free of some level of CVE is becoming more common practice. Should software not have a compliant dependency chain, it is banned from use.
3. Fixes for CVEs are made against the ""latest"" NumPy release (not sure how accurate this is, there's discussion later).

The effect of the above is that if a CVE is issued against some version of NumPy, downstream packages, like Numba:
* that are specified to depend on the NumPy version(s) with a CVE present

and
* for which there is no upgrade path available to a NumPy (probably the ""latest"") with the security fix

are also then considered ""bad"" and their use is prohibited (or similar) by dependency chain validation applications.

Admittedly the specific situation with Numba is perhaps a bit unusual as a lot of packages depending on NumPy have no upper restriction on the NumPy version used. As a result, for these packages, there's typically an upgrade path available to the latest NumPy version which most likely contains the security fix.

However, Numba specifies compatibility against the versions of NumPy it has dedicated support for and that it specifically tests, essentially it has a bounded NumPy version support limit. This is because Numba takes replicating NumPy seriously and involved testing is a large part of that. It is also very rare for Numba to need a trivial ""bump to the next NumPy version"" style patch to accommodate a new NumPy release, updating to a new version almost always involves changing algorithms or updating APIs. Whilst not directly the same, it's possible to envisage that there are other projects/entities with software stacks that face similar issues where by e.g. every declared supported version of NumPy needs testing before acceptance to production, for example a HPC centre.

Practically, there have been two cases in the last six months where Numba has had to do out-of-typical-schedule changes/releases to support the newer versions of NumPy to accommodate the situation described above.

Case 1:

Numba 0.55.0 was shipped prior to RC testing being complete and with some minor known issues in the code base so as to offer an upgrade path to NumPy 1.21 (this was against CVE-2021-33430).
https://github.com/numba/numba/blob/65cbe1cab01cc186ce873a65d94b5096acbf07f3/CHANGE_LOG#L39-L55

Case 2:

A Numba 0.55.2 had to be created out of cycle to offer an upgrade path to NumPy 1.22 due to the CVEs noted in https://github.com/numba/numba/issues/8025, it took a couple of weeks to create the support patch and then back-port and test it.

The question...

NumPy back-ports fixes for regressions and has the very useful NEP-29 in relation to support schedules. Does NumPy have or plan to have a similar back-port policy/schedule for security fixes that address CVEs?

The benefits of having such a policy that guaranteed extending security fixes back a couple of releases, or to a dedicated long term support release, would be that it would be a lot easier for downstream projects to ensure their deliberately constrained reliance on specific NumPy versions isn't inadvertently triggering software supply/dependency chain security problems.

Many thanks in advance for considering this!

CC @rgommers @seibert ",2022-06-10 09:26:19,,ENH: Extend/enhance NEP-29 to cover security fix back-ports.,"['14 - Release', 'component: NEP']"
21682,open,seberg,"I am currently running valgrind tests for the upcoming release (1.23.x branch), and this came up in the logs:
```
**265729** **********************************************************************
**265729** build/testenv/lib/python3.9/site-packages/numpy/f2py/tests/test_crackfortran.py::TestDimSpec::test_inv_array_size[2:n]
**265729** **********************************************************************
==265729== Conditional jump or move depends on uninitialised value(s)
==265729==    at 0xC79A1D4: get_inv_arr_size_2_ (tmpr5n4tml4.f90:48)
==265729==    by 0xC794DE9: f2py_rout__test_ext_module_5417_get_inv_arr_size_2 (_test_ext_module_5417module.c:778)
==265729==    by 0x52241A: _PyObject_MakeTpCall (call.c:191)
==265729==    by 0x51C124: UnknownInlinedFun (abstract.h:116)
==265729==    by 0x51C124: UnknownInlinedFun (abstract.h:103)
==265729==    by 0x51C124: UnknownInlinedFun (abstract.h:127)
==265729==    by 0x51C124: UnknownInlinedFun (ceval.c:5077)
==265729==    by 0x51C124: _PyEval_EvalFrameDefault (ceval.c:3520)
==265729==    by 0x5158A0: UnknownInlinedFun (pycore_ceval.h:40)
==265729==    by 0x5158A0: _PyEval_EvalCode (ceval.c:4329)
==265729==    by 0x52D242: _PyFunction_Vectorcall (call.c:396)
==265729==    by 0x53DFEB: UnknownInlinedFun (abstract.h:118)
==265729==    by 0x53DFEB: method_vectorcall (classobject.c:53)
==265729==    by 0x53E7C1: UnknownInlinedFun (call.c:243)
==265729==    by 0x53E7C1: UnknownInlinedFun (call.c:266)
==265729==    by 0x53E7C1: PyObject_Call (call.c:293)
==265729==    by 0x518FE8: UnknownInlinedFun (ceval.c:5125)
==265729==    by 0x518FE8: _PyEval_EvalFrameDefault (ceval.c:3582)
==265729==    by 0x5158A0: UnknownInlinedFun (pycore_ceval.h:40)
==265729==    by 0x5158A0: _PyEval_EvalCode (ceval.c:4329)
==265729==    by 0x52D242: _PyFunction_Vectorcall (call.c:396)
==265729==    by 0x518FE8: UnknownInlinedFun (ceval.c:5125)
==265729==    by 0x518FE8: _PyEval_EvalFrameDefault (ceval.c:3582)
==265729== 
==265729== LEAK SUMMARY:
==265729==    definitely lost: 3,040 (+0) bytes in 26 (+0) blocks
==265729==    indirectly lost: 808 (+0) bytes in 14 (+0) blocks
==265729==      possibly lost: 123,237,535 (+1,848) bytes in 542,404 (+12) blocks
==265729==    still reachable: 230,792,652 (+3,427) bytes in 1,710,071 (+41) blocks
==265729==                       of which reachable via heuristic:
==265729==                         length64           : 3,792 (+0) bytes in 237 (+0) blocks
==265729==                         newarray           : 28,433 (-262) bytes in 999 (-4) blocks
==265729==         suppressed: 0 (+0) bytes in 0 (+0) blocks
==265729== Reachable blocks (those to which a pointer was found) are not shown.
==265729== To see them, rerun with: --leak-check=full --show-leak-kinds=all
==265729== 
**265729** 
**265729** **********************************************************************
**265729** 
**265729** **********************************************************************
**265729** build/testenv/lib/python3.9/site-packages/numpy/f2py/tests/test_crackfortran.py::TestDimSpec::test_inv_array_size[n/2]
**265729** **********************************************************************
==265729== Conditional jump or move depends on uninitialised value(s)
==265729==    at 0xC79A2C4: get_inv_arr_size_3_ (tmpr5n4tml4.f90:65)
==265729==    by 0xC79537D: f2py_rout__test_ext_module_5417_get_inv_arr_size_3 (_test_ext_module_5417module.c:962)
==265729==    by 0x52241A: _PyObject_MakeTpCall (call.c:191)
==265729==    by 0x51C124: UnknownInlinedFun (abstract.h:116)
==265729==    by 0x51C124: UnknownInlinedFun (abstract.h:103)
==265729==    by 0x51C124: UnknownInlinedFun (abstract.h:127)
==265729==    by 0x51C124: UnknownInlinedFun (ceval.c:5077)
==265729==    by 0x51C124: _PyEval_EvalFrameDefault (ceval.c:3520)
==265729==    by 0x5158A0: UnknownInlinedFun (pycore_ceval.h:40)
```
Ping @HaoZeke maybe you can have a look?  Not sure that there is much too it.  The test is parametrized, and only these two parametrizations seem to fail.",2022-06-06 20:10:24,,BUG: Possible uninitialized value issue in f2py,"['00 - Bug', 'component: numpy.f2py']"
21676,open,pijyoi,"### Describe the issue:

When the output type is wider than the input type, a TypeError is raised. (Expected no error)
Conversely, when the output type is narrower than the input type, no TypeError is raised. (Expected error)

### Reproduce the code example:

```python
import numpy as np 

def test(dtype_in, dtype_out):
    expected = np.can_cast(dtype_in, dtype_out)

    try:
        ain = np.arange(10, dtype=dtype_in)
        mask = ain < 5
        aout = np.empty(np.count_nonzero(mask), dtype=dtype_out)
        np.compress(mask, ain, out=aout)
    except TypeError:
        success = False
    else:
        success = True

    print(f'compress {dtype_in} to {dtype_out}; {expected=}; {success=}')

test('f8', 'i4')
test('i4', 'f8')
```


### Error message:

```shell
compress f8 to i4; expected=False; success=True
compress i4 to f8; expected=True; success=False
```


### NumPy/Python version information:

1.22.4 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]",2022-06-06 02:27:01,,BUG: np.compress casting fails from narrower to wider type,"['00 - Bug', 'component: numpy._core']"
21674,open,HaoZeke,"I've recently come across this issue while using f2py in numpy 1.20.3. As a minimum working example, I have the following simple fortran module which declares two allocatable arrays (`charge` is real type and `names` is character). The subroutine ""main"" will allocate them:

```
! -*- f90 -*-

      module inputs
      implicit none
              real*8, dimension(:,:), target, allocatable :: charge
              integer is_alloc
              character(len=10), allocatable, dimension(:) :: names
      end module inputs

      subroutine main(N)
      use inputs
      implicit none

      integer ii, jj, N
      
      IF (allocated(charge)) THEN
         deallocate(charge)
      end if
      allocate(charge(N, N))
      allocate(character(len=10) :: names(N)) !, source=""TEST"")

      END subroutine
```

I can build this by running `python setup.py build_ext --inplace` with the following setup.py file (having put the above code in test.f90):
```
from numpy.distutils.core import Extension
from numpy.distutils.core import setup
from numpy.distutils.misc_util import Configuration


F90_COMBINED = ""test.f90""

calc_ext = Extension(
    name=""alloc"",
    sources=[F90_COMBINED],
    include_dirs=['.'],
    libraries=['gsl', 'gslcblas'],
    extra_f90_compile_args=['-Wargument-mismatch'],
)

def configuration(parent_package='', top_path=None):

    config = Configuration('', parent_package, top_path,
                           ext_modules=[calc_ext])
    return config

if __name__ == ""__main__"":
    setup(configuration=configuration)
```

This builds a module called `alloc`. Here's a screencap of what happens if I try to access each allocated array.
![image](https://user-images.githubusercontent.com/12849618/145152930-4b1ad058-084e-4b14-80b9-1b0ca0eec626.png)


Any suggestions would be greatly appreciated!

_Originally posted by @aelanman in https://github.com/numpy/numpy/issues/10027#issuecomment-988511053_",2022-06-05 20:03:53,,BUG: Allocation of character arrays in F2PY,['component: numpy.f2py']
21672,open,HaoZeke,"Consider `1.2.2 Character Variables and Arrays ` of the `ANSI-77_FORTRAN_Information_Document` ([Internet Archive edition](https://archive.org/details/bitsavers_decvaxlang1ANSI77FORTRANInformationDocumentApr83_3081987/page/n11/mode/2up)) which mentions:

![image](https://user-images.githubusercontent.com/4336207/132260400-338fee6e-89cc-482e-8098-7a19f958e416.png)

This is on the basis of the fact that the form of a character type-statement was simpler.

![image](https://user-images.githubusercontent.com/4336207/132260431-cb9c56d0-499e-40fd-816e-083f40518ebc.png)

In later standards the array distinction becomes clearer. To sum things up then:

```fortran
GAMMA(10)*8
```

Is problematic since F77 seems to accept this as a `character` array with `length 8`; while F90 onwards this would have `length 10` and be undefined from my reading. Practically:

```fortran
program main
  implicit none
  character*5 :: fooi(5)
  character GAMMA(10)*8
  fooi = ""12345""
  GAMMA = ""1h2d5r6""
  print*, fooi
  print*, GAMMA
end program
```
`gfortran` interprets both of these the same for some reason; that is ,`GAMMA` is taken to be of length 8 and is an array of 10 (it gets repeated). This is actually clearly a bug since by the F2018 interpretation () is interpreted first as LEN.

_Originally posted by @HaoZeke in https://github.com/numpy/numpy/pull/19388#discussion_r703071742_



FWIW this is a very low prio issue and mostly just to make sure it doesn't get buried. For all practical use-cases the current implementation works splendidly.",2022-06-05 15:12:38,,MAINT: F2PY string handling for F77 and F90,['unlabeled']
21655,open,orenbenkiki,"# Proposed new feature or change:

Provide ``numpy.asorder(a: numpy.ndarray, order: str, inplace: bool = False)`` which either creates a copy with the specified order, or modifies the array order in-place to the specified order.

## Motivation

When working with *large* data, it is *vital* for performance that it be in the correct layout (row or column major order), as this makes a *huge* impact on the performance of per-row or per-column operations (as in, an order of magnitude difference when working on data that is ~1GB).

It is easy to re-layout a matrix, e.g.:

```
column_major = np.empty(row_major.shape, dtype=row_major.dtype, order=""F"")
column_major[:] = row_major[:]
```

(BTW: calling `row_major.reshape(row_major.shape, order=""F"")` returns a row major result - is this a bug?)

Either way, this forces us to make a copy of the data, though. This is a serious concern if the data is large. Even worse, this is **needlessly slow**. Consider this code:

```
from typing import List, Tuple
from collections import namedtuple
from concurrent.futures import ThreadPoolExecutor
from math import ceil, sqrt
from timeit import timeit

from sys import argv
from sys import stdout
import numpy as np

def naive_relayout_array2d(array2d: np.ndarray, order: str) -> np.ndarray:
    result = np.empty(array2d.shape, dtype=array2d.dtype, order=order)
    result[:] = array2d[:]
    return result


def fast_relayout_array2d(array2d: np.ndarray, order: str, *, max_workers: int, small_block_size: int, large_block_size: int) -> np.ndarray:
    assert 0 < small_block_size <= large_block_size
    assert array2d.ndim == 2
    assert order in ('C', 'F')

    large_block_elements = int(sqrt(large_block_size / array2d.dtype.itemsize))
    small_block_elements = int(sqrt(small_block_size / array2d.dtype.itemsize))

    large_row_elements = array2d.shape[0]
    large_column_elements = array2d.shape[1]

    large_row_blocks = int(ceil(large_row_elements / large_block_elements))
    large_column_blocks = int(ceil(large_column_elements / small_block_elements))

    result = np.empty(array2d.shape, dtype=array2d.dtype, order=order)

    def _relayout_large_block(large_block: int) -> None:
        large_column_block = large_block // large_row_blocks
        large_row_block = large_block % large_row_blocks

        large_start_row = int(round(large_row_block * large_row_elements / large_row_blocks))
        large_stop_row = int(round((large_row_block + 1) * large_row_elements / large_row_blocks))

        large_start_column = int(round(large_column_block * large_column_elements / large_column_blocks))
        large_stop_column = int(round((large_column_block + 1) * large_column_elements / large_column_blocks))

        small_row_elements = large_stop_row - large_start_row
        small_column_elements = large_stop_column - large_start_column

        small_row_blocks = int(ceil(small_row_elements / small_block_elements))
        small_column_blocks = int(ceil(small_column_elements / small_block_elements))

        small_blocks_count = small_row_blocks * small_column_blocks

        def _relayout_small_block(small_block: int) -> None:
            small_column_block = small_block // small_row_blocks
            small_row_block = small_block % small_row_blocks

            small_start_row = large_start_row + int(round(small_row_block * small_row_elements / small_row_blocks))
            small_stop_row = large_start_row + int(round((small_row_block + 1) * small_row_elements / small_row_blocks))

            small_start_column = large_start_column + int(round(small_column_block * small_column_elements / small_column_blocks))
            small_stop_column = large_start_column + int(round((small_column_block + 1) * small_column_elements / small_column_blocks))

            result[small_start_row:small_stop_row, small_start_column:small_stop_column] = \
                array2d[small_start_row:small_stop_row, small_start_column:small_stop_column]

        for small_block in range(small_blocks_count):
            _relayout_small_block(small_block)

    large_blocks_count = large_row_blocks * large_column_blocks
    max_workers = min(max_workers, large_blocks_count)

    if max_workers == 1:
        for large_block in range(large_blocks_count):
            _relayout_large_block(large_block)
    else:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            for _ in executor.map(_relayout_large_block, range(large_blocks_count)):
                pass

    return result

def time_size(shape: Tuple[int, int]) -> Tuple[float, float, float]:
    row_major = None
    def _rand() -> None:
        nonlocal row_major
        row_major = np.random.rand(*shape)

    stdout.write(""%d,%d"" % shape)
    stdout.write("",%.3f"" % timeit(_rand, number=1))
    stdout.flush()
    assert row_major.flags[""C_CONTIGUOUS""]

    naive_column_major = None
    def _naive() -> None:
        nonlocal naive_column_major
        naive_column_major = naive_relayout_array2d(row_major, ""F"")

    stdout.write("",%.3f"" % timeit(_naive, number=1))
    stdout.flush()
    assert naive_column_major.flags[""F_CONTIGUOUS""]

    fast_column_major = None
    for max_workers in range(1, 5):
        def _fast() -> None:
            nonlocal fast_column_major
            fast_column_major = \
                fast_relayout_array2d(row_major, ""F"", max_workers=max_workers,
                                      small_block_size=9*1024, large_block_size=1024*1024)
        stdout.write("",%.3f"" % timeit(_fast, number=1))
        stdout.flush()
        assert fast_column_major.flags[""F_CONTIGUOUS""]

    def _compare():
        assert np.allclose(naive_column_major, fast_column_major)
    stdout.write("",%.3f\n"" % timeit(_compare, number=1))
    stdout.flush()


print(""rows,columns,rand,naive,1thread,2threads,3threads,4threads,compare"")

assert len(argv) == 3
shape = (int(argv[1]), int(argv[2]))
np.random.seed(123456)
time_size(shape)
```

Which gives the following timing results (median of ~10 runs):
```
rows   columns  rand    naive    1thread  2threads  3threads  4threads  compare
1024   1024     0.007   0.009    0.006    0.011     0.012     0.012     0.010
2048   2048     0.028   0.298    0.028    0.041     0.060     0.055     0.033
4096   4096     0.113   2.085    0.948    0.553     0.394     0.321     0.120
8192   8192     0.447   11.939   5.423    07820     2.147     1.733     0.548
16384  16384    1.783   47.559   22.091   12.578    8.787     7.171     4.049
32768  32768    8.117   106.071  46.188   33.141    26.273    23.220    17.082
65536  65536    34.384  366.295  81.142   80.290    79.021    71.617    60.568
```

This demonstrates that:

* By splitting the large matrix into blocks, whose size is chosen to roughly fit in the L1 and the L2 caches, it is possible to speed re-layout operations to *significantly* speed things up.

* It is also possible to gain additional performance by using multiple threads, at the cost of the data being spread out amongst multiple caches, so accessing it again from a single thread will become a bit slower. There is a complex trade-off here with the number of threads, the size of the matrix, and the size of the L3 cache, which I did not fully investigate.

Naturally, doing the re-layout in-place would halve the cache pressure allowing for even more efficient operation.

#### Edit:

It may possible to do even better with simpler code if writing this in C, and using a cache-oblivious order on the elements (and using SSE/AVX registers to do the transpose of a very small number of data elements); this would be preferable, as it wouldn't be necessary to sniff the hardware cache sizes to set up the right sizes for the nested loops.",2022-06-03 06:55:52,,ENH: Improve performance for copying/relayout of large data,['unlabeled']
21638,open,EwoutH,"### Proposed new feature or change:

Add CPU feature detection for SVE2. On wide CPU cores the Scalable Vector Extension has the potential to increase performance manyfold compared to NEON.

SVE2 is supported on most Armv9 cores, including the Arm [Cortex-A510](https://www.arm.com/products/silicon-ip-cpu/cortex-a/cortex-a510), [Cortex-A710](https://www.arm.com/products/silicon-ip-cpu/cortex-a/cortex-a710), [Cortex-X2](https://developer.arm.com/documentation/101803/0200/Scalable-Vector-Extensions-support) and [Neoverse N2](https://www.arm.com/products/silicon-ip-cpu/neoverse/neoverse-n2) CPU designs. This means it's (to be) found in a huge amount of devices.

Arm documentation: https://developer.arm.com/Architectures/SVE

> ### [Introducing SVE2](https://developer.arm.com/documentation/102340/0001/Introducing-SVE2)
> This section introduces the Scalable Vector Extension version two (SVE2) of the Arm AArch64 architecture.
> 
> Following the development of the Neon architecture extension, which has a fixed 128-bit vector length for the instruction set, Arm designed the Scalable Vector Extension (SVE). SVE is a new Single Instruction Multiple Data (SIMD) instruction set that is used as an extension to AArch64, to allow for flexible vector length implementations. SVE improves the suitability of the architecture for High Performance Computing (HPC) applications, which require very large quantities of data processing.
> 
> SVE2 is a superset of SVE and Neon. SVE2 allows for more function domains in data-level parallelism. SVE2 inherits the concept, vector registers, and operation principles of SVE. SVE and SVE2 define 32 scalable vector registers. Silicon partners can choose a suitable vector length design implementation for hardware that varies between 128 bits and 2048 bits, at 128-bit increments. The advantage of SVE and SVE2 is that only one vector instruction set uses the scalable variables.

This enhancement might be similar to https://github.com/numpy/numpy/pull/20821 and https://github.com/numpy/numpy/pull/20552.",2022-05-30 18:40:22,,ENH: Add CPU feature detection for SVE2,"['01 - Enhancement', 'component: SIMD']"
21631,open,hhoppe,"### Describe the issue:

I am using `np.format_float_positional()` to output exactly 3 digits of precision.
See the code example below.
The output is:

    0.0000314 0.000314 0.00314 0.0314 0.314 3.14 31.4 314. 3140. 31400.
    0.0000310 0.00031 0.00310 0.0310 0.310 3.10 31.0 310. 3100. 31000.
    0.0000250 0.000250 0.00250 0.0250 0.25 2.50 25.0 250. 2500. 25000.

In the second line, `0.00031` should be `0.000310`.
And in the third line, `0.25` should be `0.250`.


### Reproduce the code example:

```python
import numpy as np

for base_value in [np.pi, 3.1, 2.5]:
  print(' '.join(np.format_float_positional(value, fractional=False,
                                            precision=3, unique=False)
                 for value in base_value * 10.0**np.arange(-5, 5)))
```


### Error message:

_No response_

### NumPy/Python version information:

1.21.6 3.8.10 (default, Mar 15 2022, 12:22:08) 
[GCC 9.4.0]
",2022-05-29 17:44:46,,BUG: format_float_positional() intermittently drops trailing zeros,['00 - Bug']
21603,open,oscargus,"### Describe the issue:

According to the documentation of `zeros_like` (and related functions), providing `dtype` will override the provided data type: https://numpy.org/doc/stable/reference/generated/numpy.zeros_like.html

Basically, my interpretation is that `zeros(a.shape, dtype=foo) == zeros_like(a, dtype=foo)`.

This does not seem to be the case for `Quantity` from `astropy` (and maybe other array libraries sharing some specific mechanism).

(Which caused some issue in Astropy, see https://github.com/astropy/astropy/issues/13276, from a change in Matplotlib, see https://github.com/matplotlib/matplotlib/pull/22929. But more importantly, `zeros_like` does not appear to work as stated in the documentation. Not sure if this is a numpy or astropy issue though, but it is the numpy documentation that seems wrong.)

### Reproduce the code example:

```python
import numpy as np
import astropy.units as u

a = [1, 2, 3]*u.m

z0 = np.zeros_like(a, dtype=bool)
z1 = np.zeros(a.shape, dtype=bool)
print(repr(z0))
print(repr(z1))
```


### Error message:

_No response_

### NumPy/Python version information:

1.22.3 3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:17:03) [MSC v.1929 64 bit (AMD64)]",2022-05-26 08:56:05,,BUG: `zeros_like` does not always override `dtype`,['00 - Bug']
21539,open,vmarkovtsev,"### Describe the issue:

assert_array_equal promises to correctly compare nan-s, but if there is a nan in a struct, it raises.

### Reproduce the code example:

```python
import numpy as np
from numpy.testing import assert_array_equal

arr = np.array([(1, np.timedelta64(""NaT"", ""s""))], dtype=[(""a"", int), (""b"", ""timedelta64[s]"")])
# same result: arr = np.array([(1, np.nan)], dtype=[(""a"", int), (""b"", float)])
assert_array_equal(arr, arr)
```


### Error message:

```shell
AssertionError: 
Arrays are not equal

Mismatched elements: 1 / 1 (100%)
 x: array([(1, 'NaT')], dtype=[('a', '<i8'), ('b', '<m8[s]')])
 y: array([(1, 'NaT')], dtype=[('a', '<i8'), ('b', '<m8[s]')])
```


### NumPy/Python version information:

1.21.4",2022-05-19 18:13:06,,BUG: assert_array_equal raises on NaN-s/NaT-s in records,['00 - Bug']
21533,open,jakirkham,Would be great to have a Python API for configuring [NumPy's memory allocator]( https://numpy.org/devdocs/reference/c-api/data_memory.html ),2022-05-18 17:35:15,,Python API to allocator,"['01 - Enhancement', 'component: numpy._core']"
21532,open,Snape3058,"Please give every report a confirmation result. Refer to a report with its ID (the hex numbers in the parenthesis) for simplicity.
If there are multiple hex numbers in the parenthesis, they are the IDs for the reports in the group above or below.

For a group of reports, a summarized confirmation to the group is also OK.

If there are any questions about a report, please leave a comment with the report ID or link, and I will reply the detailed path provided by our static analysis report.

## Undecreased refcnt due to the `return NULL` statement in macro `import_array`.

This issue also affects other projects.

https://github.com/numpy/numpy/blob/59aad3cf436691d447e43038e3402500f13cd817/numpy/core/src/umath/_operand_flag_tests.c#L60
https://github.com/numpy/numpy/blob/59aad3cf436691d447e43038e3402500f13cd817/numpy/core/src/umath/_struct_ufunc_tests.c#L126
https://github.com/numpy/numpy/blob/59aad3cf436691d447e43038e3402500f13cd817/numpy/core/src/umath/_umath_tests.c.src#L705
https://github.com/numpy/numpy/blob/eecd16210ffb487d4e862c71e86cc6f88f2e9aa2/numpy/linalg/umath_linalg.cpp#L4535
https://github.com/numpy/numpy/blob/b222eb66c79b8eccba39f46f020ed8303614a87f/numpy/fft/_pocketfft.c#L2374
https://github.com/numpy/numpy/blob/b222eb66c79b8eccba39f46f020ed8303614a87f/numpy/linalg/lapack_litemodule.c#L397

(79bbae ca50d3 281b63 7ce5a3 ab547b 9bedd0)

## On failure, return NULL directly without decreasing the refcnt.

New reference returned here is inherited from its argument, but not decreased when returned on line 379. (Take the true branch on line 370) (a7dae8)
https://github.com/numpy/numpy/blob/b222eb66c79b8eccba39f46f020ed8303614a87f/numpy/core/src/multiarray/calculation.c#L362

New reference returned here is inherited from its argument, but not decreased when returned on line 410. (Take the true branch on line 390) (fbd6d9)
https://github.com/numpy/numpy/blob/b222eb66c79b8eccba39f46f020ed8303614a87f/numpy/core/src/multiarray/calculation.c#L381

New reference returned here is not decreased when returned on line 435. (347302)
https://github.com/numpy/numpy/blob/b222eb66c79b8eccba39f46f020ed8303614a87f/numpy/core/src/multiarray/descriptor.c#L424

New reference returned here is not decreased when returned on line 2714. (Not decreased on normal branch either) (d3830d)
https://github.com/numpy/numpy/blob/b222eb66c79b8eccba39f46f020ed8303614a87f/numpy/core/src/multiarray/descriptor.c#L2707

Refcnt increased here is not decreased when returned on line 2756. (f9b533)
https://github.com/numpy/numpy/blob/b222eb66c79b8eccba39f46f020ed8303614a87f/numpy/core/src/multiarray/descriptor.c#L2716

New reference returned here is not decreased when returned on line 2827. (out-of-memory raised) (b4258c)
https://github.com/numpy/numpy/blob/b222eb66c79b8eccba39f46f020ed8303614a87f/numpy/core/src/multiarray/descriptor.c#L2810

New reference returned here is not decreased when returned on line 656. (42895d)
https://github.com/numpy/numpy/blob/b222eb66c79b8eccba39f46f020ed8303614a87f/numpy/core/src/multiarray/getset.c#L648

New reference returned here is not decreased when returned on line 2145. (out-of-memory raised) (d15060)
And not decreased on line 2123 in the latest version, runtime error raised.
https://github.com/numpy/numpy/blob/b222eb66c79b8eccba39f46f020ed8303614a87f/numpy/core/src/multiarray/methods.c#L2081

Return value of `PyModule_AddObject` is not checked, the 3rd argument will leak on failure. (c7a933 ee4e9c 0b9cee afba91 08c0c8)
https://github.com/numpy/numpy/blob/b222eb66c79b8eccba39f46f020ed8303614a87f/numpy/core/src/umath/umathmodule.c#L298
https://github.com/numpy/numpy/blob/b222eb66c79b8eccba39f46f020ed8303614a87f/numpy/core/src/umath/umathmodule.c#L299
https://github.com/numpy/numpy/blob/b222eb66c79b8eccba39f46f020ed8303614a87f/numpy/core/src/umath/umathmodule.c#L300
https://github.com/numpy/numpy/blob/b222eb66c79b8eccba39f46f020ed8303614a87f/numpy/core/src/umath/umathmodule.c#L301
https://github.com/numpy/numpy/blob/b222eb66c79b8eccba39f46f020ed8303614a87f/numpy/core/src/umath/umathmodule.c#L302

## Goto a label on a failure, but the new reference is not decreased in the error handling code.

* Assume the corresponding new reference object is not NULL.

New reference returned here is not decreased after jumping to a label. (2d7c3a 3e48a7)
https://github.com/numpy/numpy/blob/59aad3cf436691d447e43038e3402500f13cd817/numpy/core/src/umath/_rational_tests.c#L1134
https://github.com/numpy/numpy/blob/b222eb66c79b8eccba39f46f020ed8303614a87f/numpy/core/src/multiarray/descriptor.c#L538

New reference returned here is not decreased after `goto finish` if the parameter `kwnames` is NULL. (f5e33e)
https://github.com/numpy/numpy/blob/b222eb66c79b8eccba39f46f020ed8303614a87f/numpy/core/src/multiarray/arrayfunction_override.c#L417
",2022-05-18 15:43:09,,BUG: Refcnt leak on failure (some static analysis reports),"['00 - Bug', 'sprintable']"
21528,open,jakirkham,"### Proposed new feature or change:

When using `multiprocessing`, Python creates a fork of the process. In some cases special handling is needed in the new child process and parent process after forking. One such case is reseeding the random number generator ( as mentioned in issue https://github.com/python/cpython/issues/74014 ). Though there may be other tasks as well. In Python 3.7, support for after-fork handlers was added ( https://github.com/python/cpython/issues/60704 ) via the [`PyOS_AfterFork_Parent`]( https://docs.python.org/3/c-api/sys.html#c.PyOS_AfterFork_Parent ) and [`PyOS_AfterFork_Child`]( https://docs.python.org/3/c-api/sys.html#c.PyOS_AfterFork_Child ) APIs. Would be helpful if NumPy used these to reseed its random number generator (and do any other operations that might be needed after-forking).",2022-05-17 18:22:02,,ENH: Use `PyOS_AfterFork_*` for after-fork operations (like w/`multiprocessing`),"['01 - Enhancement', 'component: numpy.random']"
21524,open,megies,"### Describe the issue:

I wanted to use `np.nanpercentile()` on a masked array and got a weird error `""output array is read-only""`. I'm pretty sure this used to work, since I have it in a code of mine I used before, and I don't think this part of the code was never hit before (not 100% sure there though).

There is a trivial workaround, at least for float dtypes, (see code box below), simply filling masked values with `nan` and working on a copy of the array, but this error was still quite confusing to me.

Edit: If choosing `method='nearest'` instead of interpolation, the command executes without error

### Reproduce the code example:

```python
import numpy as np

x = np.arange(20).reshape((4, 5))
# just make every second value masked
x = np.ma.masked_where(x % 2 == 0, x)

# errors out:
np.nanpercentile(x, q=95, axis=1)

# workaround, works on a copy though and does not work for all dtypes:
np.nanpercentile(x.filled(np.nan), q=95, axis=1)
```


### Error message:

```shell
In [28]: np.nanpercentile(x, q=95, axis=1)
/home/megies/miniconda3/envs/default/lib/python3.9/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.
  arr.partition(
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Input In [28], in <cell line: 1>()
----> 1 np.nanpercentile(x, q=95, axis=1)

File <__array_function__ internals>:180, in nanpercentile(*args, **kwargs)

File ~/miniconda3/envs/default/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1385, in nanpercentile(a, q, axis, out, overwrite_input, method, keepdims, interpolation)
   1383 if not function_base._quantile_is_valid(q):
   1384     raise ValueError(""Percentiles must be in the range [0, 100]"")
-> 1385 return _nanquantile_unchecked(
   1386     a, q, axis, out, overwrite_input, method, keepdims)

File ~/miniconda3/envs/default/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1560, in _nanquantile_unchecked(a, q, axis, out, overwrite_input, method, keepdims)
   1558 if a.size == 0:
   1559     return np.nanmean(a, axis, out=out, keepdims=keepdims)
-> 1560 r, k = function_base._ureduce(a,
   1561                               func=_nanquantile_ureduce_func,
   1562                               q=q,
   1563                               axis=axis,
   1564                               out=out,
   1565                               overwrite_input=overwrite_input,
   1566                               method=method)
   1567 if keepdims and keepdims is not np._NoValue:
   1568     return r.reshape(q.shape + k)

File ~/miniconda3/envs/default/lib/python3.9/site-packages/numpy/lib/function_base.py:3702, in _ureduce(a, func, **kwargs)
   3699 else:
   3700     keepdim = (1,) * a.ndim
-> 3702 r = func(a, **kwargs)
   3703 return r, keepdim

File ~/miniconda3/envs/default/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1584, in _nanquantile_ureduce_func(a, q, axis, out, overwrite_input, method)
   1582     result = _nanquantile_1d(part, q, overwrite_input, method)
   1583 else:
-> 1584     result = np.apply_along_axis(_nanquantile_1d, axis, a, q,
   1585                                  overwrite_input, method)
   1586     # apply_along_axis fills in collapsed axis with results.
   1587     # Move that axis to the beginning to match percentile's
   1588     # convention.
   1589     if q.ndim != 0:

File <__array_function__ internals>:180, in apply_along_axis(*args, **kwargs)

File ~/miniconda3/envs/default/lib/python3.9/site-packages/numpy/lib/shape_base.py:379, in apply_along_axis(func1d, axis, arr, *args, **kwargs)
    375 except StopIteration as e:
    376     raise ValueError(
    377         'Cannot apply_along_axis when any iteration dimensions are 0'
    378     ) from None
--> 379 res = asanyarray(func1d(inarr_view[ind0], *args, **kwargs))
    381 # build a buffer for storing evaluations of func1d.
    382 # remove the requested axis, and add the new ones on the end.
    383 # laid out so that each write is contiguous.
    384 # for a tuple index inds, buff[inds] = func1d(inarr_view[inds])
    385 buff = zeros(inarr_view.shape[:-1] + res.shape, res.dtype)

File ~/miniconda3/envs/default/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1608, in _nanquantile_1d(arr1d, q, overwrite_input, method)
   1604 if arr1d.size == 0:
   1605     # convert to scalar
   1606     return np.full(q.shape, np.nan, dtype=arr1d.dtype)[()]
-> 1608 return function_base._quantile_unchecked(
   1609     arr1d, q, overwrite_input=overwrite_input, method=method)

File ~/miniconda3/envs/default/lib/python3.9/site-packages/numpy/lib/function_base.py:4383, in _quantile_unchecked(a, q, axis, out, overwrite_input, method, keepdims)
   4375 def _quantile_unchecked(a,
   4376                         q,
   4377                         axis=None,
   (...)
   4380                         method=""linear"",
   4381                         keepdims=False):
   4382     """"""Assumes that q is in [0, 1], and is an ndarray""""""
-> 4383     r, k = _ureduce(a,
   4384                     func=_quantile_ureduce_func,
   4385                     q=q,
   4386                     axis=axis,
   4387                     out=out,
   4388                     overwrite_input=overwrite_input,
   4389                     method=method)
   4390     if keepdims:
   4391         return r.reshape(q.shape + k)

File ~/miniconda3/envs/default/lib/python3.9/site-packages/numpy/lib/function_base.py:3702, in _ureduce(a, func, **kwargs)
   3699 else:
   3700     keepdim = (1,) * a.ndim
-> 3702 r = func(a, **kwargs)
   3703 return r, keepdim

File ~/miniconda3/envs/default/lib/python3.9/site-packages/numpy/lib/function_base.py:4552, in _quantile_ureduce_func(a, q, axis, out, overwrite_input, method)
   4550     else:
   4551         arr = a.copy()
-> 4552 result = _quantile(arr,
   4553                    quantiles=q,
   4554                    axis=axis,
   4555                    method=method,
   4556                    out=out)
   4557 return result

File ~/miniconda3/envs/default/lib/python3.9/site-packages/numpy/lib/function_base.py:4669, in _quantile(arr, quantiles, axis, method, out)
   4667     result_shape = virtual_indexes.shape + (1,) * (arr.ndim - 1)
   4668     gamma = gamma.reshape(result_shape)
-> 4669     result = _lerp(previous,
   4670                    next,
   4671                    gamma,
   4672                    out=out)
   4673 if np.any(slices_having_nans):
   4674     if result.ndim == 0 and out is None:
   4675         # can't write to a scalar

File ~/miniconda3/envs/default/lib/python3.9/site-packages/numpy/lib/function_base.py:4489, in _lerp(a, b, t, out)
   4487 # asanyarray is a stop-gap until gh-13105
   4488 lerp_interpolation = asanyarray(add(a, diff_b_a * t, out=out))
-> 4489 subtract(b, diff_b_a * (1 - t), out=lerp_interpolation, where=t >= 0.5)
   4490 if lerp_interpolation.ndim == 0 and out is None:
   4491     lerp_interpolation = lerp_interpolation[()]  # unpack 0d arrays

ValueError: output array is read-only
```


### NumPy/Python version information:

## numpy
1.22.2

`numpy   1.22.2    py39h91f2184_0   conda-forge`


## python
3.9.10 | packaged by conda-forge | (main, Feb  1 2022, 21:24:11) 
[GCC 9.4.0]
",2022-05-17 12:06:08,,"BUG: np.nanpercentile errors out with masked array as input (""output array is read-only"")","['00 - Bug', 'component: numpy.ma']"
21520,open,WarrenWeckesser,"The function `numpy.testing.assert_array_almost_equal_nulp` has a formatting error in the code that generates the error message for a test failure when the data type is `np.longdouble`.  For example,
```
In [1]: import numpy as np

In [2]: np.__version__
Out[2]: '1.23.0.dev0+1206.gc5a999682'

In [3]: from numpy.testing import assert_array_almost_equal_nulp

In [4]: x0 = np.longdouble(1.0)

In [5]: x1 = np.nextafter(x0, np.longdouble(2.0))

In [6]: x2 = np.nextafter(x1, np.longdouble(2.0))
```
This test passes:
```
In [7]: assert_array_almost_equal_nulp(x0, x1)
```
This test should fail with an `AssertionError` that contains information about the failed comparison.  Instead, we get a `ValueError: Unsupported dtype float128`:
```
In [8]: assert_array_almost_equal_nulp(x0, x2)   # Should fail
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Input In [8], in <module>
----> 1 assert_array_almost_equal_nulp(x0, x2)

    [... skipping hidden 1 frame]

File ~/py3.10.1/lib/python3.10/site-packages/numpy/testing/_private/utils.py:1707, in nulp_diff(x, y, dtype)
   1704     diff = np.asarray(rx-ry, dtype=vdt)
   1705     return np.abs(diff)
-> 1707 rx = integer_repr(x)
   1708 ry = integer_repr(y)
   1709 return _diff(rx, ry, t)

File ~/py3.10.1/lib/python3.10/site-packages/numpy/testing/_private/utils.py:1738, in integer_repr(x)
   1736     return _integer_repr(x, np.int64, np.int64(-2**63))
   1737 else:
-> 1738     raise ValueError(f'Unsupported dtype {x.dtype}')

ValueError: Unsupported dtype float128
```
A correctly formatted error message from this function can be seen in this example, which using `float` inputs:
```
In [9]: assert_array_almost_equal_nulp(1.0, 1.5)
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
Input In [9], in <module>
----> 1 assert_array_almost_equal_nulp(1.0, 1.5)

File ~/py3.10.1/lib/python3.10/site-packages/numpy/testing/_private/utils.py:1594, in assert_array_almost_equal_nulp(x, y, nulp)
   1592     max_nulp = np.max(nulp_diff(x, y))
   1593     msg = ""X and Y are not equal to %d ULP (max is %g)"" % (nulp, max_nulp)
-> 1594 raise AssertionError(msg)

AssertionError: X and Y are not equal to 1 ULP (max is 2.2518e+15)
```


### NumPy/Python version information:

```
In [10]: import sys, numpy; print(numpy.__version__, sys.version)
1.23.0.dev0+1206.gc5a999682 3.10.1 (main, Jan 14 2022, 02:27:20) [GCC 11.2.0]
```",2022-05-16 18:50:41,,BUG: Error when formatting a failure in 'assert_array_almost_equal_nulp',"['00 - Bug', 'component: numpy.testing']"
21476,open,kalvdans,"### Describe the issue:

array_split(<Python list of lists>, <integer>, axis=1) silently throws away some elements. It works as expected if the `ary` argument is converted to and ndarray first:
```
In [7]: numpy.array_split([[1,2,3],[4,5,6]], 2, axis=1)
Out[7]: 
[array([[1],
        [4]]),
 array([[2],
        [5]])]

In [8]: numpy.array_split(numpy.array([[1,2,3],[4,5,6]]), 2, axis=1)
Out[8]: 
[array([[1, 2],
        [4, 5]]),
 array([[3],
        [6]])]
```

### Reproduce the code example:

```python
import numpy
assert numpy.array_split([[1,2,3],[4,5,6]], 2, axis=1)[0].shape == (2,2)
```


### Error message:

```shell
AssertionError:
```


### NumPy/Python version information:

1.21.5 3.8.10 (default, Nov 26 2021, 20:14:08) 
[GCC 9.3.0]
",2022-05-09 08:30:12,,BUG: array_split with list ary and nonzero axis returns wrong result,['00 - Bug']
21462,open,seberg,"In the advanced indexing code, we have the one special case that allows writing:
```
arr = np.arange(10)
arr[[]]
```
To index 0 integers.  That is, in this context, we enforce the default integer to be `intp`.  The way we do this, however, is by checking for an empty array result (no elements).

This is arguably incorrect, rather we would need to check for no dtype to be inferred, since an array-like may be 0-D but _also_ be e.g. floating point (or boolean!).

To fix this, we would need to inject setting a different dtype here: https://github.com/numpy/numpy/blob/622ca4874f229c83b75746ca5ef7ef9e8a536ea2/numpy/core/src/multiarray/ctors.c#L1724-L1726

In principle, it might otherwise also make sense to pass such a ""default"" as a minimum dtype which could then be used as an additional last (or first?) promotion, to ensure that an `int32` detected array is immediately converted as `int64`.

---

The ""quick"" 95% fix that would also be available for indexing itself, would be to only do the conversion for `float64` (i.e. the default dtype).  That will still allow incorrect indexing for float64 array-likes, but at least the boolean ones (which might return _wrong_ results) would fail. ",2022-05-06 09:46:43,,"BUG: NumPy sometimes converts non-integer array-like to integer ones due to ""integer default dtype""","['00 - Bug', 'component: numpy._core']"
21461,open,bhavukkalra,"### Issue with current documentation:

This issue is opened in regards to keep track of the Examples being added/Improved to various functions. Based on the community feedback from 2020, 2021.

What This issue Specifically focuses on
- Adding detailed How To's for certain topics. [Reference for How To's](https://diataxis.fr/how-to-guides/)
- Adding More examples in the generated documentation, `.rst` files

Connected to Reference - Issue #21351 For Doc Strings

",2022-05-06 09:24:29,,DOC: Improve the existing Examples given in the Docs - Tracking Issue,['04 - Documentation']
21457,open,seberg,"The scalar math (now) does some casting, and also the normal cast functions in `lowlevel_strided_loops.c.src` are heavily templated and complicated.
It would seems useful to create static inline functions for these (probably via C++ templating now).  That way we can simplify all casting code paths in one go.

See also https://github.com/numpy/numpy/pull/21188#discussion_r866521119 and the discussion on that PR.",2022-05-06 06:53:01,,MAINT: Create static inline functions for builtin (numeric?) casts,"['17 - Task', 'component: numpy._core', '03 - Maintenance']"
21443,open,SBS-jtemplin,"### Steps to reproduce:

I am trying to use numpy 1.22.3 with embedded python 3.9.12 or 3.10.2 compiled to a DLL on Windows 10.0.19044.

I've added numpy to embedded python by first installing it to the system python of the same version, then copying the new folders from Lib/site-packages to python3x.zip/site-packages. I have also installed numpy by first installing pip with get-pip from PyPa, but get the same results.

Importing numpy fails in three ways:

When compiling to a UWP DLL or standard DLL without unzipping python3x.zip, I get the following error:
```
ImportError('
IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!
Importing the numpy C-extensions failed. This error can happen for many reasons, often due to issues with your setup or how NumPy was installed.
We have compiled some common reasons and troubleshooting tips at: https://numpy.org/devdocs/user/troubleshooting-importerror.html
Please note and check the following:
* The Python version is: Python3.10 from ""C:\\Users\\jtemplin\\git\\PythonInterpreterDLL\\x64\\Debug\\PythonInterpreterTests\\AppX\\PythonInterpreterTests.exe""
* The NumPy version is: ""1.22.3""
and make sure that they are the versions you expect.
Please carefully study the documentation linked above for further help.
Original error was: No module named \'numpy.core._multiarray_umath\'
')
```

This happens in debug and release builds. Other recommendations on the linked troubleshooting page either didn't apply or made no difference.

However, if I unzip the python3x.zip I get different errors:

When compiling to a UWP DLL and and importing numpy in python using PyObject_CallObject I get the following error:
```
Traceback (most recent call last):
File ""C:\path-to-build\tests.py"", line 37, in import_numpy_exception
import numpy
File ""C:\path-to-build\python310.zip\site-packages\numpy\__init__.py"", line 128, in <module>
from numpy.__config__ import show as show_config
File ""C:\path-to-build\python310.zip\site-packages\numpy\__config__.py"", line 13, in <module>
os.add_dll_directory(extra_dll_dir)
File ""os.py"", line 1117, in add_dll_directory
OSError: [WinError 87] The parameter is incorrect: 'C:\\path-to-build\\python310.zip\\site-packages\\numpy\\.libs'
```

The path that os.add_dll_directory is trying to add exists, and the function works if I call it manually in a python shell.

When compiling to a standard DLL I can call os.add_dll_directory using `PyRun_SimpleString`, but still can't import numpy. I've tried `PyImport_Import(""numpy"")`, `PyImport_ImportModule(""numpy"")`, `PyImport_AddModule(""numpy"")` and they all return `nullptr`. I've also tried importing numpy in python using `PyObject_CallObject` and it fails with the following error:
`Assertion failed: !PyErr_Occurred(), file D:\a\1\s\Objects\typeobject.c, line 3264`

All of the above functions are able to import django successfully, so I don't believe it is a simple path issue. They also import numpy successfully when running in a console app.

So why does importing numpy fail in a DLL?

Here are some code samples that work with django, but not numpy:

```cpp
bool PythonInterpreter::importModule(const char* moduleName) {
    // moduleName = ""numpy"" or ""django""
    auto pyModuleName = PyUnicode_FromString(moduleName);
    auto module = PyImport_Import(pyModuleName);
    auto result1 = !(nullptr == module);
    
    auto module2 = PyImport_ImportModule(moduleName);
    auto result2 = !(nullptr == module2);
    
    auto module3 = PyImport_AddModule(moduleName);
    auto result3 = !(nullptr == module3);
    
    return result1 || result2 || result3;
}

bool PythonInterpreter::importSimpleString(const char* importString) {
    // importString = ""import numpy"" or ""import django""
    auto result = 0 == PyRun_SimpleString(importString);
    
    return result;
}

bool PythonInterpreter::importWithException(const char* functionName) {
    // functionName = ""import_numpy"" or ""import_django""
    auto importNumpyEx = PyObject_GetAttrString(this->testModule, functionName);
    auto result = _PyUnicode_AsString(PyObject_CallObject(importNumpyEx, NULL));
    
    return 0 == strcmp(result, ""PASS"");
}
```

```python
def import_django():
    try:
        import django
        return ""PASS""
    except OSError as e:
        return full_stack() # lifted from stackexchange

def import_numpy():
    try:
        import numpy
        return ""PASS""
    except OSError as e:
        return full_stack() # lifted from stackexchange

def full_stack():
    import traceback, sys
    exc = sys.exc_info()[0]
    stack = traceback.extract_stack()[:-1]  # last one would be full_stack()
    if exc is not None:  # i.e. an exception is present
        del stack[-1]       # remove call of full_stack, the printed exception
                            # will contain the caught exception caller instead
    trc = 'Traceback (most recent call last):\n'
    stackstr = trc + ''.join(traceback.format_list(stack))
    if exc is not None:
         stackstr += '  ' + traceback.format_exc().lstrip(trc)
    return
```



### Error message:

See above

### Additional information:

Also posted here: https://github.com/python/cpython/issues/92282",2022-05-04 23:18:46,,Importing numpy fails when CPython is compiled to a DLL ,"['57 - Close?', 'Embedded']"
21435,open,tflahaul,"### Proposed new feature or change:

```py
x.repeat(B, 0).repeat(H * W, 3)
```
In this case, instead of calling `repeat` twice, I'd love to call it a single time with something along the lines of
```py
x.repeat(
    repeats=(B, H * W),
    axis=(0, 3)
)
```
or by combining the number of repetitions with the axis like so:
```py
x.repeat((B, 0), (H * W, 3))
```
<br/>
I'd love to know your opinions on this, thanks a lot!",2022-05-03 18:35:44,,ENH: Allow tuple arguments for `numpy.repeat`,['unlabeled']
21424,open,tgross35,"### Issue with current documentation:

I'm working through my first issue with the project, and struggling to find information on whether there is a recommended formatter. The source currently seems to use a mix of optional styles (single vs. double quotes, backslash vs. brackets, formatting of non-python files, etc.) and I think that some more specific in the docs, possibly with recommended formatters and/or formatter config files, would help to simplify things a bit.

The only reference to style that I found was here: https://numpydoc.readthedocs.io/en/latest/format.html

### Idea or request for content:

Plenty of projects use Black to format code, and I don't think that would be a bad idea here. If there are some custom style options that Numpy prefers to stick with, then something like Autopep8 or YAPF would probably be configurable to work.

A simpler option would be to add a precommit hook that handles formatting, just to be able to easier enforce consistency. Even if the hook isn't enforced in the pipeline, it would at least help give devs some direction when working locally. At a minimum, the following hooks would be quite useful:
- Black (with line length 79 to match pycodestyle)
- flake8, recommended in the Numpy style guide
- trailing-whitespace
- end-of-file-fixer
- clang-format, to implement the config from here https://github.com/numpy/numpy/pull/19754

Optionally, some other nice to have hooks would be:
- isort
- pretty-format-ini
- pretty-format-toml
- pretty-format-yaml
- check-toml
- check-yaml
- fix-byte-order-marker
- mixed-line-ending

If there is interest in this, I would be happy to submit a PR. Of course, it is possible that this has been discussed before and there's a reason it doesn't exist. I couldn't find anything in the issues list, but I figure there must be a reason why linter.py is used instead of a precommit hook.",2022-05-02 03:34:59,,DOC: Style guide could indicate formatters / pre-commit hook,['04 - Documentation']
21419,open,mattip,"### Issue with current documentation:

In [PR 21381](https://github.com/numpy/numpy/pull/21381#issuecomment-1114150267) we noticed mathjax display was slow, for instance [the numpy.correlate](https://numpy.org/devdocs/reference/generated/numpy.correlate.html) page has a formula at the top. The reformatting is visible during page load.

### Idea or request for content:

Is there more caching that can be done to improve page load times?",2022-05-01 09:23:34,,DOC: loading pages with mathjax is slow,['04 - Documentation']
21414,open,oscargus,"### Proposed new feature or change:

It would be convenient to do something like:
```
np.less(a, b, where=c, out=False)
```
instead of something like
```
default = np.zeros_like(a, dtype=bool)
np.less(a, b, where=c, out=default)
```",2022-04-30 11:00:50,,"ENH: provide default value for `less`, `greater`, etc when used with `where`","['01 - Enhancement', 'component: numpy.ufunc']"
21409,open,HaoZeke,"The problem is that the size of long depends on the platform. This causes a problem with Fortran also, as the default integer type depends on compilation flags. The best approach is to determine the needed size and map it to either int32 (c_int) or int64 (c_longlong).

_Originally posted by @charris in https://github.com/numpy/numpy/issues/19159#issuecomment-855266685_",2022-04-29 03:28:43,,MAINT: Remove platform-dependent-width-types from F2PY,"['01 - Enhancement', 'component: numpy.f2py', '03 - Maintenance']"
21396,open,lezcano,"### Proposed new feature or change:

The current default for `np.vander` of returning the powers of the matrix in decreasing order is highly non standard (see e.g. the [wikipedia](https://en.wikipedia.org/wiki/Vandermonde_matrix)). This is problematic as, for one, if `len(x)` is even, it returns the negative of the [Vandermonde determinant](https://en.wikipedia.org/wiki/Vandermonde_polynomial). This has proven to be rather confusing for users (see this [SO post](https://stackoverflow.com/questions/63984312/how-to-make-a-simple-vandermonde-matrix-with-numpy/71758047#71758047)).

This odd default value was inherited from MATLAB as [@rgommers found here](https://github.com/pytorch/pytorch/pull/76303#issuecomment-1108505327).

In PyTorch, when adding this function to `torch.linalg`, we decided to remove the `increasing` keyword, and simply leave as default behaviour the `increasing=True` behaviour, which is standard in literature. Note that the previous behaviour can be recovered by simply doing `linalg.vander(x, N).flip(-1)`. See the discussion in https://github.com/pytorch/pytorch/pull/76303 for more context.

The proposals are then:
- Change the `increasing=False` default value to `increasing=True`
- Implement `linalg.vander` following PyTorch (perhaps even giving support for batches, as PyTorch does).

There are a number of ways to achieve this:
1. Simply implement `linalg.vander` and have it live together with `np.vander`.
2. Implement `linalg.vander` and deprecate `np.vander` and eventually remove it
3. Implement `linalg.vander` and deprecate the default behaviour of `np.vander`. This is the trickiest option and it's not even clear to me how to implement it.

cc @mruberry @rgommers @kgryte",2022-04-26 12:16:17,,ENH: `linalg.vander` and the default `increasing` value for `np.vander`,"['01 - Enhancement', '30 - API']"
21379,open,leofang,"### Proposed new feature or change:

I am creating this feature request wearing two hats (CuPy and cuQuantum Python).

Currently, `numpy.einsum_path()` supports the same calling convention as `numpy.einsum()` with regard to the input tensors and the einsum expression. So either of these is accepted:
- `numpy.einsum_path(arr1, sublist1, arr2, sublist2, ...)`
- `numpy.einsum_path(einsum_str, *(arr1, arr2, ...))`

But either of this is quite an overkill, as `einsum_path()` does not care about the array contents or most of the array attributes other than their shapes (strides aren't even taken into account during the path computation). Therefore, unlike `numpy.einsum()` it can be made *backend-agnostic*: As long as the backend array (NumPy, CuPy, PyTorch, ...) has the concept of shapes, they can all delegate to `numpy.einsum_path()` to compute the path without reinventing the wheel. 

For this purpose, it'd be nice if `numpy.einsum_path()` can additionally support the following calling conventions:
- `numpy.einsum_path(shape1, sublist1, shape2, sublist2, ...)`
- `numpy.einsum_path(einsum_str, *(shape2, shape2, ...))`

(picking one of them is also fine, if it's easier.)

On the **CuPy** side, this naive delegation to implement `cupy.einsum_path()`
```python
def einsum_path(subscripts, *operands, optimize='greedy'):
    return numpy.einsum_path(subscripts, *operands, optimize=optimize)
```
does not work because it'd be trapped in an infinite `__array_function__` dispatching loop. As a result, currently CuPy has to copy a ton of the `numpy.einsum_path` internal code. This is not fun and has maintenance overhead (we can't co-evolve with the latest improvement in NumPy).

On the **cuQuantum Python** side, we'd like to provide a path we calculate to `numpy.einsum_path()` for pretty-printing the contraction information so that we don't have to reinvent this bit of machinery (or being outdated from NumPy),
```python
path = cuquantum.contract_path(...)
path, info = numpy.einsum_path(..., optimize=['einsum_path', *path])
```
However, as we support multiple array backends this is a problem (ex: we have to create temporary NumPy arrays if the input tensors are from PyTorch or CuPy in order to do this).",2022-04-22 17:52:39,,ENH: `einsum_path` should be backend-agnostic ,['60 - Major release']
21376,open,paulmenzel,"### Describe the issue:

[Same problem reported by somebody else in https://github.com/pytorch/pytorch/issues/66782.]

On an old AMD Opteron system with Python 3.9.7 and NumPy 1.20.3, importing NumPy terminates Python with a segmentation fault.

```
$ lscpu
Architecture:            x86_64
  CPU op-mode(s):        32-bit, 64-bit
  Address sizes:         48 bits physical, 48 bits virtual
  Byte Order:            Little Endian
CPU(s):                  64
  On-line CPU(s) list:   0-63
Vendor ID:               AuthenticAMD
  Model name:            AMD Opteron(tm) Processor 6282 SE
    CPU family:          21
    Model:               1
    Thread(s) per core:  2
    Core(s) per socket:  8
    Socket(s):           4
    Stepping:            2
    Frequency boost:     enabled
    CPU max MHz:         2600.0000
    CPU min MHz:         1400.0000
    BogoMIPS:            5200.23
    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf pni pclmulqdq monitor ssse3 cx16 sse4_1 sse4_2 popcnt aes xsave avx lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 nodeid_msr topoext perfctr_core perfctr_nb cpb hw_pstate ssbd vmmcall arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold
[…]
```

    [84603702.837479] python3[26042]: segfault at 0 ip 0000000000000000 sp 00007fd459dd4db8 error 14 in python3.9[400000+1000]
    [84603702.837494] Code: Bad RIP value.

```
(gdb) bt
#0  0x0000000000000000 in ?? ()
#1  0x00007fc012033760 in blas_memory_alloc () from /pkg/python-3.9.7-0/lib/python3.9/site-packages/numpy/core/../../numpy.libs/libopenblasp-r0-5bebc122.3.13.dev.so
#2  0x00007fc012033f24 in blas_thread_server () from /pkg/python-3.9.7-0/lib/python3.9/site-packages/numpy/core/../../numpy.libs/libopenblasp-r0-5bebc122.3.13.dev.so
#3  0x00007fc015b87d8e in start_thread (arg=0x7fbfbbc4c640) at pthread_create.c:473
#4  0x00007fc015958aaf in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95
(gdb) info registers 
rax            0x0                 0
rbx            0x7fbfbbc4bdd8      140461465714136
rcx            0x0                 0
rdx            0x25890             153744
rsi            0x0                 0
rdi            0x0                 0
rbp            0x23                0x23
rsp            0x7fbfbbc4bdb8      0x7fbfbbc4bdb8
r8             0x0                 0
r9             0x0                 0
r10            0x22                34
r11            0x56b000            5681152
r12            0x7fc013bc7460      140462941566048
r13            0xffffffffffffffff  -1
r14            0x7fc013bca280      140462941577856
r15            0x7fc013bcb438      140462941582392
rip            0x0                 0x0
eflags         0x10206             [ PF IF RF ]
cs             0x33                51
ss             0x2b                43
ds             0x0                 0
es             0x0                 0
fs             0x0                 0
gs             0x0                 0
(gdb) f 1
#1  0x00007fc012033760 in blas_memory_alloc () from /pkg/python-3.9.7-0/lib/python3.9/site-packages/numpy/core/../../numpy.libs/libopenblasp-r0-5bebc122.3.13.dev.so
(gdb) bt f
#0  0x0000000000000000 in ?? ()
No symbol table info available.
#1  0x00007fc012033760 in blas_memory_alloc () from /pkg/python-3.9.7-0/lib/python3.9/site-packages/numpy/core/../../numpy.libs/libopenblasp-r0-5bebc122.3.13.dev.so
No symbol table info available.
#2  0x00007fc012033f24 in blas_thread_server () from /pkg/python-3.9.7-0/lib/python3.9/site-packages/numpy/core/../../numpy.libs/libopenblasp-r0-5bebc122.3.13.dev.so
No symbol table info available.
#3  0x00007fc015b87d8e in start_thread (arg=0x7fbfbbc4c640) at pthread_create.c:473
        ret = <optimized out>
        pd = 0x7fbfbbc4c640
        unwind_buf = {cancel_jmp_buf = {{jmp_buf = {140461465716288, 7677594563587215477, 140734205512782, 0, 140734205512783, 0, -7641698058055053195, -7706678334027067275}, mask_was_saved = 0}}, priv = {pad = {0x0, 0x0, 0x0, 0x0}, data = {prev = 0x0, cleanup = 0x0, canceltype = 0}}}
        not_first_call = 0
#4  0x00007fc015958aaf in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95
No locals.
(gdb) layout asm
```

```
│    0x7fc012033740 <blas_memory_alloc+256>  lea    -0x477(%rip),%rax        # 0x7fc0120332d0 <alloc_mmap>                                           │
│    0x7fc012033747 <blas_memory_alloc+263>  mov    %rsp,%rbx                                                                                        │
│    0x7fc01203374a <blas_memory_alloc+266>  jmp    0x7fc012033753 <blas_memory_alloc+275>                                                           │
│    0x7fc01203374c <blas_memory_alloc+268>  nopl   0x0(%rax)                                                                                        │
│    0x7fc012033750 <blas_memory_alloc+272>  mov    (%rbx),%rax                                                                                      │
│    0x7fc012033753 <blas_memory_alloc+275>  mov    0x1b95d06(%rip),%rdi        # 0x7fc013bc9460 <base_address>                                      │
│    0x7fc01203375a <blas_memory_alloc+282>  add    $0x8,%rbx                                                                                        │
│    0x7fc01203375e <blas_memory_alloc+286>  call   *%rax                                                                                            │
│  > 0x7fc012033760 <blas_memory_alloc+288>  mov    %rax,%r13                                                                                        │
│    0x7fc012033763 <blas_memory_alloc+291>  cmp    $0xffffffffffffffff,%rax                                                                         │
│    0x7fc012033767 <blas_memory_alloc+295>  je     0x7fc012033750 <blas_memory_alloc+272>                                                           │
│    0x7fc012033769 <blas_memory_alloc+297>  mov    0x1b95cf0(%rip),%rax        # 0x7fc013bc9460 <base_address>                                      │
│    0x7fc012033770 <blas_memory_alloc+304>  test   %rax,%rax                                                                                        │
│    0x7fc012033773 <blas_memory_alloc+307>  je     0x7fc012033782 <blas_memory_alloc+322>                                                           │
│    0x7fc012033775 <blas_memory_alloc+309>  add    $0x2001000,%rax                                                                                  │
│    0x7fc01203377b <blas_memory_alloc+315>  mov    %rax,0x1b95cde(%rip)        # 0x7fc013bc9460 <base_address>                                      │
│    0x7fc012033782 <blas_memory_alloc+322>  lea    0x1b95cf7(%rip),%rdi        # 0x7fc013bc9480 <alloc_lock>                                        │
│    0x7fc012033789 <blas_memory_alloc+329>  call   0x7fc011e097e0 <pthread_mutex_lock@plt>                                                          │
│    0x7fc01203378e <blas_memory_alloc+334>  mov    %rbp,%rax                                                                                        │
│    0x7fc012033791 <blas_memory_alloc+337>  lea    0x1b95ce8(%rip),%rdi        # 0x7fc013bc9480 <alloc_lock>                                        │
│    0x7fc012033798 <blas_memory_alloc+344>  shl    $0x6,%rax                                                                                        │
│    0x7fc01203379c <blas_memory_alloc+348>  add    %r12,%rax                                                                                        │
```

On a newer system it works, so the assumption is, `libopenblasp-r0-5bebc122.3.13.dev.so` was optimized for the build system and is using an unsupported instruction. But there is, for example, no AVX2 instruction at that spot, as far as I can see.

### Reproduce the code example:

```python
import numpy
```


### Error message:

```shell
Segmentation fault
```


### NumPy/Python version information:

Python 3.9.7 with NumPy 1.20.3",2022-04-21 19:44:23,,BUG: `import numpy` terminates Python with SIGSEGV in ,['00 - Bug']
21375,open,marctorsoc,"### Proposed new feature or change:

My motivation comes from this PR in sklearn https://github.com/scikit-learn/scikit-learn/pull/23183/files#diff-b62de604fe871c9d2dcbfc799daf93b5bcebef1bf08341a0a7fc7562ba20b337R148, where I wrote:

```
Wrapper for np.average, with np.nan values being ignored from the average
    This is similar to np.nanmean, but allowing to pass weights as in np.average
```

The ideas would be to add this functionality to one of the two functions, i.e:

a) add weights to np.nanmean: https://numpy.org/doc/stable/reference/generated/numpy.nanmean.html
b) add option to ignore nan's for np.average : https://numpy.org/doc/stable/reference/generated/numpy.average.html

I'm aware of https://numpy.org/doc/stable/reference/generated/numpy.ma.average.html#numpy.ma.average but to be honest, I think it'd still be useful to implicitly indicate the mask with np.nan in the value, rather than having to create a masked array.

I'm happy to (at least try) do the PR myself, but would like some feedback on the idea / scope / api, and whether I missed some already existing method.

",2022-04-21 18:35:49,,ENH: np.nanmean with weights,['01 - Enhancement']
21358,open,l-johnston,"### Describe the issue:

A Polynomial of order 0 and with coefficient value infinity emits a RuntimeWarning when dividing it by a float:

```python
>>> Polynomial([np.inf]) / 2
RuntimeWarning: invalid value encountered in multiply
Polynomial([inf], domain=[-1.,  1.], window=[-1.,  1.])
```

The return value is correct, but it shouldn't emit a warning. Multiplication works fine:

```python
>>> Polynomial([np.inf]) * 0.5
Polynomial([inf], domain=[-1.,  1.], window=[-1.,  1.])
```

A higher order polynomial with finite zero order coefficient works fine:

```python
>>> Polynomial([1, np.inf]) / 2
Polynomial([0.5, inf], domain=[-1.,  1.], window=[-1.,  1.])
```


### Reproduce the code example:

```python
import numpy as np
from numpy.polynomial import Polynomial

Polynomial([np.inf]) / 2
```


### Error message:

```shell
.../python3.9/site-packages/numpy/polynomial/polynomial.py:410: RuntimeWarning: invalid value encountered in multiply
  return c1/c2[-1], c1[:1]*0
```


### NumPy/Python version information:

1.22.3 3.9.2 (default, Mar  4 2022, 13:52:40)
[GCC 9.3.0]",2022-04-18 13:22:11,,BUG: Polynomial([inf]) divided by a float emits RuntimeWarning: invalid value encountered in multiply,['00 - Bug']
21357,open,miniufo,"### Proposed new feature or change:

I need to take centered finite difference of `data` of length N along the dimension 'X', with boundary conditions (BCs) specified in flexible ways.  This cannot be done using `xarray` (although seems quite natural given the data and coordinates), as it is based on `numpy.gradient` (see [the issue here](https://github.com/pydata/xarray/issues/6493)).  Just want to know, if we could pad `data` with different BCs (length becoming N+2) so that the indicing will not be out-of-range, and then take centered difference.

Commonly used BCs are:
1. fixed - fill with fixed values so derivatives at BCs are (BC - data[-1])/dx and (data[0] - BC)/dx;
2. extend - fill BCs with second outer-most values so that derivatives at BCs are exactly zero;
3. periodic - fill BCs cyclic so that the derivatives are also cyclic.

After the padding of proper BCs, one can:
```python
# padded with BCs into N+2, which could use `numpy.pad`
data_pad = pad_BCs(data, dim, BCs='periodic')

# padded with BCs into N+2 with mixed BCs
data_pad = pad_BCs(data, dim, BCs=['extend', 'fixed'], fill=1)

# padded with BCs into N+2 with different values at each BC
data_pad = pad_BCs(data, dim, BCs=['fixed', 'fixed'], fill=(2,3))

# it is safe to take finite difference
for i in range(len(data))
    diff[i] = (data_pad [i+1] - data_pad [i-1]) / (dx*2)
```
to get the derivative/gradient, and calculate `np.gradient` with various BCs:
```python
grad = data.gradient(dim, BCs=['extend', 'fixed'], fill=1)
```",2022-04-18 07:26:00,,ENH: boundary condition for np.gradient,['unlabeled']
21342,open,asmeurer,"### Describe the issue:

np.dot() seems to ignore proper negative zero semantics in some cases. For example:

```py
>>> np.dot(np.array([[-0.], [-0.]]), np.array([[0.]]))
array([[0.],
       [0.]])
>>> np.dot(np.array([[-0.], [-0.]]), np.array([[1.]]))
array([[0.],
       [0.]])
>>> np.dot(np.array([[-0.], [1.]]), np.array([[1.]]))
array([[0.],
       [1.]])
```

The issue isn't present when the first array is (1, 1)

```py
>>> np.dot(np.array([[-0.]]), np.array([[0.]]))
array([[-0.]])
>>> np.dot(np.array([[-0.]]), np.array([[1.]]))
array([[-0.]])
```

A similar issue was just fixed in https://github.com/numpy/numpy/pull/21218, but I tested this in main which has that PR, so it's not exactly the same issue.


### Reproduce the code example:

```python
>>> np.dot(np.array([[-0.], [-0.]]), np.array([[0.]]))
array([[0.],
       [0.]])
>>> np.dot(np.array([[-0.], [-0.]]), np.array([[1.]]))
array([[0.],
       [0.]])
```


### Error message:

_No response_

### NumPy/Python version information:

```py
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.23.0.dev0+1032.gae13307732 3.9.9 | packaged by conda-forge | (main, Dec 20 2021, 02:41:37)
[Clang 11.1.0 ]
```

---

EDIT (seberg): This is also true for `vdot` (vectors) and summing.  IMO, it's not worthwhile to spend much time on: see also discussion in PR.  For `sum` fixing could be done relatively easily now in principle.  When blas is invovled: probably not.",2022-04-14 19:57:08,,BUG: np.dot sometimes sometimes has incorrect -0. (negative zero) semantics,['00 - Bug']
21339,open,NewUserHa,"### Describe the issue:

once import NumPy, the commit size goes from ~40MB to ~280MB. is it able to use less commit size for the portable devices?NumPy

### Reproduce the code example:

```python
import numpy as np
```


### Error message:

_No response_

### NumPy/Python version information:

1.22.3 3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]",2022-04-14 00:08:57,,BUG: numpy use 280MB commit size just once import,['00 - Bug']
21332,open,scottshambaugh,"### Proposed new feature or change:

As [this stackoverflow question shows](https://stackoverflow.com/questions/37964100/creating-numpy-linspace-out-of-datetime), there is quite a bit of interest for allowing linspace() to take in datetimes, much like arange() allows for.

In other words, I would like to be able to generate an array of every day of the year 2022 with the following cmd:
`x = np.linspace(datetime.datetime(2022, 1, 1), datetime.datetime(2023, 1, 1), num=365, endpoint=False)`",2022-04-12 17:53:52,,ENH: Support for datetimes in linspace,['unlabeled']
21310,open,oscargus,"### Issue with current documentation:

Right now most of the documentation use `*` for ""multiplication"". Some rare cases use `\cdotp`. `\times` is primarily used for matrix/tensor sizes.

Is there some consensus on what should be used? I guess one can in many cases skip it, but not always. To me `*` seems like the worst option of them. (I also believe that `\cdot` is better than `cdotp` and believe that the name of `\times` indicates that it is the ""correct"" one, but realize that `\cdot` may be preferred as it is more brief.)

Then, there is the slightly related topic of how to write variable names in equations and when to use equations and when to use ""Python equations"". But maybe one at a time...

### Idea or request for content:

I can probably make a PR making it consistent, at least if that means getting rid of `*`... But won't start before I know that I'm changing in the right direction.",2022-04-07 08:10:08,,DOC: `*` vs `\cdot` vs `\cdotp` vs `\times`?,['04 - Documentation']
21305,open,t-wy,"### Describe the issue:

It is often usual to use a column of type `dtype('int64')` from an array (e.g. from csv).
`np.choose` works correctly in 64-bit Python for using `dtype('int64')` array in parameter `a`, while it returns the casting error in 32-bit Python, which leads to the inconsistency.

(It is weird for `dtype('int64')` to be necessary for array index though, so here just addresses the **inconsistency**.)

The below is just the minimal code for the problem, the use case is usually like
```
np.choose(x[x < 5], [1, 2, 3], mode=""clip"")
```
in which x is in `dtype('int64')`

A current consistent workaround for all indices within the magnitude of 2^31 would be something like
```
np.choose(np.array([2]).astype(np.int32), [1, 2, 3], mode=""clip"")
np.choose(x[x < 5].astype(np.int32), [1, 2, 3], mode=""clip"")
```

### Reproduce the code example:

```python
import numpy as np
np.choose(123456789012345, [1, 2, 3], mode=""clip"")
# Expected Result: 3, working in 64-bit Python
```


### Error message:

```shell
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<__array_function__ internals>"", line 5, in choose
  File ""C:\Users\_____\AppData\Local\Programs\Python\Python38-32\lib\site-packages\numpy\core\fromnumeric.py"", line 429, in choose
    return _wrapfunc(a, 'choose', choices, out=out, mode=mode)
  File ""C:\Users\_____\AppData\Local\Programs\Python\Python38-32\lib\site-packages\numpy\core\fromnumeric.py"", line 54, in _wrapfunc
    return _wrapit(obj, method, *args, **kwds)
  File ""C:\Users\_____\AppData\Local\Programs\Python\Python38-32\lib\site-packages\numpy\core\fromnumeric.py"", line 43, in _wrapit
    result = getattr(asarray(obj), method)(*args, **kwds)
TypeError: Cannot cast scalar from dtype('int64') to dtype('int32') according to the rule 'safe'
```


### NumPy/Python version information:

```
1.21.2 3.8.2 (tags/v3.8.2:7b3ab59, Feb 25 2020, 22:45:29) [MSC v.1916 32 bit (Intel)]
1.22.3 3.8.2 (tags/v3.8.2:7b3ab59, Feb 25 2020, 22:45:29) [MSC v.1916 32 bit (Intel)]
```",2022-04-06 23:14:25,,BUG: np.choose results in rule 'safe' cast error in 32-bit Python,['00 - Bug']
21304,open,HaoZeke,"### Proposed new feature or change: Unit testing C code

One of the primary motivations behind this change is to support [F2PY generated codes better](https://github.com/HaoZeke/f2py_derived_nep/commit/c58330350a4a4d9d4d145e58ed87b69d8df7307a). Working with `fortranobject.{c,h}` while trying to have test coverage is effectively impossible right now. Currently we only have integration tests (at the Python level) and this makes it hard to track problems sometimes.

However, beyond F2PY as well, unit testing the NumPy-C API can be done in low-boilerplate way with [cmocka](https://cmocka.org/).

Some nice features of `cmocka`:
- C only
- Surprisingly low boilerplate
- Is easily available on `conda`
- Used by a bunch of other projects (e.g. `libssh`)

Existing C-testing:
- Integration (`pytest`)
- Making small extension modules to test parts of the public API ([test_mem_policy.py](https://github.com/numpy/numpy/blob/main/numpy/core/tests/test_mem_policy.py))

Some questions which have come up in discussions are:
- Compiling on Windows is slow --> we can concatenate these on the CI
- Compiling them in monolithic files might be hard to debug (e.g. F2PY generates wrong C code)
   - This should not be an issue for the most part, compiler errors are caught by our test suite now as well, which should not change.",2022-04-06 16:05:30,,ENH: Adding `cmocka` for unit testing,"['05 - Testing', 'component: numpy.f2py', 'triaged']"
21300,open,HaoZeke,"### Proposed new feature or change:

The [C API Stability](https://docs.python.org/3/c-api/stable.html#stable) document defines a subset of functions which are (to some extent) compatible across python versions. Thankfully most of the API functions used in `fortranobject.{c,h}` are already in the stable API.

It would be good to ensure this strictly though. A solution would be effectively to ensure that all C extensions compile with `Py_LIMITED_API` set to the same lowest Python version supported by NumPy itself. [1]

In practice, there is a slight performance hit from not using functions outside the stable API, but more importantly, the reference counting is not always the same between these functions (e.g. The sequence protocol defines an ""unsafe"" `Fast_GET_ITEM` which borrows a reference instead of `GetItem` which returns a new reference). This means that some care may need to be taken to ensure equivalence.

[1] Also to make sure that none of the ""unstable"" API functions are used",2022-04-05 13:22:47,,ENH: Ensure F2PY generates Python-C stable API code,"['01 - Enhancement', 'component: numpy.f2py']"
21280,open,seberg,"### Describe the issue:

UFuncs seem to not handle array priority wrapping quite right (at least compared to `np.get_array_wrap`).  That is, they simply skip NumPy arrays and scalars, but that is not necessarily right.
(The code should probably flag their existence, and return `NULL` manually when that makes sense)

(Noticed in gh-21262)

### Reproduce the code example:

```python
import numpy as np
class myarr(np.ndarray):
    __array_priority__ = -1
a = np.arange(3)
b = np.arange(3).view(myarr)

# although priority is negative, `myarr` is used:
assert type(np.multiply(a, b)) is myarr
assert type(np.multiply(b, a)) is myarr

# But this is not true for the Python helper:
res = np.arange(3)
assert type(np.get_array_wrap(a, b)(res)) is np.ndarray
```


### Error message:

_No response_

### NumPy/Python version information:

1.22.x/1.23.<dev>


## Possible project

I think fixing this is a small nice project.  It should not be very hard, but requires to dig a bit into the code for universal functions and experience with the Python C-API (or willingness to learn).
Fixing it may also require an iteration or two to figure out the best way to do it.  But I expect we should adjust the ufunc code and that would make the issue very localized.",2022-04-01 16:27:36,,BUG(?): Array-priority for output wrapping seems wrong in ufuncs,"['00 - Bug', 'Project']"
21274,open,francois-rozet,"### Describe the issue:

When vectorizing a function having array-like outputs, if the function is called with scalar (0-d) inputs, the array-like outputs are transformed to (`object`) arrays. This results in the output types changing with respect to the inputs' dimensionality, which shouldn't be the case.

This issue comes from the fact that `np.frompyfunc` does not wrap the outputs in arrays if the inputs are scalars (which I find quite nice). Then, when the following lines are applied (in `vectorize`), the array-like outputs are transformed into arrays instead of being wrapped into scalar arrays like other types (e.g. `set`).

https://github.com/numpy/numpy/blob/a8fd84da447a5af7434cbff1e7e84a2e7a0cfb5a/numpy/lib/function_base.py#L2389-L2393

A simple solution would be to not apply `asanyarray` if the `otype` is `object`.

> P.S. The list comprehension at line 2392 is superfluous.

### Reproduce the code example:

```python
>>> import numpy as np
>>> vf = np.vectorize(lambda _: np.arange(3), otypes=[object])
>>> vf([0])
array([array([0, 1, 2])], dtype=object)
>>> vf(0)  # should be array([0, 1, 2]) or array(array([0, 1, 2]), dtype=object)
array([0, 1, 2], dtype=object)
```


### Error message:

_No response_

### NumPy/Python version information:

```
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.21.0 3.9.7 (default, Sep 16 2021, 13:09:58)
[GCC 7.5.0]
```",2022-03-31 15:00:20,,BUG: np.vectorize is inconsistent for scalar inputs and array-like outputs,['00 - Bug']
21273,open,milliams,"### Proposed new feature or change:

I'd like to add support for pretty-printing of NumPy arrays to Jupyter Notebooks via `_repr_html_` and/or `_repr_pretty_`.

At present, using `print(a)` gives a nice text-based output:

```
[[1 2 3]
 [4 5 6]
 [7 8 9]]
```

but using

```python
a
```

by itself in a cell calls the `__repr__` by default and so gives:

```
array([[1, 2, 3],
       [4, 5, 6],
       [7, 8, 9]])
```

I'd like to use [the features of IPython/Jupyter](https://ipython.readthedocs.io/en/stable/config/integrating.html) to allow it to display a rich, HTML-based format of an array which I think will aid understanding and learning of NumPy, especially for beginners.

# `_repr_html_`

This won't affect the principle that the `repr` of an object should (where possible) be eval-able to recreate it as it will be implemented as a new method `_repr_html_`.

For 1 and 2 dimensional arrays, the format would be a HTML table style, and for higher dimensions we could do the same as with the `repr` where we repeat it by rows, but with the advantage that with some outlines it becomes more obvious where the groupings are.

I'm imagining something like:
![Screenshot_20220331_135044](https://user-images.githubusercontent.com/61316/161059870-0a14f133-dbdc-4867-86f2-64339e5770e4.png)
but the dtype, shape and/or number of dimensions could be added as a caption too.

# `_repr_pretty_`

If a HTML output is not desirable, then at least implementing `_repr_pretty_` would be nice as it makes interactive use of NumPy easier. This could just return the result of `__str__`.

Changing the `repr` of course comes with backward-compatibility constraints (doctests etc.), but the output of these methods would be more free of this and we could add features as needed.

# Prior art

This sort of thing has been done in NumPy previously with [displaying polynomials as LaTeX](https://github.com/numpy/numpy/pull/11528). It is also used by Pandas to format `DataFrame`s nicely in notebooks.

# Way forwards

I'm happy to do the work to implement this, but I wanted to check first whether this was something that would be welcome before putting in the time.",2022-03-31 13:15:58,,ENH: Add support for IPython pretty printers to ndarray,['01 - Enhancement']
21261,open,maxnoe,"### Describe the issue:

Scalar bitshifts seem not to be implemented and are either upcasting to int64 or failing to upcast (for uint64).

### Reproduce the code example:

```python
import numpy as np


dtypes = np.uint16, np.uint32, np.uint64


for dtype in dtypes:
    scalar = dtype(15)
    array = np.array([15], dtype=dtype)

    print(""dtype = "", dtype)

    print(""Scalar right shift: "", end="""")
    try:
        result = scalar >> 1
        print(result, type(result))
    except:
        print(""Error"")


    print(""Scalar left shift: "", end="""")
    try:
        result = scalar << 1
        print(result, type(result))
    except:
        print(""Error"")


    print(""Array right shift: "", end="""")
    try:
        result = array >> 1
        print(result, result.dtype)
    except:
        print(""Error"")

    print(""Array left shift: "", end="""")
    try:
        result = array << 1
        print(result, result.dtype)
    except:
        print(""Error"")

    print()

```


### Error message:

```shell
❯ python bitshift_dtypes.py
dtype =  <class 'numpy.uint16'>
Scalar right shift: 7 <class 'numpy.int64'>
Scalar left shift: 30 <class 'numpy.int64'>
Array right shift: [7] uint16
Array left shift: [30] uint16

dtype =  <class 'numpy.uint32'>
Scalar right shift: 7 <class 'numpy.int64'>
Scalar left shift: 30 <class 'numpy.int64'>
Array right shift: [7] uint32
Array left shift: [30] uint32

dtype =  <class 'numpy.uint64'>
Scalar right shift: Error
Scalar left shift: Error
Array right shift: [7] uint64
Array left shift: [30] uint64
``
```


### NumPy/Python version information:

❯ python -c 'import sys, numpy; print(numpy.__version__, sys.version)'
1.21.5 3.10.2 (main, Jan 15 2022, 19:56:27) [GCC 11.1.0]",2022-03-29 13:37:47,,BUG: Bitshift operations on scalar integers change type to int64 or fail,['00 - Bug']
21239,open,Ruibin-Liu,"### Describe the issue:

Tried to make sure my `numpy` is installed correctly and found the `AssertionError` during testing.

### Reproduce the code example:

```python
import numpy as np
np.test()
```


### Error message:

```shell
self = <numpy.distutils.tests.test_system_info.TestSystemInfoReading object at 0x1507e76d14f0>

    def test_overrides(self):
        previousDir = os.getcwd()
        cfg = os.path.join(self._dir1, 'site.cfg')
        shutil.copy(self._sitecfg, cfg)
        try:
            os.chdir(self._dir1)
            # Check that the '[ALL]' section does not override
            # missing values from other sections
            info = mkl_info()
            lib_dirs = info.cp['ALL']['library_dirs'].split(os.pathsep)
            assert info.get_lib_dirs() != lib_dirs
    
            # But if we copy the values to a '[mkl]' section the value
            # is correct
            with open(cfg, 'r') as fid:
                mkl = fid.read().replace('ALL', 'mkl')
            with open(cfg, 'w') as fid:
                fid.write(mkl)
            info = mkl_info()
            assert info.get_lib_dirs() == lib_dirs
    
            # Also, the values will be taken from a section named '[DEFAULT]'
            with open(cfg, 'r') as fid:
                dflt = fid.read().replace('mkl', 'DEFAULT')
            with open(cfg, 'w') as fid:
                fid.write(dflt)
            info = mkl_info()
>           assert info.get_lib_dirs() == lib_dirs
E           AssertionError: assert ['/home/rliu/anaconda3/lib'] == ['/tmp/tmpa4n.../tmpdrk7ha99']
E             At index 0 diff: '/home/rliu/anaconda3/lib' != '/tmp/tmpa4n9hm2s'
E             Right contains one more item: '/tmp/tmpdrk7ha99'
E             Full diff:
E             - ['/tmp/tmpa4n9hm2s', '/tmp/tmpdrk7ha99']
E             + ['/home/rliu/anaconda3/lib']

cfg        = '/tmp/tmpa4n9hm2s/site.cfg'
dflt       = '\n[DEFAULT]\nlibrary_dirs = /tmp/tmpa4n9hm2s:/tmp/tmpdrk7ha99\nlibraries = /tmp/tmpa4n9hm2s/libfoo.so,/tmp/tmpdrk7ha9.../tmpdrk7ha99\n\n[duplicate_options]\nmylib_libs = /tmp/tmpa4n9hm2s/libfoo.so\nlibraries = /tmp/tmpdrk7ha99/libbar.so\n'
fid        = <_io.TextIOWrapper name='/tmp/tmpa4n9hm2s/site.cfg' mode='w' encoding='UTF-8'>
info       = <numpy.distutils.system_info.mkl_info object at 0x1507e75f1f10>
lib_dirs   = ['/tmp/tmpa4n9hm2s', '/tmp/tmpdrk7ha99']
mkl        = '\n[mkl]\nlibrary_dirs = /tmp/tmpa4n9hm2s:/tmp/tmpdrk7ha99\nlibraries = /tmp/tmpa4n9hm2s/libfoo.so,/tmp/tmpdrk7ha99/li.../tmpdrk7ha99\n\n[duplicate_options]\nmylib_libs = /tmp/tmpa4n9hm2s/libfoo.so\nlibraries = /tmp/tmpdrk7ha99/libbar.so\n'
previousDir = '/home/rliu'
self       = <numpy.distutils.tests.test_system_info.TestSystemInfoReading object at 0x1507e76d14f0>

anaconda3/lib/python3.9/site-packages/numpy/distutils/tests/test_system_info.py:284: AssertionError
```


### NumPy/Python version information:

`numpy` version: 1.20.3
`python` version: 3.9.7",2022-03-24 17:55:42,,BUG: tests failed with AssertionError from test_system_info.py,['00 - Bug']
21223,open,jhgoebbert,"### Describe the issue:

Calling `import numpy` at the same time in two different threads can lead to a race-condition
This happens for example with Xpra when loading the encoder nvjpeg: 
```
2022-03-20 12:54:59,298  cannot load enc_nvjpeg (nvjpeg encoder)
Traceback (most recent call last):
  File ""<pythondir>/lib/python3.9/site-packages/xpra/codecs/loader.py"", line 52, in codec_import_check
    ic =  __import__(class_module, {}, {}, classnames)
  File ""xpra/codecs/nvjpeg/encoder.pyx"", line 8, in init xpra.codecs.nvjpeg.encoder
  File ""<pythondir>/lib/python3.9/site-packages/numpy/__init__.py"", line 150, in <module>
    from . import core
  File ""<pythondir>/lib/python3.9/site-packages/numpy/core/__init__.py"", line 51, in <module>
    del os.environ[envkey]
  File ""<pythondir>/lib/python3.9/os.py"", line 695, in __delitem__
    raise KeyError(key) from None
KeyError: 'OPENBLAS_MAIN_FREE'
```

The problem seems to come from numpy directly.

Here the environment variable OPENBLAS_MAIN_FREE is set:
https://github.com/numpy/numpy/blob/maintenance/1.21.x/numpy/core/__init__.py#L18
and short after that it is deleted
https://github.com/numpy/numpy/blob/maintenance/1.21.x/numpy/core/__init__.py#L51
But this deletion fails ... 

To me this looks like a threading issue in numpy. A lock would need to be set here.

### Reproduce the code example:

```python
Here the environment variable OPENBLAS_MAIN_FREE is set:
https://github.com/numpy/numpy/blob/maintenance/1.21.x/numpy/core/__init__.py#L18
and short after that it is deleted
https://github.com/numpy/numpy/blob/maintenance/1.21.x/numpy/core/__init__.py#L51

If two threads call this funtion at the same time we get a race-condition.
The deletion fails ...
```


### Error message:

```shell
File ""<pythondir>/lib/python3.9/site-packages/numpy/core/__init__.py"", line 51, in <module>
    del os.environ[envkey]
  File ""<pythondir>/lib/python3.9/os.py"", line 695, in __delitem__
    raise KeyError(key) from None
KeyError: 'OPENBLAS_MAIN_FREE'
```
```


### NumPy/Python version information:

numpy 1.21.3
Python 3.9.6",2022-03-20 12:21:01,,BUG: Calling `import numpy` at the same time in two different threads can lead to a race-condition,['00 - Bug']
21220,open,h-vetinari,"The docstring of `all_close` [says](https://github.com/numpy/numpy/blob/v1.22.3/numpy/testing/_private/utils.py#L1476-L1480):
> Raises an AssertionError if two objects are not equal up to desired tolerance.

It's therefore IMO not unreasonable to assume that it should be possible to compare whether given arrays of integers and floats are within said tolerance of each other.

However, if the integers do not fit into the standard integer types anymore (which is still reasonable to compare against floats though!), this bombs with some casting error:
```
>>> import numpy as np
>>> from numpy.testing import assert_allclose
>>> from scipy.special import factorial
>>> assert_allclose(factorial(100), factorial(100, exact=True))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\[...]\.conda\envs\dev\lib\site-packages\numpy\testing\_private\utils.py"", line 1530, in assert_allclose
    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
  File ""C:\Users\[...]\.conda\envs\dev\lib\site-packages\numpy\testing\_private\utils.py"", line 792, in assert_array_compare
    val = comparison(x, y)
  File ""C:\Users\[...]\.conda\envs\dev\lib\site-packages\numpy\testing\_private\utils.py"", line 1525, in compare
    return np.core.numeric.isclose(x, y, rtol=rtol, atol=atol,
  File ""<__array_function__ internals>"", line 180, in isclose
  File ""C:\Users\[...]\.conda\envs\dev\lib\site-packages\numpy\core\numeric.py"", line 2359, in isclose
    yfin = isfinite(y)
TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''

# for reference
>>> factorial(100)
9.332621544394415e+157
>>> np.finfo(np.float64).max
1.7976931348623157e+308
>>> np.__version__
'1.22.3'
```

The same thing happens if the values are wrapped into `np.array()`. or `np.array([])`.

CC @seberg",2022-03-19 06:37:17,,BUG: assert_allclose chokes on large integers,['unlabeled']
21219,open,h-vetinari,"While writing tests for `factorial2` in scipy, I found that `np.prod` silently creates garbage values. I'm pretty sure it has to do with silently overflowing and wrapping with int32, hence it's probably relevant that the below is happening on windows.

```
>>> from functools import reduce
>>> from operator import mul
>>> import numpy as np
>>> from scipy.special import factorial2
>>> n = 20
>>> factorial2(n, exact=True)
3715891200
>>> reduce(mul, list(range(n, 0, -2)), 1)
3715891200
>>> np.prod(list(range(n, 0, -2)))
-579076096
>>> np.iinfo(np.int32).max
2147483647
>>> np.iinfo(np.int32).min + (3715891200 - (np.iinfo(np.int32).max + 1))
-579076096
>>> np.__version__
'1.22.3'
```

IMO this should implicitly bump the type (and error loudly when overflow happens...)
",2022-03-19 06:18:02,,BUG: silent overflow in np.prod,['unlabeled']
21213,open,honno,"### Describe the issue:

The Array API spec has the following special case [`__pow__()`](https://data-apis.org/array-api/latest/API_specification/generated/signatures.array_object.array.__pow__.html)/ `__ipow__()`

> If `x1_i`  is `-0`, `x2_i` is greater than `0`, and `x2_i` is not an odd integer value, the result is `+0`.

`array_api` arrays currently follow this special case for `__pow__()`—`__ipow__()`  seems to likewise follow this case _except_ when `x2_i=.5`, where a _negative_ 0 is returned instead. cc @asmeurer @seberg

Also discovered a similar problem when `self=-inf` https://github.com/numpy/numpy/issues/21213#issuecomment-1072257597

### Reproduce the code example:

```python
>>> x1 = xp.asarray(-0.)
>>> x2 = xp.asarray(0.5)
>>> x1 ** x2
Array(0., dtype=float64)
>>> x1 **= x2
>>> x1
Array(-0., dtype=float64)  # should be positive 0
...
>>> x3 = xp.asarray(-0.)
>>> x4 = xp.asarray(2.)
>>> x3 **= x4; x3
Array(0., dtype=float64)  # as expected
...
>>> x5 = xp.asarray(-0.)
>>> x6 = xp.asarray(2.5)
>>> x5 **= x6; x5
Array(0., dtype=float64)  # as expected
...
>>> x7 = xp.asarray(-0.)
>>> x8 = xp.asarray(.6)
>>> x7 **= x8; x7
Array(0., dtype=float64)  # as expected
```


### Error message:

_No response_

### NumPy/Python version information:

1.22.3 3.8.12 (default, Mar 13 2022, 19:12:08) 
[GCC 9.4.0]",2022-03-17 18:57:37,,"BUG: Incorrect `__ipow__()` result for `array_api` arrays when `self` is `-0.` or `-inf`, and `other=0.5`","['00 - Bug', 'component: numpy.array_api']"
21194,open,YairMZ,"### Describe the issue:

It may not be a bug (as it may be by design), but I think it is.

Trying to set the output of `numpy.sign` to integers with a floating-point input raises an error due to casting.
The same casting on the original array (without calling the sign function) works.

If it is indeed deemed as a bug, I'll be happy to submit a fix, but I may need some guidance as this is my first issue here.

The workaround I show below can resolve the issue. However, the behavior is inconsistent as casting is allowed when not calling the `sign`, especially, as by the mathematical definition of the sign function, this casting will not result in any loss in precision.

### Reproduce the code example:

```python
import numpy as np
a = np.array(list(range(-5, 5)),dtype=np.float_)  # a is floating point array
b = np.sign(a, dtype=np.int_) # this row raises an exception
c = np.array(a, dtype=np.int_) # this works
d = np.sign(a, dtype=np.int_,,casting='unsafe') # a workaround
```


### Error message:

```shell
Traceback (most recent call last):
  File ""/Users/yairmazal/.virtualenv/LDPC/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3251, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-4-719e498ab119>"", line 1, in <module>
    b = np.sign(a, dtype=np.int_)
numpy.core._exceptions._UFuncInputCastingError: Cannot cast ufunc 'sign' input from dtype('float64') to dtype('int64') with casting rule 'same_kind'
```


### NumPy/Python version information:

1.22.1 3.9.10 (main, Jan 15 2022, 11:48:00) 
[Clang 13.0.0 (clang-1300.0.29.3)]",2022-03-14 09:51:54,,BUG: Casting issue when using sign,['00 - Bug']
21174,open,andriyor,"### Proposed new feature or change:

https://github.com/matplotlib/matplotlib/pull/18971

related issue with versioneer https://github.com/python-versioneer/python-versioneer/issues/193",2022-03-10 17:48:03,,Suggestion to switch from versioneer to setuptools_scm as matplotlib did it,['unlabeled']
21164,open,seberg,"Since a very long time (predating clang, I believe), NumPy uses `NPY_GCC_OPT_3` in a few places.  This macro is useful to locally enable a high optimization level for functions we know should be optimized:  tight, simple (usually 1-D) loops.

However, due to its age, the macro only applies to `GCC` (unless clang picks it up?).  It would be nice to generalize the macro a bit to other compilers, probably using `#pragma` depending on the compilers.
This may need some care, since different compilers have different ideas of what `O3` means, IIRC.  So it may be that e.g. the Intel compiler enables unsafe fast-math when GCC does not.

The task are:
1. Check how various compilers change the optimization level for a single function
2. Add additional branches for those compilers to the `#define` (maybe renaming it)
3. Check benchmarks of functions that should modified it with the compilers in question
4. Run the test suite
5. Double check the compiler documentation to be sure that no unsafe fast-math is enabled.  We cannot trust our test-suite on all accounts (e.g. floating point error flags).

In some cases functions that currently use this, may end up as universal-intrinsics eventually.  But I somewhat expect that this macro will stay useful for things where maximum performance is less important or just as a stop gap, because it adds no complexity.

EDIT: I expect this is a fairly nice sprintable thing to investigate, although it is best if an MSVC setup is available.  (Clang likely supports the gcc attributes, but I am not sure.  This is a useful reference probably: https://stackoverflow.com/questions/31373885/how-to-change-optimization-level-of-one-function.)",2022-03-07 15:39:54,,ENH: Investigate `NPY_GCC_OPT_3` macro for other compilers,"['01 - Enhancement', 'sprintable']"
21163,open,riedgar-ms,"### Describe the issue:

I'm having trouble with `np.unique()` when the source of the data is a pandas DataFrame. It appears that the datatype is not being correctly identified from pandas.

### Reproduce the code example:

```python
import numpy as np
import pandas as pd

data = [np.NaN, 'a', 'a', 'b']
df = pd.DataFrame({'col': data })

np.unique(data) # This works
np.unique(df['col']) # This fails
```


### Error message:

```shell
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<__array_function__ internals>"", line 180, in unique
  File ""<snip>\numpy\lib\arraysetops.py"", line 272, in unique
    ret = _unique1d(ar, return_index, return_inverse, return_counts)
  File ""<snip>\numpy\lib\arraysetops.py"", line 333, in _unique1d
    ar.sort()
TypeError: '<' not supported between instances of 'str' and 'float'
```


### NumPy/Python version information:

1.22.2 3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:19:05) [MSC v.1916 64 bit (AMD64)]",2022-03-07 14:13:04,,BUG: unique failing when data extracted from DataFrame,['33 - Question']
21161,open,HaoZeke,"### Proposed new feature or change:

#20881 highlighted and #20884 elaborated on the fact that due to historical reasons, there is some duplication within the NumPy-C and F2PY generated C code.

These should be reduced for maximally efficient maintainable code. For over a decade now, F2PY is distributed with NumPy and so this issue stipulates that instead of `Python.h` and `typedef` structures in F2PY, we should instead depend explicitly on `<numpy/npy_common.h>` as discussed [here](https://github.com/numpy/numpy/pull/20884#discussion_r790675831).",2022-03-06 23:05:42,,ENH: F2PY harmonization with NumPy-C API,['component: numpy.f2py']
21160,open,HaoZeke,"### Proposed new feature or change:

Post #20770 there are some points remaining:

- [x] Scalar numeric types (c_float, c_double)
- [ ] Derived types containing arrays of numeric data
- [ ] Arrays of derived types

The general implementation is to have two separate implementation structures:
- One for the intrinsic types which are defined to map to C structures
- Another, which is possibly more flexible but also slightly more fragile, is the opaque pointer mechanism",2022-03-06 22:54:29,,ENH: Derived type support checklist,['component: numpy.f2py']
21147,open,simehaa,"### Describe the issue:

When using `np.searchsorted(a, v, side=""right"")`, if both `v` is of dtype=object, _which can often be the case when dealing with pandas dataframes_, and an element in `v` is NaN, then unexpected behaviour occurs.

The resulting array correctly gives the NaN element `len(a)` as the bin value, however, also the subsequent two elements are wrongly given `len(a)` as values.


### Reproduce the code example:

```python
import numpy as np

# Correct behaviour, dtype=float or np.float64
a = np.array([0, 5, 10, 15])
v = np.array([9.0, 11.0, np.NaN, 2.0, 13.0, 6.0], dtype=float)
print(np.searchsorted(a, v, side=""right"")) # [2 3 4 1 3 2]

# Wrong behaviour
v = np.array([9.0, 11.0, np.NaN, 2.0, 13.0, 6.0], dtype=object)
# The two subsequent elements after the NaN are also given the bin len(a)
print(np.searchsorted(a, v, side=""right"")) # [2 3 4 4 4 2]

# However, side=""left"" works
print(np.searchsorted(a, v, side=""left"")) # [2 3 0 1 3 2]
```


### Error message:

_No response_

### NumPy/Python version information:

Tested with Python 3.10.2 and NumPy 1.22.2",2022-03-03 10:14:39,,BUG: searchsorted has unexpected behaviour for object datatype,['00 - Bug']
21136,open,kadykov,"### Proposed new feature or change:

Numpy has many functions that support operations over multiple dimensions like `min`, `max`, `mean`, `average`, `sum`, etc.
All of them has `axis `parameter to specify the axes along which the function has to be applied.

In most cases, the accepted types for this parameter are `None`, `int`, and a `tuple of ints`, like in `ptp`, `percentile`, `nanpercentile`, `quantile`, `nanquantile`, `average`, `mean`, `std`, `var`, `nanmean`, `nanstd`, `nanvar`, `sum`, `prod`, `nanprod`, `nansum`, and `gradient`.

Is there any reason not to use a `sequence of ints` for the `axis` parameter where a `tuple of ints` is allowed?

For instance, the functions `median` and `nanmedian` support a `sequence of ints`, therefore users can use `sets` and `frozensets` to skip unnecessary checks for repeated values.",2022-03-02 14:11:24,,"ENH: Use sequence of ints instead of tuple of ints in the axis parameter for mean, sum, min, max, etc.",['unlabeled']
21135,open,vnmabus,"### Proposed new feature or change:

I was testing how to add support for the array API standard to a small project of mine. However, I also wanted to remain compatible with Numpy `ndarray`, as it is what everyone uses right now. However, the differences in the API between the standard and `ndarray`, and the decision to conform to the minimal implementation of the standard, make really difficult to support both use cases with the same code.

For example, in order to create a copy of the input array, which should be a simple thing to do, using the array standard I would need to call `x = xp.asarray(x, copy=True)`. However, when I receive a numpy ndarray I fall back to `xp = np`, and `np.asarray` does not have the `copy` parameter.

I can't just convert the `ndarray` to `Array`, both because I would be returning a different type than the input type, and because `Array` does not allow the `object` dtype, and I explicitly allow `ndarray`s containing `Fraction` or `Decimal` objects.

The ideal choice would be either to make the basic Numpy `ndarray` functions compatible with the standard, or to expose an advanced version of the `array_api` that deals directly with `ndarray` objects and support all Numpy functionalities in a manner compatible with the standard. Otherwise, making existing code compatible with both `ndarray` and the API standard will require a lot of effort and duplicated code to accommodate both.",2022-03-02 12:41:30,,ENH: Array API standard and Numpy compatibility,['component: numpy.array_api']
21094,open,QuLogic,"### Proposed new feature or change:

PowerPC is [transitioning from AIX longdouble to IEEE longdouble](https://fedoraproject.org/wiki/Changes/PPC64LE_Float128_Transition). Fortunately, NumPy's [check for longdouble format](https://github.com/numpy/numpy/blob/main/numpy/core/setup_common.py#L348) works through this transition and does not break any tests.

However, `test_ppc64_ibm_double_double128` is keyed only on ppc64, and has started failing when the system uses IEEE long doubles (since they're 'normal' and don't have the semantics that this test is checking.) I don't know how to determine (at runtime) what long double format NumPy determined (at build time), but if possible, this test should check that and only run for AIX long doubles.",2022-02-19 22:59:58,,ENH: Support IEEE long double on ppc64 in tests,['unlabeled']
21092,open,gitgithan,"### Describe the issue:

I want to have empty list as default value of np.select.

When the default value is selected to be an empty list or tuple, `choicelist` will be overwritten from `[array(1), array(2), array(3), array([], dtype=float64)]`  to `[array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=float64)]` 
after `choicelist = np.broadcast_arrays(*choicelist)` in `function_base.py`.

The result is that I get `array([], dtype=float64)` returned when i expect 3 to be returned (because the conditions are False,False,True corresponding to 1,2,3).



### Reproduce the code example:

```python
import numpy as np

conds = [False, 
         False, 
         True]

print(np.select(conds, [1,2,3], default=[]))
```
```


### Error message:

_No response_

### NumPy/Python version information:

1.21.4 3.8.12 (default, Dec  8 2021, 11:22:00) 
[Clang 12.0.5 (clang-1205.0.22.11)]",2022-02-19 08:53:55,,BUG: np.select does not return values in choicelist when default used is empty list,"['00 - Bug', '04 - Documentation']"
21091,open,jessexknight,"### Describe the issue:

1) When one or more `inf` or `-inf` are present in the argument to `np.quantile` (or `np.nanquantile`), the results often include `nan`, when `+/-inf` could be reasonably returned -- e.g. if there are 10 `-inf`s in 100-long `x`, `np.quantile(x,.05)` should probably return `-inf`, not `nan`.

2) `np.quantile(-inf,0)` =/= `np.quantile(-inf,0.)` (int vs float, and similarly for `1` vs `1.`)

3) The behaviour for `-inf` and `inf` is possibly different in some situations -- e.g. compare actual outputs 7 vs 9 below: the median in 7 that averages 2 and `inf` returns `nan` while the median in 9 that averages `-inf` and 3 returns `-inf`.

Likely related: #12282

### Reproduce the code example:

```python
import numpy as np
inf = np.inf
nan = np.nan
eps = 1e-9
x_pos_even = [1,2,inf,inf]
x_pos_odd  = [1,2,3,inf,inf]
x_neg_even = [-inf,-inf,3,4]
x_neg_odd  = [-inf,-inf,3,4,5]
q_even     = [0,1/3,2/3,1]
q_odd      = [0,.25,.5,.75,1]
printfun = lambda r: print(np.round(r,2))
printfun(np.quantile(x_neg_even,0))
printfun(np.quantile(x_neg_even,0.))
printfun(np.quantile(x_pos_even,q_even))
printfun(np.quantile(x_pos_odd, q_even))
printfun(np.quantile(x_neg_even,q_even))
printfun(np.quantile(x_neg_odd, q_even))
printfun(np.quantile(x_pos_even,q_odd))
printfun(np.quantile(x_pos_odd, q_odd))
printfun(np.quantile(x_neg_even,q_odd))
printfun(np.quantile(x_neg_odd, q_odd))

# expected output -- nan* = truly undefined behaviour, though could possibly be +/- inf
# -inf
# -inf
# [ 1.  , 2.  , inf , inf ]
# [ 1.  , 2.33, nan*, inf ]
# [-inf ,-inf , 3.  , 4.  ]
# [-inf , nan*, 3.67, 5.  ]
# [ 1.  , 1.75, nan*, inf , inf ]
# [ 1.  , 2.  , 3.  , inf , inf ]
# [-inf ,-inf , nan*, 3.25, 4.  ]
# [-inf ,-inf , 3.  , 4.  , 5.  ]

# actual output (spacing adjusted for readability)
# -inf
# nan
# [ 1.  , nan , nan , nan ]
# [ 1.  , 2.33, nan , nan ]
# [ nan , nan , 3.  , 4.  ]
# [ nan , nan , 3.67, 5.  ]
# [ 1.  , 1.75, nan , nan , nan ]
# [ 1.  , 2.  , nan , nan , nan ]
# [ nan , nan ,-inf , 3.25, 4.  ]
# [ nan , nan , 3.  , 4.  , 5.  ]
```


### Error message:

```shell
/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py:4486: RuntimeWarning: invalid value encountered in subtract
  diff_b_a = subtract(b, a)
/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py:4488: RuntimeWarning: invalid value encountered in multiply
  lerp_interpolation = asanyarray(add(a, diff_b_a * t, out=out))
/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py:4489: RuntimeWarning: invalid value encountered in subtract
  subtract(b, diff_b_a * (1 - t), out=lerp_interpolation, where=t >= 0.5)
/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py:4488: RuntimeWarning: invalid value encountered in add
  lerp_interpolation = asanyarray(add(a, diff_b_a * t, out=out))
```


### NumPy/Python version information:

1.22.2 3.8.10 (default, Nov 26 2021, 20:14:08) 
[GCC 9.3.0]
",2022-02-19 03:33:13,,BUG: inf in quantile has undefined behaviour (and possibly different for -inf vs +inf),['00 - Bug']
21078,open,rdebroiz,"### Issue with current documentation:

I run into a performance problem I had difficulties to diagnostic. Performance was linearly decreasing as i was increasing the number of process performing matrix computations. 

The problem is that I reach the CPU's cache limits of my machines. It was not obvious to find out for a neophyte like me.

I saw this post on SO which give a good description of my problem (after i found it by myself :/, maybe i should reconsider my research skills ).
https://stackoverflow.com/questions/29358872/inefficient-multiprocessing-of-numpy-based-calculations

### Idea or request for content:

It would be great to add a section about performance in the documentation describing what can be the cause of bad performances.
And maybe also hints about what could be done to improve them.",2022-02-17 14:01:23,,DOC: Give hints about causes of bad performances,['04 - Documentation']
21070,open,rossbar,"The `runtests.py` script has a `--doctests` command line option with the following description:

>   --doctests            Run doctests in module

However, AFAICT this always raises a `ValueError`. I tried several invocations: `python runtests.py --doctests`, `python runtests.py --doctests numpy/lib/histograms.py` (in an attempt to limit the tests to a single module). No matter what, this seems to call down to [_pytesttester.py](https://github.com/numpy/numpy/blob/5a53fa9326047e33c251d238a6ce344980cbe038/numpy/_pytesttester.py#L169-L170) which simply raises a `ValueError` saying that doctests are not supported. 

I assume this is just a vestigial option that should be removed from `runtests.py`, but I wanted to check if this was expected to work.",2022-02-16 05:12:20,,DEV: `--doctests` option to `runtests.py` doesn't do anything,['16 - Development']
21069,open,rossbar,"Recently there was an instance of a doctest failure caught in a downstream project that was not caught by the numpy test suite: see #21064 .

It's worth checking that the infrastructure for picking up and running doctests is appropriately hitting the ufunc docstrings defined in `numpy/core/code_generators/ufunc_docstrings.py`.",2022-02-16 03:27:07,,TASK: Verify doctest is running on ufunc docstring examples,['17 - Task']
21065,open,igordertigor,"### Describe the issue:

When comparing dtypes to sets and lists of dtypes, I observe inconsistent behaviour. When checking if a dtype is in a set of dtypes, the result is always `False`. Checking against a list of dtypes this is not the case.

This might be related to #6295

### Reproduce the code example:

```python
import numpy as np

uint_types = {np.uint8, np.uint16, np.uint32}
a = np.array([0], np.uint8)

assert a.dtype in uint_types, ""This fails""

# However:
assert a.dtype in list(uint_types), ""This passes""
```


### Error message:

_No response_

### NumPy/Python version information:

numpy version: 1.21.5
python version: 3.8.10 (default, Nov 26 2021, 20:14:08) \n[GCC 9.3.0]",2022-02-15 15:59:24,,"BUG: Checking if dtype is in a set of dtypes fails, but same check with list passes.",['00 - Bug']
21014,open,jbrockmendel,"The `__init__.pxd` file that downstream libraries use to access the numpy C API via cython has the old PyArray_IterNew etc but not the new NpyIter_New etc.

It'd be nice to have the newer ones.

Side-note: it'd be really convenient to have a nicer API for getitem/setitem for cnp.flatiter and cnp.broadcast objects.  At the moment doing a ND-compatible `out[i] = func(values[i])` requires:

```
item = <object>(<PyObject**>cnp.PyArray_MultiIter_DATA(mi, 1))[0]

res = func(item)

(<int64_t*>cnp.PyArray_MultiIter_DATA(mi, 0))[0] = res
```
",2022-02-08 00:16:26,,BUG: add NpyIter_New etc to __init__.pxd ,['00 - Bug']
21012,open,alex-rakowski,"### Describe the issue:

`np.tensordot` can overflow if the dtypes of the arrays are not sufficient for the output. `np.dot` and `np.einsum` allow using a preallocated `out` array, and `np.einsum` allows forcing the calculation to use a specified `dtype`. 

It would be useful have this flexibility in `np.tensordot` to avoid overflow or at least highlight the potential for this error in the docs.

As background we use use `np.tensordot` and `np.einsum` to multiple reduce a 4D dataset with a 2D boolean mask, along the last two axes of the 4D array.  

### Reproduce the code example:

```python
import numpy as np
# looking to reduce 4D dataset to 2D
# by applying 2D boolean mask along last two axes of 4D dataset
# dtype is np.uint16

# make a fake array, using (1<<16)-1 to exemplify the issue
# array sizes are typically ~ 100x100x1024x1024 to 100x100x4098x4098
a = np.array([(1<<16) -1], dtype=np.uint16).repeat(100).reshape(1,1,10,10)
# make a boolean mask
b = np.zeros((10,10), dtype=np.bool_)
b[4:7,4:7] = True

# uint16 [[65527]]
c_uint = np.tensordot(a,b, axes=((2,3),(0,1)))
print(c_uint.dtype, c_uint)

a = np.array([(1<<16) -1], dtype=np.int64).repeat(100).reshape(1,1,10,10)

b = np.zeros((10,10), dtype=np.bool_)

b[4:7,4:7] = True

c_int64 = np.tensordot(a,b, axes=((2,3),(0,1)))

# int64 [[589815]]
print(c_int64.dtype, c_int64)


#### Using np.einsum ####

# without forcing calculation dtype or preallocating results in same error
a = np.array([(1<<16) -1], dtype=np.uint16).repeat(100).reshape(1,1,10,10)
b = np.zeros((10,10), dtype=np.bool_)
b[4:7,4:7] = True
c_einsum = np.einsum('ijnm,nm->ij', a,b)
# uint16 [[65527]]
print(c_einsum.dtype, c_einsum)

# Forcing calculation datatype
a = np.array([(1<<16) -1], dtype=np.uint16).repeat(100).reshape(1,1,10,10)
b = np.zeros((10,10), dtype=np.bool_)
b[4:7,4:7] = True
c_einsum = np.einsum('ijnm,nm->ij', a,b, dtype=np.int64)
# float64 [[589815]]
print(c_einsum.dtype, c_einsum)

# Preallocating output array
a = np.array([(1<<16) -1], dtype=np.uint16).repeat(100).reshape(1,1,10,10)
b = np.zeros((10,10), dtype=np.bool_)
b[4:7,4:7] = True
c_out = np.empty((1,1), dtype=np.int64)
np.einsum('ijnm,nm->ij', a,b, out=c_out)
# int64 [[589815]]
print(c_out.dtype, c_out)
```


### Error message:

_No response_

### NumPy/Python version information:

Multiple platforms tested: 

> 1.19.5 3.8.11 (default, Aug 16 2021, 12:04:33) [Clang 12.0.0 ]

> 1.21.5 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) [GCC 9.4.0]

> 1.19.1 3.8.5 (default, Sep  4 2020, 07:30:14) [GCC 7.3.0]
",2022-02-07 22:31:51,,BUG: np.tensordot overflows ,['01 - Enhancement']
20998,open,astrojuanlu,"### Issue with current documentation:

I was reading the wonderful [Interoperability with NumPy](https://numpy.org/devdocs/user/basics.interoperability.html) guide introduced in #20185 (thanks!) and it does a very good job at explaining what the user _can_ do, by giving (a) detailed explanations of what the different methods do and (b) showing examples from popular libraries that apply such methods.

However, if one dives deep enough there seems to be some conflicting information. For example, NEP 18 (2018) says:

> There are no general requirements on the return value from `__array_function__`, although most sensible implementations should probably return array(s) with the same type as one of the function's arguments.

However, NEP 37 (2019) recollects

> `__array_function__` has significant implications for libraries that use it: [...] users expect NumPy functions like `np.concatenate` to return NumPy arrays. [...] Libraries like Dask and CuPy have looked at and accepted the backwards incompatibility impact of `__array_function__`; it would still have been better for them if that impact didn't exist"".

Therefore, from my understanding (and I'd love to be corrected here if I'm wrong!) even though NEP 18 left the return value of `__array_function__` loosely defined, there was some friction when projects tried to adopt it. I think these findings that were only possible after putting the code in the hands of users are extremely valuable for new library authors.

In a way, I believe this goes to the core of something the guide briefly mentions at the beginning (inspiration from the chart at the top of https://github.com/data-apis/array-api/issues/1): the difference between ""array providers"" (CuPy, pydata/sparse, dask.array) and ""array consumers"" (xarray), including the gray area between the two (pandas Series consume NumPy arrays but they also offer an array interface, and same goes to astropy.units).

### Idea or request for content:

Expand the current interoperability guide (or add another guide/cookbook/whatever) that addresses practical questions like

- Should new implementers of NEP 18 return ndarrays, or custom classes?
- What is the best way to make your array consumer library interoperate with several array providers?
- Should your custom class adopt the Python array API standard?
- What are the performance implications of each dispatch mechanism?

(random questions off the top of my head)

If folks are happy to discuss these, either here or in the mailing list, I'd be happy to try to sort out those thoughts and contribute something to the docs myself.",2022-02-04 16:46:56,,DOC: Document interoperability best practices,['04 - Documentation']
20995,open,geniuskey,"### Proposed new feature or change:

I'm using bunch of large 2D arrays (about 10000 x 10000 ) and I need to compress and save data quickly. 
The problem is that the savez_compressed() function uses only default compress level of zipfile.ZipFile class.
(The default compress level of zipfile.ZIP_DEFLATED is '6')

If I could use compress level 1 for savez_compressed() function 

- w/ Compress level 1: Compression ratio: 66.8%, exec. time: about 3.2 seconds 
- w/ Compress level 6: Compression ratio: 69.5%, exec. time: about 24.7 seconds 
--> The compress level 1 is the most suitable for my project. 
    Therefore I want to configure the compression level in savez_compressed() function.



Here is my solution. I could solve it with adding just few lines.

my modification code snippet of _savez() function definition of the numpy/lib/npyio.py file.
```python
def _savez(file, args, kwds, compress, allow_pickle=True, pickle_kwargs=None):
    # Import is postponed to here since zipfile depends on gzip, an optional
    # component of the so-called standard library.
    import zipfile

    if not hasattr(file, 'write'):
        file = os_fspath(file)
        if not file.endswith('.npz'):
            file = file + '.npz'

    namedict = kwds
    for i, val in enumerate(args):
        key = 'arr_%d' % i
        if key in namedict.keys():
            raise ValueError(
                ""Cannot use un-named variables and keyword %s"" % key)
        namedict[key] = val

    if compress:
        compression = zipfile.ZIP_DEFLATED
    else:
        compression = zipfile.ZIP_STORED

    if 'compresslevel' in namedict:
        compresslevel = namedict['compresslevel']
        if not isinstance(compresslevel, int) or compresslevel < 1 or compresslevel > 9:
            compresslevel = None
    else:
        compresslevel = None

    zipf = zipfile_factory(file, mode=""w"", compression=compression, compresslevel=compresslevel)

    for key, val in namedict.items():
        fname = key + '.npy'
        val = np.asanyarray(val)
        # always force zip64, gh-10776
        with zipf.open(fname, 'w', force_zip64=True) as fid:
            format.write_array(fid, val,
                               allow_pickle=allow_pickle,
                               pickle_kwargs=pickle_kwargs)

    zipf.close()
```
It works!!



I haven't understand the github process from my idea to the merge in this kind of big open source repository cause I have no experience of github upstream activity yet. So... just let me share my idea through an issue.  

thanks.
",2022-02-04 08:04:08,,ENH: Add compresslevel input parameter in savez_compressed() function,['unlabeled']
20978,open,gtoombs-microsaas,"### Issue with current documentation:

The current documentation for [accumulate](https://numpy.org/devdocs/reference/generated/numpy.ufunc.accumulate.html#numpy.ufunc.accumulate) says:

> For a one-dimensional array, accumulate produces results equivalent to:

```python
r = np.empty(len(A))
t = op.identity        # op = the ufunc being applied to A's  elements
for i in range(len(A)):
    t = op(t, A[i])
    r[i] = t
return r
```

But this seems untrue. MRE:

```python
import numpy as np


def fun(prior: float, i: int) -> float:
    print(f'Prior should be 1000 but is instead {prior}')
    exit()


np.frompyfunc(
    fun, nin=2, nout=1, identity=1000,
).accumulate(np.arange(20))
```

### Idea or request for content:

Describe what actually happens: the identity is totally ignored, and the first call to the inner function is given the first two elements of the array.",2022-02-02 21:02:10,,DOC: Accumulate ignores identity,['04 - Documentation']
20962,open,obackhouse,"### Describe the issue:

For the following contraction, even in the case of `optimize=""optimal""` the optimal order of operations is not found. The output of the code example is:

```
  Complete contraction:  Qij,Qkk->ij
         Naive scaling:  4
     Optimized scaling:  4
      Naive FLOP count:  2.000e+09
  Optimized FLOP count:  2.000e+09
   Theoretical speedup:  1.000
  Largest intermediate:  1.000e+04 elements
--------------------------------------------------------------------------
scaling                  current                                remaining
--------------------------------------------------------------------------
   4                 Qkk,Qij->ij                                   ij->ij
```

when in fact a more optimal order of contractions would be to first trace over the second and third axes of `b`, with a lower overall scaling of 3 and fewer FLOPs, i.e.:

```
tmp = np.einsum(""Qkk->Q"", b)
res = np.einsum(""Qij,Q->ij"", a, tmp)
```

Is this a bug, or a side effect of intended behaviour?

### Reproduce the code example:

```python
import numpy as np

q = 1000
n = 100

a = np.random.random((q, n, n))
b = np.random.random((q, n, n))

for line in np.einsum_path(""Qij,Qkk->ij"", a, b, optimize='optimal'):
    print(line)
```


### Error message:

_No response_

### NumPy/Python version information:

1.20.1 3.7.0 | packaged by conda-forge | (default, Nov 12 2018, 20:15:55) 
[GCC 7.3.0]",2022-02-01 14:37:26,,BUG: `einsum` not optimizing an internal trace,['00 - Bug']
20951,open,nvedata,"### Describe the issue:

## Expected Behavior:
`np.cov(X, rowvar=False)` returns array with same number of dimensions as `X`

## Actual Behavior:
`np.cov(X, rowvar=False)` returns array with zero dimesions if `X.shape[1] == 1`



### Reproduce the code example:

```python
import numpy as np

# keep dimensions as expected
a = np.random.rand(100, 0)
assert np.cov(a, rowvar=False).ndim == a.ndim

c = np.random.rand(100, 2)
assert np.cov(c, rowvar=False).ndim == c.ndim

# doesn't keep dimensions
b = np.random.rand(100, 1)
assert np.cov(b, rowvar=False).ndim == b.ndim
```


### Error message:

_No response_

### NumPy/Python version information:

1.21.5 3.7.3 (default, Jan 22 2021, 20:04:44) 
[GCC 8.3.0]",2022-01-31 14:47:10,,BUG: numpy.cov doesn't keep dimensions for matrix with exactly one column,['00 - Bug']
20941,open,QuLogic,"### Describe the issue:

In #18658, `/usr/include` was removed from the include search path. This has broken searching for FFTW. It appears that the FFTW variants are the only ones that check for a header, or else other things would have broken.

### Reproduce the code example:

On a Fedora 35 container with only `python3-numpy` and `fftw3-devel` installed:

```python
$ python3 -m numpy.distutils.system_info -v fftw_info
set_threshold: setting threshold to DEBUG level, it can be changed only with force argument
fftw_info:
( library_dirs = /usr/local/lib64:/usr/local/lib:/usr/lib64:/usr/lib:/usr/lib/ )
( include_dirs = /usr/local/include )
new_compiler returns <class 'distutils.unixccompiler.UnixCCompiler'>
customize UnixCCompiler
(paths: )
(paths: )
(paths: /usr/lib64/libfftw3.so)
(paths: )
  fftw3 not found
( library_dirs = /usr/local/lib64:/usr/local/lib:/usr/lib64:/usr/lib:/usr/lib/ )
( include_dirs = /usr/local/include )
(paths: )
(paths: )
(paths: )
(paths: )
(paths: )
(paths: )
(paths: )
(paths: )
(paths: )
(paths: )
(paths: )
(paths: )
(paths: )
(paths: )
(paths: )
(paths: )
(paths: )
(paths: )
(paths: )
(paths: )
  libraries rfftw,fftw not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/']
  fftw2 not found
  NOT AVAILABLE
```

### NumPy/Python version information:

1.21.5 3.10.2 (main, Jan 17 2022, 00:00:00) [GCC 11.2.1 20211203 (Red Hat 11.2.1-7)]",2022-01-30 02:13:35,,BUG: fftw can no longer be found by np.distutils.system_info,"['00 - Bug', 'component: numpy.distutils']"
20933,open,maxnoe,"### Describe the issue:

When using a masked array of integer indexes as index for another array, also the masked values are used.

This was quite surprising to me, since normally, masked entries are ignored for all kinds of operations and we planned to use masked arrays to signal that for a given input, no valid index could be found.

### Reproduce the code example:

```python
import numpy as np

data = np.array([1.0, 2.0, 3.0])
index = np.ma.masked_array([0, 1, 2], mask=[False, True, False])

print(data[index])

assert len(data[index]) == 2, ""FAILS, masked values are not ignored.""
```


### NumPy/Python version information:

1.21.5 3.10.2 (main, Jan 15 2022, 19:56:27) [GCC 11.1.0]",2022-01-28 17:58:07,,BUG: Using a masked array as index into an array ignores the mask,"['00 - Bug', 'component: numpy.ma']"
20923,open,jbrockmendel,"### Proposed new feature or change:

When setting values into an ndarray, there are at least two ways to get a ValueError (1.22.1):

```
arr = np.arange(5)

>>> arr[[1, 2]] = [4, 5, 6]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ValueError: shape mismatch: value array of shape (3,) could not be broadcast to indexing result of shape (2,)

>>> arr[0] = ""foo""
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ValueError: invalid literal for int() with base 10: 'foo'
```

For the relevant (pandas) use case, I want to only catch the latter.  This would be simplified by having a ValueError subclass   e.g. `NonCastableValueError(ValueError)`",2022-01-27 23:50:17,,ENH: ValueError subclass for invalid ndarray.__setitem__ values,"['01 - Enhancement', 'Project']"
20905,open,paigeweber13,"# Describe the issue:

## Expected Behavior:
I expect to create a python `Fraction` from numpy integer types with no loss of precision

## Actual Behavior:
Types are coerced to `numpy.float64` and precision is lost

# Reproducible code example:

```python
import numpy as np
from fractions import Fraction

nums = np.array([1])
denoms = np.array([18446744073709550592], dtype='uint64')
f = Fraction(nums[0], denoms[0]) # denom has been rounded, is now a float
print(f) # 1.0/1.8446744073709552e+19
print(type(f.denominator)) # <class 'numpy.float64'>
```

# Error message:

_No response_

# NumPy/Python version information:

Numpy: 1.21.2 
Python: 3.7.9 (default, Aug 31 2020, 12:42:55) [GCC 7.3.0]
OS: Debian GNU/Linux 10 (buster)

# Other information that may be helpful:
I discovered this behavior while creating fractions from parallel arrays representing the numerators and denominators of fractions. This loss of precision is problematic because the float is rounded up so that it is out of range for `uint64`, despite the original value being valid. In my case I have a workaround, as converting the values to python `int` before inputting them to the `Fraction` constructor prevents this issue. However, I wanted to let you know because it is unexpected that an operation with two numpy integer types produced a value with floating point types.

I briefly looked into the `Fractions` constructor and I discovered that this behavior comes from lines 175-180 of `fractions.py`:

```python
        elif (isinstance(numerator, numbers.Rational) and
            isinstance(denominator, numbers.Rational)):
            numerator, denominator = (
                numerator.numerator * denominator.denominator,
                denominator.numerator * numerator.denominator
                )
```

`numpy.uint64` is an instance of `numbers.Rational`:

```python
>>> import numpy as np
>>> import numbers
>>> denoms = np.array([18446744073709550592], dtype='uint64')
>>> isinstance(denoms[0], numbers.Rational)
True
```

Inspecting the reproducible snippet above with `pdb` reveals that the multiplication on lines 178-179 of `fractions.py` is the moment when the type is changed.

Thank you for your time (:",2022-01-26 21:13:28,,BUG: Binary operations between uint64 and intX results in float64 ,['00 - Bug']
20895,open,46cv8,"### Describe the issue:

There appears to be some kind of issue where opencv when combined with numpy 1.22 generates lots of warning messages.

UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.

During opencv importation it calls to numpy and it generates an object and puts it into the cache with the correct smallest_subnormal array(1.e-45, dtype=float32). However checking the numpy cache immediately after it appears as if the smallest_subnormal value has no changed to 0. I have no explanation for how or why this would happen. (last reproduction code example)

The workaround to avoid seeing the warnings appears to be to import numpy and attempt to read values from the cache for those types that are generating a warning before importing opencv. I'm not sure why this works but it appears to stop the warnings. In my case this looks like the following.

```
import numpy as np

np.finfo(np.dtype(""float32""))
np.finfo(np.dtype(""float64""))
import cv2
```

### Reproduce the code example:

```python
Python 3.8.10 (default, Nov 26 2021, 20:14:08) 
[GCC 9.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import cv2
>>> import numpy
>>> numpy.finfo(numpy.dtype('float32')).smallest_subnormal
/usr/local/lib/python3.8/dist-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.
  setattr(self, word, getattr(machar, word).flat[0])
/usr/local/lib/python3.8/dist-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.
  return self._float_to_str(self.smallest_subnormal)
0.000000000000000000000000000000000000000000001
>>> exit()

Python 3.8.10 (default, Nov 26 2021, 20:14:08) 
[GCC 9.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy
>>> import cv2
>>> numpy.finfo(numpy.dtype('float32')).smallest_subnormal
/usr/local/lib/python3.8/dist-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.
  setattr(self, word, getattr(machar, word).flat[0])
/usr/local/lib/python3.8/dist-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.
  return self._float_to_str(self.smallest_subnormal)
0.000000000000000000000000000000000000000000001

Python 3.8.10 (default, Nov 26 2021, 20:14:08) 
[GCC 9.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy
>>> numpy.finfo(numpy.dtype('float32')).smallest_subnormal
1e-45
>>> import cv2
>>> numpy.finfo(numpy.dtype('float32')).smallest_subnormal
0.000000000000000000000000000000000000000000001
>>> exit()


Python 3.8.10 (default, Nov 26 2021, 20:14:08) 
[GCC 9.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy
>>> from numpy.core.getlimits import _get_machar
>>> x = _get_machar(numpy.float32)
>>> x
<numpy.core.getlimits.MachArLike object at 0x7fdd79c9ddc0>
>>> x.smallest_subnormal
array(1.e-45, dtype=float32)
>>> import cv2
>>> x = _get_machar(numpy.float32)
>>> x
<numpy.core.getlimits.MachArLike object at 0x7fdd79c9ddc0>
>>> x.smallest_subnormal
<stdin>:1: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.
array(0., dtype=float32)
```


### Error message:

```shell
UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.
```


### NumPy/Python version information:

>>> import sys, numpy; print(numpy.__version__, sys.version)
1.22.0 3.8.10 (default, Nov 26 2021, 20:14:08) 
[GCC 9.3.0]
>>> cv2.__version__
'4.5.5'

The opencv used was built as part of the docker image available here so this may be related. (However I've found 2 reports of this same issue online in the last 2 weeks already)
https://github.com/datamachines/cuda_tensorflow_opencv",2022-01-26 03:59:51,,DOC: issue with opencv altering smallest subnormal for <class 'numpy.float32'>,['04 - Documentation']
20880,open,matthew-brett,"### Proposed new feature or change:

This is an issue to discuss the future of the `npymath` library in Numpy.

Numpy ships with a static library in `core/lib` named `npymath.lib` (Windows) or `libnpymath.a` (Unices).

It provides a suitable platform-specific implementation of various math routines.

Scipy links to `npymath` during its build process, in various places.

We (Scipy) have run into problems linking against Npymath, because of the combination of Numpy's use of the latest Visual Studio toolchain, and Scipy's use of the Mingw-w64 toolchain - discussed in https://github.com/MacPython/numpy-wheels/issues/145 .

The issue that arises is that it is in general more difficult to link static libraries across toolchains, than it is to link against dynamic libraries (e.g. DLLs) or to recompile the sources.   This issue is to discuss whether we should think of other solutions.   Possible options are:

* Leave things as they are.
* Ship the C header and sources for Npymath, so external libraries and packages can recompile them.
* Create a dynamic library from Npymath, and ship that, for linkage against external libraries.
* Your option here.

This issue is to house the discussion of different options.",2022-01-23 16:27:32,,ENH: Future of npymath library,"['54 - Needs decision', 'component: npy_math']"
20873,open,dstansby,"### Describe the issue:

Running `np.unique` on a flat array with multiple `NaN`s only returns one `NaN`. Running along an axis that has duplicated rows containing `NaN`s returns both rows however.

I suspect this is related to https://github.com/numpy/numpy/issues/20326 - whatever the decision here I think the result should be consistent between `axis=None` and `axis=0`.

### Reproduce the code example:

```python
import numpy as np

print(np.unique([np.nan, np.nan, 2.]))
# [ 2. nan]

a = np.array([[np.nan, 2.],
              [np.nan, 2.]])
print(np.unique(a))
# [ 2. nan]
print(np.unique(a, axis=0))
# [[nan  2.]
#  [nan  2.]]
#
# Expected:
# [[nan  2.]]
```


### Error message:

_No response_

### NumPy/Python version information:

1.22.1
3.9.7 (default, Sep 16 2021, 08:50:36) 
[Clang 10.0.0 ]",2022-01-21 21:28:04,,BUG: `unique` with NaNs and along an axis inconsistent with flat version,['00 - Bug']
20871,open,djpine,"numpy.polynomial.polynomial.polyfit does not return covariance matrix.  The older numpy.polyfit does, but using numpy.polynomial.polynomial.polyfit is prefered as it is part of the new numpy.polynomial package that uses a more sensible ordering of polynomial coefficients.  

The covariance matrix is needed in order to get estimates of the uncertainties in the fitted polynomial coefficients.  For reasons unknown, it was not included when the newer numpy.polynomial.polynomial.polyfit was developed.  This feature should be restored in the newer version of polyfit.",2022-01-21 16:40:46,,numpy.polynomial.polynomial.polyfit does not return covariance matrix,"['01 - Enhancement', 'component: numpy.polynomial']"
20864,open,aspfohl,"### Describe the issue:

replace parameter uses truth rather than boolean behavior, meaning if you accidentally specify the probabilities without the replace default, it will automatically use replacement and disregard the probabilities

### Reproduce the code example:

```python
from numpy.random import choice
choice([""a"", ""b"", ""c""], 100, [.01, .98, .01])
# array(['c', 'a', 'c', 'c', 'b', 'b', 'b', 'c', 'a', 'b', 'a', 'c', 'b',
#        'c', 'c', 'c', 'b', 'a', 'a', 'b', 'c', 'c', 'a', 'c', 'c', 'a',
#        'c', 'c', 'b', 'a', 'b', 'a', 'a', 'c', 'a', 'c', 'b', 'b', 'b',
#        'a', 'c', 'a', 'c', 'a', 'a', 'b', 'c', 'a', 'b', 'a', 'a', 'a',
#        'a', 'b', 'b', 'c', 'a', 'a', 'a', 'b', 'b', 'c', 'b', 'b', 'c',
#        'b', 'a', 'a', 'c', 'a', 'a', 'b', 'c', 'b', 'b', 'a', 'a', 'b',
#        'a', 'a', 'a', 'b', 'c', 'c', 'a', 'a', 'c', 'a', 'b', 'b', 'a',
#        'a', 'c', 'a', 'c', 'b', 'c', 'b', 'c', 'b'], dtype='<U1')

# Expected sampling behavior (no error thrown in example above)
choice([""a"", ""b"", ""c""], 100, True, p=[.01, .98, .01])
# array(['b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b',
#        'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b',
#        'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'c', 'c', 'b', 'b',
#        'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b',
#        'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b',
#        'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b',
#        'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'a',
#        'b', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b'], dtype='<U1')
```


### Error message:

(None)


### NumPy/Python version information:

numpy: 1.21.4
python: 3.9.6 v3.9.6:db3ff76da1, Jun 28 2021, 11:14:58",2022-01-20 15:13:44,,BUG: numpy.random.choice replace parameter uses truth rather than boolean behavior,"['00 - Bug', 'component: numpy.random']"
20860,open,Kai-Striega,"### Describe the issue:

It is possible to create a ``Polynomial`` using the ``Decimal`` datatype. However when trying to evaluate the polynomial at a given point (`p(x)`) it fails due to a type error.


### Reproduce the code example:

```python
Python 3.9.7 (default, Sep 16 2021, 13:09:58) 
Type 'copyright', 'credits' or 'license' for more information
IPython 7.29.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import numpy as np

In [2]: from numpy.polynomial import Polynomial

In [3]: from decimal import Decimal

In [4]: l = [Decimal(x) for x in range(5)]

In [5]: p = Polynomial(l)

In [6]: p

Out[6]: 
Polynomial([Decimal('0'), Decimal('1'), Decimal('2'), Decimal('3'),
       Decimal('4')], dtype=object, domain=[-1,  1], window=[-1,  1])
```


### Error message:

```shell
In [7]: p(5)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-7-5a786ac4beee> in <module>
----> 1 p(5)

~/anaconda3/lib/python3.9/site-packages/numpy/polynomial/_polybase.py in __call__(self, arg)
    481         off, scl = pu.mapparms(self.domain, self.window)
    482         arg = off + scl*arg
--> 483         return self._val(arg, self.coef)
    484 
    485     def __iter__(self):

~/anaconda3/lib/python3.9/site-packages/numpy/polynomial/polynomial.py in polyval(x, c, tensor)
    752         c = c.reshape(c.shape + (1,)*x.ndim)
    753 
--> 754     c0 = c[-1] + x*0
    755     for i in range(2, len(c) + 1):
    756         c0 = c[-i] + c0*x

TypeError: unsupported operand type(s) for +: 'decimal.Decimal' and 'float'

In [8]: p(Decimal(5))
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-8-8e5350f97fd4> in <module>
----> 1 p(Decimal(5))

~/anaconda3/lib/python3.9/site-packages/numpy/polynomial/_polybase.py in __call__(self, arg)
    480     def __call__(self, arg):
    481         off, scl = pu.mapparms(self.domain, self.window)
--> 482         arg = off + scl*arg
    483         return self._val(arg, self.coef)
    484 

TypeError: unsupported operand type(s) for *: 'float' and 'decimal.Decimal'
```


### NumPy/Python version information:

```python
In [9]: import sys, numpy; print(numpy.__version__, sys.version)
1.20.3 3.9.7 (default, Sep 16 2021, 13:09:58) 
[GCC 7.5.0]
```",2022-01-20 03:57:00,,BUG: Polynomial cannot be evaluated with `Decimal` datatype,['00 - Bug']
20859,open,erezinman,"### Describe the issue:

There's an inconsistency in between `np.nan` and `np.nat` handling when it comes to `nanmin`, `nanmax`, etc., on non-`ndarray` arrays. See the example below.

### Reproduce the code example:

```python
arr_dt = [np.datetime64('nat'), np.datetime64('2002-01-01')]
print(np.nanmin(np.asarray(arr_dt)))  # -> 2002-01-01
print(np.nanmin(arr_dt))              # -> NaT

arr_fl = [float('nan'), 142.]
print(np.nanmin(np.asarray(arr_fl)))  # -> 142.0
print(np.nanmin(arr_fl))              # -> 142.0
```


### Error message:

_No response_

### NumPy/Python version information:

1.19.5 3.6.9 (default, Jan 26 2021, 15:33:00) 
[GCC 8.4.0]",2022-01-19 17:57:53,,BUG: Inconsistency in `np.nanmin` and similar with `nat` in non-array,['00 - Bug']
20855,open,seberg,"This change means that genfromtxt now ignores `names=('column1','column2')` when `unpack=True` is passed. Instead of giving a named/rec array as before, a list of arrays with no names is given. Is this the correct behavior? It means that passing the names explicitly into genfromtxt does nothing and the output is not an array of arrays/columns, which was very useful. Lists can't be indexed the same way. Can I be guaranteed that the returned list is in the right order so I can manually assign the names? Should I open an issue about this?

The example I am thinking of is, say we have a simple csv or space delimited document with 4 columns:

val1 val2 valignore1 valignore2
val3 val4 valignore3 valignore4
val5 val6 valignore5 valignore6

I read this in using `np.genfromtxt(data.dat, unpack=True, names=('column1', 'column2'), usecols=(0,1))`, the result is a list and the names are ignored. Previously, the names would be attached and the result would be a two column array that can be indexed with the given names. The list obviously can't be. 

Now we get [array(val1, val3, val5), array(val1, val3, val5)], previously it would be an array of arrays with names (used to be called recarray I think but maybe that name has changed).

_Originally posted by @emirkmo in https://github.com/numpy/numpy/issues/16650#issuecomment-1016558482_",2022-01-19 15:12:34,,"BUG: genfromtxt now ignores `names=('column1','column2')` when `unpack=True`","['06 - Regression', '57 - Close?']"
20850,open,axil,"### Issue with current documentation:

This parameter is one of the the first things you see when you build a masked array.

Yet there is no explanation of the purpose of this parameter.

The [overview page of the ma module](https://numpy.org/doc/stable/reference/maskedarray.generic.html) omits it entirely,

The [documentation page on the MaskedArray class](https://numpy.org/doc/stable/reference/maskedarray.baseclass.html#the-maskedarray-class) gives the following brief description:

> A fill_value, a value that may be used to replace the invalid entries in order to return a standard numpy.ndarray.

It doesn't say _when_ or _how_ it is used to replace the invalid entries.

There is a dedicated section in the docs '[Filling a masked array](https://numpy.org/doc/stable/reference/routines.ma.html#filling-a-masked-array)'. It consists of 8 functions dealing with  `fill_value` but none of the descriptions explain the purpose of this field:
* `ma.common_fill_value(a, b)` Return the common filling value of two masked arrays, if any.
* `ma.default_fill_value(obj)` Return the default fill value for the argument object.
The default filling value depends on the datatype of the input array or the type of the input scalar:
* `ma.maximum_fill_value(obj)` Return the minimum value that can be represented by the dtype of an object.
This function is useful for calculating a fill value suitable for taking the maximum of an array with a given dtype.
* `ma.set_fill_value(a, fill_value)` Set the filling value of a, if a is a masked array.
This function changes the fill value of the masked array a in place. If a is not a masked array, the function returns silently, without doing anything.
* `ma.MaskedArray.get_fill_value()` The filling value of the masked array is a scalar. When setting, None will set to a default based on the data type.
and finally,
* [`MaskedArray.fill_value`](https://numpy.org/doc/stable/reference/maskedarray.baseclass.html#numpy.ma.MaskedArray.fill_value) The filling value of the masked array is a scalar. When setting, None will set to a default based on the data type.

As a sidenote, 

> [Encyclopaedia Cosmica](https://en.wikipedia.org/wiki/Sepulka) gives the following definition of the word 'Sepulka':
> Sepulka – pl.: sepulkas, a prominent element of the civilization of Ardrites from the planet of Enteropia; see ""Sepulkaria""
> Sepulkaria – sing: sepulkarium, establishments used for sepuling; see ""Sepuling""
> Sepuling – an activity of Ardrites from the planet of Enteropia; see ""Sepulka""
> Although omnipresent in art and commercials of the alien civilisation Tichy visits, discussions of sepulkas is a major taboo and all of Tichy's attempts to learn about them are seen by the locals as a faux pas.  :)

At the documentation meeting, Matti said it is used when unmasking the array.

Here is the documentation page for unmasking:
https://numpy.org/doc/stable/reference/maskedarray.generic.html#unmasking-an-entry

None of the methods listed there use `fill_value`.

In the follow-up discussion Matti said that the relevant info can be found in the [documentation page for `ma.filled`](https://numpy.org/doc/stable/reference/generated/numpy.ma.filled.html#numpy.ma.filled). But how is user supposed to know it is on that particular page? 

What makes things worse, the usage of `a.fill_value` is inconsistent across different functions:

* ma.MaskedArray.filled(fill_value=None) uses a.fill_value when fill_value is None
* a.mask = ma.unmasked ignores a.fill_value
* ma.fix_invalid(a, mask=False, copy=True, fill_value=None) uses a.fill_value when fill_value is None
* ma.MaskedArray.tolist(fill_value=None)  ignores a.fill_value
* ma.MaskedArray.tobytes(fill_value=None, order='C') uses a.fill_value when fill_value is None
* ma.min/ma.max/ma.argmin/ma.argmax ignore a.fill_value

I believe that in every particular case there're good reasons why this or that behavior is chosen, yet I don't see how storing fill_value in the MaskedArray object can be useful. 

Melissa suggested that is a standard practice - it is not. For example, in Pandas `fill_value` is not stored in the Int64 object. I feel there must be a use case where storing `fill_value` in the `MaskedArray` object is beneficial over providing it as an argument (to `filled()` for example). Does anyone know what it is?

### Idea or request for content:

Include a use case demonstrating the purpose of this parameter.",2022-01-18 18:54:42,,DOC: What is the purpose of fill_value in masked arrays?,['04 - Documentation']
20836,open,BvB93,"Currently all deprecations are treated as if they're completed in the typing stub files, causing type checkers to produce warnings when using deprecated functions/arguments similar to how `DeprecationWarning`s are issued during runtime. This behavior is not documented anywhere though and could use a paragraph or two in the typing docs.",2022-01-17 10:26:18,,DOC: Document how numpy deprecations are treated with static typing,"['04 - Documentation', 'Static typing']"
20802,open,aldanor,"### Describe the issue:

Format strings generated for certain dtypes cannot be parsed by `np.core.internal._dtype_from_pep3118`, see below for a simple example.

What is unclear: is it a format string generation bug (i.e., nested subarrays should be squashed into a single one), or is it a format string parsing bug (i.e. multiple subarray prefixes should be allowed)?

### Reproduce the code example:

```python
from numpy.core.internal import _dtype_from_pep3118

format_in = '^x(2)3i'
dtype = _dtype_from_pep3118(format_in)
format_out = np.zeros(0, dtype).data.format

print(dtype)
# {'names':['f0'], 'formats':[(('<i4', (3,)), (2,))], 'offsets':[1], 'itemsize':25}

print(format_out)
# T{x(2)(3)=i:f0:}

assert _dtype_from_pep_3118(format_out) == dtype
# unable to parse because of multiple subarray prefixes: '(2)(3)'
```


### Error message:

```shell
ValueError: Unknown PEP 3118 data type specifier '(3)=i:f0:}'
```


### NumPy/Python version information:

- NumPy 1.20.3
- Python 3.8.12",2022-01-12 12:49:04,,BUG: Format strings generated by NumPy for nested subarrays cannot be parsed back by _dtype_from_pep3118,['00 - Bug']
20799,open,didickman,"### Describe the issue:

Starting with numpy 1.20rc1 and also happening in every release through 1.22.0, the tests cause a segfault on OpenBSD/i386. I've tested this with both python 3.8 and python 3.9. numpy 1.19.5 appears to allow the test suite to run.

The backtrace from gdb shows ""solve_diophantine"" may be where it's crashing?

In case helpful to know, on OpenBSD, netlib Blas is used.

### Reproduce the code example:

```python
$ egdb python3
GNU gdb (GDB) 9.2
Copyright (C) 2020 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
Type ""show copying"" and ""show warranty"" for details.
This GDB was configured as ""i386-unknown-openbsd7.0"".
Type ""show configuration"" for configuration details.
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>.
Find the GDB manual and other documentation resources online at:
    <http://www.gnu.org/software/gdb/documentation/>.

For help, type ""help"".
Type ""apropos word"" to search for commands related to ""word""...
Reading symbols from python3...
(No debugging symbols found in python3)
(gdb) run
Starting program: /usr/local/bin/python3
Python 3.8.12 (default, Jan 11 2022, 03:22:43)
[Clang 13.0.0 ] on openbsd7
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy
>>> numpy.test(verbose=2)
NumPy version 1.20.0rc1
NumPy relaxed strides checking option: True
NumPy CPU features: SSE SSE2 SSE3* SSSE3* SSE41* POPCNT* SSE42* AVX? F16C* FMA3? AVX2? AVX512F? AVX512CD? AVX512_KNL? AVX512_KNM? AVX512_SKX? AVX512_CLX? AVX512_CNL? AVX512_ICL?
===================================================================================================== test session starts ======================================================================================================
platform openbsd7 -- Python 3.8.12, pytest-6.2.5, py-1.8.2, pluggy-1.0.0
rootdir: /home/didickman/numpy, configfile: pytest.ini
plugins: hypothesis-6.35.0
collecting 11938 items / 4 errors / 11934 selected
Program received signal SIGSEGV, Segmentation fault.
0x0aba548b in solve_diophantine () from /usr/local/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38.so
(gdb) bt
#0  0x0aba548b in solve_diophantine () from /usr/local/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38.so
#1  0x0aba681b in solve_may_share_memory () from /usr/local/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38.so
#2  0x0ab7c8bf in PyUFunc_GenericFunction_int () from /usr/local/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38.so
#3  0x0ab7d001 in ufunc_generic_call () from /usr/local/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38.so
#4  0x08c40e6f in _PyObject_MakeTpCall () from /usr/local/lib/libpython3.8.so.0.0
#5  0x08c42e04 in object_vacall () from /usr/local/lib/libpython3.8.so.0.0
#6  0x08c42f71 in PyObject_CallFunctionObjArgs () from /usr/local/lib/libpython3.8.so.0.0
#7  0x0aa02e31 in PyArray_GenericInplaceBinaryFunction () from /usr/local/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38.so
#8  0x08c2a072 in PyNumber_InPlaceMultiply () from /usr/local/lib/libpython3.8.so.0.0
#9  0x08cffd87 in _PyEval_EvalFrameDefault () from /usr/local/lib/libpython3.8.so.0.0
#10 0x08d0875e in _PyEval_EvalCodeWithName () from /usr/local/lib/libpython3.8.so.0.0
#11 0x08c4193a in _PyFunction_Vectorcall () from /usr/local/lib/libpython3.8.so.0.0
#12 0x08c411be in PyVectorcall_Call () from /usr/local/lib/libpython3.8.so.0.0
#13 0x08c412ac in PyObject_Call () from /usr/local/lib/libpython3.8.so.0.0
#14 0x0a93935c in array_implement_array_function () from /usr/local/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38.so
#15 0x08c41481 in cfunction_call_varargs () from /usr/local/lib/libpython3.8.so.0.0
#16 0x08c41f8a in PyCFunction_Call () from /usr/local/lib/libpython3.8.so.0.0
#17 0x08c40e6f in _PyObject_MakeTpCall () from /usr/local/lib/libpython3.8.so.0.0
#18 0x08d07b86 in call_function () from /usr/local/lib/libpython3.8.so.0.0
#19 0x08d04965 in _PyEval_EvalFrameDefault () from /usr/local/lib/libpython3.8.so.0.0
#20 0x08d0875e in _PyEval_EvalCodeWithName () from /usr/local/lib/libpython3.8.so.0.0
#21 0x08c4193a in _PyFunction_Vectorcall () from /usr/local/lib/libpython3.8.so.0.0
#22 0x08d07b44 in call_function () from /usr/local/lib/libpython3.8.so.0.0
<snipped by @seberg>
```


### Error message:

```shell
Program received signal SIGSEGV, Segmentation fault.
```


### NumPy/Python version information:

1.20.0rc1 3.8.12 (default, Jan 11 2022, 03:22:43)
[Clang 13.0.0 ]
",2022-01-12 00:28:10,,BUG: on OpenBSD/i386 numpy segfaults while running tests (1.20rc1 to 1.22.0 crash / 1.19.5 is the last known working version),"['00 - Bug', 'component: SIMD']"
20785,open,mattip,"In PR #17989 changes to unrelated code changed argmax/argmin benchmarks significantly. We should be more consistent about benchmarking PRs so we have a baseline. Perhaps the benchmark itself is unstable, perhaps somehow this PR changed some compiler branching. It would be nice to set up daily/weekly/per PR/ benchmark runs and compare them before merging PRs.

For this particular problem, we should try different dtypes and benchmark ordering to see what is causing the change in argmax/argmin.",2022-01-11 07:14:05,,BENCH: consistently test benchmarks (specifically argmax/argmin),"['05 - Testing', '28 - Benchmark']"
20771,open,WarrenWeckesser,"(This was originally a [SciPy issue](https://github.com/scipy/scipy/issues/15386).)

When building scipy `1.9.0.dev0+1241.f916acd` with numpy 1.22.0, the following warnings are generated by f2py:
```
[...]
INFO:   adding 'build/src.linux-x86_64-3.10/build/src.linux-x86_64-3.10/scipy/linalg/_fblas-f2pywrappers.f' to sources.
INFO: building extension ""scipy.linalg._flapack"" sources
INFO: from_template:> build/src.linux-x86_64-3.10/scipy/linalg/flapack.pyf
INFO: f2py options: []
INFO: f2py: build/src.linux-x86_64-3.10/scipy/linalg/flapack.pyf
/home/warren/py3.10.1/lib/python3.10/site-packages/numpy/f2py/symbolic.py:1508: ExprWarning: fromstring: treating ""'A'||"" as symbol (original=(compute_v?(2*(*range=='A'||(*range=='I' && iu-il+1==n)?n:0)):0))
  ewarn(
/home/warren/py3.10.1/lib/python3.10/site-packages/numpy/f2py/symbolic.py:1508: ExprWarning: fromstring: treating ""'I' && iu"" as symbol (original=(compute_v?(2*(*range=='A'||(*range=='I' && iu-il+1==n)?n:0)):0))
  ewarn(
/home/warren/py3.10.1/lib/python3.10/site-packages/numpy/f2py/symbolic.py:1508: ExprWarning: fromstring: treating '(jobt == 0)&&' as symbol (original=((jobt == 0)&&(jobu == 3)?0:m))
  ewarn(
/home/warren/py3.10.1/lib/python3.10/site-packages/numpy/f2py/symbolic.py:1508: ExprWarning: fromstring: treating '(jobt == 0)&&' as symbol (original=((jobt == 0)&&(jobu == 3)?0:(jobu == 1?m:n)))
  ewarn(
/home/warren/py3.10.1/lib/python3.10/site-packages/numpy/f2py/symbolic.py:1508: ExprWarning: fromstring: treating '(jobt == 0)&&' as symbol (original=((jobt == 0)&&(jobv == 3)?0:ldv))
  ewarn(
/home/warren/py3.10.1/lib/python3.10/site-packages/numpy/f2py/symbolic.py:1508: ExprWarning: fromstring: treating '(jobt == 0)&&' as symbol (original=((jobt == 0)&&(jobv == 3)?0:n))
  ewarn(
Reading fortran codes...
	Reading file 'build/src.linux-x86_64-3.10/scipy/linalg/flapack.pyf' (format:free)
Post-processing...
[...]
```

The warnings appear to be triggered with expressions in `dimension` attributes in a `.pyf` that use a mix of comparisons, logical operators and ternary operators.  E.g.
```
    integer intent(out),dimension((compute_v?(2*(*range=='A'||(*range=='I' && iu-il+1==n)?n:0)):0)),depend(n,iu,il,compute_v,range) :: isuppz

```

Further down the build log, we have similar warnings when building `specfun`:
```
[...]
creating build/src.linux-x86_64-3.10/scipy/special
INFO: f2py options: ['--no-wrap-functions']
INFO: f2py: scipy/special/specfun.pyf
/home/warren/py3.10.1/lib/python3.10/site-packages/numpy/f2py/symbolic.py:1508: ExprWarning: fromstring: treating '(int)v' as symbol (original=(int)v+1)
  ewarn(
/home/warren/py3.10.1/lib/python3.10/site-packages/numpy/f2py/symbolic.py:1508: ExprWarning: fromstring: treating '(int)v' as symbol (original=abs((int)v)+2)
  ewarn(
Reading fortran codes...
	Reading file 'scipy/special/specfun.pyf' (format:free)
Post-processing...
[...]
```
The relevant code in `specfun.pyf` is
```
            double precision intent(out),depend(v),dimension((int)v+1) :: vl
            double precision intent(out),depend(v),dimension((int)v+1) :: dl
```

@pearu [explained](https://github.com/scipy/scipy/issues/15386#issuecomment-1008246141) that this is an issue with the dimension expression parser that was added in numpy 1.22.
",2022-01-09 16:35:09,,BUG: f2py warnings with `dimension` expressions,"['00 - Bug', 'component: numpy.f2py']"
20761,open,takluyver,"### Describe the issue:

From NumPy 1.21.5, h5py's ppc64le builds on Travis have started failing, with the error message added in #20464 (""RuntimeError: Broken toolchain: cannot link a simple C++ program. note: A compiler with support for C++11 language features is required.""). It was able to install 1.21.4 successfully just before 1.21.5 was released.

I'm reporting this as a bug as I don't see anything in the release notes indicating that the compiler requirements changed for 1.21.5 (and it would be surprising if they did for a minor release). So it appears that the check is failing where building NumPy would otherwise work.

Cross-linking https://github.com/h5py/h5py/issues/2030

### Reproduce the code example:

```python
pip install numpy
```


### Error message:

```shell
RuntimeError: Broken toolchain: cannot link a simple C++ program. note: A compiler with support for C++11 language features is required.
```


### NumPy/Python version information:

1.21.5, 3.7.1",2022-01-07 11:50:52,,BUG: Compiler check fails where build previously succeeded,['00 - Bug']
20755,open,BrunoZockt,"Python 3.8.12
Numpy 1.21.2

I'm not sure if this is a bug or intended behaviour but I think it should be improved in either way.
I'm trying to print a numpy int array with the ints in binary representation and am losing my mind.
Code example:
```
array = np.zeros(256, dtype=np.uint32)
np.savetxt(""test.out"", np.column_stack(array), fmt='%032b')
```

Error and relevant part of the traceback:
```
File ""C:\My\Path\to\numpy\lib\npyio.py"", line 1435, in savetxt
    v = format % tuple(row) + newline
ValueError: unsupported format character 'b' (0x62) at index 4
```

Using 'x' or 'o' instead of 'b' works as intended and saves the numbers in hexadecimal or octal representation respectively.
[The documentation](https://numpy.org/doc/stable/reference/generated/numpy.savetxt.html) links to [Format Specification Mini-Language](https://docs.python.org/library/string.html#format-specification-mini-language) ""for an exhaustive specification."" So it seems like numpys format (or fmt) is intended to implement/mimic pythons format but does not. I'm guessing that there are more differences between the two, but this is the one I found.

I couldn't come up with a cleaner work-around other than iterating over the whole array and formatting each value with pythons format function, saving it as a new string type array and then putting that one through savetxt so any suggestions on a cleaner work-around are also welcome!",2022-01-06 19:49:35,,Numpy fmt parameter (for example in savetxt) does not support binary even though pythons .format does.,['unlabeled']
20733,open,madphysicist,"### Proposed new feature or change:

There is a good tutorial for writing ufuncs in C here: https://numpy.org/doc/stable/user/c-info.ufunc-tutorial.html. It would be wonderful if there was something similar for regular functions. I wouldn't have to bug devs so much if it existed.

Some specific questions that I'd like to have authoritative answers for:

- How to harness the code generation machinery to generate multiple loops.
- How to dispatch to those loops correctly.
- How to unpack arguments from the python code in preparation for the dispatcher.
- All the different layers of python and C wrappers and where they go in the numpy codebase. 
- Where to put docstrings.

Actual usecase #1: I wrote a ufunc based on the tutorial: https://github.com/madphysicist/is_integer_ufunc. Now I want to add some arguments, but since ufuncs can't accept arbitrary arguments, I have to turn my ufunc into an ordinary function, but with multiple loops.

Actual usecase #2: I'd like to write up an O(n) version of weighted partitioning/quantiles. This will look something similar to `sort`, which is a regular function.",2022-01-05 07:38:10,,DOC/ENH: Tutorial for adding C functions,['unlabeled']
20715,open,Wikstahl,"### Describe the issue:

I was naive trying to change `dtype` of an array from `float64` to `complex64` by simply overwriting the `dtype` as `arr.dtype = np.complex64`. However this caused all elements in my array to either become super big or super small. I now know that this is the wrong way to change the `dtype` of an array. However, I think that an error-message or warning should have been in place.


### Reproduce the code example:

```python
import numpy as np
arr = np.random.rand(5,5)
print(arr)
arr.dtype = np.complex64
print(arr)
```


### Error message:

_No response_

### NumPy/Python version information:

1.21.5 3.9.9 (main, Nov 21 2021, 03:23:44) 
[Clang 13.0.0 (clang-1300.0.29.3)]",2022-01-03 10:39:16,,DEP: Disallow setting `arr.dtype` (or limit it?),['07 - Deprecation']
20705,open,madphysicist,"### Proposed new feature or change:

Here is a minimal example:

```
>>> x = np.arange(10)[::2]  #Create a non-contiguous array
>>> x.dtype
dtype('int64')   # This could be int32, in which case adjust the following lines with float32 and int16 as appropriate
>>> x.view(np.float64)
array([0.e+000, 1.e-323, 2.e-323, 3.e-323, 4.e-323])   # So far so good

# This makes perfect sense:
>>> x.view(np.int32)
ValueError: To change to a dtype of a different size, the array must be C-contiguous

# But this does not
>>> x[:, None].view(np.int32)
ValueError: To change to a dtype of a different size, the array must be C-contiguous
```

When the last dimension is 1, and the new dtype's `itemsize` is an even divisor of the original's, there is nothing stopping us from creating the view meaningfully.

I can understand the `np.ndarray` constructor and `np.ndarray.view` being user-facing and therefore trying to be a bit conservative. However, something like `np.lib.stride_tricks.as_strided` certainly should have the option of bypassing the dtype check: if I'm a consenting adult that's able to decide how to mess up my memory layout, I should be allowed to do it will the full force of all the tools available to me. To that end, I made a very naive attempt at adding an `offset` and `dtype` parameter to `as_strided`: https://github.com/madphysicist/numpy/commit/e42218658aeb35b236226bcdeac11fc9c6a7d3b1. This has the same failure as the example above.

The issue came up because of my work on #20694, where I am trying to add slicing to char arrays. It's actually an ideal example of how switching dtypes can be very useful. Additionally, it shows a workaround: I drill down to the base-most array, and either get a C-contiguous block of numpy-allocated memory, or construct a view of the buffer that's contiguous and large enough for the operations I need. The key is that even if the underlying memory is not contiguous for some reason, I am doing extensive checks to make sure I don't overrun the parts I know for sure I have access to: I just need to jump through hoops to convince `np.ndarray` that I know what I'm doing, but I very strongly feel that I shouldn't have to.

The workaround in #20694 is functional, but quite hacky. A better solution would be to allow `as_strided`, and potentially `ndarray` to just do what you ask them. At the very least, an exception to the dtype check should happen when the last dimension is 1 and the source array's dtype is a multiple of the target dtype's itemsize. But I would go further: if I can call `ndarray` and set an arbitrary offset, which is way more dangerous, why not allow unchecked dtype reassignments?

I propose a solution like this:

- Split `np.ndarray` into a ""checked"" and ""unchecked"" version (I have no clear idea how the code works right now, so this is just a concept)
- `as_strided` becomes a wrapper for the unchecked version: user has infinite power
- The actual function `np.ndarray` and its wrappers/siblings stay exactly the same, using the checked version.

I'd be happy to work on this issue if there is any interest in it.",2022-01-02 03:38:15,,ENH? BUG? Unexpeced restrictions in behavior of view creating when switching dtypes,['unlabeled']
20687,open,mdickinson,"### Describe the issue:

Conversion from a Python `int` to an `np.float32` value does not appear to be correctly rounded, and does not always match the equivalent conversion from `np.int64` to `np.float32`.

I'm a bit hesitant to call this a bug rather than a feature request, since I'm not aware of anywhere where it's specified that `int` to `np.float32` conversions _should_ be correctly rounded. But whether bug or not, correct rounding seems like a desirable property here, and consistency between `int` and `np.int64` also seems desirable.

In IEEE 754 binary32 floating-point, `2**54 - 2**30` and `2**54` are both exactly representable, and are neighboring floating-point values. The halfway point between them is `2**54 - 2**29`. Under correct rounding (ties-to-even, as usual), integers `n` in the interval `[2**54 - 2**29, 2**54)` should round up to `2**54` when converted to `np.float32`, while integers in `(2**54-2**30, 2**54-2**29)` should round down to `2**54 - 2**30`. In particular, `2**54 - 2**29 - 1` should round down, but as the code example below shows, it rounds up.

In contrast, the exact same input value represented as an `int64` instead of an `int` rounds down (as expected).

I haven't looked at the implementation, but my strong suspicion is that there's a double rounding going on here: that the incoming `int` is converted first to `float64`, and then to `float32`.


### Reproduce the code example:

```python
>>> import numpy as np
>>> np.float32(2**54 - 2**29 - 1) == 2**54 - 2**30  # expect True, get False
False
>>> np.float32(np.int64(2**54 - 2**29 - 1)) == 2**54 - 2**30  # ok: expect True, get True
True
```

Second example, to emphasise the unexpected difference in behaviour between `int` and `np.int64`:

```python
>>> import numpy as np
>>> x = 2**54 - 2**29 - 1
>>> y = np.int64(x)
>>> x == y
True
>>> np.float32(x) == np.float32(y)  # expect True, get False
False
```

### Error message:

_No response_

### NumPy/Python version information:

```
>>> import sys, numpy
>>> print(numpy.__version__)
1.21.4
>>> print(sys.version)
3.9.9 (main, Nov 16 2021, 07:23:17) 
[Clang 10.0.1 (clang-1001.0.46.4)]
```",2021-12-31 16:08:58,,BUG: int to np.float32 conversion is not correctly rounded,['00 - Bug']
20684,open,AnkitSharmai,"### Describe the issue:

For example there is an array _arr_ that is [n X 3]. Then the following two statements give different results:
import numpy as np
arrMean = np.mean(arr, axis=0)
arrMean2 = np.mean(arr[:, 2])

arrMean[2] IS NOT EQUAL TO arrMean2

### Reproduce the code example:

```python
import numpy as np

arr = np.array([[ 3.03586   ,  2.009392  , -0.10294339],
       [ 3.0333118 ,  2.0153277 , -0.10294339],
       [ 3.0311642 ,  2.0215352 , -0.10294339],
       [ 3.0903606 ,  2.0145807 , -0.10294339],
       [ 3.0876164 ,  2.0204794 , -0.10294339],
       [ 3.0847762 ,  2.0263193 , -0.10294339],
       [ 3.0965116 ,  2.0807695 , -0.10294339],
       [ 3.0925503 ,  2.0859516 , -0.10294339],
       [ 3.0884955 ,  2.0910687 , -0.10294339],
       [ 3.2995787 ,  1.8064666 , -0.10294339],
       [ 3.2974312 ,  1.8127781 , -0.10294339],
       [ 3.2951815 ,  1.8190379 , -0.10294339],
       [ 3.2219813 ,  2.021145  , -0.10294339],
       [ 3.219052  ,  2.0271454 , -0.10294339],
       [ 3.2158618 ,  2.0329835 , -0.10294339],
       [ 3.2845802 ,  1.9735749 , -0.10294339],
       [ 3.282012  ,  1.9798362 , -0.10294339],
       [ 3.2793922 ,  1.9860705 , -0.10294339]], dtype=np.float32)

arrMean = np.mean(arr, axis=0)
arrMean2 = np.mean(arr[:, 2])

print(arrMean[2])
print(arrMean2)
assert(arrMean[2] == arrMean2)
```


### Error message:

```shell
-0.10294341
-0.10294339
Traceback (most recent call last):

  File ""C:\my_dev\untitled0.py"", line 34, in <module>
    assert(arrMean[2] == arrMean2)

AssertionError
```


### NumPy/Python version information:

1.20.1 3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]",2021-12-30 09:22:28,,ENH: numpy.mean() gives inconsistant results with and without 'axis' argument,"['01 - Enhancement', '23 - Wish List']"
20671,open,rgommers,"xref https://github.com/pypa/pip/issues/4582#issuecomment-996193345, where @tgamblin pointed out that the [`archspec`(https://github.com/archspec/archspec) library may enable us to build and distribute more optimized binaries (e.g., using a higher `--cpu-baseline` so we use more SIMD instructions). The catch (of course) is that the package manager must understand `archspec`, so the correct binary for the users' system will be chosen.

The [archspec paper](https://tgamblin.github.io/pubs/archspec-canopie-hpc-2020.pdf) is a good read to understand the details.

This will not be immediately useful, but if we can for example figure out a wheel labelling scheme where installers that do not (yet) understand `archspec` naming will choose the baseline, and the ones that do can pick the more optimized wheel, that would be a win.

PyPI will likely be the biggest challenge; Spack already knows how to do this so once it adds more binaries that'll become a better option for the average user; `conda` does not yet know and would be an easier target than PyPI/wheels.

For now this issue is just to keep track of the idea and add to it. It'd be a lot of work to integrate this into a package manager.

Also note that NumPy does do dynamic dispatch based on CPU capabilities already (https://numpy.org/devdocs/reference/simd/index.html), so it's not like `archspec` is a magic bullet. However, currently we do have to choose a CPU baseline and a limited set of other compile options in order to make a trade-off between performance benefit and binary size (because we have to stuff everything into a single binary conda/wheel package). Having the ability to build and distribute a single optimized binary package without it running the risk of users installing them on CPUs that aren't supported by that binary would be quite nice. For other projects it'd be a larger gain (they'd not have to reinvent the dynamic dispatch wheel).",2021-12-27 14:49:00,,Investigate if/how we can use `archspec` for distributing more optimized binaries,"['01 - Enhancement', 'component: distribution', 'component: SIMD']"
20662,open,xor2k,"### Issue with current documentation:

When Googling for ""numpy datatypes"" I find the following documentation:

https://numpy.org/doc/stable/user/basics.types.html
https://numpy.org/doc/stable/reference/arrays.dtypes.html

both these docs have ""C main types"" (compare https://en.wikipedia.org/wiki/C_data_types) like char, int, long as their starting points. To get an overview of ""fixed width types"", one needs to click on ""Sized aliases"" to get to

https://numpy.org/doc/stable/reference/arrays.scalars.html#sized-aliases

Should it not be the other way round? According to my understanding and to 

https://matt.sh/howto-c

fixed width types should be the norm and char, int, long etc. obsolete, as for types like uint8_t, int32_t

1. their size is known at programming time and can match requirements better (like ""this variable can only take on 4 values, uint8_t will do"" or ""this will never fit into 32 bits, need to be 64 bits"")
2. is more portable, especially for binary file formats

Nowadays, architecture wise, I guess almost everything Numpy runs on is basically 64 bit. And if manual tuning is required (micro controllers, Raspberry Pis, GPUs) one would start with fixed types anyway.

Do you agree or am I missing out on something here?

### Idea or request for content:

A good starting point would be

https://axil.github.io/numpy-data-types.html or
https://numpy.org/doc/stable/reference/arrays.scalars.html#sized-aliases",2021-12-26 11:03:49,,DOC: make fixed data types default,['04 - Documentation']
20596,open,mattip,"From [this comment](https://github.com/numpy/numpy/pull/20593/files#r770103845). 

We may be catching too many other things with blanket `PyErr_Clear()` calls. For instance we should never clear KeyboardInterrupt or MemoryError.",2021-12-16 07:36:51,,BUG review all uses of PyErr_Clear() to exempt KeyboardInterrupt and MemoryError,['unlabeled']
20586,open,mwjury,"### Describe the issue:

I am using np.ma.corrcoef on masked 2d data with varying gaps.
- np.ma.corrcoef returns values well above 1 and below -1.
- additionally returned data values within +-1 are sometimes wrong.

When using corrcoef to obtain correlation between the single vectors:
- np.corrcoef produces correct values when reducing vectors to overlapping data (unmasked), regardless of using np.corrcoef x and y array inputs
- np.ma.corrcoef produces correct values when using masked vectors for x and y
- np.ma.corrcoef produces incorrect values when using a masked array for x

Thanks for any help. Please retag if this is not a Bug but an user error.
(Sorry for the large sample data, I tried to cut it down best I can to reproduce the numbers)

### Reproduce the code example:

```python
import numpy as np

data = np.array([[np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,
         0.05416667461395264, 0.3083333174387614, 1.1375001271565754, np.nan,
         np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,
         np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, 0.12083332737286885,
         0.3583333492279053, 0.23749999205271402, 0.4541666905085246, np.nan,
         np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,
         np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,
         np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,
         np.nan],
        [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,
         np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,
         np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,
         np.nan, np.nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19583332538604736,
         0.22083336114883423, 0.5874999761581421, 0.20833333333333334,
         1.0708333651224773, 1.6249998410542805, 0.23749999205271402,
         0.033333333830038704, 0.0, 0.0, 0.14999999602635702, 0.0,
         0.07916667064030965, 0.0, 0.0, 0.0, 0.004166666728754838,
         0.22916666666666666, 0.5416666666666666, 0.5458333094914755,
         0.833333412806193, 0.5124999682108561, 1.3375000953674316,
         1.9749999046325684, 0.07500000298023224, 0.0, 0.0,
         0.012500000496705374, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
         0.0, 0.004166666728754838, 0.0, 0.0, 0.23333332935969034,
         0.5250000158945719, 0.0, 0.016666666915019352],
        [0.12916666269302368, 0.0, 0.0, 0.037499999006589256,
         0.033333333830038704, 0.0, 0.033333333830038704,
         0.14166666070620218, 0.0, 0.12916666269302368,
         0.12916667262713113, 0.0, 0.0, 0.0, 0.0, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,
         np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,
         np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, 0.0, 0.0,
         0.0, 0.0, 0.0, 0.0, 0.029166666169961292, 0.9416666030883789,
         0.10416667660077412, 0.1958333452542623, 0.27916667858759564,
         0.7833333015441895, 0.020833333333333332, 0.050000001986821495,
         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02916666865348816,
         0.22916666666666666, 0.24999998013178507, 0.125,
         0.20833333333333334, 0.3958333333333333, 0.4583332935969035,
         1.2458333174387615, 0.0, 0.0, 0.0, 0.0, 0.004166666728754838,
         0.0, 0.0, 0.0, 0.0, 0.0, 0.08333333333333333, 0.0, 0.0,
         0.23749999205271402, 0.0833333432674408, 0.008333333457509676,
         0.11666666467984517, 0.2916666865348816, 0.2208333412806193,
         0.10000000397364299],
        [0.0, 0.0, 0.0, 0.0, 0.0, 0.06666666766007741,
         0.016666666915019352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
         0.0, 0.0, 0.0, 0.0, 0.0, 0.041666666666666664, 0.0, 0.0, 0.0,
         0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.033333333830038704,
         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
         0.0, 0.0, 0.0, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,
         np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,
         np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, 0.0,
         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])

def cor_single_vectors_x(data):
    """"""correct: reducing vectors to overlap.""""""
    cor = np.ones((data.shape[0], data.shape[0]))
    for inda in np.arange(data.shape[0]):
        for indb in np.arange(data.shape[0]):
            if indb > inda:
                a, b = data[inda], data[indb]
                m = ~a.mask & ~b.mask
                sub = np.array([a.data[m], b.data[m]])
                cor_coeff = np.corrcoef(sub)[0, 1]
                cor[inda, indb] = cor_coeff
                cor[indb, inda] = cor_coeff
    return cor

def cor_single_vectors_ma_x_and_y(data):
    """"""correct: using masked vectors for x and y""""""
    cor = np.ma.ones((data.shape[0], data.shape[0]))
    for inda in np.arange(data.shape[0]):
        for indb in np.arange(data.shape[0]):
            if indb > inda:
                cor_coeff = np.ma.corrcoef(data[inda], data[indb])[0, 1]
                cor[inda, indb] = cor_coeff
                cor[indb, inda] = cor_coeff
    return cor

def cor_single_vectors_ma_x(data):
    cor = np.ma.ones((data.shape[0], data.shape[0]))
    for inda in np.arange(data.shape[0]):
        for indb in np.arange(data.shape[0]):
            if indb > inda:
                x = np.ma.array([data[inda], data[indb]])
                cor_coeff = np.ma.corrcoef(x)[0, 1]
                cor[inda, indb] = cor_coeff
                cor[indb, inda] = cor_coeff
    return cor

data = np.ma.masked_invalid(data)
cor = np.ma.corrcoef(data) # incorrect
cor_x = cor_single_vectors_x(data) # correct
cor_ma_x_and_y = cor_single_vectors_ma_x_and_y(data) # correct
cor_ma_x = cor_single_vectors_ma_x(data) # incorrect

# aside from the too large/small values
# cor[2,3] is incorrect
# cor_x[2,3] is correct
```


### Error message:

_No response_

### NumPy/Python version information:

1.21.2 3.9.7 (default, Sep 16 2021, 13:09:58) 
[GCC 7.5.0]",2021-12-15 11:57:32,,BUG: np.ma.corrcoef returns incorrect data,"['00 - Bug', 'component: numpy.ma']"
20566,open,roryyorke,"### Issue with current documentation:

This follows from https://stackoverflow.com/q/70312146/1008142.

Where is it documented that, say, `np.exp` will call `instance.exp()` in some cases (presumably dtype object arrays) ?

I've checked these, and couldn't see it documented (apologies if I missed it):
  - https://numpy.org/devdocs/reference/ufuncs.html
  - https://numpy.org/devdocs/user/basics.ufuncs.html#ufuncs-basics
  - https://numpy.org/devdocs/reference/arrays.classes.html#arrays-classes

Other things that should probably be documented:
  - Why does this not work for, say, `np.heaviside` in this case:

```
import numpy as np


class foo:
    
    def exp(self):
        return foo()

    def heaviside(self, other):
        return foo()


print(f'{np.exp(foo()) = }')
print(f'{np.heaviside(foo(), foo()) = }')
```

which produces (Python 3.9.7, Numpy 1.21.4, Ubuntu 20.04 x64):

```
np.exp(foo()) = <__main__.foo object at 0x7efcdbba1ac0>
Traceback (most recent call last):
  File ""/home/rory/hack/stackoverflow/q70312146/heaviside.py"", line 14, in <module>
    print(f'{np.heaviside(foo(), foo()) = }')
TypeError: ufunc 'heaviside' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
```
  - Does `__array_ufunc__` take priority over this?


### Idea or request for content:

Document this, I guess at https://numpy.org/devdocs/user/basics.ufuncs.html#overriding-ufunc-behavior.",2021-12-11 06:31:03,,DOC: ufuncs falling back to object methods not documented,['04 - Documentation']
20545,open,vsbuffalo,"### Proposed new feature or change:

Currently the unary operator `-` flips the sign bit on unsigned integers (as far as I can tell) without warning which, while it makes sense, is subtle and can lead to bugs in numeric operations where an unsigned integer is used. Here is an example of the expected, but subtle behavior (followed by alternatives that lead to the more obvious result):

```
> L = np.array([4123], dtype='uint32')
> t = np.array([1e-4], dtype='float64')
> -L*t
array([429496.3173])
-1 * L*t
> array([-0.4123])
-(L*t)
array([-0.4123])
-t*L
array([-0.4123])
```

This lead to a large bug in some recent code, and I wondered if perhaps there could be a numpy option to trigger a warning when unary `-` is applied to unsigned integers? ",2021-12-08 18:34:44,,ENH: throw a warning when the unary '-' operator is applied to an unsigned int,['unlabeled']
20514,open,FudgeMunkey,"### Describe the issue:

Running `np.round` adds `0.5` to some whole integer floats. For example, `3061040371728385.0` becomes `3061040371728385.0`. This error occurs if decimal values of `2, 5, 8` are used.

This becomes more significant with larger numbers such as `6.2768919806476296e16` which rounds up `8.0` numbers. This error occurs if decimal values of `1, 4, 7, 10` are used. Note, this isn't a problem if `62768919806476296` is used instead regardless of decimal value. 

*I checked decimcal values in the range `[0, 15]`

*This bug was found using [Hypothesis](https://hypothesis.readthedocs.io/).

### Reproduce the code example:

```python
A = np.array([3061040371728385.0])
out = np.round(A, 2)
diff = out - A # 0.5

assert_array_equal(out, A)

A = np.array([6.2768919806476296e16])
out = np.round(A, 1)
diff = out - A # 8.0

assert_array_equal(out, A)
```


### Error message:

```shell
# For np.array([3061040371728385.0]), 2
Mismatched elements: 1 / 1 (100%)
Max absolute difference: 0.5
Max relative difference: 1.63343158e-16
 x: array([3.06104e+15])
 y: array([3.06104e+15])

# For np.array([6.2768919806476296e16]), 1
Mismatched elements: 1 / 1 (100%)
Max absolute difference: 8.
Max relative difference: 1.27451612e-16
 x: array([6.276892e+16])
 y: array([6.276892e+16])
```


### NumPy/Python version information:

```
1.21.4 3.8.10 (default, Sep 28 2021, 16:10:42) 
[GCC 9.3.0]
```",2021-12-04 04:03:45,,BUG: Rounding floats which are already equal to an integer changes the value,['00 - Bug']
20508,open,olafx,"### Issue with current documentation:

I discovered that the transpose function (`transpose(array)`) just reverses the dimensions and changes from row major ('contiguousarray') storage to column major ('fortranarray') storage. Nothing wrong with this, is smart, sort of a lazy evaluation.
It only actually moves data when I use `ascontiguousarray(transpose(array))`, which again makes sense, no problem.
The problem is, this is undocumented. It says nothing [here](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html) about changing from the standard contiguous array layout to Fortran array layout. This isn't obvious at all that it would do this, and can make for unexpected results, because NumPy row major and column major arrays aren't interchangeable just because functions work with both. (I discovered it due to writing raw data and found unexpected behavior, namely that transpose did nothing, so proof is in the pudding.)

### Idea or request for content:

No mention of changing memory layout from contiguous to fortran in `numpy.reverse` as a means of lazy evaluation.",2021-12-03 14:05:38,,DOC: lazy transpose function behavior is undocumented,['04 - Documentation']
20506,open,scratchmex,"### Describe the issue:

I think the operators `/` and `/=` should behave the same as one is just a shorthand for the other. They do not with masked arrays and with nans as you see in the code example. The `/` auto masks the `nan` and `/=` does not. 
We should see why `__truedivide__` and `__itruedivide__` are implemented different for `MaskedArray` class.

### Reproduce the code example:

```python
>> a = np.ma.array([1, np.nan])
>> b = [1, np.nan]

>>> a / b
[1.0 --]

>> a /= b
>>> a
[ 1. nan]
```


### Error message:

_No response_

### NumPy/Python version information:
this happens in `main` right now.
`1.22.0.dev0+1977.g114d91919 3.8.10 (default, Sep 28 2021, 16:10:42) [GCC 9.3.0]`",2021-12-02 23:03:01,,BUG: `/` and `/=` behaves different in masked arrays with `nan`,"['00 - Bug', 'component: numpy.ma']"
20501,open,honno,"### Describe the issue:

[`np.finfo()`](https://numpy.org/doc/stable/reference/generated/numpy.finfo.html) and [`np.iinfo()`](https://numpy.org/doc/stable/reference/generated/numpy.iinfo.html) document some returned attributes with `int` or `float`, when actually they return NumPy scalars.

This behaviour makes sense, as beyond 64bits you could not represent some of these attributes with Python scalars. However I think it's important the docs note this, as a user might expect a Python scalar. This tripped me up recently ([HypothesisWorks/hypothesis#3156](https://github.com/HypothesisWorks/hypothesis/pull/3156#issuecomment-984003898)) as a CI job was failing due to an unexpected NumPy scalar from `np.finfo(float32).tiny`—I had just assumed the docs meant Python scalars so didn't immediately recognise the problem.

@mattip suggested something like changing

> tiny: `float`

to

> tiny: floating point number of the appropriate type

How does using that language sound for all the attributes?

### Reproduce the code example:

```python
>>> type(np.finfo(np.float32).tiny)
numpy.float32  # I expected a Python float - probably the doc should change
>>> type(np.finfo(np.float128).tiny)
numpy.float128  # could not be represented as a Python float
```

### NumPy/Python version information:

1.23.0.dev0+81.gc8fdb53ee 3.9.9 (main, Nov 16 2021, 03:08:02) ",2021-12-02 13:13:02,,"DOC: Docs suggest returned `iinfo`/`finfo` attributes are Python scalars, not NumPy scalars",['00 - Bug']
20494,open,dirteat,"### Describe the issue:

Reading a multilines file into a structured data-type returns an iterable ndarray object. Perfect!
Problems arise when the same code is reading a single line file, the returned object by loadtxt is still a ndarray but with 0 dimension and a shape (), i.e., not iterable, even though it contains one element. This breaks any possible code using iterators afterwards, but not only. The difference in behaviour for many functions between 0 dimensional and non-zero dimensional array is a pain to deal with. An easy fix would be to return a 1 dimension ndarray in that precise case, see below.

Try the following code on a file like this, it crashes with one line, but if you uncomment Arthur and Martha, it goes fine.

```
# my mini address book
Robert	  1.23039
#Arthur	  3.14159
#Martha	  9.99999
```

### Reproduce the code example:

```python
import numpy as np

def load(filename):
    mytype = np.dtype([('Name', np.unicode_, 60),
                     ('Kindness',np.dtype(float))])

    
    funnyArray = np.loadtxt(filename,dtype=mytype)
# this is the fix
#    if (funnyArray.ndim == 0):
#        funnyArray = np.reshape(funnyArray,1)        
        
    return funnyArray


miniAddressBook = load('myfile.txt')

print('the best of all:',max(miniAddressBook['Kindness']))
```


### Error message:

```shell
Traceback (most recent call last):
  File ""./loadzero.py"", line 19, in <module>
    print('the best of all:',max(miniAddressBook['Kindness']))
TypeError: iteration over a 0-d array
```


### NumPy/Python version information:

import sys, numpy; print(numpy.__version__, sys.version)
1.19.4 3.8.12 (default, Sep 12 2021, 19:57:22) 
[GCC 10.3.0]
",2021-11-30 12:54:28,,loadtxt returns 0-dimensional ndarray on reading single line file,['00 - Bug']
20493,open,WarrenWeckesser,"### Describe the issue:

I suspect the problem reported in [this question on stackoverflow](https://stackoverflow.com/questions/70163035/overflow-encountered-in-multiply-in-np-var-while-np-nanvar-works-just-fine) is the result of code in `var` not actually ignoring the values where `where` is False.  Here's a simple example that (I think) demonstrates the problem reported in that question:

```
In [4]: rng = np.random.default_rng()

In [5]: t = np.linspace(0, 10, 2000)

In [6]: mask = rng.choice([False, False, True], size=t.shape)

In [7]: y = np.sin(t, where=mask)

In [8]: np.var(y, where=mask)
Out[8]: 0.43815108139227454

In [9]: y = np.sin(t, where=mask)

In [10]: np.var(y, where=mask)
/Users/warren/a202111/lib/python3.9/site-packages/numpy/core/_methods.py:232: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
Out[10]: 0.43815108139227454
```
The `RuntimeWarning` is the result of the uninitialized values in `y`:
```
In [11]: y.max()
Out[11]: nan

In [12]: np.nanmax(y)
Out[12]: 0.9999877595103673

In [13]: np.nanmin(y)
Out[13]: -1.5092481258755539e+304
```
In [this comment](https://github.com/numpy/numpy/issues/17192#issuecomment-982500954), @eric-wieser said

> I guess the fix is as easy as adding where=where to that um.multiply, but the concern is that that worsens performance.

so perhaps this type of warning is unavoidable.


### Reproduce the code example:

```python
See above.
```


### Error message:

```shell
See above.
```


### NumPy/Python version information:

1.20.3 3.9.7 (default, Sep 16 2021, 08:50:36) 
[Clang 10.0.0 ]

(That's the numpy version in Anaconda that I happened to be using when I ran the above code.  The problem is also reproducible with the main development version `1.23.0.dev0+...`.)",2021-11-30 11:10:03,,BUG: Spurious warning generated by `var` with `where` parameter.,['00 - Bug']
20461,open,fangq,"### Proposed new feature or change:

I am leading a project, NeuroJSON (http://neurojson.org), funded by the National Institute of Health (NIH), to develop portable data formats and annotations for the global neuroimaging community. One of our focuses is to standardize storage of complex scientific data structures as lightweight JSON based data annotations, so that complex datasets can be shared among different programming environments (Python, MATLAB, C, ...), and eventually disseminate datasets via JSON/binary JSON files or NoSQL databases.

N-D arrays has been the most important data structures in the neuroimaging community, and numpy is one of the most commonly used modules among many python based neuroimaging tools. A present obstacle is that by default numpy `ndarray` objects can not be directly serialized in the form of JSON. 

We have developed a lightweight/portable solution to serialize `ndarray` objects by using our ""[annotated N-D array format](https://github.com/NeuroJSON/jdata/blob/master/JData_specification.md#annotated-storage-of-n-d-arrays)"" defined in our [JData specification](https://github.com/NeuroJSON/jdata/blob/master/JData_specification.md).

The current module (jdata, https://pypi.org/project/jdata/) to handle encoding/decoding such N-D array annotations from and to an `ndarray` object can be found here

https://github.com/NeuroJSON/pyjdata/blob/63301d41c7b97fc678fa0ab0829f76c762a16354/jdata/jdata.py#L72-L97
https://github.com/NeuroJSON/pyjdata/blob/63301d41c7b97fc678fa0ab0829f76c762a16354/jdata/jdata.py#L126-L160

because exporting ndarray to JSON has been a frequent request in the community (#16432, #12481, #18994, pallets/flask#4012, openmm/openmm#3202, zarr-developers/zarr-python#354), I am wondering if the numpy team is interested in adopting these lightweight data packing notations (`_ArraySize_,_ArrayType_,_ArrayData_`, often times, for large arrays, compressed data is much more compact and efficient, a few more tags will be handled in the jdata module: `_ArrayZipType_,_ArrayZipSize_,_ArrayZipData_`) in a built-in JSON import/export filter.

If the team is interested, we are happy to either contribute our existing JData-encoding/decoding functions in the form of an PR if the team let me know where is the best place to add this feature, or we can further explain the data annotation to the developers and discuss the best implementation approach.

I also wrote [JSONLab](https://github.com/fangq/jsonlab) - a widely used MATLAB/Octave JSON reader/writer, where these JData annotation tags were initially developed to import/export complex MATLAB data structures. It would be great to see python/numpy also adds such capability to exchange complex data structures, such as ndarray, with other programming environment via JSON.

Happy to discuss this further or set up a meeting to explain our approach and module.",2021-11-25 18:45:16,,ENH: Making ndarray object JSON serializable by adopting JData-based portable N-D array annotations,['unlabeled']
20451,open,rhoudroge,"### Describe the issue:

The return type from genfromtxt is incoherent when there is only one line to read: instead of returning a list containing a single element, it returns the element itself.

### Reproduce the code example:

```python
from io import StringIO
import numpy as np

def printdata(inputdata):
    data = np.genfromtxt(StringIO(inputdata), names=""a, b, c"", usecols=(""a"", ""c""))
    for i in data['a']:
        print(i)

# No issue here, column 'a' can be iterated
printdata(""1 2 3\n4 5 6"")

# Instead of an array, a single element is returned, resulting in a TypeError
printdata(""1 2 3"")
```


### Error message:

```shell
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
TypeError: iteration over a 0-d array
```


### NumPy/Python version information:

1.20.1 3.8.8 (default, Apr 13 2021, 19:58:26)
[GCC 7.3.0]",2021-11-24 13:19:47,,BUG: genfromtxt return type incoherent when there is only one line to read,['33 - Question']
20441,open,melissawm,"### Issue with current documentation:

Currently, the array subclassing features (namely, `__array_wrap__`, `__array_finalize__` and `__array_priority__` are mentioned in two separate pages with different approaches.

- In the [Beyond the basics](https://numpy.org/devdocs/user/c-info.beyond-basics.html#specific-features-of-ndarray-sub-typing) document
- In the [Subclassing ndarray](https://numpy.org/devdocs/user/basics.subclassing.html) document

In addition they will also be mentioned in the new [Interoperability with NumPy](https://github.com/numpy/numpy/pull/20185) doc.

### Idea or request for content:

Can we review, unify or at least cross-link these documents so the information is more easily found?",2021-11-23 14:54:58,,DOC: Unify information about array subclassing features,['04 - Documentation']
20438,open,NickleDave,"### Issue with current documentation:

Currently the [""Indexing on ndarrays""](https://github.com/numpy/numpy/blob/b95cfa5eb7b3e6714ddcab7f95ebb4c7a3b77433/doc/source/user/basics.indexing.rst?plain=1#L27) page uses the phrase ""selection tuple"" without defining it.

The first occurrence is here:
https://github.com/numpy/numpy/blob/b95cfa5eb7b3e6714ddcab7f95ebb4c7a3b77433/doc/source/user/basics.indexing.rst?plain=1#L179

```rst
- If the number of objects in the selection tuple is less than the 
  *N* dimensions of the array, then ``:`` is assumed for any subsequent dimensions.
  For example::

      >>> x = np.array([[[1],[2],[3]], [[4],[5],[6]]])
      >>> x.shape
      (2, 3, 1)
      >>> x[1:2]
      array([[[4],
              [5],
              [6]]])
```

I had never heard this term before.

When reading the page trying to understand the indexing specification, it really added to my cognitive load trying to guess what a ""selection tuple"" is supposed to be.

From the callout at the top of the page, I think ""selection tuple"" refers to the ""implied"" tuple, for which our friend the square brackets serves as syntactic sugar:
https://github.com/numpy/numpy/blob/b95cfa5eb7b3e6714ddcab7f95ebb4c7a3b77433/doc/source/user/basics.indexing.rst?plain=1#L27

```rst
Note that in Python, ``x[(exp1, exp2, ..., expN)]`` is equivalent to
``x[exp1, exp2, ..., expN]``; the latter is just syntactic sugar
for the former.
```

I don't know if ""selection tuple"" is a term the developers use frequently, a search of other issues didn't turn up many occurrences.

I guess from the link drawing people's attention to the ""basic indexing"" page at the top, this is not meant to be the friendliest page.

I ended up here from a Google search, though, so maybe it would be worth revising?


### Idea or request for content:

I'm not sure if the use of the term adds to the definition, and I think it could just be replaced with simpler language. E.g.,
```rst
- If the number of slices in the square brackets is less than the 
  *N* dimensions of the array, then ``:`` is assumed for any remaining dimensions.
  For example::

      >>> x = np.array([[[1],[2],[3]], [[4],[5],[6]]])
      >>> x.shape
      (2, 3, 1)
      >>> x[1:2]
      array([[[4],
              [5],
              [6]]])
```

I find seven occurrences of the term--that might be a lot of revising.

An alternative would be just to explicitly define it in the callout at the top of the page.

```rst
Note that in Python, ``x[(exp1, exp2, ..., expN)]`` is equivalent to
``x[exp1, exp2, ..., expN]``; the latter is just syntactic sugar
for the former. Below we refer to this as a ""selection tuple"".
```",2021-11-23 00:49:19,,"DOC: define ""selection tuple"" on Indexing page, or revise to avoid using the term",['04 - Documentation']
20401,open,fourpoints,"### Describe the issue:

I'm almost sure this is intended, but it caught me by surprise.

We have that `min(a.min(), b.min()) != np.min([a, b])` when `(a, b)` are masked arrays, but I expected them to be the same.

This also applies to other functions, such as `np.max`, `np.nanmin`, `np.nanmax`, and probably other reduce functions.

### Reproduce the code example:

```python
ma = np.ma.array([0, 1], mask=[True, False])

assert np.min(ma) == np.min([ma])  # AssertionError, 1 == 0
assert np.min([np.min(ma), np.min(ma)]) == np.min([ma, ma])  # AssertionError, (1, 1) == (0, 0)
```


### Error message:

_No response_

### NumPy/Python version information:

1.21.4 3.8.1 (tags/v3.8.1:1b293b6, Dec 18 2019, 23:11:46) [MSC v.1916 64 bit (AMD64)]",2021-11-18 15:25:45,,BUG: np.min not respecting masks on list of masked arrays,"['00 - Bug', 'component: numpy.ma']"
20384,open,stanleyjs,"### Describe the issue:

SVD fails (without an exception) when factorizing a large matrix.

### Reproduce the code example:

```python
import numpy as np
x = np.random.randn(27000,27000)
u,s,v = np.linalg.svd(x)
```


### Error message:

```shell
init_dgesdd failed init
```


### NumPy/Python version information:

1.21.2 3.9.5 (default, Jun  4 2021, 12:28:51) 
[GCC 7.5.0]
",2021-11-16 13:27:42,,BUG: init_dgesdd failed init for large SVD,"['00 - Bug', 'component: numpy.linalg']"
20362,open,zmoon,"### Describe the issue:

Related to [this Python issue](https://bugs.python.org/issue42114).

I found that passing `winmode=0` to `ctypes.CDLL()` allows the library to be loaded successfully, as mentioned in that issue.

Full context of code example below: https://github.com/zmoon/vorts/blob/81df335a146993d7d7fdc92bab745de0c46865ed/vorts/f/api.py

### Reproduce the code example:

```python
np.ctypeslib.load_library(""libvortsf"", ""./_build"")
```


### Error message:

```shell
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\zmoon\.conda\envs\vorts-dev\lib\site-packages\numpy\ctypeslib.py"", line 149, in load_library
    return ctypes.cdll[libpath]
  File ""C:\Users\zmoon\.conda\envs\vorts-dev\lib\ctypes\__init__.py"", line 448, in __getitem__
    return getattr(self, name)
  File ""C:\Users\zmoon\.conda\envs\vorts-dev\lib\ctypes\__init__.py"", line 443, in __getattr__
    dll = self._dlltype(name)
  File ""C:\Users\zmoon\.conda\envs\vorts-dev\lib\ctypes\__init__.py"", line 373, in __init__
    self._handle = _dlopen(self._name, mode)
FileNotFoundError: Could not find module 'C:\Users\zmoon\git\vorts\vorts\f\_build\libvortsf.dll' (or one of its dependencies). Try using the full path with constructor syntax.
```


### NumPy/Python version information:

1.21.4 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 15:50:08) [MSC v.1916 64 bit (AMD64)]",2021-11-12 21:16:05,,BUG: `numpy.ctypeslib.load_library()` from custom directory fails on Windows with Python 3.8+,['00 - Bug']
20351,open,simonrp84,"### Describe the issue:

When working with a numpy `datetime64` it is sometimes useful to convert it into a python `datetime.datetime` type. However, in some cases this conversion is not successful: Rather than a `datetime` being returned, the user instead gets an `int` that appears to be nanoseconds since 1970.
See the example below. The first case successfully returns `datetime` (`2020-09-01 00:50:13.386840`) but the second returns `int` (`1598921413386840000`).

### Reproduce the code example:

```python
from datetime import datetime
from numpy import datetime64
my_date = datetime64('2020-09-01T00:50:13.386840')
print(type(my_date.astype(datetime)))
my_date = datetime64('2020-09-01T00:50:13.3868400')
print(type(my_date.astype(datetime)))
```


### Error message:

_No response_

### NumPy/Python version information:

1.20.3 3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:22:46) [MSC v.1916 64 bit (AMD64)]",2021-11-11 13:25:26,,DOC: `np.datetime64.astype(datetime)` does not always return a datetime,['04 - Documentation']
20332,open,UmarJ,"### Describe the issue:

While investigating #20291, I found out that multiplication of a complex number having `np.inf` as one of its parts with

1. a real number (imaginary part `0`)
2. a purely complex number (real part `0`)

returns an incorrect result.

### Reproduce the code example:

```python
import numpy as np

print((np.inf + 1j) * 2)
print((np.inf + 1j) * 2j)
print(complex(1, np.inf) * 2)
print(complex(1, np.inf) * 2j)
```


### Error message:

```shell
(inf+nanj)
(nan+infj)
(nan+infj)
(-inf+nanj)
```


### NumPy/Python version information:

1.21.4 3.9.7 (default, Oct 10 2021, 15:13:22) 
[GCC 11.1.0]",2021-11-09 14:18:53,,BUG: Complex numbers having an np.inf part return the wrong value when multiplied,['00 - Bug']
20329,open,WarrenWeckesser,"### Describe the issue:

If `genfromtxt` is given an `encoding` with a bad type (e.g. `encoding=int`, which is nonsense, of course), the error message in the ""outer"" exception is wrong:
```
In [25]: np.genfromtxt('data.csv', encoding=int)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/mc39numpy/lib/python3.9/site-packages/numpy/lib/npyio.py in genfromtxt(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, like)
   1812         if isinstance(fname, str):
-> 1813             fid = np.lib._datasource.open(fname, 'rt', encoding=encoding)
   1814             fid_ctx = contextlib.closing(fid)

~/mc39numpy/lib/python3.9/site-packages/numpy/lib/_datasource.py in open(path, mode, destpath, encoding, newline)
    192     ds = DataSource(destpath)
--> 193     return ds.open(path, mode, encoding=encoding, newline=newline)
    194 

~/mc39numpy/lib/python3.9/site-packages/numpy/lib/_datasource.py in open(self, path, mode, encoding, newline)
    528                 mode.replace(""+"", """")
--> 529             return _file_openers[ext](found, mode=mode,
    530                                       encoding=encoding, newline=newline)

TypeError: open() argument 'encoding' must be str or None, not type

The above exception was the direct cause of the following exception:

TypeError                                 Traceback (most recent call last)
<ipython-input-25-b2096b5fdc92> in <module>
----> 1 np.genfromtxt('data.csv', encoding=int)

~/mc39numpy/lib/python3.9/site-packages/numpy/lib/npyio.py in genfromtxt(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, like)
   1818         fhd = iter(fid)
   1819     except TypeError as e:
-> 1820         raise TypeError(
   1821             f""fname must be a string, filehandle, list of strings,\n""
   1822             f""or generator. Got {type(fname)} instead.""

TypeError: fname must be a string, filehandle, list of strings,
or generator. Got <class 'str'> instead.
```
The problem is that the `try/except` statement at https://github.com/numpy/numpy/blob/79b381f364818cd197d53ed8b767c8fa3bdcb33f/numpy/lib/npyio.py#L1809-L1823
doesn't take into account that a `TypeError` can be raised for reasons other than a bad filename.

### Reproduce the code example:

```python
See above.
```


### Error message:

```shell
See above.
```


### NumPy/Python version information:

1.22.0.dev0+1698.g79b381f36 3.9.7 (default, Sep 16 2021, 13:09:58) 
",2021-11-09 00:02:39,,BUG: lib: Incorrect error raised when `genfromtxt` is given an `encoding` with the wrong type.,"['00 - Bug', 'component: numpy.lib']"
20312,open,billtubbs,"### Describe the issue:

While this is not a bug, I thought I'd raise it since it could be a common mistake people make and it would make debugging easier if the traceback message was less cryptic at least.

See this stack overflow post today:
[How do I fix this error IndexError: too many indices for array](https://stackoverflow.com/q/69858557/1609514)

The traceback is raised here in `piecewise`:
https://github.com/numpy/numpy/blob/b235f9e701e14ed6f6f6dcba885f7986a833743f/numpy/lib/function_base.py#L614

But the cause goes back to these lines I think:

https://github.com/numpy/numpy/blob/b235f9e701e14ed6f6f6dcba885f7986a833743f/numpy/lib/function_base.py#L591-L596

The intent of the above I think is to promote a single list or ndarray into a list containing one list or array.  However, if the user passes a list of some other 'array-like' objects, such as a Pandas Series, `condlist` will get turned into a 3d array.  By the time the execution gets to line 614, `cond` is a 2d array instead of a 1d array.

Maybe a simple fix could be:
```python
    # undocumented: single condition is promoted to a list of one condition
    if isscalar(condlist) or (isscalar(condlist[0]) and x.ndim != 0):
        condlist = [condlist]
```

(Someone with more experience than me should think about this).

### Reproduce the code example:

```python
import numpy as np
import pandas as pd

t = np.arange(10)
f1, f2 = lambda t: t**2, lambda t: t**2 + 1
t0 = 5

# Correct use of piecewise
condlist = [t <= t0, t > t0]
funclist = [f1, f2]
y = np.piecewise(t, condlist, funclist)
print(y)

# Incorrect use with Pandas argument
t = pd.Series(np.arange(10))
condlist = [t <= t0, t > t0]  # this is a list of pd.Series!
funclist = [f1, f2]
y2 = np.piecewise(t, condlist, funclist)
# Raises IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
```


### Error message:

```shell
Traceback (most recent call last):
  File ""/Users/billtubbs/stackoverflow/piecewise_bug.py"", line 18, in <module>
    y2 = np.piecewise(t, condlist, funclist)
  File ""<__array_function__ internals>"", line 5, in piecewise
  File ""/Users/billtubbs/anaconda3/envs/torch/lib/python3.9/site-packages/numpy/lib/function_base.py"", line 614, in piecewise
    vals = x[cond]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
```

While the documentation says that condlist must be a
> list of bool arrays or bool scalars

I think it would be nice to better handle the case where it is a list of Pandas.Series (either raising a nicer traceback or functioning correctly).

### NumPy/Python version information:

np.__version__
 '1.20.3'

python_version()
'3.9.7'",2021-11-05 22:29:50,,BUG: np.piecewise raises 'IndexError: too many indices for array' when a Pandas Series is mistakenly passed as the condlist argument instead of an ndarray,['00 - Bug']
20291,open,zerothi,"### Describe the issue:

Comparing two complex values with one of them having an infinite part (either real or complex) results in the comparison just failing.

This does _not_ happen for proper real `np.inf` values.

The below code snippet should in my view return all True's.

### Reproduce the code example:

```python
import numpy as np
print(np.allclose(1., 1.))
print(np.allclose(1., 1. + 0j))
print(np.allclose(np.inf, np.inf))
print(np.allclose(np.inf + 0j, np.inf))
print(np.allclose(np.inf + 0j, np.inf + 0j))
print(np.allclose(1 + np.inf * 1j, 1 + np.inf * 1j))
```


### Error message:

```shell
True
True
True
/opt/gnu/11.2.0/python/packages/3.9.7/numpy/1.21.3/lib/python3.9/site-packages/numpy/core/numeric.py:2365: RuntimeWarning: invalid value encountered in multiply
  x = x * ones_like(cond)
False
/opt/gnu/11.2.0/python/packages/3.9.7/numpy/1.21.3/lib/python3.9/site-packages/numpy/core/numeric.py:2366: RuntimeWarning: invalid value encountered in multiply
  y = y * ones_like(cond)
False
False
```


### NumPy/Python version information:

1.21.3 3.9.7 (default, Nov  2 2021, 13:08:39) 
[GCC 11.2.0]
",2021-11-03 20:10:01,,BUG: np.allclose complex numbers with a part np.inf,['00 - Bug']
20280,open,fdraxler,"### Describe the issue:

`np.linalg.eigh` and `np.linalg.eigvalsh` produce unexpected results for certain inputs containing nan (see code example with outputs). They do so inconsistently (first example).

I would argue that any nan in a matrix should result in at least one eigenvalue set to nan. To be more precise, if the matrix is a block diagonal matrix, only the blocks containing nan should return nan.
To avoid block identification, it might be sufficient to check if a matrix contains nan and return all eigenvalues set to nan in this case. This would be an improvement over the current outputs.


### Reproduce the code example:

```python
import numpy as np

matrices = np.array([
    [
        # Expected: [nan, nan]
        [float(""nan""), 0],
        [0, float(""nan"")]
    ],
    [
        # Expected: [nan, nan]
        [float(""nan""), .1],
        [.1, 1]
    ],
    [
        # Expected: [nan, nan]
        [float(""inf""), 0],
        [0, 1]
    ],
    [
        # Expected: [0, 0]
        [0, 0],
        [0, 0]
    ]
])

print(np.linalg.eigvalsh(matrices))
# [[ 0.         -0.        ]
#  [-0.14142136  0.14142136]
#  [        nan         nan]
#  [ 0.          0.        ]]


values, _ = np.linalg.eigh(matrices)
print(values)
# [[        nan         nan]
#  [-0.14142136  0.14142136]
#  [        nan         nan]
#  [ 0.          0.        ]]
```


### Error message:

_No response_

### NumPy/Python version information:

1.21.3 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 20:33:18) 
[Clang 11.1.0 ]",2021-11-02 16:53:24,,"BUG: np.linalg.eigvalsh and np.linalg.eigh returns arbitrary values for nan input, but they make different mistakes","['00 - Bug', 'component: numpy.linalg']"
20273,open,matthew-brett,"### Describe the issue:

There seems to be some inconsistency in the Numpy Cython complex `_t` definitions:

https://github.com/numpy/numpy/blob/main/numpy/__init__.cython-30.pxd

Note the difference between `complex128_t` and `clongdouble_t`:

```
ctypedef double complex complex128_t
...
ctypedef npy_clongdouble clongdouble_t
```
with previous:

```
    ctypedef struct npy_clongdouble:
        long double real
        long double imag
```

As a result of these definitions, when a complex array has type `clongdouble_t`, indexing for an element returns a struct, thence a dict with keys `real` and `imag`.  This is also true of the alias `complex256` (on a 64 bit system).
All other complex types generate the complex value itself, and not a dict.

Consider the following Cython file:

```
import numpy as np
cimport numpy as cnp


def one_long_double():
    cdef:
        cnp.ndarray[cnp.npy_longdouble] output

    output = np.array([1], dtype=np.longdouble)
    print(output[0])


def one_c128():
    cdef:
        cnp.ndarray[cnp.complex128_t] output

    output = np.array([1 + 1j], dtype=np.complex128)
    print(output[0])


def one_c256():
    cdef:
        cnp.ndarray[cnp.npy_complex256] output

    output = np.array([1 + 1j], dtype=np.complex256)
    print(output[0])


def one_clong_double():
    cdef:
        cnp.ndarray[cnp.clongdouble_t] output

    output = np.array([1 + 1j], dtype=np.clongdouble)
    print(output[0])


def one_long_double_complex():
    cdef:
        cnp.ndarray[long double complex] output

    output = np.array([1 + 1j], dtype=np.clongdouble)
    print(output[0])


print('long double')
one_long_double()
print('c128')
one_c128()
print('c256')
one_c256()
print('clongdouble')
one_clong_double()
print('long double complex')
one_long_double_complex()
```

Code at: https://github.com/matthew-brett/cyext

This gives output:

```
long double
1.0
c128
(1+1j)
c256
{'real': 1.0, 'imag': 1.0}
clongdouble
{'real': 1.0, 'imag': 1.0}
long double complex
(1+1j)
```

This seems confusing to me - that the `<type>_t` definitions are of different form for `clongdouble` than for the other complex types.  Was it intended?  Would it be reasonable to use:

```
ctypedef long double complex clongdouble_t
```

instead? (and for `complex256_t`)?

### Reproduce the code example:

```python
See above.
```


### Error message:

```shell
None.
```


### NumPy/Python version information:

```
1.21.3 3.9.7 (default, Oct 13 2021, 06:45:31) 
[Clang 13.0.0 (clang-1300.0.29.3)]
```",2021-11-01 22:36:45,,BUG: clongdouble_t element access inconsistent with other complex types,['00 - Bug']
20259,open,david-cortes,"### Describe the issue:

Calling `issubdtype` on a `complex64` does not produce the correct output.

### Reproduce the code example:

```python
import numpy as np
np.issubdtype(np.array([], dtype=""F"").dtype, complex)
np.issubdtype(np.array([], dtype=""F"").dtype, np.complex_)
```
```
False
False
```

### Error message:

_No response_

### NumPy/Python version information:

1.21 and current master (10460874b9f083871a846542b8b3e87c876934e0)",2021-10-31 23:21:16,,DOC: docs for issubdtype doesn't explain why `complex64` is not a subdtype of `complex` or `complex_`,['04 - Documentation']
20233,open,leobiec,"### Describe the issue:

I have a calculus of an inverse matrix in the middle of my code (some module developed by me) and debugging I found that numpy could not provide and inverse matrix. Also, only a few elements where reported as nan (don´t know if this is as expected or the complete matrix should be nan).

Anyway, if I stop the calculus at this point, and call again the inverse, it does provide an inverse matrix. Seems that something is changed in the numpy module in the first call, and not restored, so the next calls I have a different output.

The code I provide as an example does not work stand alone. I tried executing at the beginning of my complete code and it does provide the same result. So, something in my code previously has an effect on how numpy gets configured to the place where it does fail. 

Sorry I cant provide the complete code and libraries. Don’t know how can I provide some stand alone code to reproduce this failure. Can I help the debugging of this issue in an other way?

Regards

Leonardo

### Reproduce the code example:

```python
a = np.array([[ 1.06902137e-08+1.03397577e-25j, -8.29800043e-09+1.46316136e-09j,
         1.06153905e-08+7.03229683e-10j, -8.26255952e-09+1.45691217e-09j],
       [-8.29800043e-09-1.46316136e-09j,  1.06902137e-08+0.00000000e+00j,
        -8.26255952e-09-1.45691217e-09j,  1.06153905e-08-7.03229683e-10j],
       [ 1.06153905e-08-7.03229683e-10j, -8.26255952e-09+1.45691217e-09j,
         1.06902137e-08+7.23783036e-25j, -8.29800043e-09+1.46316136e-09j],
       [-8.26255952e-09-1.45691217e-09j,  1.06153905e-08+7.03229683e-10j,
        -8.29800043e-09-1.46316136e-09j,  1.06902137e-08-7.23783036e-25j]])
print(np.linalg.inv(a))
print(np.linalg.inv(a))

[[            nan           +nanj -3.75115133e+11-5.40693281e+10j
  -3.79340139e+11-1.19099169e+11j  3.72964747e+11-6.57637077e+10j]
 [            nan           +nanj  3.98070598e+11-1.64825002e-01j
   3.72964747e+11+6.57637077e+10j -3.79340139e+11+1.19099169e+11j]
 [            nan           +nanj  3.72964747e+11-6.57637077e+10j
   3.98070588e+11-1.88123676e-01j -3.34000150e+11+1.79105405e+11j]
 [            nan+6.57637077e+10j -3.79340139e+11-1.19099169e+11j
  -3.34000150e+11-1.79105405e+11j  3.98070588e+11-1.64492045e-01j]]
[[ 3.98070598e+11-1.88316777e-01j -3.75115133e+11-5.40693281e+10j
  -3.79340139e+11-1.19099169e+11j  3.72964747e+11-6.57637077e+10j]
 [-3.75115133e+11+5.40693281e+10j  3.98070598e+11-1.64825002e-01j
   3.72964747e+11+6.57637077e+10j -3.79340139e+11+1.19099169e+11j]
 [-3.79340139e+11+1.19099169e+11j  3.72964747e+11-6.57637077e+10j
   3.98070588e+11-1.88123676e-01j -3.34000150e+11+1.79105405e+11j]
 [ 3.72964747e+11+6.57637077e+10j -3.79340139e+11-1.19099169e+11j
  -3.34000150e+11-1.79105405e+11j  3.98070588e+11-1.64492045e-01j]]
```


### Error message:

_No response_

### NumPy/Python version information:

numpy version 1.20.1",2021-10-29 14:31:10,,BUG: numpy.linalg.inv returning different values on consecutive calls with some nan elements,['00 - Bug']
20227,open,HaoZeke,"### Issue with current documentation:

Although changes have been made to make some commands more portable (`f2py`-->`python -m numpy.f2py` , there are still no clear indications as to the method of using F2PY on windows.

As of this issue, there are only [two compilers](https://fortran-lang.org/compilers) which have first class support for Windows:
- Silverfrost (F95 only)
- Intel oneAPI

The most common approach is to mix an existing version of MSVC with minGW from either binary installs or a management system like Choco or MSYS2.

These various installation methods combined with differing pythons (from `conda`, from the Windows store, from Intel etc.) which are the source of many fun bugs like the seven or [more ones here](https://github.com/numpy/numpy/issues?q=is%3Aopen+is%3Aissue+label%3A%22component%3A+numpy.f2py%22+windows).

### Idea or request for content:

A single comprehensive source of truth regarding Windows usage for F2PY. Essentially I'm thinking of (with VS-Code as the IDE):
- WSL
- Intel everything
- MSYS + MSVC (recommended by VS-Code)
- Choco + MSVC

Combined with the following Python sources:
- Conda
- Intel Python
- Windows store Python

Let me know if something is missing.",2021-10-29 10:04:28,,DOC: F2PY on Windows,"['04 - Documentation', 'component: numpy.f2py']"
20226,open,honno,"### Describe the issue:

Given a distance `stop - start` which very largely outsizes `step`, `np.arange()` can produce an array `x` which has `x.size` as 1 off the expected size. My guess is that internally, the non-integer result of distance divided by the step `(stop - start) / step` becomes represented as an ""integer"" with floating-point representation, and thus does not indicate that one more array element should be generated.

Notably `np.arange()` with float arguments has a great many related issues—it looks like the inprecision in those scenarios seems to be considered ""acceptable behaviour"" and has been [noted in the docs](https://numpy.org/doc/stable/reference/generated/numpy.arange.html) as such. However as in #18881, because the dtype is specified as an integer, people might have different thoughts. cc @asmeurer 

I found this via a [Hypothesis-powered test method](https://github.com/data-apis/array-api-tests/blob/58af4245e915d993de5bcde489ef667378dbca81/array_api_tests/test_creation_functions.py#L79) I wrote for the Array API test suite.

### Reproduce the code example:

```python
>>> start, stop, step = 0, 108086391056891901, 1080863910568919
>>> x = np.arange(start, stop, step, dtype=np.uint64)
>>> x.size
100
>>> r = range(start, stop, step)
>>> len(r)
101
...
>>> x[-1]
107005527146322981
>>> x[-1] + np.uint64(step)
108086391056891900  # i.e. 1 less than stop
...
>>> 108086391056891901 == 100 * 1080863910568919 + 1
True
>>> n = 108086391056891901 / 1080863910568919
>>> n
100.0
>>> n.is_integer()
True
```

### NumPy/Python version information:

```
0.3.0+27065.g0169af739 3.8.10 (default, Sep 28 2021, 16:10:42) 
[GCC 9.3.0]
```",2021-10-29 09:32:11,,BUG: `np.arange()` with int dtype can return inprecisely-sized arrays,['00 - Bug']
20215,open,denieboy,"### Describe the issue:

- np.historgram2d() does not support argument 'auto' bin selection
- and it is not clear from the description what does it use as default
- while this does work:

`x_edges = np.histogram_bin_edges(x, bins='auto')` 
`y_edges = np.histogram_bin_edges(y, bins='auto')` 
`np.histogram2d(x, y, bins=[x_edges, y_edges])`

### Reproduce the code example:

```python
import numpy as np

np.histogram2d(x, y, bins=['auto','auto'])
or 
np.histogram2d(x, y, bins='auto')
```


### Error message:

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-22-f1386dd21766> in <module>()
----> 1 np.histogram2d(x, y, bins=['auto','auto'])
      2 
      3 # np.histogram_bin_edges(x, bins='auto')

<__array_function__ internals> in histogram2d(*args, **kwargs)

1 frames
<__array_function__ internals> in histogramdd(*args, **kwargs)

/usr/local/lib/python3.7/dist-packages/numpy/lib/histograms.py in histogramdd(sample, bins, range, normed, weights, density)
   1045     for i in _range(D):
   1046         if np.ndim(bins[i]) == 0:
-> 1047             if bins[i] < 1:
   1048                 raise ValueError(
   1049                     '`bins[{}]` must be positive, when an integer'.format(i))

TypeError: '<' not supported between instances of 'str' and 'int'
```


### NumPy/Python version information:

1.21.3 3.7.12 (default, Sep 10 2021, 00:21:48) 
[GCC 7.5.0]",2021-10-28 07:24:49,,ENH: np.histogram2d() does not support argument 'auto' for `bins`.,"['01 - Enhancement', 'component: numpy.lib']"
20202,open,HaoZeke,"### Issue with current documentation:

Currently it is rather difficult to ascertain which features / language standards are supported by `f2py`. Additionally more canonical examples would help.

### Idea or request for content:

I think a good plan would be to filch / adapt a progress and support document like the one [for LFortran](https://docs.lfortran.org/progress/).


 I asked the LFortran people (@certik) for permission to mimic it and got the green light for it ^_^

The plan is to show by language standard every feature f2py supports; plus it would clear up the signature file and cmap confusion I often see.",2021-10-26 21:12:23,,DOC: F2PY wishlist,"['04 - Documentation', 'component: numpy.f2py']"
20167,open,Apteryks,"### Describe the issue:

Hello,

While updating Numpy to 1.21.3 on GNU Guix, I've encountered the following single test failure:

```
........                                                                 [100%]
=================================== FAILURES ===================================
____________________ TestUfunc.test_pickle_name_is_qualname ____________________

self = <numpy.core.tests.test_ufunc.TestUfunc object at 0x7fff1ba501c0>

    def test_pickle_name_is_qualname(self):
        # This tests that a simplification of our ufunc pickle code will
        # lead to allowing qualnames as names.  Future ufuncs should
        # possible add a specific qualname, or a hook into pickling instead
        # (dask+numba may benefit).
        _pickleable_module_global.ufunc = umt._pickleable_module_global_ufunc
>       obj = pickle.loads(pickle.dumps(_pickleable_module_global.ufunc))

self       = <numpy.core.tests.test_ufunc.TestUfunc object at 0x7fff1ba501c0>

numpy/core/tests/test_ufunc.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Cython.Shadow.CythonCImports object at 0x7fff2fee1630>
item = '_pickleable_module_global'

    def __getattr__(self, item):
        if item.startswith('__') and item.endswith('__'):
            raise AttributeError(item)
>       return __import__(item)
E       ModuleNotFoundError: No module named '_pickleable_module_global'

item       = '_pickleable_module_global'
self       = <Cython.Shadow.CythonCImports object at 0x7fff2fee1630>

/gnu/store/cpxk6m09b5b2g13bxqylasy94a8bdpi3-python-cython-0.29.24-0.c25c87d/lib/python3.10/site-packages/Cython/Shadow.py:542: ModuleNotFoundError
=========================== short test summary info ============================
FAILED numpy/core/tests/test_ufunc.py::TestUfunc::test_pickle_name_is_qualname
1 failed, 14993 passed, 616 skipped, 1253 deselected, 19 xfailed, 3 xpassed in 155.08s (0:02:35)
```

It may be due to Python 3.10.

The direct dependencies used are:
```
gfortran@10.3.0 openblas@0.3.13 python-cython@0.29.24-0.c25c87d python-hypothesis@6.0.2
+ python-pytest-xdist@2.1.0 python-pytest@6.2.5
```

### Reproduce the code example:

```python
I run the test suite with:

./runtests.py -j 24
```

Curiously the test don't occur in parallel (only one CPU core gets used to 100%).
```


### Error message:

_No response_

### NumPy/Python version information:

Numpy 1.21.3
Python 3.10.0",2021-10-23 06:43:45,,ENH: Allow (and have) `__module__` on ufuncs,['01 - Enhancement']
20157,open,akesandgren,"### Describe the issue:

Building numpy with Intel causes TestF90Callback to fail with compiler errors.
```
E                                  /scratch/eb-ake-tmp/eb-5rz83rix/tmp35nrlnrx/src.linux-x86_64-3.9/_test_ext_module_5407-f2pywrappers2.f90(12): error #6428: This name has already been used as a dummy procedure name.   [F]
E                                            external f
E                                  -------------------^
E                                  /scratch/eb-ake-tmp/eb-5rz83rix/tmp35nrlnrx/src.linux-x86_64-3.9/_test_ext_module_5407-f2pywrappers2.f90(23): error #6633: The type of the actual argument differs from the type of the dummy argument.   [F]
E                                        gh17797f2pywrap = gh17797(f, y)
E                                  --------------------------------^
E                                  compilation aborted for /scratch/eb-ake-tmp/eb-5rz83rix/tmp35nrlnrx/src.linux-x86_64-3.9/_test_ext_module_5407-f2pywrappers2.f90 (code 1)
```

Picking that code out and manually trying to build it with any Intel compiler >= 2019.1.144 (at least) produces the same error.
The problem is that both ""subroutine f2pywrapgh17797"" and ""interface function gh17797"" need their own interface definition for ""f"", or it needs to be put into a module that can then be used in f2pywrapgh17797  and gh17797.

The generated fortran code is:
```
!     -*- f90 -*-
!     This file is autogenerated with f2py (version:1.20.3)
!     It contains Fortran 90 wrappers to fortran functions.

      subroutine f2pywrapgh17797 (gh17797f2pywrap, f, y, f2py_y_d0)
      external f
      integer f2py_y_d0
      integer(kind=8) y(f2py_y_d0)
      integer(kind=8) gh17797f2pywrap
      interface
      function gh17797(f,y) result (r) 
          external f
          integer(kind=8), dimension(:) :: y
          integer(kind=8) :: r
          interface  
              function f(e_0_e) result (r) 
                  integer :: e_0_e
                  integer(kind=8) :: r
              end function f
          end interface 
      end function gh17797
      end interface
      gh17797f2pywrap = gh17797(f, y)
      end
```

### Reproduce the code example:

```python
Build numpy 1.20.3 with Intel compiler >= 2019.1.144
```


### Error message:

_No response_

### NumPy/Python version information:

1.20.3",2021-10-21 17:51:22,,BUG: TestF90Callback in numpy 1.20.3 seem to be producing incorrect fortran code,"['00 - Bug', 'component: numpy.f2py']"
20148,open,HaoZeke,"### Describe the issue:

Consider the following Fortran subroutine with an infinite loop. Please note that no one in their right mind would write this, but it illustrates the point.
```fortran
! bla.f90
subroutine main
  do
   print*,""zzz...""
  end do
end subroutine main
```
Now this can be compiled into a nice extension module `f2py -m sloop -c bla.f90` as can be seen in the reproducing code example.

Handling custom signals set via Python's `signal` library is out of scope for this issue.


### Reproduce the code example:

```python
import sloop
sloop.main()
```


### Error message:

This now ignores all `CTRL-C` and other signals. Note that as implemented, callback functions do handle signals in a sense (i.e. they are mostly `python` which does work with signals and avoid executing `fortran` when a signal is caught).

### NumPy/Python version information:

```bash
1.21.2 3.9.6 | packaged by conda-forge | (default, Jul 11 2021, 03:36:15)
[Clang 11.1.0 ]
```",2021-10-20 19:39:36,,ENH: `f2py` wrappers should respect SIGINT (CTRL-C),"['23 - Wish List', 'component: numpy.f2py']"
20135,open,HaoZeke,"Currently there are no checks for the `f2py` CLI itself (the `main` and `run_main` of `f2py2e.py`), though the underlying functionality is checked via the unit tests. This is disingenuous since breaking changes to the CLI cannot be checked.",2021-10-19 13:34:01,,MAINT: Add tests for `f2py` CLI,"['05 - Testing', 'component: numpy.f2py', '03 - Maintenance']"
20121,open,melissawm,"### Issue with current documentation:

The [current page for the numpy.random API reference](https://numpy.org/doc/stable/reference/random/index.html) contains explanatory material, but should also include a flat list of functions and methods available at the submodule.

### Idea or request for content:

A concrete suggestion is to have something like the following structure:

Include API table/list with functions/methods similar to [what is listed for the fft submodule](https://numpy.org/devdocs/reference/routines.fft.html). The list can be annotated with headings and one-line or one-paragraph descriptions, something like:

```
Generator interface
===============

This is the recommended interface ... <etc - list methods too>

default_rng
Generator
Generator.random
Generator.integers

Interface for unit testing and legacy code
===============================

<short explainer>
np.random.random_sample
np.random.randint
RandomState
```

See related discussion in the [mailing list archives](https://mail.python.org/archives/list/numpy-discussion@python.org/thread/5QTWCGP577LRNWFUXVIPRAPCW4ISW3TJ/). Relevant link: https://albertcthomas.github.io/good-practices-random-number-generators/

Related issue: #19420",2021-10-15 14:38:33,,DOC: List API reference for numpy.random,"['04 - Documentation', 'component: numpy.random', 'sprintable']"
20114,open,asmeurer,"### Describe the issue:

I'm not clear if this should be considered a bug, or simply an expected nature of the algorithm. The [docstring of slogdet](https://numpy.org/doc/stable/reference/generated/numpy.linalg.slogdet.html) says ""If the determinant is zero, then sign will be 0 and logdet will be -Inf."" However, we have

```
>>> x = np.full((5, 5), 6143., dtype=np.float32)
>>> np.linalg.slogdet(x)
(1.0, -323.75266)
>>> np.linalg.det(x)
0.0
```

(note the matrix is trivially singular)

The same matrix with float64 precision does not give 0 for det.

```
>>> x = np.full((5, 5), 6143., dtype=np.float64)
>>> np.linalg.det(x)
2.4888852543280577e-141
>>> np.linalg.slogdet(x)
(1.0, -323.75266319095135)
```

So my question is if this is expected behavior, and if the admonition in the docstring should literally mean `det(x) == 0 -> slogdet(x)[1] == -inf`.

### Reproduce the code example:

```python
>>> x = np.full((5, 5), 6143., dtype=np.float32)
>>> np.linalg.slogdet(x)
(1.0, -323.75266)
>>> np.linalg.det(x)
0.0
```


### Error message:

_No response_

### NumPy/Python version information:

1.20.2 3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 05:02:20)
[Clang 11.0.1 ]",2021-10-13 20:44:57,,BUG: slogdet does not return -inf when det returns 0,['00 - Bug']
20112,open,Mukulikaa,We aim to add a list of operations that return copies and ones that return views to the [Copies and views](https://github.com/numpy/numpy/pull/19791) document in the future. This is a tracking issue off of discussion in #19791.,2021-10-13 18:12:26,,DOC: Create a list of operations that return copies vs views ,['04 - Documentation']
20103,open,ShrirajHegde,"### Describe the issue:

A detailed description is at [Stackoverflow, here](https://stackoverflow.com/questions/69537576/f2py-gives-error-when-subroutine-contains-internal-procedures-but-compiles-succ)

to summarize, a procedure like this

```f90
subroutine example(array)
    implicit none
    real*8::array(:,:)
    INTEGER::i,j
!f2py intent(inout)::array

    do i=1,3
        do j=1,3
            array(i,j)=10*i+j
        enddo
        ! print*,''
    enddo
    call tentimes(array)
    RETURN

    contains

    subroutine tentimes(array)
        implicit none
!f2py intent(inout):array
        real*8::array(:,:)
        array=array*10
    end subroutine tentimes
end subroutine example

```

should produce a wrapper function like this
```f90
  subroutine f2pywrapexample (array, f2py_array_d0, f2py_array_d1)
      integer f2py_array_d0
      integer f2py_array_d1
      real*8 array(f2py_array_d0,f2py_array_d1)
      interface
      
            subroutine example(array) 
                real*8, dimension(:,:),intent(inout) :: array
            end subroutine example
                      !separate
            subroutine tentimes(array) 
                real*8, dimension(:,:) :: array
            end subroutine tentimes
      end interface
      call example(array)
      end
```

instead f2py produces a wrapper function like this (notice the nested procedures even in the interface)
```f90
!     -*- f90 -*-
!     This file is autogenerated with f2py (version:1.21.2)
!     It contains Fortran 90 wrappers to fortran functions.

      subroutine f2pywrapexample (array, f2py_array_d0, f2py_array_d1)
      integer f2py_array_d0
      integer f2py_array_d1
      real*8 array(f2py_array_d0,f2py_array_d1)
      interface
      
            subroutine example(array) 
                real*8, dimension(:,:),intent(inout) :: array
                subroutine tentimes(array) 
                    real*8, dimension(:,:) :: array
                end subroutine tentimes
            end subroutine example
      end interface
      call example(array)
      end
```

The bad interface fails to compile. This happens every single time when there are nested procedures.

### Reproduce the code example:

```python
import ftest
import numpy as np

a=np.zeros((3,3),order=""F"")

print(a)
res=ftest.example(a)
print(a)
```


### Error message:

```shell
running build
running config_cc
unifing config_cc, config, build_clib, build_ext, build commands --compiler options
running config_fc
unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options
running build_src
build_src
building extension ""ftest"" sources
f2py options: []
f2py:> /tmp/tmpvsa9ua1w/src.linux-x86_64-3.9/ftestmodule.c
creating /tmp/tmpvsa9ua1w/src.linux-x86_64-3.9
Reading fortran codes...
	Reading file 'allocate.f90' (format:free)
{'before': '', 'this': 'intent', 'after': '(inout):array'}
Line #20 in allocate.f90:""      intent(inout):array""
	analyzeline: no name pattern found in intent statement for ':array'. Skipping.
Post-processing...
	Block: ftest
			Block: example
			Block: dummy
Post-processing (stage 2)...
Building modules...
	Building module ""ftest""...
		Creating wrapper for Fortran subroutine ""example""(""example"")...
		Constructing wrapper function ""example""...
		  example(array)
	Wrote C/API module ""ftest"" to file ""/tmp/tmpvsa9ua1w/src.linux-x86_64-3.9/ftestmodule.c""
	Fortran 90 wrappers are saved to ""/tmp/tmpvsa9ua1w/src.linux-x86_64-3.9/ftest-f2pywrappers2.f90""
  adding '/tmp/tmpvsa9ua1w/src.linux-x86_64-3.9/fortranobject.c' to sources.
  adding '/tmp/tmpvsa9ua1w/src.linux-x86_64-3.9' to include_dirs.
copying /home/srj/.local/lib/python3.9/site-packages/numpy/f2py/src/fortranobject.c -> /tmp/tmpvsa9ua1w/src.linux-x86_64-3.9
copying /home/srj/.local/lib/python3.9/site-packages/numpy/f2py/src/fortranobject.h -> /tmp/tmpvsa9ua1w/src.linux-x86_64-3.9
  adding '/tmp/tmpvsa9ua1w/src.linux-x86_64-3.9/ftest-f2pywrappers2.f90' to sources.
build_src: building npy-pkg config files
running build_ext
customize UnixCCompiler
customize UnixCCompiler using build_ext
get_default_fcompiler: matching types: '['gnu95', 'intel', 'lahey', 'pg', 'nv', 'absoft', 'nag', 'vast', 'compaq', 'intele', 'intelem', 'gnu', 'g95', 'pathf95', 'nagfor', 'fujitsu']'
customize Gnu95FCompiler
Found executable /usr/bin/gfortran
customize Gnu95FCompiler
customize Gnu95FCompiler using build_ext
building 'ftest' extension
compiling C sources
C compiler: gcc -pthread -Wno-unused-result -Wsign-compare -DDYNAMIC_ANNOTATIONS_ENABLED=1 -DNDEBUG -O2 -fexceptions -g -grecord-gcc-switches -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -Wp,-D_GLIBCXX_ASSERTIONS -fstack-protector-strong -m64 -mtune=generic -fasynchronous-unwind-tables -fstack-clash-protection -fcf-protection -D_GNU_SOURCE -fPIC -fwrapv -O2 -fexceptions -g -grecord-gcc-switches -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -Wp,-D_GLIBCXX_ASSERTIONS -fstack-protector-strong -m64 -mtune=generic -fasynchronous-unwind-tables -fstack-clash-protection -fcf-protection -D_GNU_SOURCE -fPIC -fwrapv -O2 -fexceptions -g -grecord-gcc-switches -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -Wp,-D_GLIBCXX_ASSERTIONS -fstack-protector-strong -m64 -mtune=generic -fasynchronous-unwind-tables -fstack-clash-protection -fcf-protection -D_GNU_SOURCE -fPIC -fwrapv -fPIC

creating /tmp/tmpvsa9ua1w/tmp
creating /tmp/tmpvsa9ua1w/tmp/tmpvsa9ua1w
creating /tmp/tmpvsa9ua1w/tmp/tmpvsa9ua1w/src.linux-x86_64-3.9
compile options: '-DNPY_DISABLE_OPTIMIZATION=1 -I/tmp/tmpvsa9ua1w/src.linux-x86_64-3.9 -I/home/srj/.local/lib/python3.9/site-packages/numpy/core/include -I/usr/include/python3.9 -c'
gcc: /tmp/tmpvsa9ua1w/src.linux-x86_64-3.9/ftestmodule.c
gcc: /tmp/tmpvsa9ua1w/src.linux-x86_64-3.9/fortranobject.c
In file included from /home/srj/.local/lib/python3.9/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969,
                 from /home/srj/.local/lib/python3.9/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,
                 from /home/srj/.local/lib/python3.9/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from /tmp/tmpvsa9ua1w/src.linux-x86_64-3.9/fortranobject.h:13,
                 from /tmp/tmpvsa9ua1w/src.linux-x86_64-3.9/ftestmodule.c:16:
/home/srj/.local/lib/python3.9/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning ""Using deprecated NumPy API, disable it with "" ""#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION"" [-Wcpp]
   17 | #warning ""Using deprecated NumPy API, disable it with "" \
      |  ^~~~~~~
In file included from /home/srj/.local/lib/python3.9/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969,
                 from /home/srj/.local/lib/python3.9/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,
                 from /home/srj/.local/lib/python3.9/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from /tmp/tmpvsa9ua1w/src.linux-x86_64-3.9/fortranobject.h:13,
                 from /tmp/tmpvsa9ua1w/src.linux-x86_64-3.9/fortranobject.c:2:
/home/srj/.local/lib/python3.9/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning ""Using deprecated NumPy API, disable it with "" ""#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION"" [-Wcpp]
   17 | #warning ""Using deprecated NumPy API, disable it with "" \
      |  ^~~~~~~
/tmp/tmpvsa9ua1w/src.linux-x86_64-3.9/ftestmodule.c:137:12: warning: ‘f2py_size’ defined but not used [-Wunused-function]
  137 | static int f2py_size(PyArrayObject* var, ...)
      |            ^~~~~~~~~
compiling Fortran sources
Fortran f77 compiler: /usr/bin/gfortran -Wall -g -ffixed-form -fno-second-underscore -fPIC -O3 -funroll-loops
Fortran f90 compiler: /usr/bin/gfortran -Wall -g -fno-second-underscore -fPIC -O3 -funroll-loops
Fortran fix compiler: /usr/bin/gfortran -Wall -g -ffixed-form -fno-second-underscore -Wall -g -fno-second-underscore -fPIC -O3 -funroll-loops
compile options: '-I/tmp/tmpvsa9ua1w/src.linux-x86_64-3.9 -I/home/srj/.local/lib/python3.9/site-packages/numpy/core/include -I/usr/include/python3.9 -c'
gfortran:f90: allocate.f90
gfortran:f90: /tmp/tmpvsa9ua1w/src.linux-x86_64-3.9/ftest-f2pywrappers2.f90
/tmp/tmpvsa9ua1w/src.linux-x86_64-3.9/ftest-f2pywrappers2.f90:13:17:

   13 |                 subroutine tentimes(array)
      |                 1
Error: Unclassifiable statement at (1)
/tmp/tmpvsa9ua1w/src.linux-x86_64-3.9/ftest-f2pywrappers2.f90:14:51:

   14 |                     real*8, dimension(:,:) :: array
      |                                                   1
Error: Symbol ‘array’ at (1) already has basic type of REAL
/tmp/tmpvsa9ua1w/src.linux-x86_64-3.9/ftest-f2pywrappers2.f90:15:39:

   15 |                 end subroutine tentimes
      |                                       1
Error: Expected label ‘example’ for END SUBROUTINE statement at (1)
error: Command ""/usr/bin/gfortran -Wall -g -fno-second-underscore -fPIC -O3 -funroll-loops -I/tmp/tmpvsa9ua1w/src.linux-x86_64-3.9 -I/home/srj/.local/lib/python3.9/site-packages/numpy/core/include -I/usr/include/python3.9 -c -c /tmp/tmpvsa9ua1w/src.linux-x86_64-3.9/ftest-f2pywrappers2.f90 -o /tmp/tmpvsa9ua1w/tmp/tmpvsa9ua1w/src.linux-x86_64-3.9/ftest-f2pywrappers2.o"" failed with exit status 1
```


### NumPy/Python version information:

```
❯ python          
Python 3.9.7 (default, Aug 30 2021, 00:00:00) 
[GCC 11.2.1 20210728 (Red Hat 11.2.1-1)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy
>>> numpy.version.version
'1.21.2'
>>> 
```
```
❯ python
Python 3.9.7 (default, Aug 30 2021, 00:00:00) 
[GCC 11.2.1 20210728 (Red Hat 11.2.1-1)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.21.2 3.9.7 (default, Aug 30 2021, 00:00:00) 
[GCC 11.2.1 20210728 (Red Hat 11.2.1-1)]
>>> 

```",2021-10-12 12:19:32,,BUG:  F2py creates bad wrapper functions when fortran code has nested procedures,"['00 - Bug', 'component: numpy.f2py']"
20099,open,vnmabus,"### Describe the issue:

A operation between a `NDArray[Any]` array and an integer, as in the code example, makes Mypy infer the result as
`numpy.ndarray[Any, numpy.dtype[numpy.signedinteger[Any]]]`. This is an error, as the result could also be a floating point array, or even a complex one.

### Reproduce the code example:

```python
from numpy.typing import NDArray

def f(arr: NDArray[Any]) -> None:
    reveal_type(arr + 2)
```


### Error message:

_No response_

### NumPy/Python version information:

1.21.2 3.7.6 (default, Jan  8 2020, 19:59:22) 
[GCC 7.3.0]",2021-10-11 18:20:19,,BUG: `NDArray[Any]` methods ignore overload ambiguity,"['00 - Bug', 'Static typing']"
20095,open,HaoZeke,"### Issue with current documentation:

At a recent docs meeting, it was noted that the ""Python as Glue"" is written in a style unlike the rest of the docs.

Note that this is not a priority, and simply a minor tracking issue.

### Idea or request for content:

_No response_",2021-10-11 17:00:07,,"DOC:  Refactor ""Python as Glue""","['04 - Documentation', 'component: numpy.f2py']"
20092,open,MaxGhenis,"### Proposed new feature or change:

`numpy.isin` is equivalent to `pandas.{Series,DataFrame}.isin`. Adding `numpy.ndarray.isin` would produce consistency with `pandas` and save 4 characters:
```python
np.isin(x, y)  # Current
x.isin(y)      # Proposed
```

I've posted this idea to the forum [here](https://mail.python.org/archives/list/numpy-discussion@python.org/thread/6QJJLJSM4T562YOOU3XJ774PYM6ADQVI/).",2021-10-11 14:34:00,,ENH: `ndarray.isin`,['01 - Enhancement']
20090,open,azatsman,"### Describe the issue:

The result of 1D np.correlate has the order reversed compared to what would be expected from the API documentation.

### Reproduce the code example:

```python
import numpy as np

# From Numpy documentation for nump.correlate in
# https://numpy.org/doc/stable/reference/generated/numpy.correlate.html:
#
# c_{av}[k] = sum_n a[n+k] * conj(v[n])
#
# Thus if a = [1,1] and v = [1,2,3,4,5,6] we would expect c_{av}[0] = 3
# (I.e., c_{av}[0] = a[0]*v[0] + a[1]*v[1]).
#
# But: in Numpy 1.17.4, Python 3.8.10, Linux Mint 20.2, the result is 11, and
# the full output of the code below is
#
# a :  [1 1]
# v :  [1 2 3 4 5 6]
# c_{av} :  [11  9  7  5  3]

a   = np.array ([1,1]);
v   = np.arange (1,7);
cav = np.correlate (a,v, mode='valid');

print (""a      : "", a)
print (""v      : "", v)
print (""c_{av} : "", cav);
```


### Error message:

_No response_

### NumPy/Python version information:

1.17.4 3.8.10 (default, Sep 28 2021, 16:10:42) 
[GCC 9.3.0]
",2021-10-11 13:19:42,,BUG: numpy.correlate result does not match the documentation,"['04 - Documentation', '33 - Question', 'sprintable']"
20072,open,rdbisme,"### Describe the issue:

In my code I'm using a slightly customized numpy subclass. When I sum two numpy arrays viewed as this class, the result has a different type. I'd expect the sum to be of the same type as the two addends. 

### Reproduce the code example:

```python
import numpy as np


class State(np.ndarray):
    pass


a: State = np.random.rand(10, 5).view(State)
b: State = np.random.rand(10, 5).view(State)

reveal_type(a)
reveal_type(b)
reveal_type(a + b)
```


### Error message:

```shell
test-mypy-numpy.py:12: note: Revealed type is ""test-mypy-numpy.State""
test-mypy-numpy.py:13: note: Revealed type is ""test-mypy-numpy.State""
test-mypy-numpy.py:14: note: Revealed type is ""numpy.ndarray[Any, numpy.dtype[numpy.bool_]]""
```


### NumPy/Python version information:
Python 3.9.5
Numpy: 1.21.2
Mypy: 0.910",2021-10-08 12:46:37,,BUG: Sum of ndarray subclassess incorrectly typed as np.ndarray,"['00 - Bug', 'Static typing']"
20062,open,DimitriPapadopoulos,"### Issue with current documentation:

1. The documentation In [`numpy/core/defchararray.py`](https://github.com/numpy/numpy/blob/main/numpy/core/defchararray.py) still makes a distinction between `str` and `unicode`, for historical reasons I suppose:
https://github.com/numpy/numpy/blob/8d2ddb53a9dae2d90a752ad4995c30817049fe06/numpy/core/defchararray.py#L6-L13
The code itself, including internal function names, still uses the terms `unicode` and `str`, and as far as I can see always as aliases of `bytes` and `str`. As a first step, without modifying the code itself , shouldn't occurrences of `unicode` be removed from the documentation? Or `unicode` and `str` changed to `str` and `bytes`? I am not sure as I haven't understood yet the expected type of `_vec_string()` arguments.
I suppose ""_if the corresponding string method is  available in your version of Python_"" refers to Python 2. Shouldn't this sentence be removed?

2. I can still see [`string_`](https://numpy.org/doc/1.21/reference/arrays.scalars.html#numpy.string_) and [`unicode_`](https://numpy.org/doc/1.21/reference/arrays.scalars.html#numpy.unicode_) used in the documentation, but nowadays these are aliases of [`bytes_`](https://numpy.org/doc/1.21/reference/arrays.scalars.html#numpy.bytes_) and [`str_`](https://numpy.org/doc/1.21/reference/arrays.scalars.html#numpy.str_) respectively. Shouldn't all occurrences of [`string_`](https://numpy.org/doc/1.21/reference/arrays.scalars.html#numpy.string_) and [`unicode_`](https://numpy.org/doc/1.21/reference/arrays.scalars.html#numpy.unicode_) be changed to [`bytes_`](https://numpy.org/doc/1.21/reference/arrays.scalars.html#numpy.bytes_) and [`str_`](https://numpy.org/doc/1.21/reference/arrays.scalars.html#numpy.str_)?

3. Then perhaps the code itself in [`numpy/core/defchararray.py`](https://github.com/numpy/numpy/blob/main/numpy/core/defchararray.py) should be changed accordingly.

4. Shouldn't the `unicode` keyword argument disappear? For example:
    `class numpy.chararray(shape, itemsize=1, unicode=False, buffer=None, offset=0, strides=None, order=None)`
 
5. Finally:
https://github.com/numpy/numpy/blob/8d2ddb53a9dae2d90a752ad4995c30817049fe06/numpy/core/defchararray.py#L15
Yet the documentation has two duplicate entries. Shouldn't the second entry point to the first one?
   * [`numpy.chararray`](https://numpy.org/doc/1.21/reference/generated/numpy.chararray.html)
   * [`numpy.char.chararray`](https://numpy.org/doc/1.21/reference/generated/numpy.char.chararray.html)


### Idea or request for content:

_No response_",2021-10-07 09:36:27,,DOC: unicode/str/bytes in Python 3,['04 - Documentation']
20053,open,HaoZeke,"Well, fair enough, this is messed up anyway.  Although, I don't think it makes sense to ignore _these_ errors.  It may make sense to ignore some (python) errors, but these are blanket ignores that may make writing clean python code hard.

_Originally posted by @seberg in https://github.com/numpy/numpy/pull/20043#discussion_r722771699_",2021-10-06 16:01:11,,"ENH,MAINT: Fix up `fortranobject.c` and generated `C` code from F2PY",['component: numpy.f2py']
20044,open,jbrockmendel,"### Proposed new feature or change:

Context: I'm trying to implement `pandas.Index.__array_ufunc__` and `pandas.core.arrays.ExtensionArray.__array_ufunc__`

It would be helpful if there were a `np.default_array_ufunc` such that:

```
def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):
     return np.default_array_ufunc(self, ufunc, method, *inputs, **kwargs)
```

would be equivalent to not implementing `__array_ufunc__` at all.

This would make it feasible to implement `__array_ufunc__` incrementally, in this case by using [maybe_dispatch_ufunc_to_dunder_op](https://github.com/pandas-dev/pandas/blob/master/pandas/_libs/ops_dispatch.pyx#L63) (a little bit like a reversed `NDArrayOperatorsMixin`) 

```
def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):
    result = maybe_dispatch_ufunc_to_dunder_op(self, ufunc, method, *inputs, **kwargs)
    if result is not NotImplemented:
        return result
    return np.default_array_ufunc(self, ufunc, method, *inputs, **kwargs)
```

My not-quite-right implementation reads

```
def default_array_ufunc(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):
    """"""
    Fallback to the behavior we would get if we did not define __array_ufunc__.

    Notes
    -----
    We are assuming that `self` is among `inputs`.
    """"""
    if not any(x is self for x in inputs):
        raise NotImplementedError

    new_inputs = [x if x is not self else np.asarray(x) for x in inputs]

    return getattr(ufunc, method)(*new_inputs, **kwargs)
```

This doesn't _quite_ achieve the goal of behaving as if `__array_ufunc__` were not implemented.  If it did, I think it would be better to live in numpy than in pandas.

xref #18186 motivated by trying to implement `__array_function__` incrementally.",2021-10-05 20:21:42,,ENH: np.default_array_ufunc,['unlabeled']
20028,open,anntzer,"### Issue with current documentation:

Consider e.g.
```python
np.interp(x, [0, 1, 1, 2], [a, b, c, d])
```
If `x` does not contain the value 1, then the interpolation is well defined (it's a discontinuous piecewise linear function, going from a to b on [0, 1] and then from c to d on [1, 2]).  AFAICT, np.interp indeed implements this correctly.

However, the docs of interp explicitly state that `xp` needs to be (strictly) increasing (thus without any duplicated values).  It may be worth explicitly stating whether the behavior in the presence of duplicated xp's (i.e., modelling a discontinuous piecewise linear function) is guaranteed, or whether one cannot rely on it.

### Idea or request for content:

_No response_",2021-10-04 15:44:19,,DOC: Clarify behavior of interp in presence of duplicated x's (= discontinuities),['04 - Documentation']
20022,open,iam-abbas,"### Describe the issue:

`np.ma.testutils.assert_equal(np.nan, np.nan)` throws AssertionError while `np.testing.assert_equal(np.nan, np.nan)` works as expected.

It is duplicate of #6661 which was opened in 2015 and it is still unfixed.

### Reproduce the code example:

```python
import numpy as np

np.ma.testutils.assert_equal(np.nan, np.nan)
```


### Error message:

```shell
Traceback (most recent call last)
<ipython-input-5-e131339d780a> in <module>()
----> 1 assert_equal(np.nan, np.nan)

/usr/local/lib/python3.7/dist-packages/numpy/ma/testutils.py in assert_equal(actual, desired, err_msg)
    127         msg = build_err_msg([actual, desired], err_msg,)
    128         if not desired == actual:
--> 129             raise AssertionError(msg)
    130         return
    131     # Case #4. arrays or equivalent

AssertionError: 
Items are not equal:
 ACTUAL: nan
 DESIRED: nan
```


### NumPy/Python version information:

1.19.5 3.7.12 (default, Sep 10 2021, 00:21:48) 
[GCC 7.5.0]",2021-10-02 19:46:21,,BUG: np.ma.testutils.assert_equal error on two np.nan,['00 - Bug']
19996,open,HaoZeke,"Although perhaps it would be best to remove the sizeof logic altogether and assign the members (`real`, `imaginary`) of the struct `npy_c<type` instead.

_Originally posted by @HaoZeke in https://github.com/numpy/numpy/pull/19978#discussion_r718151763_",2021-09-29 05:07:06,,Use `npy_c<TYPE>` for `f2py`,['component: numpy.f2py']
19975,open,charris,"Some of the entries in the blocklist may be fixed in later versions. In particular,
```
#if defined(_MSC_VER) && (_MSC_VER >= 1900)
```
may be overly broad, version 1900 was Visual Studio 2015.",2021-09-27 19:06:47,,Update blocklist in numpy/core/src/common/npy_config.h,['17 - Task']
19970,open,xegulon,"## Feature

<!-- If you're looking to request a new feature or change in functionality, including
adding or changing the meaning of arguments to an existing function, please
post your idea on the [numpy-discussion mailing list]
(https://mail.python.org/mailman/listinfo/numpy-discussion) to explain your
reasoning in addition to opening an issue or pull request. You can also check
out our [Contributor Guide]
(https://github.com/numpy/numpy/blob/main/doc/source/dev/index.rst) if you
need more information. -->

The [`np.array_split`](https://numpy.org/doc/stable/reference/generated/numpy.array_split.html?highlight=array_split#numpy-array-split) function takes an `indices_or_sections` argument which, when it is an integer, makes the function return a _list of `indices_or_sections` subarrays_.

For example:
```python
import numpy as np
l = 17 * ['a']
np.array_split(np.array(l), 4)
>>> [array(['a', 'a', 'a', 'a', 'a'], dtype='<U1'),
     array(['a', 'a', 'a', 'a'], dtype='<U1'),
     array(['a', 'a', 'a', 'a'], dtype='<U1'),
     array(['a', 'a', 'a', 'a'], dtype='<U1')]
```

What I would like it to return it a list of subarrays, all of size `batch_size` (except maybe the last).
Example of what I want:
```python
import numpy as np
l = 17 * ['a']
np.array_split(np.array(l), batch_size=4)
>>> [array(['a', 'a', 'a', 'a'], dtype='<U1'),
     array(['a', 'a', 'a', 'a'], dtype='<U1'),
     array(['a', 'a', 'a', 'a'], dtype='<U1'),
     array(['a', 'a', 'a', 'a'], dtype='<U1'),
     array(['a'], dtype='<U1')]
```",2021-09-27 13:04:04,,Add ability to use `batch_size` parameter in `np.array_split`,['unlabeled']
19923,open,david-bentley,"When I try to do `np.sqrt(1j * masked_array)`, where `masked_array` is a MaskedArray containing complex values, the result is `[-- --]`. The values in `np.sqrt(1j * masked_array).data` are not as expected, but I can obtain the expected results using `np.sqrt(1j * masked_array.data)` directly.

### Reproducing code example:

The following code demonstrates the issue, in particular the `print(np.sqrt(1j * k_complex_masked))` line

```python
import numpy as np

N = 2

k_real = np.random.random(N)
k_complex = np.random.random(N) + 1j * np.random.random(N)
k_real_masked = np.ma.MaskedArray(k_real, mask=False, fill_value=1e20)
k_complex_masked = np.ma.MaskedArray(k_complex, mask=False, fill_value=1e20)

print(np.sqrt(k_real))
print(np.sqrt(k_complex))
print(np.sqrt(k_real_masked))
print(np.sqrt(k_complex_masked))

print(np.sqrt(1j * k_real))
print(np.sqrt(1j * k_complex))
print(np.sqrt(1j * k_real_masked))
print(np.sqrt(1j * k_complex_masked))

print(np.sqrt(1j * k_complex_masked.data))
print(np.sqrt(1j * k_complex_masked).data)
```

gives

```python
[0.92273101 0.44957708]
[0.69839729+0.51669031j 0.54124365+0.538087j  ]
[0.9227310061479015 0.4495770836357023]
[(0.6983972916989899+0.5166903094712504j)
 (0.5412436537163824+0.5380870012221408j)]
[0.65246935+0.65246935j 0.317899  +0.317899j  ]
[0.12848624+0.85919668j 0.00223209+0.76320203j]
[(0.652469351658267+0.652469351658267j)
 (0.31789900450487674+0.31789900450487674j)]
[-- --]
[0.12848624+0.85919668j 0.00223209+0.76320203j]
[0.+0.j 0.+0.j]
```

### NumPy/Python version information:

1.21.2 3.7.11 (default, Jul 27 2021, 14:32:16) 
[GCC 7.5.0]",2021-09-22 15:06:55,,np.sqrt(1j * MaskedArray) returns a masked array with invalid data values,"['00 - Bug', 'component: numpy.ma']"
19896,open,dcaliste,"## Feature

This feature would be based on Fortran derived-types support in `f2py`. When defining a new Fortran type, one can also define module procedures that overrides the assignment or operators on this derived-type.

#15006 is providing support to properly parse operator() and assigment() definitions from a module file. Depending on how Fortran derived-type support is implemented in f2py, operator() and assigment() definitions could then be used.",2021-09-20 12:16:53,,Add support for operator() and assignment() module procedures from Fortran (f2py),['component: numpy.f2py']
19891,open,Kai-Striega,"<!-- Please describe the issue in detail here, and fill in the fields below -->
I'm trying to follow the documentation from [example-numpy-ufunc-for-one-dtype](https://numpy.org/doc/stable/user/c-info.ufunc-tutorial.html#example-numpy-ufunc-for-one-dtype) to produce a simple numpy ufunc. 

The full build log is included at the end of the issue (as it's quite long) however the module builds the file ``single_type_logit.o`` as expected raising no errors but two warnings.

```
In file included from /home/kai/CLionProjects/examples/ufunc/venv/lib/python3.8/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969,
                 from single_type_logit.c:3:
/home/kai/CLionProjects/examples/ufunc/venv/lib/python3.8/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning ""Using deprecated NumPy API, disable it with "" ""#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION"" [-Wcpp]
   17 | #warning ""Using deprecated NumPy API, disable it with "" \
      |  ^~~~~~~
single_type_logit.c:52:36: warning: initialization of ‘void (*)(char **, const npy_intp *, const npy_intp *, void *)’ {aka ‘void (*)(char **, const long int *, const long int *, void *)’} from incompatible pointer type ‘void (*)(char **, npy_intp *, npy_intp *, void *)’ {aka ‘void (*)(char **, long int *, long int *, void *)’} [-Wincompatible-pointer-types]
   52 | PyUFuncGenericFunction funcs[1] = {&double_logit};
      |                                    ^
single_type_logit.c:52:36: note: (near initialization for ‘funcs[0]’)
```

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->
1. Copy code from tutorial into ``single_type_logit.c``and ``setup.py`` files
2. python setup.py clean
3. python setup.py build_ext --inplace

```python 
>>> import numpy as np
>>> import npufunc
```

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/main/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'npufunc'
```

### NumPy/Python version information:

```
>>> np.__version__
'1.21.2'
>>> import sys
>>> sys.version
'3.8.10 (default, Jun  2 2021, 10:49:15) \n[GCC 9.4.0]'
```
<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

```
running build_ext
running build_src
build_src
building extension ""npufunc_directory.npufunc"" sources
build_src: building npy-pkg config files
customize UnixCCompiler
customize UnixCCompiler using build_ext
CCompilerOpt.cc_test_flags[1013] : testing flags (-march=native)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

creating /tmp/tmpm22vu8i0/home
creating /tmp/tmpm22vu8i0/home/kai
creating /tmp/tmpm22vu8i0/home/kai/CLionProjects
creating /tmp/tmpm22vu8i0/home/kai/CLionProjects/examples
creating /tmp/tmpm22vu8i0/home/kai/CLionProjects/examples/ufunc
creating /tmp/tmpm22vu8i0/home/kai/CLionProjects/examples/ufunc/venv
creating /tmp/tmpm22vu8i0/home/kai/CLionProjects/examples/ufunc/venv/lib
creating /tmp/tmpm22vu8i0/home/kai/CLionProjects/examples/ufunc/venv/lib/python3.8
creating /tmp/tmpm22vu8i0/home/kai/CLionProjects/examples/ufunc/venv/lib/python3.8/site-packages
creating /tmp/tmpm22vu8i0/home/kai/CLionProjects/examples/ufunc/venv/lib/python3.8/site-packages/numpy
creating /tmp/tmpm22vu8i0/home/kai/CLionProjects/examples/ufunc/venv/lib/python3.8/site-packages/numpy/distutils
creating /tmp/tmpm22vu8i0/home/kai/CLionProjects/examples/ufunc/venv/lib/python3.8/site-packages/numpy/distutils/checks
compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-march=native'
CCompilerOpt.cc_test_flags[1013] : testing flags (-O3)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-O3'
CCompilerOpt.cc_test_flags[1013] : testing flags (-Werror)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-Werror'
CCompilerOpt.__init__[1701] : check requested baseline
CCompilerOpt.cc_test_flags[1013] : testing flags (-msse)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse'
CCompilerOpt.cc_test_flags[1013] : testing flags (-msse2)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse2'
CCompilerOpt.feature_test[1466] : testing feature 'SSE2' with flags (-msse -msse2)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -Werror'
CCompilerOpt.feature_test[1466] : testing feature 'SSE' with flags (-msse -msse2)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -Werror'
CCompilerOpt.cc_test_flags[1013] : testing flags (-msse3)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse3'
CCompilerOpt.feature_test[1466] : testing feature 'SSE3' with flags (-msse -msse2 -msse3)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3 -Werror'
CCompilerOpt.__init__[1710] : check requested dispatch-able features
CCompilerOpt.cc_test_flags[1013] : testing flags (-mssse3)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-mssse3'
CCompilerOpt.cc_test_flags[1013] : testing flags (-msse4.1)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse4.1'
CCompilerOpt.cc_test_flags[1013] : testing flags (-mpopcnt)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-mpopcnt'
CCompilerOpt.cc_test_flags[1013] : testing flags (-msse4.2)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse4.2'
CCompilerOpt.feature_test[1466] : testing feature 'SSE42' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -Werror'
CCompilerOpt.feature_test[1466] : testing feature 'POPCNT' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -Werror'
CCompilerOpt.feature_test[1466] : testing feature 'SSE41' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -Werror'
CCompilerOpt.feature_test[1466] : testing feature 'SSSE3' with flags (-msse -msse2 -msse3 -mssse3)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3 -mssse3 -Werror'
CCompilerOpt.cc_test_flags[1013] : testing flags (-mavx)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-mavx'
CCompilerOpt.feature_test[1466] : testing feature 'AVX' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -Werror'
CCompilerOpt.cc_test_flags[1013] : testing flags (-mf16c)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-mf16c'
CCompilerOpt.cc_test_flags[1013] : testing flags (-mfma)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-mfma'
CCompilerOpt.cc_test_flags[1013] : testing flags (-mavx2)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-mavx2'
CCompilerOpt.cc_test_flags[1013] : testing flags (-mavx512f)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-mavx512f'
CCompilerOpt.feature_test[1466] : testing feature 'AVX512F' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -Werror'
CCompilerOpt.feature_test[1466] : testing feature 'FMA3' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -Werror'
CCompilerOpt.feature_test[1466] : testing feature 'F16C' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -Werror'
CCompilerOpt.feature_test[1466] : testing feature 'AVX2' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mavx2)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mavx2 -Werror'
CCompilerOpt.cc_test_flags[1013] : testing flags (-mavx512cd)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-mavx512cd'
CCompilerOpt.feature_test[1466] : testing feature 'AVX512CD' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -Werror'
CCompilerOpt.cc_test_flags[1013] : testing flags (-mavx512vl -mavx512bw -mavx512dq)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-mavx512vl -mavx512bw -mavx512dq'
CCompilerOpt.feature_test[1466] : testing feature 'AVX512_SKX' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq -Werror'
CCompilerOpt.cc_test_flags[1013] : testing flags (-mavx512ifma -mavx512vbmi)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-mavx512ifma -mavx512vbmi'
CCompilerOpt.feature_test[1466] : testing feature 'AVX512_CNL' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq -mavx512ifma -mavx512vbmi)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq -mavx512ifma -mavx512vbmi -Werror'
CCompilerOpt.cc_test_flags[1013] : testing flags (-mavx512er -mavx512pf)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-mavx512er -mavx512pf'
CCompilerOpt.feature_test[1466] : testing feature 'AVX512_KNL' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512er -mavx512pf)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512er -mavx512pf -Werror'
CCompilerOpt.cc_test_flags[1013] : testing flags (-mavx5124fmaps -mavx5124vnniw -mavx512vpopcntdq)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-mavx5124fmaps -mavx5124vnniw -mavx512vpopcntdq'
CCompilerOpt.feature_test[1466] : testing feature 'AVX512_KNM' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512er -mavx512pf -mavx5124fmaps -mavx5124vnniw -mavx512vpopcntdq)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512er -mavx512pf -mavx5124fmaps -mavx5124vnniw -mavx512vpopcntdq -Werror'
CCompilerOpt.cc_test_flags[1013] : testing flags (-mavx512vnni)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-mavx512vnni'
CCompilerOpt.feature_test[1466] : testing feature 'AVX512_CLX' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq -mavx512vnni)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq -mavx512vnni -Werror'
CCompilerOpt.cc_test_flags[1013] : testing flags (-mavx512vbmi2 -mavx512bitalg -mavx512vpopcntdq)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-mavx512vbmi2 -mavx512bitalg -mavx512vpopcntdq'
CCompilerOpt.feature_test[1466] : testing feature 'AVX512_ICL' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq -mavx512vnni -mavx512ifma -mavx512vbmi -mavx512vbmi2 -mavx512bitalg -mavx512vpopcntdq)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq -mavx512vnni -mavx512ifma -mavx512vbmi -mavx512vbmi2 -mavx512bitalg -mavx512vpopcntdq -Werror'
CCompilerOpt.__init__[1722] : skip features (SSE SSE3 SSE2) since its part of baseline
CCompilerOpt.__init__[1726] : initialize targets groups
CCompilerOpt.__init__[1728] : parse target group simd_test
CCompilerOpt._parse_target_tokens[1939] : skip targets (FMA4 VSX3 VSX VSX2 NEON ASIMD XOP) not part of baseline or dispatch-able features
CCompilerOpt._parse_policy_not_keepbase[2051] : skip baseline features (SSE2)
CCompilerOpt.generate_dispatch_header[2272] : generate CPU dispatch header: (build/src.linux-x86_64-3.8/numpy/distutils/include/npy_cpu_dispatch_config.h)
CCompilerOpt.feature_extra_checks[1546] : Testing extra checks for feature 'AVX512F' (AVX512F_REDUCE)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -Werror'
CCompilerOpt.feature_extra_checks[1546] : Testing extra checks for feature 'AVX512_SKX' (AVX512BW_MASK AVX512DQ_MASK)
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq -Werror'
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq -Werror'
building 'npufunc_directory.npufunc' extension
compiling C sources
C compiler: x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC

compile options: '-I/home/kai/CLionProjects/examples/ufunc/venv/lib/python3.8/site-packages/numpy/core/include -Ibuild/src.linux-x86_64-3.8/numpy/distutils/include -I/home/kai/CLionProjects/examples/ufunc/venv/include -I/usr/include/python3.8 -c'
extra options: '-msse -msse2 -msse3'
x86_64-linux-gnu-gcc: single_type_logit.c
x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/single_type_logit.o -o npufunc_directory/npufunc.cpython-38-x86_64-linux-gnu.so

########### EXT COMPILER OPTIMIZATION ###########
Platform      : 
  Architecture: x64
  Compiler    : gcc

CPU baseline  : 
  Requested   : 'min'
  Enabled     : SSE SSE2 SSE3
  Flags       : -msse -msse2 -msse3
  Extra checks: none

CPU dispatch  : 
  Requested   : 'max -xop -fma4'
  Enabled     : SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512F AVX512CD AVX512_KNL AVX512_KNM AVX512_SKX AVX512_CLX AVX512_CNL AVX512_ICL
  Generated   : none
CCompilerOpt.cache_flush[809] : write cache to path -> /home/kai/CLionProjects/examples/ufunc/build/temp.linux-x86_64-3.8/ccompiler_opt_cache_ext.py
```",2021-09-19 07:28:54,,``example-numpy-ufunc-for-one-dtype`` fails to import new ufunc,['unlabeled']
19877,open,sebschub,"For masked arrays, `isin()` and `in1d()` produce wrong results. In particular, the result is masked at wrong places. The issue was discussed initially on [stackoverflow](https://stackoverflow.com/questions/69160969/) and it seems that the involved sorting does not handle the masked value correctly.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy.ma as ma
a = ma.MaskedArray([[1,2,3],[4,5,6]], [[True,False,False],[False,False,False]])
ta = ma.array([1,4,5])
ma.isin(a, ta)
```
The output of ma.isin() is wrong (see below). I use the following workaround:
```python
import numpy as np
def ma_isin(array, comparison):
    return ma.MaskedArray(
        data=np.isin(array, comparison),
        mask=array.mask.copy())
```


### Output and expected results:
These are the results from the code above
```python
>>> a
masked_array(
  data=[[--, 2, 3],
        [4, 5, 6]],
  mask=[[ True, False, False],
        [False, False, False]],
  fill_value=999999)
>>> ta
masked_array(data=[1, 4, 5],
             mask=False,
       fill_value=999999)
>>> ma.isin(a, ta)
masked_array(
  data=[[False, False, False],
        [True, True, --]],
  mask=[[False, False, False],
        [False, False,  True]],
  fill_value=True)
```
The issue is mainly that the last element in the result is masked, and probably also that the first element is not masked. I would expect the following output:
```python
>>> ma_isin(a, ta)
masked_array(
  data=[[--, False, False],
        [True, True, False]],
  mask=[[ True, False, False],
        [False, False, False]],
  fill_value=True)
```

`in1d()` shows an analogue issue.


### NumPy/Python version information:

1.21.2 3.9.4 (default, Apr  9 2021, 16:34:09) 
[GCC 7.3.0]

",2021-09-15 04:22:48,,Wrong results of ma.isin() and ma.in1d() for masked arrays,"['00 - Bug', 'component: numpy.ma']"
19876,open,WarrenWeckesser,"
In [`ufunclab`](https://github.com/WarrenWeckesser/ufunclab), I create the gufunc `min_argmin` (source: [`minmax_gufunc.c.src`](https://github.com/WarrenWeckesser/ufunclab/blob/main/src/minmax/minmax_gufunc.c.src)).  The function `min_argmin` is a gufunc with signature `'(i)->(),()'`:

```
In [305]: from ufunclab import min_argmin

In [306]: min_argmin.signature
Out[306]: '(i)->(),()'
```

For example,

```
In [308]: a
Out[308]: 
array([[ 0.2,  2.6,  0.4, -0.7, -1.2,  0.3, -1.6, -1.3,  0.7],
       [-0.4,  0.6, -1.8, -0.6, -0.2, -1.4,  0.7, -0.1,  0.6]])

In [309]: a.shape
Out[309]: (2, 9)
```

By default, such a gufunc operates along the last axis:

```
In [310]: vals, inds = min_argmin(a)

In [311]: vals
Out[311]: array([-1.6, -1.8])

In [312]: inds
Out[312]: array([6, 2])
```
`axis=0` tells it to operate along the first axis:
```
In [313]: vals, inds = min_argmin(a, axis=0)

In [314]: vals
Out[314]: array([-0.4,  0.6, -1.8, -0.7, -1.2, -1.4, -1.6, -1.3,  0.6])

In [315]: inds
Out[315]: array([1, 1, 1, 0, 0, 1, 0, 0, 1])
```
The gufunc automatically includes the `keepdims` parameter, and it works as expected when combined with the `axis` parameter:
```
In [316]: vals, inds = min_argmin(a, keepdims=True)

In [317]: vals
Out[317]: 
array([[-1.6],
       [-1.8]])

In [318]: vals.shape
Out[318]: (2, 1)

In [319]: vals, inds = min_argmin(a, keepdims=True, axis=0)

In [320]: vals
Out[320]: array([[-0.4,  0.6, -1.8, -0.7, -1.2, -1.4, -1.6, -1.3,  0.6]])

In [321]: vals.shape
Out[321]: (1, 9)
```
The gufunc also accepts the `axes` parameter.  Given this gufunc's signature, we should be able to use `axes=[(0,)]` or `axes=[(1,)]`, and those should be equivalent to `axis=0` or `axis=1`, respectively. Without `keepdims`, it works as expected:

```

In [322]: min_argmin(a, axes=[(0,)])
Out[322]: 
(array([-0.4,  0.6, -1.8, -0.7, -1.2, -1.4, -1.6, -1.3,  0.6]),
 array([1, 1, 1, 0, 0, 1, 0, 0, 1]))

In [323]: min_argmin(a, axes=[(1,)])
Out[323]: (array([-1.6, -1.8]), array([6, 2]))
```

But when both `axes=[(0,)]` and `keepdims=True` are given, the shapes of the output arrays are not correct--they appear to transposed:

```
In [325]: vals, inds = min_argmin(a, axes=[(0,)], keepdims=True)

In [326]: vals.shape  # This should be (1, 9)
Out[326]: (9, 1)
```

I haven't found any obvious problems in my code in `minmax_gufunc.c.src`, and the handling of `axis`, `axes` and `keepdims` is included automatically as part of the gufunc machinery, so I suspect the bug is in NumPy.

### NumPy/Python version information:

Python 3.9.5
NumPy 1.21.2


",2021-09-15 02:31:44,,BUG: gufunc: Incorrect shape returned when both `axes` and `keepdims` given.,"['00 - Bug', 'component: numpy.ufunc']"
19875,open,leycec,"## Documentation

<!-- If this is an issue with the current documentation for NumPy (e.g.
incomplete/inaccurate docstring, unclear explanation in any part of the
documentation), make sure to leave a reference to the document/code you're
referring to. You can also check the development version of the documentation
and see if this issue has already been addressed: https://numpy.org/devdocs/
-->

<!-- If this is an idea or a request for content, please describe as clearly as
possible what topics you think are missing from the current documentation. Make
sure to check https://github.com/numpy/numpy-tutorials and see if this issue
might be more appropriate there. -->

The [well-documented **Typing** subsection](https://numpy.org/devdocs/reference/typing.html) exhaustively details our typed NumPy API (which is great), complete with concise examples (which is even greater). With [one notable exception pertaining to mypy](https://numpy.org/devdocs/reference/typing.html#examples), however, this subsection lacks a substantive discussion of actual *usage* – how NumPy type hints are actually used, applied, and enforced in real-world practice (which is less than great).

A trailing subsubsection briefly enumerating usage practicalities could prove useful, especially if doing so curtails discussion churn elsewhere (e.g., on [Gitter](https://gitter.im/numpy/numpy) or this issue tracker).

Since NumPy itself and Python interpreters in general silently ignore annotations, NumPy type hints have no force or effect in and of themselves; by design, they reduce to noops in the absence of external third-party support. There only exist two PEP-compliant type checkers explicitly supporting the typed NumPy API. <sup>*...to my limited knowledge*</sup> Usage of NumPy type hints thus presupposes use of one or both of these type checkers.

**Drumroll, please.** These are...

## Static Type Checking

Static type checkers explicitly supporting the typed NumPy API include:

* [**mypy**](http://mypy-lang.org), of course.

There exist three other well-maintained static type checkers, but it's unclear whether they currently support the typed NumPy API:

* [FaceBook's **Pyre**](https://pyre-check.org).
* [Google's **pytype**](https://github.com/google/pytype).
* [Microsoft's **pyright**](https://github.com/Microsoft/pyright).

They probably don't (yet), but probably will (eventually). A simple list or table enumerating which of these checkers support NumPy type hints – and as of which checker version – would go a long way to addressing usage. Likewise...

## Runtime Type Checking

Runtime type checkers explicitly supporting the typed NumPy API include:

* [**beartype**](https://github.com/beartype/beartype#numpy-type-hints), surprisingly.

There exist two other well-maintained runtime type checkers that sadly both fail to support the typed NumPy API:

* [**Pydantic**](https://pydantic-docs.helpmanual.io).
* [**typical**](https://github.com/seandstewart/typical).

Again, a simple list or table enumerating which of these checkers support NumPy type hints – and as of which checker version – would simplify everyone's hectic focus on revolutionizing the world with NumPy.

## That's All the Data Scientist Wrote

Thanks for all the jaw-dropping volunteerism, NumPy doco evangelists! I remain in awe of your commitment to sound data science in Python, the language of AI. Everyone here deserves all the thunderous applause I have to give. Sadly, these emoji will have to do.

:clap: :zap: :clap: ",2021-09-15 02:16:24,,DOC: Typed NumPY API usage,"['04 - Documentation', 'Static typing']"
19861,open,defoishugo,"Hi,

When filling *inf* values generated by impossible casting, the *numpy.nan_to_num* fails setting *inf* values to another value.

### Reproducing code example:

```
import numpy as np

array_with_non_float16_values = [
    [839735.3786387637, 83.37],
    [0.0, 839735.3786387637],
    [123, 95.87]
]

dtype = [
    (""a"", ""float16""),
    (""b"", ""float16"")
]

np_array = np.array(array_with_non_float16_values, dtype=dtype)
np_array_without_nan = np.nan_to_num(np_array, nan=0, posinf=0, neginf=0)

print(np_array_without_nan)
```

### Output obtained:

```
> python test.py
[[(  inf,   inf) ( 83.4,  83.4)]
 [(  0. ,   0. ) (  inf,   inf)]
 [(123. , 123. ) ( 95.9,  95.9)]]
```

### Output desired:

```
> python test.py
[[(  0. ,   0. ) ( 83.4,  83.4)]
 [(  0. ,   0. ) (  0. ,   0. )]
 [(123. , 123. ) ( 95.9,  95.9)]]
```

### NumPy/Python version information:

1.21.1 3.8.10 | packaged by conda-forge | (default, May 11 2021, 06:25:23) [MSC v.1916 64 bit (AMD64)]

### Comments:

I would like to work on resolving this issue.
I can do a pull-request as soon as possible.

This issue is blocking some of my projects, so I really need to provide a fix.
@seberg Just tell me if this is ok for you.",2021-09-10 18:02:53,,BUG: `np.nan_to_num` does nothing for structured dtypes,['unlabeled']
19853,open,seberg,"It would be nice to have python code coverage as part of CI (same as C code coverage).  Unlike the C code coverage, it should be even easier to get very good coverage (since impossible to test error paths are fairly common in C).

I think it would be generally good.  One thing where I am noticing it now is that `loadtxt` ~is~ has terrible test coverage, and code coverage of the existing code would also help with replacing that code (I can run that local for that concrete use-case).  It might also identify some sprint-able/LGTM style small contributions.

Plus, I think it might simplify review a bit: Instead of asking to make sure that error paths are covered, we can ask to make sure code coverage annotations are gone, giving a clearer picture of how many tests are missing (and why that is not good).",2021-09-08 23:51:01,,CI: Add code coverage for Python code,"['17 - Task', 'Project', 'component: CI']"
19852,open,seberg,"gh-8211 proposed adding a `cov2corr` function to calculate the `corrcoef` directly from a known covariance matrix.  The discussion there also suggested to add this as an option to `corrcoef` instead.

This should probably be discussed again as a new API addition.  gh-8211 may be a good start for anyone interested working on it.",2021-09-08 17:18:59,,ENH: Corrcoef could allow providing the covariance matrix,"['01 - Enhancement', '62 - Python API']"
19848,open,Snape3058,"### Reproducing code example:

**Static analysis results, no POC.**
This static analysis report has been manually reviewed to verify its validity.

### Error message:

The path provided by the static analyzer is as follows.

1. A new reference is returned from function `PyUnicode_FromEncodedObject`, assigned to variable `str_object`.
https://github.com/numpy/numpy/blob/04ab04d93d4d7a4d241fe0ceb725436a8b6c8c2e/numpy/core/src/umath/ufunc_object.c#L4604

2. Assume take the true branch here.
https://github.com/numpy/numpy/blob/04ab04d93d4d7a4d241fe0ceb725436a8b6c8c2e/numpy/core/src/umath/ufunc_object.c#L4629

3. Release the reference.
https://github.com/numpy/numpy/blob/04ab04d93d4d7a4d241fe0ceb725436a8b6c8c2e/numpy/core/src/umath/ufunc_object.c#L4630

4. Call a function with all references released.
https://github.com/numpy/numpy/blob/04ab04d93d4d7a4d241fe0ceb725436a8b6c8c2e/numpy/core/src/umath/ufunc_object.c#L4637

As in step 3, all references are released. If the reference released on line 4630 is the last one, the object will be destructed. The function call on line 4637 will lead to a use-after-free bug.

### NumPy/Python version information:

Static analysis carried out on commit 04ab04d. The issue still exists in the latest version on the main branch.
",2021-09-08 05:12:43,,Potential use-after-free bug caused by using a PyObject after all its references are released,['00 - Bug']
19836,open,skoehler,"When processing large amount of data with Python, I rely on numpy to do the processing.
I seem to have no way of loading large text files with binary numbers in them, something like

```
101 11 111
110 01 000
```

I have found that `loadtxt` does handle the 0x prefix but not the 0b prefix (as available in Python), probably since you just use some strtoull function for the conversion from ASCII to numbers. On the other hand, the strtou* functions would allow me to specify a radix (2, 8, 10, 16, or even 13). This feature is currently not exposed. Also, the documentation of `loadtxt` does not seem to describe the currently implemented logic (how are prefixes like 0x handled? do leading zeros indicate octal literals? etc.).

I would like to propose that I can specify the default radix. Also, it would be nice if numpy would handle the typical Python prefixes (0x, 0o, 0b).  Please note that this is not the out-of-the-box strtou* behavior. If given a radix, it will disable prefix detection, AFAIK.

",2021-09-06 16:47:46,,loadtxt does not allow specifying a default radix,['component: numpy.lib']
19831,open,uellue,"## Feature

Currently, NumPy allows out of bounds slicing so that the following code works without error:

```python
a = np.array((1, 2, 3))

a[13:17] = 23
```

In a more complex application where the slice bounds are calculated to direct operations to specific parts of an array, miscalculating the indices will lead to wrong results without triggering an error with this behavior. Strict bounds checking would make it easier to catch such bugs.

In https://stackoverflow.com/questions/69047282/how-to-prevent-accidental-assignment-into-empty-numpy-views I've asked for possible solutions. See also https://stackoverflow.com/questions/54632256/numpy-slicing-with-bound-checks

In both cases, two options emerged:

1. Calculate the indices for the assignment to trigger an error
2. Manually validate the array bounds, for example by overriding `__getitem__()` and `_setitem__()` or by other means.

The first option carries a severe performance penalty and becomes complicated for multidimensional arrays and if indexing methods are mixed. The second option becomes complicated if all options for NumPy indexing are to be supported, and it requires either subclassing or proxying the array, or protecting each instance where the array is sliced. Both are rather cumbersome in a large code base.

Since likely some form of check is already performed within NumPy to truncate the slice to the actual array limits, it would probably be much easier to trigger an error within NumPy. I could imagine the following API:

```python
with numpy.strict_slicing():
    a[13:17] = 23  # Raises IndexError
```

Refs https://github.com/LiberTEM/LiberTEM/issues/1026",2021-09-06 07:56:53,,Allow optional strict bounds checking for slices,['unlabeled']
19825,open,charris,"There is `math.h`, which is in the C standard library, and also in five numpy locations:
```
charris@fc [numpy.git (cleanup-includes)]$ find numpy -name math.h
numpy/core/src/common/simd/vsx/math.h
numpy/core/src/common/simd/sse/math.h
numpy/core/src/common/simd/avx512/math.h
numpy/core/src/common/simd/avx2/math.h
numpy/core/src/common/simd/neon/math.h
```
This is not a problem as long as the includes are local, but it is a bit confusing.

Then there is `arrayobject.h` which is defined in two places:
```
charris@fc [numpy.git (cleanup-includes)]$ find numpy -name arrayobject.h
numpy/core/include/numpy/arrayobject.h
numpy/core/src/multiarray/arrayobject.h
```
In this case, both files can be included because they have non-standard guards.",2021-09-04 01:21:42,,"Duplicate include file names, ",['component: SIMD']
19820,open,Shulyaka,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

Unable to compile on ARMv7

### NumPy/Python version information:

Numpy 1.21.2
Python 3.8.10

### Error message:

<details>

```
root@homeassistant:~# cat /etc/alpine-release
3.13.0
root@homeassistant:~# cat /proc/cpuinfo
processor       : 0
model name      : ARMv7 Processor rev 1 (v7l)
BogoMIPS        : 1600.00
Features        : half thumb fastmult vfp edsp neon vfpv3 tls vfpd32
CPU implementer : 0x41
CPU architecture: 7
CPU variant     : 0x4
CPU part        : 0xc09
CPU revision    : 1

processor       : 1
model name      : ARMv7 Processor rev 1 (v7l)
BogoMIPS        : 1600.00
Features        : half thumb fastmult vfp edsp neon vfpv3 tls vfpd32
CPU implementer : 0x41
CPU architecture: 7
CPU variant     : 0x4
CPU part        : 0xc09
CPU revision    : 1

Hardware        : Marvell Armada 380/385 (Device Tree)
Revision        : 0000
Serial          : 0000000000000000
root@homeassistant:~# pip3 install --upgrade numpy==1.21.2
Collecting numpy==1.21.2
  Using cached numpy-1.21.2.zip (10.3 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
    Preparing wheel metadata ... done
Building wheels for collected packages: numpy
  Building wheel for numpy (PEP 517) ... error
  ERROR: Command errored out with exit status 1:
   command: /usr/bin/python3 /usr/lib/python3.8/site-packages/pip/_vendor/pep517/_in_process.py build_wheel /tmp/tmpzp04q267
       cwd: /tmp/pip-install-lzjs_nw2/numpy
  Complete output (878 lines):
  Processing numpy/random/_bounded_integers.pxd.in
  Processing numpy/random/_mt19937.pyx
  Processing numpy/random/_generator.pyx
  Processing numpy/random/_philox.pyx
  Processing numpy/random/_bounded_integers.pyx.in
  Processing numpy/random/_common.pyx
  Processing numpy/random/_sfc64.pyx
  Processing numpy/random/bit_generator.pyx
  Processing numpy/random/_pcg64.pyx
  Processing numpy/random/mtrand.pyx
  Cythonizing sources
  blas_opt_info:
  blas_mkl_info:
  customize UnixCCompiler
    libraries mkl_rt not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
    NOT AVAILABLE

  blis_info:
    libraries blis not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
    NOT AVAILABLE

  openblas_info:
    libraries openblas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
    NOT AVAILABLE

  accelerate_info:
    NOT AVAILABLE

  atlas_3_10_blas_threads_info:
  Setting PTATLAS=ATLAS
    libraries tatlas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
    NOT AVAILABLE

  atlas_3_10_blas_info:
    libraries satlas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
    NOT AVAILABLE

  atlas_blas_threads_info:
  Setting PTATLAS=ATLAS
    libraries ptf77blas,ptcblas,atlas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
    NOT AVAILABLE

  atlas_blas_info:
    libraries f77blas,cblas,atlas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
    NOT AVAILABLE

  blas_info:
    libraries blas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
    NOT AVAILABLE

  blas_src_info:
    NOT AVAILABLE

    NOT AVAILABLE

  non-existing path in 'numpy/distutils': 'site.cfg'
  lapack_opt_info:
  lapack_mkl_info:
    libraries mkl_rt not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
    NOT AVAILABLE

  openblas_lapack_info:
    libraries openblas not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
    NOT AVAILABLE

  openblas_clapack_info:
    libraries openblas,lapack not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
    NOT AVAILABLE

  flame_info:
    libraries flame not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
    NOT AVAILABLE

  atlas_3_10_threads_info:
  Setting PTATLAS=ATLAS
    libraries lapack_atlas not found in /usr/local/lib
    libraries tatlas,tatlas not found in /usr/local/lib
    libraries lapack_atlas not found in /usr/lib
    libraries tatlas,tatlas not found in /usr/lib
    libraries lapack_atlas not found in /usr/lib/
    libraries tatlas,tatlas not found in /usr/lib/
  <class 'numpy.distutils.system_info.atlas_3_10_threads_info'>
    NOT AVAILABLE

  atlas_3_10_info:
    libraries lapack_atlas not found in /usr/local/lib
    libraries satlas,satlas not found in /usr/local/lib
    libraries lapack_atlas not found in /usr/lib
    libraries satlas,satlas not found in /usr/lib
    libraries lapack_atlas not found in /usr/lib/
    libraries satlas,satlas not found in /usr/lib/
  <class 'numpy.distutils.system_info.atlas_3_10_info'>
    NOT AVAILABLE

  atlas_threads_info:
  Setting PTATLAS=ATLAS
    libraries lapack_atlas not found in /usr/local/lib
    libraries ptf77blas,ptcblas,atlas not found in /usr/local/lib
    libraries lapack_atlas not found in /usr/lib
    libraries ptf77blas,ptcblas,atlas not found in /usr/lib
    libraries lapack_atlas not found in /usr/lib/
    libraries ptf77blas,ptcblas,atlas not found in /usr/lib/
  <class 'numpy.distutils.system_info.atlas_threads_info'>
    NOT AVAILABLE

  atlas_info:
    libraries lapack_atlas not found in /usr/local/lib
    libraries f77blas,cblas,atlas not found in /usr/local/lib
    libraries lapack_atlas not found in /usr/lib
    libraries f77blas,cblas,atlas not found in /usr/lib
    libraries lapack_atlas not found in /usr/lib/
    libraries f77blas,cblas,atlas not found in /usr/lib/
  <class 'numpy.distutils.system_info.atlas_info'>
    NOT AVAILABLE

  lapack_info:
    libraries lapack not found in ['/usr/local/lib', '/usr/lib', '/usr/lib/']
    NOT AVAILABLE

  lapack_src_info:
    NOT AVAILABLE

    NOT AVAILABLE

  numpy_linalg_lapack_lite:
    FOUND:
      language = c

  running bdist_wheel
  running build
  running config_cc
  unifing config_cc, config, build_clib, build_ext, build commands --compiler options
  running config_fc
  unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options
  running build_src
  build_src
  building py_modules sources
  creating build
  creating build/src.linux-armv7l-3.8
  creating build/src.linux-armv7l-3.8/numpy
  creating build/src.linux-armv7l-3.8/numpy/distutils
  building library ""npymath"" sources
  Could not locate executable gfortran
  Could not locate executable f95
  Could not locate executable ifort
  Could not locate executable ifc
  Could not locate executable lf95
  Could not locate executable pgfortran
  Could not locate executable nvfortran
  Could not locate executable f90
  Could not locate executable f77
  Could not locate executable fort
  Could not locate executable efort
  Could not locate executable efc
  Could not locate executable g77
  Could not locate executable g95
  Could not locate executable pathf95
  Could not locate executable nagfor
  Could not locate executable frt
  don't know how to compile Fortran code on platform 'posix'
  creating build/src.linux-armv7l-3.8/numpy/core
  creating build/src.linux-armv7l-3.8/numpy/core/src
  creating build/src.linux-armv7l-3.8/numpy/core/src/npymath
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/npymath/npy_math_internal.h
    adding 'build/src.linux-armv7l-3.8/numpy/core/src/npymath' to include_dirs.
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/npymath/ieee754.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/npymath/npy_math_complex.c
  None - nothing done with h_files = ['build/src.linux-armv7l-3.8/numpy/core/src/npymath/npy_math_internal.h']
  building library ""npyrandom"" sources
  building extension ""numpy.core._multiarray_tests"" sources
  creating build/src.linux-armv7l-3.8/numpy/core/src/multiarray
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/multiarray/_multiarray_tests.c
  building extension ""numpy.core._multiarray_umath"" sources
  non-existing path in 'numpy/core': 'build/src.linux-armv7l-3.8/numpy/core/src/common'
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/multiarray/arraytypes.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/multiarray/einsum.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/multiarray/einsum_sumprod.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/multiarray/lowlevel_strided_loops.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/multiarray/nditer_templ.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/multiarray/scalartypes.c
  creating build/src.linux-armv7l-3.8/numpy/core/src/common
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/common/npy_sort.h
    adding 'build/src.linux-armv7l-3.8/numpy/core/src/common' to include_dirs.
  creating build/src.linux-armv7l-3.8/numpy/core/src/npysort
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/npysort/quicksort.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/npysort/mergesort.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/npysort/timsort.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/npysort/heapsort.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/npysort/radixsort.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/common/npy_partition.h
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/npysort/selection.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/common/npy_binsearch.h
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/npysort/binsearch.c
  creating build/src.linux-armv7l-3.8/numpy/core/src/umath
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/umath/funcs.inc
    adding 'build/src.linux-armv7l-3.8/numpy/core/src/umath' to include_dirs.
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/umath/simd.inc
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/umath/loops.h
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/umath/loops_utils.h
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/umath/loops.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/umath/loops_unary_fp.dispatch.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/umath/loops_arithm_fp.dispatch.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/umath/loops_arithmetic.dispatch.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/umath/loops_trigonometric.dispatch.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/umath/loops_exponent_log.dispatch.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/umath/matmul.h
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/umath/matmul.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/umath/clip.h
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/umath/clip.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/umath/scalarmath.c
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/common/templ_common.h
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/common/npy_cpu_features.c
  numpy.core - nothing done with h_files = ['build/src.linux-armv7l-3.8/numpy/core/src/common/npy_sort.h', 'build/src.linux-armv7l-3.8/numpy/core/src/common/npy_partition.h', 'build/src.linux-armv7l-3.8/numpy/core/src/common/npy_binsearch.h', 'build/src.linux-armv7l-3.8/numpy/core/src/umath/funcs.inc', 'build/src.linux-armv7l-3.8/numpy/core/src/umath/simd.inc', 'build/src.linux-armv7l-3.8/numpy/core/src/umath/loops.h', 'build/src.linux-armv7l-3.8/numpy/core/src/umath/loops_utils.h', 'build/src.linux-armv7l-3.8/numpy/core/src/umath/matmul.h', 'build/src.linux-armv7l-3.8/numpy/core/src/umath/clip.h', 'build/src.linux-armv7l-3.8/numpy/core/src/common/templ_common.h', 'build/src.linux-armv7l-3.8/numpy/core/include/numpy/config.h', 'build/src.linux-armv7l-3.8/numpy/core/include/numpy/_numpyconfig.h', 'build/src.linux-armv7l-3.8/numpy/core/include/numpy/__multiarray_api.h', 'build/src.linux-armv7l-3.8/numpy/core/include/numpy/__ufunc_api.h']
  building extension ""numpy.core._umath_tests"" sources
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/umath/_umath_tests.c
  building extension ""numpy.core._rational_tests"" sources
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/umath/_rational_tests.c
  building extension ""numpy.core._struct_ufunc_tests"" sources
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/umath/_struct_ufunc_tests.c
  building extension ""numpy.core._operand_flag_tests"" sources
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/umath/_operand_flag_tests.c
  building extension ""numpy.core._simd"" sources
  creating build/src.linux-armv7l-3.8/numpy/core/src/_simd
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/_simd/_simd_inc.h
    adding 'build/src.linux-armv7l-3.8/numpy/core/src/_simd' to include_dirs.
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/_simd/_simd_data.inc
  conv_template:> build/src.linux-armv7l-3.8/numpy/core/src/_simd/_simd.dispatch.c
  numpy.core - nothing done with h_files = ['build/src.linux-armv7l-3.8/numpy/core/src/_simd/_simd_inc.h', 'build/src.linux-armv7l-3.8/numpy/core/src/_simd/_simd_data.inc']
  building extension ""numpy.fft._pocketfft_internal"" sources
  building extension ""numpy.linalg.lapack_lite"" sources
  creating build/src.linux-armv7l-3.8/numpy/linalg
  ### Warning:  Using unoptimized lapack ###
  building extension ""numpy.linalg._umath_linalg"" sources
  ### Warning:  Using unoptimized lapack ###
  conv_template:> build/src.linux-armv7l-3.8/numpy/linalg/umath_linalg.c
  building extension ""numpy.random._mt19937"" sources
  building extension ""numpy.random._philox"" sources
  building extension ""numpy.random._pcg64"" sources
  building extension ""numpy.random._sfc64"" sources
  building extension ""numpy.random._common"" sources
  building extension ""numpy.random.bit_generator"" sources
  building extension ""numpy.random._generator"" sources
  building extension ""numpy.random._bounded_integers"" sources
  building extension ""numpy.random.mtrand"" sources
  building data_files sources
  build_src: building npy-pkg config files
  running build_py
  creating build/lib.linux-armv7l-3.8
  creating build/lib.linux-armv7l-3.8/numpy
  copying numpy/_version.py -> build/lib.linux-armv7l-3.8/numpy
  copying numpy/setup.py -> build/lib.linux-armv7l-3.8/numpy
  copying numpy/__init__.py -> build/lib.linux-armv7l-3.8/numpy
  copying numpy/dual.py -> build/lib.linux-armv7l-3.8/numpy
  copying numpy/_globals.py -> build/lib.linux-armv7l-3.8/numpy
  copying numpy/_distributor_init.py -> build/lib.linux-armv7l-3.8/numpy
  copying numpy/conftest.py -> build/lib.linux-armv7l-3.8/numpy
  copying numpy/matlib.py -> build/lib.linux-armv7l-3.8/numpy
  copying numpy/version.py -> build/lib.linux-armv7l-3.8/numpy
  copying numpy/_pytesttester.py -> build/lib.linux-armv7l-3.8/numpy
  copying numpy/ctypeslib.py -> build/lib.linux-armv7l-3.8/numpy
  copying build/src.linux-armv7l-3.8/numpy/__config__.py -> build/lib.linux-armv7l-3.8/numpy
  creating build/lib.linux-armv7l-3.8/numpy/compat
  copying numpy/compat/setup.py -> build/lib.linux-armv7l-3.8/numpy/compat
  copying numpy/compat/__init__.py -> build/lib.linux-armv7l-3.8/numpy/compat
  copying numpy/compat/_inspect.py -> build/lib.linux-armv7l-3.8/numpy/compat
  copying numpy/compat/py3k.py -> build/lib.linux-armv7l-3.8/numpy/compat
  creating build/lib.linux-armv7l-3.8/numpy/compat/tests
  copying numpy/compat/tests/__init__.py -> build/lib.linux-armv7l-3.8/numpy/compat/tests
  copying numpy/compat/tests/test_compat.py -> build/lib.linux-armv7l-3.8/numpy/compat/tests
  creating build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/_add_newdocs_scalars.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/_internal.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/fromnumeric.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/_string_helpers.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/getlimits.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/_add_newdocs.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/defchararray.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/_type_aliases.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/einsumfunc.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/setup.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/_asarray.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/__init__.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/records.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/memmap.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/function_base.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/multiarray.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/arrayprint.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/overrides.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/numeric.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/_dtype_ctypes.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/_dtype.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/cversions.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/umath.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/numerictypes.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/shape_base.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/_exceptions.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/umath_tests.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/_ufunc_config.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/setup_common.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/machar.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/_methods.py -> build/lib.linux-armv7l-3.8/numpy/core
  copying numpy/core/code_generators/generate_numpy_api.py -> build/lib.linux-armv7l-3.8/numpy/core
  creating build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_half.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_extint128.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_umath_accuracy.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_arraymethod.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_protocols.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_simd_module.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_function_base.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_ufunc.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test__exceptions.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_nditer.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_umath_complex.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/__init__.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_memmap.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_item_selection.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_shape_base.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_argparse.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_mem_overlap.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_records.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_cpu_dispatcher.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_cython.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_array_coercion.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_numerictypes.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_defchararray.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_scalar_methods.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_scalarinherit.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_dtype.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_arrayprint.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_datetime.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_scalarbuffer.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_longdouble.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_numeric.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_einsum.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_abc.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_indexing.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/_locales.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_scalarmath.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_api.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_overrides.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_multiarray.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_simd.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_unicode.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_scalarprint.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_conversion_utils.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_regression.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_casting_unittests.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_deprecations.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_cpu_features.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_print.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_scalar_ctors.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_getlimits.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_machar.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_errstate.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_indexerrors.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  copying numpy/core/tests/test_umath.py -> build/lib.linux-armv7l-3.8/numpy/core/tests
  creating build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/exec_command.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/core.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/npy_pkg_config.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/mingw32ccompiler.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/setup.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/__init__.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/conv_template.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/unixccompiler.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/from_template.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/numpy_distribution.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/misc_util.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/msvc9compiler.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/pathccompiler.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/ccompiler.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/system_info.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/lib2def.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/intelccompiler.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/cpuinfo.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/_shell_utils.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/msvccompiler.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/log.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/line_endings.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/ccompiler_opt.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying numpy/distutils/extension.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  copying build/src.linux-armv7l-3.8/numpy/distutils/__config__.py -> build/lib.linux-armv7l-3.8/numpy/distutils
  creating build/lib.linux-armv7l-3.8/numpy/distutils/command
  copying numpy/distutils/command/sdist.py -> build/lib.linux-armv7l-3.8/numpy/distutils/command
  copying numpy/distutils/command/config_compiler.py -> build/lib.linux-armv7l-3.8/numpy/distutils/command
  copying numpy/distutils/command/build_scripts.py -> build/lib.linux-armv7l-3.8/numpy/distutils/command
  copying numpy/distutils/command/config.py -> build/lib.linux-armv7l-3.8/numpy/distutils/command
  copying numpy/distutils/command/autodist.py -> build/lib.linux-armv7l-3.8/numpy/distutils/command
  copying numpy/distutils/command/build_src.py -> build/lib.linux-armv7l-3.8/numpy/distutils/command
  copying numpy/distutils/command/__init__.py -> build/lib.linux-armv7l-3.8/numpy/distutils/command
  copying numpy/distutils/command/install.py -> build/lib.linux-armv7l-3.8/numpy/distutils/command
  copying numpy/distutils/command/build_py.py -> build/lib.linux-armv7l-3.8/numpy/distutils/command
  copying numpy/distutils/command/install_data.py -> build/lib.linux-armv7l-3.8/numpy/distutils/command
  copying numpy/distutils/command/build_clib.py -> build/lib.linux-armv7l-3.8/numpy/distutils/command
  copying numpy/distutils/command/build_ext.py -> build/lib.linux-armv7l-3.8/numpy/distutils/command
  copying numpy/distutils/command/bdist_rpm.py -> build/lib.linux-armv7l-3.8/numpy/distutils/command
  copying numpy/distutils/command/install_headers.py -> build/lib.linux-armv7l-3.8/numpy/distutils/command
  copying numpy/distutils/command/install_clib.py -> build/lib.linux-armv7l-3.8/numpy/distutils/command
  copying numpy/distutils/command/egg_info.py -> build/lib.linux-armv7l-3.8/numpy/distutils/command
  copying numpy/distutils/command/build.py -> build/lib.linux-armv7l-3.8/numpy/distutils/command
  copying numpy/distutils/command/develop.py -> build/lib.linux-armv7l-3.8/numpy/distutils/command
  creating build/lib.linux-armv7l-3.8/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/nag.py -> build/lib.linux-armv7l-3.8/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/pathf95.py -> build/lib.linux-armv7l-3.8/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/intel.py -> build/lib.linux-armv7l-3.8/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/none.py -> build/lib.linux-armv7l-3.8/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/__init__.py -> build/lib.linux-armv7l-3.8/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/vast.py -> build/lib.linux-armv7l-3.8/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/lahey.py -> build/lib.linux-armv7l-3.8/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/mips.py -> build/lib.linux-armv7l-3.8/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/hpux.py -> build/lib.linux-armv7l-3.8/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/nv.py -> build/lib.linux-armv7l-3.8/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/sun.py -> build/lib.linux-armv7l-3.8/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/environment.py -> build/lib.linux-armv7l-3.8/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/pg.py -> build/lib.linux-armv7l-3.8/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/gnu.py -> build/lib.linux-armv7l-3.8/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/g95.py -> build/lib.linux-armv7l-3.8/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/compaq.py -> build/lib.linux-armv7l-3.8/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/fujitsu.py -> build/lib.linux-armv7l-3.8/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/absoft.py -> build/lib.linux-armv7l-3.8/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/ibm.py -> build/lib.linux-armv7l-3.8/numpy/distutils/fcompiler
  creating build/lib.linux-armv7l-3.8/numpy/distutils/tests
  copying numpy/distutils/tests/test_shell_utils.py -> build/lib.linux-armv7l-3.8/numpy/distutils/tests
  copying numpy/distutils/tests/test_from_template.py -> build/lib.linux-armv7l-3.8/numpy/distutils/tests
  copying numpy/distutils/tests/test_ccompiler_opt_conf.py -> build/lib.linux-armv7l-3.8/numpy/distutils/tests
  copying numpy/distutils/tests/test_fcompiler_nagfor.py -> build/lib.linux-armv7l-3.8/numpy/distutils/tests
  copying numpy/distutils/tests/test_misc_util.py -> build/lib.linux-armv7l-3.8/numpy/distutils/tests
  copying numpy/distutils/tests/__init__.py -> build/lib.linux-armv7l-3.8/numpy/distutils/tests
  copying numpy/distutils/tests/test_fcompiler.py -> build/lib.linux-armv7l-3.8/numpy/distutils/tests
  copying numpy/distutils/tests/test_fcompiler_intel.py -> build/lib.linux-armv7l-3.8/numpy/distutils/tests
  copying numpy/distutils/tests/test_fcompiler_gnu.py -> build/lib.linux-armv7l-3.8/numpy/distutils/tests
  copying numpy/distutils/tests/test_npy_pkg_config.py -> build/lib.linux-armv7l-3.8/numpy/distutils/tests
  copying numpy/distutils/tests/test_exec_command.py -> build/lib.linux-armv7l-3.8/numpy/distutils/tests
  copying numpy/distutils/tests/test_system_info.py -> build/lib.linux-armv7l-3.8/numpy/distutils/tests
  copying numpy/distutils/tests/test_mingw32ccompiler.py -> build/lib.linux-armv7l-3.8/numpy/distutils/tests
  copying numpy/distutils/tests/test_build_ext.py -> build/lib.linux-armv7l-3.8/numpy/distutils/tests
  copying numpy/distutils/tests/test_ccompiler_opt.py -> build/lib.linux-armv7l-3.8/numpy/distutils/tests
  creating build/lib.linux-armv7l-3.8/numpy/doc
  copying numpy/doc/__init__.py -> build/lib.linux-armv7l-3.8/numpy/doc
  copying numpy/doc/constants.py -> build/lib.linux-armv7l-3.8/numpy/doc
  copying numpy/doc/ufuncs.py -> build/lib.linux-armv7l-3.8/numpy/doc
  creating build/lib.linux-armv7l-3.8/numpy/f2py
  copying numpy/f2py/f2py_testing.py -> build/lib.linux-armv7l-3.8/numpy/f2py
  copying numpy/f2py/setup.py -> build/lib.linux-armv7l-3.8/numpy/f2py
  copying numpy/f2py/__init__.py -> build/lib.linux-armv7l-3.8/numpy/f2py
  copying numpy/f2py/crackfortran.py -> build/lib.linux-armv7l-3.8/numpy/f2py
  copying numpy/f2py/rules.py -> build/lib.linux-armv7l-3.8/numpy/f2py
  copying numpy/f2py/f2py2e.py -> build/lib.linux-armv7l-3.8/numpy/f2py
  copying numpy/f2py/diagnose.py -> build/lib.linux-armv7l-3.8/numpy/f2py
  copying numpy/f2py/cb_rules.py -> build/lib.linux-armv7l-3.8/numpy/f2py
  copying numpy/f2py/cfuncs.py -> build/lib.linux-armv7l-3.8/numpy/f2py
  copying numpy/f2py/f90mod_rules.py -> build/lib.linux-armv7l-3.8/numpy/f2py
  copying numpy/f2py/auxfuncs.py -> build/lib.linux-armv7l-3.8/numpy/f2py
  copying numpy/f2py/__main__.py -> build/lib.linux-armv7l-3.8/numpy/f2py
  copying numpy/f2py/func2subr.py -> build/lib.linux-armv7l-3.8/numpy/f2py
  copying numpy/f2py/use_rules.py -> build/lib.linux-armv7l-3.8/numpy/f2py
  copying numpy/f2py/__version__.py -> build/lib.linux-armv7l-3.8/numpy/f2py
  copying numpy/f2py/capi_maps.py -> build/lib.linux-armv7l-3.8/numpy/f2py
  copying numpy/f2py/common_rules.py -> build/lib.linux-armv7l-3.8/numpy/f2py
  creating build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_return_character.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_string.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_semicolon_split.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/__init__.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_callback.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_module_doc.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_return_real.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_array_from_pyobj.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_return_integer.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_return_logical.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_block_docstring.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_return_complex.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_crackfortran.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_abstract_interface.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_size.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_kind.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_quoted_character.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/util.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_regression.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_assumed_shape.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_parameter.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_common.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_compile_function.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  copying numpy/f2py/tests/test_mixed.py -> build/lib.linux-armv7l-3.8/numpy/f2py/tests
  creating build/lib.linux-armv7l-3.8/numpy/fft
  copying numpy/fft/_pocketfft.py -> build/lib.linux-armv7l-3.8/numpy/fft
  copying numpy/fft/setup.py -> build/lib.linux-armv7l-3.8/numpy/fft
  copying numpy/fft/__init__.py -> build/lib.linux-armv7l-3.8/numpy/fft
  copying numpy/fft/helper.py -> build/lib.linux-armv7l-3.8/numpy/fft
  creating build/lib.linux-armv7l-3.8/numpy/fft/tests
  copying numpy/fft/tests/__init__.py -> build/lib.linux-armv7l-3.8/numpy/fft/tests
  copying numpy/fft/tests/test_helper.py -> build/lib.linux-armv7l-3.8/numpy/fft/tests
  copying numpy/fft/tests/test_pocketfft.py -> build/lib.linux-armv7l-3.8/numpy/fft/tests
  creating build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/_datasource.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/arrayterator.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/utils.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/_version.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/scimath.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/ufunclike.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/setup.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/__init__.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/nanfunctions.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/_iotools.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/function_base.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/npyio.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/polynomial.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/user_array.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/arraysetops.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/twodim_base.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/index_tricks.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/shape_base.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/stride_tricks.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/type_check.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/histograms.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/arraypad.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/format.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/mixins.py -> build/lib.linux-armv7l-3.8/numpy/lib
  copying numpy/lib/recfunctions.py -> build/lib.linux-armv7l-3.8/numpy/lib
  creating build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_histograms.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_arrayterator.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_ufunclike.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_financial_expired.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_function_base.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_nanfunctions.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_arraypad.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_recfunctions.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/__init__.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_index_tricks.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_shape_base.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_utils.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_mixins.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_twodim_base.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test__version.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_io.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test__datasource.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_type_check.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_regression.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_packbits.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test__iotools.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_stride_tricks.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_arraysetops.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_polynomial.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  copying numpy/lib/tests/test_format.py -> build/lib.linux-armv7l-3.8/numpy/lib/tests
  creating build/lib.linux-armv7l-3.8/numpy/linalg
  copying numpy/linalg/setup.py -> build/lib.linux-armv7l-3.8/numpy/linalg
  copying numpy/linalg/__init__.py -> build/lib.linux-armv7l-3.8/numpy/linalg
  copying numpy/linalg/linalg.py -> build/lib.linux-armv7l-3.8/numpy/linalg
  creating build/lib.linux-armv7l-3.8/numpy/linalg/tests
  copying numpy/linalg/tests/__init__.py -> build/lib.linux-armv7l-3.8/numpy/linalg/tests
  copying numpy/linalg/tests/test_build.py -> build/lib.linux-armv7l-3.8/numpy/linalg/tests
  copying numpy/linalg/tests/test_regression.py -> build/lib.linux-armv7l-3.8/numpy/linalg/tests
  copying numpy/linalg/tests/test_deprecations.py -> build/lib.linux-armv7l-3.8/numpy/linalg/tests
  copying numpy/linalg/tests/test_linalg.py -> build/lib.linux-armv7l-3.8/numpy/linalg/tests
  creating build/lib.linux-armv7l-3.8/numpy/ma
  copying numpy/ma/core.py -> build/lib.linux-armv7l-3.8/numpy/ma
  copying numpy/ma/timer_comparison.py -> build/lib.linux-armv7l-3.8/numpy/ma
  copying numpy/ma/setup.py -> build/lib.linux-armv7l-3.8/numpy/ma
  copying numpy/ma/testutils.py -> build/lib.linux-armv7l-3.8/numpy/ma
  copying numpy/ma/bench.py -> build/lib.linux-armv7l-3.8/numpy/ma
  copying numpy/ma/__init__.py -> build/lib.linux-armv7l-3.8/numpy/ma
  copying numpy/ma/extras.py -> build/lib.linux-armv7l-3.8/numpy/ma
  copying numpy/ma/mrecords.py -> build/lib.linux-armv7l-3.8/numpy/ma
  creating build/lib.linux-armv7l-3.8/numpy/ma/tests
  copying numpy/ma/tests/test_core.py -> build/lib.linux-armv7l-3.8/numpy/ma/tests
  copying numpy/ma/tests/test_extras.py -> build/lib.linux-armv7l-3.8/numpy/ma/tests
  copying numpy/ma/tests/__init__.py -> build/lib.linux-armv7l-3.8/numpy/ma/tests
  copying numpy/ma/tests/test_old_ma.py -> build/lib.linux-armv7l-3.8/numpy/ma/tests
  copying numpy/ma/tests/test_subclassing.py -> build/lib.linux-armv7l-3.8/numpy/ma/tests
  copying numpy/ma/tests/test_mrecords.py -> build/lib.linux-armv7l-3.8/numpy/ma/tests
  copying numpy/ma/tests/test_regression.py -> build/lib.linux-armv7l-3.8/numpy/ma/tests
  copying numpy/ma/tests/test_deprecations.py -> build/lib.linux-armv7l-3.8/numpy/ma/tests
  creating build/lib.linux-armv7l-3.8/numpy/matrixlib
  copying numpy/matrixlib/setup.py -> build/lib.linux-armv7l-3.8/numpy/matrixlib
  copying numpy/matrixlib/__init__.py -> build/lib.linux-armv7l-3.8/numpy/matrixlib
  copying numpy/matrixlib/defmatrix.py -> build/lib.linux-armv7l-3.8/numpy/matrixlib
  creating build/lib.linux-armv7l-3.8/numpy/matrixlib/tests
  copying numpy/matrixlib/tests/__init__.py -> build/lib.linux-armv7l-3.8/numpy/matrixlib/tests
  copying numpy/matrixlib/tests/test_defmatrix.py -> build/lib.linux-armv7l-3.8/numpy/matrixlib/tests
  copying numpy/matrixlib/tests/test_numeric.py -> build/lib.linux-armv7l-3.8/numpy/matrixlib/tests
  copying numpy/matrixlib/tests/test_masked_matrix.py -> build/lib.linux-armv7l-3.8/numpy/matrixlib/tests
  copying numpy/matrixlib/tests/test_multiarray.py -> build/lib.linux-armv7l-3.8/numpy/matrixlib/tests
  copying numpy/matrixlib/tests/test_matrix_linalg.py -> build/lib.linux-armv7l-3.8/numpy/matrixlib/tests
  copying numpy/matrixlib/tests/test_regression.py -> build/lib.linux-armv7l-3.8/numpy/matrixlib/tests
  copying numpy/matrixlib/tests/test_interaction.py -> build/lib.linux-armv7l-3.8/numpy/matrixlib/tests
  creating build/lib.linux-armv7l-3.8/numpy/polynomial
  copying numpy/polynomial/hermite.py -> build/lib.linux-armv7l-3.8/numpy/polynomial
  copying numpy/polynomial/setup.py -> build/lib.linux-armv7l-3.8/numpy/polynomial
  copying numpy/polynomial/__init__.py -> build/lib.linux-armv7l-3.8/numpy/polynomial
  copying numpy/polynomial/laguerre.py -> build/lib.linux-armv7l-3.8/numpy/polynomial
  copying numpy/polynomial/legendre.py -> build/lib.linux-armv7l-3.8/numpy/polynomial
  copying numpy/polynomial/polynomial.py -> build/lib.linux-armv7l-3.8/numpy/polynomial
  copying numpy/polynomial/chebyshev.py -> build/lib.linux-armv7l-3.8/numpy/polynomial
  copying numpy/polynomial/polyutils.py -> build/lib.linux-armv7l-3.8/numpy/polynomial
  copying numpy/polynomial/hermite_e.py -> build/lib.linux-armv7l-3.8/numpy/polynomial
  copying numpy/polynomial/_polybase.py -> build/lib.linux-armv7l-3.8/numpy/polynomial
  creating build/lib.linux-armv7l-3.8/numpy/polynomial/tests
  copying numpy/polynomial/tests/test_classes.py -> build/lib.linux-armv7l-3.8/numpy/polynomial/tests
  copying numpy/polynomial/tests/test_legendre.py -> build/lib.linux-armv7l-3.8/numpy/polynomial/tests
  copying numpy/polynomial/tests/__init__.py -> build/lib.linux-armv7l-3.8/numpy/polynomial/tests
  copying numpy/polynomial/tests/test_hermite.py -> build/lib.linux-armv7l-3.8/numpy/polynomial/tests
  copying numpy/polynomial/tests/test_chebyshev.py -> build/lib.linux-armv7l-3.8/numpy/polynomial/tests
  copying numpy/polynomial/tests/test_printing.py -> build/lib.linux-armv7l-3.8/numpy/polynomial/tests
  copying numpy/polynomial/tests/test_hermite_e.py -> build/lib.linux-armv7l-3.8/numpy/polynomial/tests
  copying numpy/polynomial/tests/test_polyutils.py -> build/lib.linux-armv7l-3.8/numpy/polynomial/tests
  copying numpy/polynomial/tests/test_laguerre.py -> build/lib.linux-armv7l-3.8/numpy/polynomial/tests
  copying numpy/polynomial/tests/test_polynomial.py -> build/lib.linux-armv7l-3.8/numpy/polynomial/tests
  creating build/lib.linux-armv7l-3.8/numpy/random
  copying numpy/random/setup.py -> build/lib.linux-armv7l-3.8/numpy/random
  copying numpy/random/__init__.py -> build/lib.linux-armv7l-3.8/numpy/random
  copying numpy/random/_pickle.py -> build/lib.linux-armv7l-3.8/numpy/random
  creating build/lib.linux-armv7l-3.8/numpy/random/tests
  copying numpy/random/tests/test_generator_mt19937_regressions.py -> build/lib.linux-armv7l-3.8/numpy/random/tests
  copying numpy/random/tests/test_extending.py -> build/lib.linux-armv7l-3.8/numpy/random/tests
  copying numpy/random/tests/test_seed_sequence.py -> build/lib.linux-armv7l-3.8/numpy/random/tests
  copying numpy/random/tests/__init__.py -> build/lib.linux-armv7l-3.8/numpy/random/tests
  copying numpy/random/tests/test_randomstate.py -> build/lib.linux-armv7l-3.8/numpy/random/tests
  copying numpy/random/tests/test_smoke.py -> build/lib.linux-armv7l-3.8/numpy/random/tests
  copying numpy/random/tests/test_generator_mt19937.py -> build/lib.linux-armv7l-3.8/numpy/random/tests
  copying numpy/random/tests/test_randomstate_regression.py -> build/lib.linux-armv7l-3.8/numpy/random/tests
  copying numpy/random/tests/test_regression.py -> build/lib.linux-armv7l-3.8/numpy/random/tests
  copying numpy/random/tests/test_direct.py -> build/lib.linux-armv7l-3.8/numpy/random/tests
  copying numpy/random/tests/test_random.py -> build/lib.linux-armv7l-3.8/numpy/random/tests
  creating build/lib.linux-armv7l-3.8/numpy/testing
  copying numpy/testing/utils.py -> build/lib.linux-armv7l-3.8/numpy/testing
  copying numpy/testing/setup.py -> build/lib.linux-armv7l-3.8/numpy/testing
  copying numpy/testing/__init__.py -> build/lib.linux-armv7l-3.8/numpy/testing
  copying numpy/testing/print_coercion_tables.py -> build/lib.linux-armv7l-3.8/numpy/testing
  creating build/lib.linux-armv7l-3.8/numpy/testing/_private
  copying numpy/testing/_private/utils.py -> build/lib.linux-armv7l-3.8/numpy/testing/_private
  copying numpy/testing/_private/__init__.py -> build/lib.linux-armv7l-3.8/numpy/testing/_private
  copying numpy/testing/_private/nosetester.py -> build/lib.linux-armv7l-3.8/numpy/testing/_private
  copying numpy/testing/_private/noseclasses.py -> build/lib.linux-armv7l-3.8/numpy/testing/_private
  copying numpy/testing/_private/decorators.py -> build/lib.linux-armv7l-3.8/numpy/testing/_private
  copying numpy/testing/_private/parameterized.py -> build/lib.linux-armv7l-3.8/numpy/testing/_private
  creating build/lib.linux-armv7l-3.8/numpy/testing/tests
  copying numpy/testing/tests/__init__.py -> build/lib.linux-armv7l-3.8/numpy/testing/tests
  copying numpy/testing/tests/test_doctesting.py -> build/lib.linux-armv7l-3.8/numpy/testing/tests
  copying numpy/testing/tests/test_utils.py -> build/lib.linux-armv7l-3.8/numpy/testing/tests
  creating build/lib.linux-armv7l-3.8/numpy/typing
  copying numpy/typing/mypy_plugin.py -> build/lib.linux-armv7l-3.8/numpy/typing
  copying numpy/typing/_generic_alias.py -> build/lib.linux-armv7l-3.8/numpy/typing
  copying numpy/typing/_array_like.py -> build/lib.linux-armv7l-3.8/numpy/typing
  copying numpy/typing/_add_docstring.py -> build/lib.linux-armv7l-3.8/numpy/typing
  copying numpy/typing/setup.py -> build/lib.linux-armv7l-3.8/numpy/typing
  copying numpy/typing/__init__.py -> build/lib.linux-armv7l-3.8/numpy/typing
  copying numpy/typing/_extended_precision.py -> build/lib.linux-armv7l-3.8/numpy/typing
  copying numpy/typing/_dtype_like.py -> build/lib.linux-armv7l-3.8/numpy/typing
  copying numpy/typing/_char_codes.py -> build/lib.linux-armv7l-3.8/numpy/typing
  copying numpy/typing/_nbit.py -> build/lib.linux-armv7l-3.8/numpy/typing
  copying numpy/typing/_shape.py -> build/lib.linux-armv7l-3.8/numpy/typing
  copying numpy/typing/_scalars.py -> build/lib.linux-armv7l-3.8/numpy/typing
  copying numpy/typing/_callable.py -> build/lib.linux-armv7l-3.8/numpy/typing
  creating build/lib.linux-armv7l-3.8/numpy/typing/tests
  copying numpy/typing/tests/__init__.py -> build/lib.linux-armv7l-3.8/numpy/typing/tests
  copying numpy/typing/tests/test_runtime.py -> build/lib.linux-armv7l-3.8/numpy/typing/tests
  copying numpy/typing/tests/test_typing_extensions.py -> build/lib.linux-armv7l-3.8/numpy/typing/tests
  copying numpy/typing/tests/test_generic_alias.py -> build/lib.linux-armv7l-3.8/numpy/typing/tests
  copying numpy/typing/tests/test_typing.py -> build/lib.linux-armv7l-3.8/numpy/typing/tests
  copying numpy/typing/tests/test_isfile.py -> build/lib.linux-armv7l-3.8/numpy/typing/tests
  creating build/lib.linux-armv7l-3.8/numpy/tests
  copying numpy/tests/test_scripts.py -> build/lib.linux-armv7l-3.8/numpy/tests
  copying numpy/tests/__init__.py -> build/lib.linux-armv7l-3.8/numpy/tests
  copying numpy/tests/test_matlib.py -> build/lib.linux-armv7l-3.8/numpy/tests
  copying numpy/tests/test_numpy_version.py -> build/lib.linux-armv7l-3.8/numpy/tests
  copying numpy/tests/test_reloading.py -> build/lib.linux-armv7l-3.8/numpy/tests
  copying numpy/tests/test_public_api.py -> build/lib.linux-armv7l-3.8/numpy/tests
  copying numpy/tests/test_ctypeslib.py -> build/lib.linux-armv7l-3.8/numpy/tests
  copying numpy/tests/test_warnings.py -> build/lib.linux-armv7l-3.8/numpy/tests
  UPDATING build/lib.linux-armv7l-3.8/numpy/_version.py
  set build/lib.linux-armv7l-3.8/numpy/_version.py to '1.21.2'
  running build_clib
  customize UnixCCompiler
  customize UnixCCompiler using new_build_clib
  CCompilerOpt.cc_test_flags[1013] : testing flags (-march=native)
  C compiler: gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC

  creating /tmp/tmpl_hng7jf/tmp
  creating /tmp/tmpl_hng7jf/tmp/pip-install-lzjs_nw2
  creating /tmp/tmpl_hng7jf/tmp/pip-install-lzjs_nw2/numpy
  creating /tmp/tmpl_hng7jf/tmp/pip-install-lzjs_nw2/numpy/numpy
  creating /tmp/tmpl_hng7jf/tmp/pip-install-lzjs_nw2/numpy/numpy/distutils
  creating /tmp/tmpl_hng7jf/tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks
  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c'
  extra options: '-march=native'
  CCompilerOpt.cc_test_flags[1013] : testing flags (-O3)
  C compiler: gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c'
  extra options: '-O3'
  CCompilerOpt.cc_test_flags[1013] : testing flags (-Werror)
  C compiler: gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c'
  extra options: '-Werror'
  CCompilerOpt.__init__[1701] : check requested baseline
  CCompilerOpt.__init__[1710] : check requested dispatch-able features
  CCompilerOpt.cc_test_flags[1013] : testing flags (-mfpu=neon)
  C compiler: gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c'
  extra options: '-mfpu=neon'
  CCompilerOpt.cc_test_flags[1013] : testing flags (-mfpu=neon-fp16 -mfp16-format=ieee)
  C compiler: gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c'
  extra options: '-mfpu=neon-fp16 -mfp16-format=ieee'
  CCompilerOpt.feature_test[1466] : testing feature 'NEON_FP16' with flags (-mfpu=neon-fp16 -mfp16-format=ieee)
  C compiler: gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c'
  extra options: '-mfpu=neon-fp16 -mfp16-format=ieee -Werror'
  CCompilerOpt.cc_test_flags[1013] : testing flags (-mfpu=neon-vfpv4)
  C compiler: gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c'
  extra options: '-mfpu=neon-vfpv4'
  CCompilerOpt.feature_test[1466] : testing feature 'NEON_VFPV4' with flags (-mfp16-format=ieee -mfpu=neon-vfpv4)
  C compiler: gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c'
  extra options: '-mfp16-format=ieee -mfpu=neon-vfpv4 -Werror'
  CCompilerOpt.feature_test[1466] : testing feature 'NEON' with flags (-mfpu=neon)
  C compiler: gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c'
  extra options: '-mfpu=neon -Werror'
  CCompilerOpt.cc_test_flags[1013] : testing flags (-mfpu=neon-fp-armv8 -march=armv8-a+simd)
  C compiler: gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c'
  extra options: '-mfpu=neon-fp-armv8 -march=armv8-a+simd'
  CCompilerOpt.feature_test[1466] : testing feature 'ASIMD' with flags (-mfp16-format=ieee -mfpu=neon-fp-armv8 -march=armv8-a+simd)
  C compiler: gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c'
  extra options: '-mfp16-format=ieee -mfpu=neon-fp-armv8 -march=armv8-a+simd -Werror'
  CCompilerOpt.cc_test_flags[1013] : testing flags (-march=armv8.2-a+dotprod)
  C compiler: gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c'
  extra options: '-march=armv8.2-a+dotprod'
  CCompilerOpt.feature_test[1466] : testing feature 'ASIMDDP' with flags (-mfp16-format=ieee -mfpu=neon-fp-armv8 -march=armv8.2-a+dotprod)
  C compiler: gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c'
  extra options: '-mfp16-format=ieee -mfpu=neon-fp-armv8 -march=armv8.2-a+dotprod -Werror'
  CCompilerOpt.dist_test[581] : CCompilerOpt._dist_test_spawn[716] : Command (gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimddp.c -o /tmp/tmpl_hng7jf/tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimddp.o -MMD -MF /tmp/tmpl_hng7jf/tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimddp.o.d -mfp16-format=ieee -mfpu=neon-fp-armv8 -march=armv8.2-a+dotprod -Werror) failed with exit status 1 output ->
  /tmp/ccCfFfNe.s: Assembler messages:
  /tmp/ccCfFfNe.s:79: Error: selected processor does not support `vudot.u8 q8,q9,q10' in ARM mode

  CCompilerOpt.feature_test[1482] : testing failed
  CCompilerOpt.cc_test_flags[1013] : testing flags (-march=armv8.2-a+fp16)
  C compiler: gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c'
  extra options: '-march=armv8.2-a+fp16'
  CCompilerOpt.feature_test[1466] : testing feature 'ASIMDHP' with flags (-mfp16-format=ieee -mfpu=neon-fp-armv8 -march=armv8.2-a+fp16)
  C compiler: gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c'
  extra options: '-mfp16-format=ieee -mfpu=neon-fp-armv8 -march=armv8.2-a+fp16 -Werror'
  CCompilerOpt.cc_test_flags[1013] : testing flags (-march=armv8.2-a+fp16fml)
  C compiler: gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c'
  extra options: '-march=armv8.2-a+fp16fml'
  CCompilerOpt.feature_test[1466] : testing feature 'ASIMDFHM' with flags (-mfp16-format=ieee -mfpu=neon-fp-armv8 -march=armv8.2-a+fp16+fp16fml)
  C compiler: gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c'
  extra options: '-mfp16-format=ieee -mfpu=neon-fp-armv8 -march=armv8.2-a+fp16+fp16fml -Werror'
  CCompilerOpt.dist_test[581] : CCompilerOpt._dist_test_spawn[716] : Command (gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimdfhm.c -o /tmp/tmpl_hng7jf/tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimdfhm.o -MMD -MF /tmp/tmpl_hng7jf/tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimdfhm.o.d -mfp16-format=ieee -mfpu=neon-fp-armv8 -march=armv8.2-a+fp16+fp16fml -Werror) failed with exit status 1 output ->
  /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimdfhm.c: In function 'main':
  /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimdfhm.c:13:35: error: implicit declaration of function 'vfmlal_low_u32'; did you mean 'vfmlal_low_f16'? [-Werror=implicit-function-declaration]
     13 |     int ret  = (int)vget_lane_f32(vfmlal_low_u32(vlf, vlhp, vlhp), 0);
        |                                   ^~~~~~~~~~~~~~
        |                                   vfmlal_low_f16
  /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimdfhm.c:13:35: error: incompatible type for argument 1 of 'vget_lane_f32'
     13 |     int ret  = (int)vget_lane_f32(vfmlal_low_u32(vlf, vlhp, vlhp), 0);
        |                                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        |                                   |
        |                                   int
  In file included from /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimdfhm.c:4:
  /usr/lib/gcc/armv6-alpine-linux-musleabihf/10.2.1/include/arm_neon.h:6171:28: note: expected 'float32x2_t' but argument is of type 'int'
   6171 | vget_lane_f32 (float32x2_t __a, const int __b)
        |                ~~~~~~~~~~~~^~~
  /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimdfhm.c:14:36: error: implicit declaration of function 'vfmlslq_high_u32'; did you mean 'vfmlslq_high_f16'? [-Werror=implicit-function-declaration]
     14 |         ret += (int)vgetq_lane_f32(vfmlslq_high_u32(vf, vhp, vhp), 0);
        |                                    ^~~~~~~~~~~~~~~~
        |                                    vfmlslq_high_f16
  /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimdfhm.c:14:36: error: incompatible type for argument 1 of 'vgetq_lane_f32'
     14 |         ret += (int)vgetq_lane_f32(vfmlslq_high_u32(vf, vhp, vhp), 0);
        |                                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        |                                    |
        |                                    int
  In file included from /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimdfhm.c:4:
  /usr/lib/gcc/armv6-alpine-linux-musleabihf/10.2.1/include/arm_neon.h:6269:29: note: expected 'float32x4_t' but argument is of type 'int'
   6269 | vgetq_lane_f32 (float32x4_t __a, const int __b)
        |                 ~~~~~~~~~~~~^~~
  cc1: all warnings being treated as errors

  CCompilerOpt.feature_test[1482] : testing failed
  CCompilerOpt.__init__[1726] : initialize targets groups
  CCompilerOpt.__init__[1728] : parse target group simd_test
  CCompilerOpt._parse_target_tokens[1939] : skip targets (AVX512_SKX VSX VSX2 FMA4 XOP SSE2 VSX3 AVX512F (AVX2 FMA3) SSE42) not part of baseline or dispatch-able features
  CCompilerOpt.generate_dispatch_header[2272] : generate CPU dispatch header: (build/src.linux-armv7l-3.8/numpy/distutils/include/npy_cpu_dispatch_config.h)
  CCompilerOpt.generate_dispatch_header[2281] : dispatch header dir build/src.linux-armv7l-3.8/numpy/distutils/include does not exist, creating it
  building 'npymath' library
  compiling C sources
  C compiler: gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC

  creating build/temp.linux-armv7l-3.8
  creating build/temp.linux-armv7l-3.8/numpy
  creating build/temp.linux-armv7l-3.8/numpy/core
  creating build/temp.linux-armv7l-3.8/numpy/core/src
  creating build/temp.linux-armv7l-3.8/numpy/core/src/npymath
  creating build/temp.linux-armv7l-3.8/build
  creating build/temp.linux-armv7l-3.8/build/src.linux-armv7l-3.8
  creating build/temp.linux-armv7l-3.8/build/src.linux-armv7l-3.8/numpy
  creating build/temp.linux-armv7l-3.8/build/src.linux-armv7l-3.8/numpy/core
  creating build/temp.linux-armv7l-3.8/build/src.linux-armv7l-3.8/numpy/core/src
  creating build/temp.linux-armv7l-3.8/build/src.linux-armv7l-3.8/numpy/core/src/npymath
  compile options: '-Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -Inumpy/core/include -Ibuild/src.linux-armv7l-3.8/numpy/core/include/numpy -Ibuild/src.linux-armv7l-3.8/numpy/distutils/include -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c'
  Running from numpy source directory.
  /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/system_info.py:2026: UserWarning:
      Optimized (vendor) Blas libraries are not found.
      Falls back to netlib Blas library which has worse performance.
      A better performance should be easily gained by switching
      Blas library.
    if self._calc_info(blas):
  /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/system_info.py:2026: UserWarning:
      Blas (http://www.netlib.org/blas/) libraries not found.
      Directories to search for the libraries can be specified in the
      numpy/distutils/site.cfg file (section [blas]) or by setting
      the BLAS environment variable.
    if self._calc_info(blas):
  /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/system_info.py:2026: UserWarning:
      Blas (http://www.netlib.org/blas/) sources not found.
      Directories to search for the sources can be specified in the
      numpy/distutils/site.cfg file (section [blas_src]) or by setting
      the BLAS_SRC environment variable.
    if self._calc_info(blas):
  /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/system_info.py:1858: UserWarning:
      Lapack (http://www.netlib.org/lapack/) libraries not found.
      Directories to search for the libraries can be specified in the
      numpy/distutils/site.cfg file (section [lapack]) or by setting
      the LAPACK environment variable.
    return getattr(self, '_calc_info_{}'.format(name))()
  /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/system_info.py:1858: UserWarning:
      Lapack (http://www.netlib.org/lapack/) sources not found.
      Directories to search for the sources can be specified in the
      numpy/distutils/site.cfg file (section [lapack_src]) or by setting
      the LAPACK_SRC environment variable.
    return getattr(self, '_calc_info_{}'.format(name))()
  Warning: attempted relative import with no known parent package
  /usr/lib/python3.8/distutils/dist.py:274: UserWarning: Unknown distribution option: 'define_macros'
    warnings.warn(msg)

  ########### CLIB COMPILER OPTIMIZATION ###########
  Platform      :
    Architecture: armhf
    Compiler    : gcc

  CPU baseline  :
    Requested   : 'min'
    Enabled     : none
    Flags       : none
    Extra checks: none

  CPU dispatch  :
    Requested   : 'max -xop -fma4'
    Enabled     : NEON NEON_FP16 NEON_VFPV4 ASIMD ASIMDHP
    Generated   : none
  CCompilerOpt.cache_flush[809] : write cache to path -> /tmp/pip-install-lzjs_nw2/numpy/build/temp.linux-armv7l-3.8/ccompiler_opt_cache_clib.py
  error: [Errno 2] No such file or directory
  ----------------------------------------
  ERROR: Failed building wheel for numpy
Failed to build numpy
ERROR: Could not build wheels for numpy which use PEP 517 and cannot be installed directly
```

</details>

```
  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c'
  extra options: '-mfp16-format=ieee -mfpu=neon-fp-armv8 -march=armv8.2-a+fp16+fp16fml -Werror'
  CCompilerOpt.dist_test[581] : CCompilerOpt._dist_test_spawn[716] : Command (gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -fomit-frame-pointer -g -fno-semantic-interposition -DTHREAD_STACK_SIZE=0x100000 -fPIC -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/usr/include/python3.8 -Ibuild/src.linux-armv7l-3.8/numpy/core/src/common -Ibuild/src.linux-armv7l-3.8/numpy/core/src/npymath -c /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimdfhm.c -o /tmp/tmpl_hng7jf/tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimdfhm.o -MMD -MF /tmp/tmpl_hng7jf/tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimdfhm.o.d -mfp16-format=ieee -mfpu=neon-fp-armv8 -march=armv8.2-a+fp16+fp16fml -Werror) failed with exit status 1 output ->
  /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimdfhm.c: In function 'main':
  /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimdfhm.c:13:35: error: implicit declaration of function 'vfmlal_low_u32'; did you mean 'vfmlal_low_f16'? [-Werror=implicit-function-declaration]
     13 |     int ret  = (int)vget_lane_f32(vfmlal_low_u32(vlf, vlhp, vlhp), 0);
        |                                   ^~~~~~~~~~~~~~
        |                                   vfmlal_low_f16
  /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimdfhm.c:13:35: error: incompatible type for argument 1 of 'vget_lane_f32'
     13 |     int ret  = (int)vget_lane_f32(vfmlal_low_u32(vlf, vlhp, vlhp), 0);
        |                                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        |                                   |
        |                                   int
  In file included from /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimdfhm.c:4:
  /usr/lib/gcc/armv6-alpine-linux-musleabihf/10.2.1/include/arm_neon.h:6171:28: note: expected 'float32x2_t' but argument is of type 'int'
   6171 | vget_lane_f32 (float32x2_t __a, const int __b)
        |                ~~~~~~~~~~~~^~~
  /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimdfhm.c:14:36: error: implicit declaration of function 'vfmlslq_high_u32'; did you mean 'vfmlslq_high_f16'? [-Werror=implicit-function-declaration]
     14 |         ret += (int)vgetq_lane_f32(vfmlslq_high_u32(vf, vhp, vhp), 0);
        |                                    ^~~~~~~~~~~~~~~~
        |                                    vfmlslq_high_f16
  /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimdfhm.c:14:36: error: incompatible type for argument 1 of 'vgetq_lane_f32'
     14 |         ret += (int)vgetq_lane_f32(vfmlslq_high_u32(vf, vhp, vhp), 0);
        |                                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        |                                    |
        |                                    int
  In file included from /tmp/pip-install-lzjs_nw2/numpy/numpy/distutils/checks/cpu_asimdfhm.c:4:
  /usr/lib/gcc/armv6-alpine-linux-musleabihf/10.2.1/include/arm_neon.h:6269:29: note: expected 'float32x4_t' but argument is of type 'int'
   6269 | vgetq_lane_f32 (float32x4_t __a, const int __b)
        |                 ~~~~~~~~~~~~^~~
  cc1: all warnings being treated as errors

..


ERROR: Could not build wheels for numpy which use PEP 517 and cannot be installed directly
```",2021-09-03 00:04:32,,BLD: Failed building wheel for numpy on armv7,"['32 - Installation', 'component: SIMD']"
19808,open,jakpiase,"## Feature
Hi,
I am working at PaddlePaddle(chinese DL framework). We and other DL frameworks would extremely benefit from integrated bfloat16 numpy datatype. I have seen that TF added its own implementation and lately a standalone pip package [bfloat16](https://pypi.org/project/bfloat16/) was released. 
I have also seen NEP 41, 42, 43 which from what I understand will allow adding new, user-defined datatypes. Are you guys planning to integrate bfloat16 into core numpy? If you don't have the bandwidth to do that, is there anyone that can guide me how to implement that so I can make a PR? And how are these NEPs going? 
",2021-09-01 16:48:54,,ENH: numpy bfloat16 support,"['01 - Enhancement', '33 - Question', 'Tracking / planning']"
19786,open,hdkire,"Creating large byte string values can result in MemoryError and somewhat strange behavior.
It seems like they are limited to < 2 ** 31 characters, in my setup, is this the intended or expected behavior?

### Reproducing code example 1:

```python
import numpy as np
# OK
x = np.empty(1, dtype=f'S{2**30}')
x = np.empty(1, dtype=f'S{2**32}')
# Fail
x = np.empty(1, dtype=f'S{2**31}')
```

### Error message:

MemoryError: Unable to allocate -2.00 GiB for an array with shape (1,) and data type |S-2147483648

### Strange behavior:

When using even larger numbers, 2 ** 32 or more, there is no error but the resulting itemsize is 1.

###  Reproducing code example 2:

Reproduces also when tested on windows.

```python
import numpy as np
# OK
x = np.array(b'a' * (2 ** 30))
x = np.array(b'a' * (2 ** 31 - 1))

# Fail
x = np.array(b'a' * (2 ** 31))
```

### Error message:

MemoryError: Unable to allocate -2.00 GiB for an array with shape () and data type |S-2147483648


###  Reproducing code example 3:

```python
import numpy as np
# Fail
x = np.array(b'a' * (2 ** 32 ))
```

### Error message:

SystemError: <built-in function array> returned a result with an error set


### NumPy/Python version information:

1.20.3 3.9.5 (default, May 11 2021, 08:20:37) 
[GCC 10.3.0]


",2021-08-30 13:59:11,,MemoryError allocating large byte string values,"['00 - Bug', 'component: numpy.dtype']"
19782,open,hec10r,"When converting a `datetime64[ns]` array into object type, it returns the date as an integer instead of `datetime.datetime` object

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
>>> import numpy as np
>>> from datetime import datetime
>>> d = datetime.now()
>>> np.array(d)
array(datetime.datetime(2021, 8, 29, 20, 1, 32, 350877), dtype=object)
>>> np.array(d, dtype=""datetime64[ns]"")
array('2021-08-29T20:01:32.350877000', dtype='datetime64[ns]')
>>> np.array(d).astype(""O"") # Keeps the same, as expected
array(datetime.datetime(2021, 8, 29, 20, 1, 32, 350877), dtype=object)
>>> np.array(d, dtype=""datetime64[ns]"").astype(""O"") # Transforms to int. Unexpected
array(1630267292350877000, dtype=object)
>>> np.array(d, dtype=""datetime64[ns]"").view(np.int64) # Transforms to int. Expected
array(1630267292350877000)
```

IMO, the results from `np.array(d).astype(""O"")` and `np.array(d, dtype=""datetime64[ns]"").astype(""O"")` should be equal

### NumPy/Python version information:
```
1.18.5 3.8.3 (default, Jul  2 2020, 11:26:31) 
[Clang 10.0.0 ]
```",2021-08-30 01:16:56,,DOC: Inconsistencies in converting array with `datetime` values into object array,"['04 - Documentation', 'component: numpy.datetime64']"
19778,open,mattip,"Both of these sources state
> If you’re unsure where to start or how your skills fit in, reach out! You can ask on the mailing list or here, on GitHub, by opening a new issue or leaving a comment on a relevant issue that is already open.

I think the last bit is leading to comments on the issues ""I am new, can I work on this?"". We should clarify what we mean here: do we really want these kinds of comments? Perhaps we need a top-level ""new contributor"" walkthrough like

Code fix?
- build numpy locally or on gitpod
- change an existing test that exersizes the code path for the issue (or, unlikely, add a new test), make sure it fails
- fix the issue
- make sure the test passes
- submit a PR linking to the issue (fixes gh-XXXX).

Documentation fix?
- make sure the documentation still builds correctly
- spell checker? ",2021-08-29 06:55:51,,DOC: Tighten up first-time contributor suggestion on README.md and dev.index.rst,"['04 - Documentation', 'sprintable']"
19767,open,HaoZeke,"Currently trying to wrap a function which is not in a module silently errors at runtime. This should not be the case. Subroutines work without being encapsulated in a module.
### Reproducing code example:
Consider:
```fortran
real(kind=8) function mysqrt(x) result(r)
  real(kind=8), intent(in) :: x
  r = x*x
end function
```

This actually compiles via `f2py -m sq -c mys.f90` however, on usage it invariably returns `0.0`.

```python
import sq
sq.mysqrt(3.5)
# 0.0
```

The same code works when wrapped in a module:

```fortran
module test_sq
  implicit none
  contains

real(kind=8) function mysqrt(x) result(r)
  real(kind=8), intent(in) :: x
  r = x*x
end function

end module
```

Now we get the expected:
```python
import sq
sq.test_sq.mysqrt(3.5)
#12.25
```

Note that as mentioned this also works in a subroutine without a module.

```fortran
subroutine mysqrt(x,r)
  real(kind=8), intent(in) :: x
  real(kind=8), intent(out) :: r
  r = x*x
end subroutine
```
Which returns the correct result as well.

```python
import sq
sq.mysqrt(3.5)
#12.25
```
### NumPy/Python version information:

`1.21.2 3.9.6 | packaged by conda-forge | (default, Jul 11 2021, 03:36:15)
[Clang 11.1.0 ]`
",2021-08-27 23:50:14,,f2py: wrapping functions without modules,"['00 - Bug', 'component: numpy.f2py']"
19755,open,dibgerge,"<!-- Please describe the issue in detail here, and fill in the fields below -->

The function `piecewise(x, condlist, funclist)` returns an output which has the same `dtype` as the input `x`. However, this input is processed by one of the functions in `funclist` and thus it should return an array which dtype same as those function's output. 

 for example
```python
def func1(v):
    return v + 0.1

def func2(v):
    return v + 0.2

funclist = [func1, func2]
condlist = [[True, False], [False, True]]

x = np.array([1, 2])
np.piecewise(x, condlist, funclist)

# array([1, 2])
```

However, the actual expected answer should be  array([1.1, 2.2]), which can be attained, but we must make sure the input is specified with `float` dtype, e.g.:  `x = array([1.0, 2.0])`.

Since we have functions here operating on each element of `x`, we expect the output to be same as functions outputs.

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.19.5 3.7.9 (default, Aug 31 2020, 12:42:55) 
[GCC 7.3.0]

",2021-08-26 01:09:47,,"`piecewise` returns array with same `dtype` as input, which may have undesired outcomes ",['04 - Documentation']
19704,open,itamarst,"In the documentation for `np.take_along_axis()`, an alternative implementation is provided: https://github.com/numpy/numpy/blob/623bc1fae1d47df24e7f1e29321d0c0ba2771ce0/numpy/lib/shape_base.py#L90

I am using this alternative implementation to implement a first pass of `take_along_axis()` in Numba.

Turns out, however, that in some cases the output between the two doesn't match.

### Reproducing code example:

```python
import numpy as np
from numpy import ndindex, s_

def take_along_axis_2(a, indices, axis):
    # This is the alternative implementation from the NumPy docs (""Notes"" section):
    # https://numpy.org/doc/stable/reference/generated/numpy.take_along_axis.html
    Ni, M, Nk = a.shape[:axis], a.shape[axis], a.shape[axis+1:]
    J = indices.shape[axis]  # Need not equal M
    out = np.empty(Ni + (J,) + Nk)

    for ii in ndindex(Ni):
        for kk in ndindex(Nk):
            a_1d       = a      [ii + s_[:,] + kk]
            indices_1d = indices[ii + s_[:,] + kk]
            out_1d     = out    [ii + s_[:,] + kk]
            for j in range(J):
                out_1d[j] = a_1d[indices_1d[j]]
    return out

# Based on https://github.com/numpy/numpy/blob/v1.21.0/numpy/lib/tests/test_shape_base.py#L74-L79
arr  = np.ones((3, 4, 1))
ai = np.ones((1, 2, 5), dtype=np.intp)

actual = np.take_along_axis(arr, ai, axis=1)
print(""np.take_along_axis() returned array of shape"", actual.shape)
alternative = take_along_axis_2(arr, ai, axis=1)
print(""Alternative take_along_axis() returned array of shape"", alternative.shape)
assert actual == alternative
```

### Error message:

```shell-session
$ python example2.py 
np.take_along_axis() returned array of shape (3, 2, 5)
Traceback (most recent call last):
  File ""example2.py"", line 26, in <module>
    alternative = take_along_axis_2(arr, ai, axis=1)
  File ""example2.py"", line 14, in take_along_axis_2
    indices_1d = indices[ii + s_[:,] + kk]
IndexError: index 1 is out of bounds for axis 0 with size 1
```

### NumPy/Python version information:

Python 3.8, NumPy 1.20.2.

",2021-08-18 13:48:26,,take_along_axis alternative implementation doesn't match real implementation,['unlabeled']
19690,open,nschloe,"Right now, `np.isscalar` returns `True` iff the input is a Python scalar:
```python
import numpy as np

np.isscalar(3.14)              # True
np.isscalar(np.array(3.14))    # False
np.isscalar(np.array([3.14]))  # False
```
However, `np.array`s with `ndim == 0` smell, taste, and behave like Python scalars in almost all situations, so you almost always want to admit ndim-0 arrays when writing `if np.isscalar(a): ...`. Unfortunately, downstream uses `isscalar` far more often (e.g. scipy: 61 `isscalar`, 9 `ndim`). This begs the question if `np.isscalar` should be deprecated or its behavior changed to
```python
def isscalar(x):
    return np.ndim(x) == 0
```",2021-08-17 14:42:35,,"deprecate isscalar, change behavior?",['unlabeled']
19685,open,dsaoijhlgfdngfsd,"Multiplication of a matrix with its transpose causes segfault for ""large"" matrices. Multiplication works for ""small"" matrices and also works if transpose is copied (`np.copy()`). Specific examples of large/small matrices below.

I do have sufficient amount of memory (>350GB) on the machine, and I observe the same behavior on EC2 instance (both Ubuntu and Amazon Linux) as well as in a Docker container.

### Reproducing code example:
**This fails:**
```python
import numpy as np
matrix = np.random.rand(80_000, 3072)
out = matrix.dot(matrix.T)
```

**This also fails:**
```python
import numpy as np
matrix = np.random.rand(80_000, 3072)
out = matrix @ matrix.T
```

**All these work:**
```python
import numpy as np
matrix = np.random.rand(75_000, 3072)
out = matrix @ matrix.T
```
```python
import numpy as np
matrix = np.random.rand(75_000, 3072)
out = matrix.dot(matrix.T)
```
```python
import numpy as np
matrix = np.random.rand(80_000, 3072)
out = matrix @ np.copy(matrix.T)
```

### Error message:
```
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
Core was generated by `python'.
Program terminated with signal SIGSEGV, Segmentation fault.
#0  0x00007fc6f108cd49 in dgemm_oncopy_SKYLAKEX ()
   from /opt/bitnami/python/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libopenblasp-r0-2d23e62b.3.17.so
[Current thread is 1 (Thread 0x7fc6f2a92f00 (LWP 676))]
(gdb) where
#0  0x00007fc6f108cd49 in dgemm_oncopy_SKYLAKEX ()
   from /opt/bitnami/python/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libopenblasp-r0-2d23e62b.3.17.so
#1  0x00007fc6f01a0734 in inner_thread ()
   from /opt/bitnami/python/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libopenblasp-r0-2d23e62b.3.17.so
#2  0x00007fc6f02c06f5 in exec_blas ()
   from /opt/bitnami/python/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libopenblasp-r0-2d23e62b.3.17.so
#3  0x00007fc6f01a10f3 in dsyrk_thread_LT ()
   from /opt/bitnami/python/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libopenblasp-r0-2d23e62b.3.17.so
#4  0x00007fc6f00b08ab in cblas_dsyrk ()
   from /opt/bitnami/python/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libopenblasp-r0-2d23e62b.3.17.so
#5  0x00007fc6f21e45cd in DOUBLE_matmul_matrixmatrix.constprop.6 ()
   from /opt/bitnami/python/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so
#6  0x00007fc6f21e86fb in DOUBLE_matmul ()
   from /opt/bitnami/python/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so
#7  0x00007fc6f21f7b49 in PyUFunc_GeneralizedFunctionInternal ()
```

<!-- Full error message, if any (starting from line Traceback: ...) -->

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```
1.21.2 3.8.10 (default, Jun  2 2021, 10:49:15)
[GCC 9.4.0]
```
",2021-08-17 09:06:12,,"Multiplication of a matrix with its transpose causes segfault for ""large"" matrices",['unlabeled']
19655,open,hdkire,"The new behavior of np.unique seems inconsistent with masked arrays.
Release notes for 1.2.1 https://numpy.org/devdocs/release/1.21.0-notes.html#np-unique-now-returns-single-nan

### Reproducing code example:

```python
import numpy as np
a1 = np.ma.masked_array([1.0, 2.0, np.nan, np.nan],
                        [False, True, False, False])
unique_a1 = np.unique(a1)
print('a1', a1)
# ... a1 [1.0 -- nan nan]
print('unique_a1', unique_a1)
# ... unique_a1 [1.0 -- nan]
# Ok, single NaN.
assert np.count_nonzero(np.isnan(unique_a1)) == 1

a2 = np.ma.masked_array([1.0, 2.0, np.nan, np.nan, 3],
                        [False, True, False, False, True])
unique_a2 = np.unique(a2)
print('a2', a2)
# ... a2 [1.0 -- nan nan --]
print('unique_a2', unique_a2)
# ... unique_a2 [1.0 -- nan nan --]
# Fail, 2 NaNs.
assert np.count_nonzero(np.isnan(unique_a2)) == 1
```

### NumPy/Python version information:

1.21.0 3.9.5 (default, May 11 2021, 08:20:37) 
[GCC 10.3.0]

",2021-08-12 14:02:09,,Multiple NaN from np.unique with masked arrays,['component: numpy.ma']
19650,open,akhmerov,"Numpy dot product can experience a dramatic slowdown on non-contiguous data:

```python
import numpy as np
print(np.__version__)
# 1.21.1
a = np.random.randn(1000, 100, 1000)
%timeit a[:, 0, :] @ a[:, :, 20]
# 1.59 s ± 184 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
%timeit np.ascontiguousarray(a[:, 0, :]) @ np.ascontiguousarray(a[:, :, 20])
# 8.06 ms ± 410 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```
While the workaround shown above is straightforward, the performance regression is hard to detect, and many users won't even be aware about it. Therefore I propose to improve the dot product to identify when data is highly discontinuous and copy it before performing the multiplication.",2021-08-11 19:10:05,,Improve dot performance on data that isn't contiguous,"['01 - Enhancement', 'component: numpy.ufunc']"
19647,open,axil,"### Reproducing code example:

```python
import numpy as np
a = np.random.rand(100, 100, 100)

%timeit np.matmul(a,a)
16 ms ± 2.76 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

%timeit np.einsum('qij,qjk->qik', a, a)
174 ms ± 37.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

%timeit np.einsum('qij,qjk->qik', a, a, optimize=True)
101 ms ± 1.48 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.21.1 3.7.7 (tags/v3.7.7:d7c567b08f, Mar 10 2020, 10:41:24) [MSC v.1900 64 bit (AMD64)]
",2021-08-11 17:13:30,,einsum 6 times slower than matmul,['01 - Enhancement']
19623,open,anntzer,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np, io
# (np.dtype([(""a"", int), (""b"", float), (""c"", int)])[[""a"", ""c""]]) constructs a non-contiguous dtype with
# the a and c fields)
np.loadtxt(io.StringIO(""1 3\n5 7""), dtype=np.dtype([(""a"", int), (""c"", int)]))  # OK
np.loadtxt(io.StringIO(""1 3\n5 7""), dtype=np.dtype([(""a"", int), (""b"", float), (""c"", int)])[[""a"", ""c""]])  # OK
np.genfromtxt(io.StringIO(""1 3\n5 7""), dtype=np.dtype([(""a"", int), (""c"", int)]))  # OK
np.genfromtxt(io.StringIO(""1 3\n5 7""), dtype=np.dtype([(""a"", int), (""b"", float), (""c"", int)])[[""a"", ""c""]])  # fails
```

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/main/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->
```
/usr/lib/python3.9/site-packages/numpy/lib/npyio.py in genfromtxt(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, like)
   2220             else:
   2221                 rows = np.array(data, dtype=[('', _) for _ in dtype_flat])
-> 2222                 output = rows.view(dtype)
   2223             # Now, process the rowmasks the same way
   2224             if usemask:

ValueError: When changing to a larger dtype, its size must be a divisor of the total size in bytes of the last axis of the array.
```

This occurs because genfromtxt handles dtypes by first ""flattening"" them (lifting nested fields to the toplevel, but also throwing away alignment info), constructing an array with that flattened dtype, and then `.view()`ing it with the original dtype; it is that last step that fails as the view cannot be done as the memory layout changes.

(OTOH, loadtxt directly constructs an output with the right dtype by using a recursive ""row packer"" that constructs a nested list/tuple with the right shape.)

I was hoping to implement a similar ""flat-dtype"" optimization for loadtxt, so that'll likely involve alignment-handling code on both sides.

... or can we just claim that non-contiguous dtypes are not supported by loadtxt/genfromtxt?  (I expect the use cases of having to loadtxt() into an array with a specific, non-contiguous alignment to be exceedingly rare; you can always to a copy later if needed, which should have negligible cost compared to the (rather slow) loadtxt.)

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```
1.21.1 3.9.6 (default, Jun 30 2021, 10:22:16) 
[GCC 11.1.0]
```

",2021-08-06 17:40:03,,genfromtxt fails when a non-contiguous dtype is requested,['unlabeled']
19604,open,rossbar,"If a user passes in an argument for `fname` that is of the wrong type, a `ValueError` is raised instead of a `TypeError`:

```python
>>> np.loadtxt(1)
Traceback (most recent call last):
   ...
ValueError: fname must be a string, file handle, or generator
```

In principle, this should be a `TypeError`. This is a very minor issue though, so it might be best to just mark `wontfix` and close. Only opening here so that this issue doesn't prevent progress on #19593 .

Arguments in favor of changing:
 - technical correctness
 - Consistency with `genfromtxt`, which [does raise a `TypeError` in this scenario](https://github.com/numpy/numpy/blob/41977b24ae011a51f64faa75cb524c7350fdedd9/numpy/lib/npyio.py#L1802-L1805)
 
Arguments against changing:
 - It's been a `ValueError` for [11+ years](https://github.com/numpy/numpy/blame/cdf83879fd0d050cade0fe7852550fcbcc177648/numpy/lib/npyio.py#L969); changing it now has the potential to break some users for very little benefit.

### NumPy/Python version information:

1.22.0.dev0+617.g2c1a34daa 3.9.6 (default, Jun 30 2021, 10:22:16) 
[GCC 11.1.0]
",2021-08-03 11:08:24,,MAINT: Wrong exception type raised when `fname` input to `loadtxt` has incorrect type,['54 - Needs decision']
19588,open,Qiyu8,"### Reproducing code example:

```python
import numpy as np
arr = np.arange(3 * 4 * 5).reshape(3, 4, 5)
print(arr.strides)  #output (80, 20, 4)
res = np.delete(arr, [1], 1)
print(res.strides)  #output (20, 60, 4), should be (60, 20, 4)
print(res.flags.c_contiguous) #output false, should be true, I think this is the reason. 
```

### Error message:
No Error message

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.21.1 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]

###  Possible solution
add the array order back to the result.",2021-07-31 01:47:46,,BUG: Incorrect strides calculation after np.delete.,"['01 - Enhancement', 'Priority: low']"
19579,open,mikeronayne,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

```python
import numpy as np
>>> np.iinfo(np.int64).max + 1 == np.int64(np.iinfo(np.int64).max)
True
>>> np.iinfo(np.int64).max + 2 == np.int64(np.iinfo(np.int64).max)
True
>>> np.iinfo(np.int64).max + 1536 == np.int64(np.iinfo(np.int64).max) # 9223372036854777343
True
>>> np.iinfo(np.int64).max + 1537 == np.int64(np.iinfo(np.int64).max)
False
```

After searching `9223372036854777343` on the web, I'm thinking maybe one (or both) of the operands are being casted to float behind the scenes?

### NumPy/Python version information:

1.21.1 3.9.4 (tags/v3.9.4:1f2e308, Apr  6 2021, 13:40:21) [MSC v.1928 64 bit (AMD64)]

",2021-07-28 22:13:33,,Some Python Native int Comparison Fails with np.int64 Max,['50 - Duplicate']
19573,open,Snape3058,"### Reproducing code example:

**Static analysis results, no POC.**
This static analysis report has been manually reviewed to verify its validity.

### Error message:

In Python API documentation, function [`PyDict_SetItemString`](https://docs.python.org/3/c-api/dict.html#c.PyDict_SetItemString) will not steal a refcnt for the third argument. This will lead to many refcnt leaks in this file.

* In function `F2PyDict_SetItemString`
https://github.com/numpy/numpy/blob/04ab04d93d4d7a4d241fe0ceb725436a8b6c8c2e/numpy/f2py/src/fortranobject.c#L30

* On the true branch of
https://github.com/numpy/numpy/blob/04ab04d93d4d7a4d241fe0ceb725436a8b6c8c2e/numpy/f2py/src/fortranobject.c#L373

* On the true branch of
https://github.com/numpy/numpy/blob/04ab04d93d4d7a4d241fe0ceb725436a8b6c8c2e/numpy/f2py/src/fortranobject.c#L379

* After the function returns here
https://github.com/numpy/numpy/blob/04ab04d93d4d7a4d241fe0ceb725436a8b6c8c2e/numpy/f2py/src/fortranobject.c#L450

### NumPy/Python version information:

Static analysis carried out on commit 04ab04d.

### Suggested fixes:

Fix the leak in function `F2PyDict_SetItemString`, and use this function instead of `PyDict_SetItemString` in this file. Corresponding nullity checks on the third argument should be removed simultaneously for simplicity.",2021-07-28 13:27:05,,F2PY refcnt leak for API function `PyDict_SetItemString` in file fortranobject.c,['component: numpy.f2py']
19544,open,seberg,"A task-list of possible or necessary post UFunc refactor cleanups:

* [ ] The ufunc cache can currently grow indefinitely, in practice it probably doesn't matter, but if someone writes a hypothesis test for a SciPy ufunc with 4 inputs, the explosion of input DType combination could become problematic.
  * Either needs a hard maximum cache size, or a smart caching scheme (or both).
* [x] The PR does not address weak promotion.  Making this only accessible through the legacy path, and only as the old-style ""value-based"" promotion at this time.  It would be nice to fix this, but it is probably cleaner to do so after we attempt getting rid of value-based casting in (currently a `TODO: we need to special case scalars here` comment in `ufunc_object.c`). (See also https://github.com/numpy/numpy/pull/19384#discussion_r675224885)
",2021-07-22 16:53:15,,MAINT: Post UFunc refactor check-list,"['component: numpy.ufunc', 'component: numpy.dtype']"
19532,open,max3-2,"### Reproducing code example:

This is hard to provide since it seems to depend on the input quite strongly. I try to describe my issue to pinpoint the direction for building a better MWE. FWIW, the included MWE does perform all the basic tasks , the input is however completely random (`x`) - in case of the true code it is a genetic algorithm that modifies a fixed input (`x`) by small tolerances to produce a set of many different `x`. The MWE however **does not reproduce** the error.
During a full optimization, there are easily 1e7 calls to `np.linalg.inv`. I would estimate that every third optimization fail, independent of convergence or progression.

```python
""""""
Segfault tester
""""""
import numpy as np

while True:
    x = np.random.random((2, 17)) * 1000.  # set of 2D points
    x = np.vstack((x, np.ones((1, np.shape(x)[1])))). # make 'homogeneous'

    m = np.mean(x[:2], axis=1)
    x[0, :] -= m[0]
    x[1, :] -= m[1]

    avdst = np.mean(np.sqrt((x[0, :] ** 2 + x[1, :] ** 2)))
    scale = np.sqrt(2) / avdst

    x[0:2] *= scale

   # This matrix norms the points and adapts the scale
    Tx = np.array([[scale, 0, -scale*m[0]],
                   [0, scale, -scale*m[1]],
                   [0, 0, 1]])
 
   # THIS is where it crashes, the inverse is needed later on
    P = np.linalg.inv(Tx)
```
Due to the inv call, I added checks for singularity and `np.isclose(np.linalg.det(Tx), 0.)` to prevent issues to no avail. 

### Error message:

```python
Fatal Python error: Segmentation fault

Current thread 0x000070000bf94000 (most recent call first):
  File ""/usr/local/lib/python3.9/site-packages/numpy/linalg/linalg.py"", line 545 in inv
  File ""<__array_function__ internals>"", line 5 in inv
```
Any following traceback just runs through my code and does not provide any more information.

### NumPy/Python version information:

macOS 11. on an Intel machine! This will not occur on Windows - I have quite some ppl running the code without reporting the issue.

```python
print(numpy.__version__, sys.version)
1.21.1 3.9.6 (default, Jun 29 2021, 05:25:02) 
[Clang 12.0.5 (clang-1205.0.22.9)]
```

```python
>>> np.__config__.show()
blas_mkl_info:
  NOT AVAILABLE
blis_info:
  NOT AVAILABLE
openblas_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/usr/local/lib']
blas_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/usr/local/lib']
lapack_mkl_info:
  NOT AVAILABLE
openblas_lapack_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/usr/local/lib']
lapack_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/usr/local/lib']
Supported SIMD extensions in this NumPy install:
    baseline = SSE,SSE2,SSE3
    found = SSSE3,SSE41,POPCNT,SSE42,AVX,F16C,FMA3,AVX2
    not found = AVX512F,AVX512CD,AVX512_KNL,AVX512_SKX,AVX512_CLX,AVX512_CNL,AVX512_ICL
```

",2021-07-21 13:43:14,,`np.linalg.inv` with occasional SEGFAULT on macOS,['unlabeled']
19520,open,Mukulikaa,"Most functions have a `source` link along with their documentation that leads to the source of the ~~docstrings~~ functions in GitHub. For example, [numpy.ndarray](https://numpy.org/devdocs/reference/generated/numpy.ndarray.html#numpy-ndarray), [numpy.argmax](https://numpy.org/devdocs/reference/generated/numpy.argmax.html#numpy-argmax) etc. It is a feature of the Sphinx theme that NumPy uses. It makes it very easy for contributors to locate the docstrings/code and modify them.

However, some functions like [numpy.ndarray.transpose](https://numpy.org/devdocs/reference/generated/numpy.ndarray.transpose.html#numpy.ndarray.transpose), [numpy.array](https://numpy.org/devdocs/reference/generated/numpy.array.html?highlight=numpy%20array#numpy.array) etc. don't have the `source` link. These are functions whose docstrings reside separately in `numpy/core/_add_newdocs.py` because they are defined in C-extension modules. It would be nice if we could find a way to make the feature work for these ~~docstrings~~ functions as well. 
Edit: The `source` link would lead to the C file like in [numpy.ndarray](https://numpy.org/devdocs/reference/generated/numpy.ndarray.html#numpy-ndarray).

Looking at the code [here](https://github.com/numpy/numpy/blob/1cc3f07ad3e0987da7d399dd8febf01ef3b48b45/doc/source/conf.py#L367) could be a starting point.",2021-07-19 17:25:57,,DOC: No `source` link for some functions in the API reference guide,['04 - Documentation']
19515,open,hawkinsp,"NumPy has surprisingly high memory usage for printing arrays with many dimensions.

### Reproducing code example:

```
import numpy as np
x = np.ones([2]*25)
str(x)
```

On my Mac, this script has a peak memory usage of 9.3GB and it takes 298s to run.

This array takes 268MB in memory, and its printed form as measured by `len(str(x))` is 570MB. So 9.3GB memory usage by NumPy seems... awfully large? Not to mention 300s is a rather long time to take when printing.

(This was originally reported to JAX as https://github.com/google/jax/issues/7301; JAX simply calls NumPy to print arrays.)

### Error message:

n/a

### NumPy/Python version information:

```
In [1]: import sys, numpy; print(numpy.__version__, sys.version)
1.21.1 3.7.2 (default, Jan 13 2021, 20:29:55)
[Clang 11.0.0 (clang-1100.0.33.17)]
```",2021-07-19 13:39:46,,High memory usage when printing arrays with many dimensions,['01 - Enhancement']
19511,open,seberg,"See this and the following discussion, we should likely fix the double-double value for `np.finfo(np.longdouble).smallest_normal`. @wkschwartz, lets continue discussion here.

I might be missing some context here, so forgive me if I'm confusing matters. It appears that you are able to compute the smallest positive subnormal 𝜎, and a few lines below you define `macheps`, so I take it you are able to compute the machine epsilon 𝜀. From there, you can in fact compute the smallest positive normal 𝜔 as 𝜎 ⨸ 𝜀, where ⨸ means division in the relevant floating point system (which is what the `/` operator does in C and Python). In Python terms, you can do approximately the following, but you'll need to move up the definition of `macheps` from below.

```Python
    # Leave the same value for the smallest subnormal as double
    smallest_subnormal_dd = ld(nextafter(0., 1.))
    smallest_normal_dd = smallest_subnormal / macheps
```

_Originally posted by @wkschwartz in https://github.com/numpy/numpy/pull/18536#discussion_r670442245_",2021-07-18 13:20:03,,ENH: Fix double-double smallest normal number,['unlabeled']
19494,open,ganesh-k13,"### New UFuncs developer documentation

As discussed on newcomers hour, it would be nice to have a developer doc for adding new UFuncs. 
Right now a good PR to refer will be LCM&GCD PR (#8774) and perhaps a more in-depth example will be bit_count (#19355)",2021-07-16 16:46:51,,DOC: Add some documentation for adding new UFuncs,['04 - Documentation']
19473,open,YunChunRui,"<!-- Please describe the issue in detail here, and fill in the fields below -->

When I use the numpy function(**np.linalg.matrix_rank**) to calculate the rank of the matrix, the function returns the rank of the matrix as 0, and then when I use matlab to calculate the rank of the same matrix, matlab got the correct result(30). I compared numpy and matlab calculations The source code of the matrix rank, but there seems to be no difference between the two ways. 
Sometimes, numpy will prompt RuntimeWarning
How should i solve this problem？

The link to the matrix is as follows
[1.zip](https://github.com/numpy/numpy/files/6813848/1.zip)

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
mat = np.loadtxt(""./1.csv"",delimiter=',')
rk = np.linalg.matrix_rank(mat)
print(rk)
```

```matlab
% matlab code
mat = csvread('1.csv')
rank(a)
```

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/main/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->
/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/numpy/linalg/linalg.py:1855: RuntimeWarning: invalid value encountered in greater
  return count_nonzero(S > tol, axis=-1)
### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

```python
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.16.4 3.7.4 (default, Aug 13 2019, 20:35:49) 
[GCC 7.3.0]
>>>np.show_config()
blas_mkl_info:
  NOT AVAILABLE
blis_info:
  NOT AVAILABLE
openblas_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
blas_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
lapack_mkl_info:
  NOT AVAILABLE
openblas_lapack_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
lapack_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
```",2021-07-14 06:02:22,,np.linalg.matrix_rank calculates the rank of the matrix and gives the wrong result,['unlabeled']
19472,open,DAGwood1018,"<!-- Please describe the issue in detail here, and fill in the fields below -->

While diagonalizing the blocks of a large matrix I encountered several Hermitian blocks that numpy.linalg.eigh does not appear to be able to handle. By this I mean the eigenvalues it returns do not add up to the trace of the matrix indicating they must be wrong. 

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->
Example of a matrix that causes eigh fails to diagonalize correctly is contained in zip file:
[Matrix.zip](https://github.com/numpy/numpy/files/6813181/Matrix.zip)

```python
import numpy as np
import os

path= os.getcwd()
with open(os.path.join(path,'mat.npy'),'rb') as file:
    M= np.load(file, allow_pickle=False)

w, v= np.linalg.eigh(M)
    
trace= np.trace(M)
eigSum= np.sum(w)
if not round(trace,7)==round(eigSum,7):
    print('Diagonalization Failed?')
    print('Trace=', trace)
    print('Eigval Sum=', eigSum)
```

### Error message:
Numpy does not raise any error message but it is clear that the diagonalization cannot be correct since the trace does not equal the sum of the eigenvalues. 

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/main/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### NumPy/Python version information:
Python 3.8
Numpy 1.19.2

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2021-07-14 03:04:26,,numpy.linalg.eigh fails to properly diagonalize some matrices while raising no error,['unlabeled']
19464,open,kshitij12345,"As per `array-api`,
```
The returned array must have the same data type as x .
```

However,
```python
>>> np.__version__
'1.21.0'
>>> np.trunc(2).dtype
dtype('float64')
>>> np.trunc(np.ones(2, dtype=np.int32)).dtype
dtype('float64')
```

Ref: https://data-apis.org/array-api/latest/API_specification/elementwise_functions.html#trunc-x",2021-07-13 07:09:32,,`np.trunc` is inconsistent with array-api,['01 - Enhancement']
19420,open,teodorkostov,"## Documentation

[API Reference 1.21](https://numpy.org/doc/1.21/reference/index.html)

The only way the this can be used currently is by searching in a third party search engine. If your users have to use your search to get information on your site - this is bad. If your users have to consistently use another search engine (e.g. Google) to get the page of your site they are looking for - that's even worse.

The point of this issue is not to just criticize the current visual structure of the API Reference. The idea is to provoke a conversation and redesign of the API Reference documentation visuals so that the following is achieved:

* All of the framework functionality is efficiently presented and described on a single page
* All of the framework functionality is efficiently searchable from within a single page (Ctrl+F)
* The detailed structure of the functionality and the documentation is not a burden but a enhancement for the user experience

### Poor documentation visual structure for API Reference

[This](https://lodash.com/docs/4.17.15) looks like an API Reference. [This](https://numpy.org/doc/1.21/reference/index.html) does __not__ look like an API Reference, it looks like some general documentation. Imagine a new user trying to find a specific function - let's say we want to understand how to create a new array and fill it with a specific value. Let's look for `fill`. No `fill` [here](https://numpy.org/doc/1.21/reference/index.html). Now we have to either read the entire page (highly unlikely) or just search. [Boom](https://numpy.org/doc/1.21/search.html?q=fill), 30+ results. Wtf? So we go to [another search engine](https://duckduckgo.com/?q=numpy+fill&ia=web). It turns out that we are probably looking for `full`.

Now don't get me wrong, I'm not saying that `full` should not be under _Array creation routines_. This is absolutely accurate and absolutely useless if you want to find something.

[This view](https://numpy.org/doc/1.21/reference/routines.array-creation.html) looks a bit more like an API Reference. We have a list of functions and some short descriptions. Sweet! But when wanting to understand what the framework can do probably we would need to use functionality that fits into different documentation sections. For example, we created our array with `full` now we want to split it. Oh, no `split` [here](https://numpy.org/doc/1.21/reference/routines.array-creation.html). [Here we go](https://numpy.org/doc/1.21/search.html?q=split) [again](https://duckduckgo.com/?q=numpy+split&ia=web)!

I hope you get the point - the users of your framework should not be required to guess or know the taxonomy for the functionality in order to use that functionality. An API Reference should present everything that's available in the framework in a succinct and efficient manner.

### Complex search is probably pointless

Let's face it - you are not Google. No point in reinventing the search. If the search functionality on your page is anything more than a simple JavaScript filter across a few tags then you are probably doing it wrong. This is effort wasted that could have been used more productively. Again, 30+ results of all `fill` methods is absolutely accurate information but useless.",2021-07-06 12:53:28,,DOC: API Reference documentation has tragic visual structure,['04 - Documentation']
19410,open,joseph-long,"It appears that under some circumstances the SVD (LAPACK gesdd or gesvd?) wrapper in NumPy will fail and *also* fail to raise an exception.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
$ python
Python 3.9.5 (default, Jun  4 2021, 12:28:51)
[GCC 7.5.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy as np
>>> x = np.random.randn(30000,30000)
>>> u,s,vt = np.linalg.svd(x, full_matrices=False)
init_dgesdd failed init
>>> np.all(u==0)
True
>>> np.all(s==0)
False
>>> s[s!=0]
array([5.367297e-318])
>>> np.all(vt==0)
True
```

### Error message:

No exception is raised, but `init_dgesdd failed init` is printed. The resulting matrices `u, s, vt` are zero or uninitialized.

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

```
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.20.2 3.9.5 (default, Jun  4 2021, 12:28:51)
[GCC 7.5.0]
>>> np.show_config()
blas_mkl_info:
    libraries = ['mkl_rt', 'pthread']
    library_dirs = ['/groups/jrmales/josephlong/miniconda3/envs/defaults_mkl/lib']
    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
    include_dirs = ['/groups/jrmales/josephlong/miniconda3/envs/defaults_mkl/include']
blas_opt_info:
    libraries = ['mkl_rt', 'pthread']
    library_dirs = ['/groups/jrmales/josephlong/miniconda3/envs/defaults_mkl/lib']
    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
    include_dirs = ['/groups/jrmales/josephlong/miniconda3/envs/defaults_mkl/include']
lapack_mkl_info:
    libraries = ['mkl_rt', 'pthread']
    library_dirs = ['/groups/jrmales/josephlong/miniconda3/envs/defaults_mkl/lib']
    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
    include_dirs = ['/groups/jrmales/josephlong/miniconda3/envs/defaults_mkl/include']
lapack_opt_info:
    libraries = ['mkl_rt', 'pthread']
    library_dirs = ['/groups/jrmales/josephlong/miniconda3/envs/defaults_mkl/lib']
    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
    include_dirs = ['/groups/jrmales/josephlong/miniconda3/envs/defaults_mkl/include']
```

```
$ conda list
# packages in environment at /groups/jrmales/josephlong/miniconda3/envs/defaults_mkl:
#
# Name                    Version                   Build  Channel
_libgcc_mutex             0.1                        main
_openmp_mutex             4.5                       1_gnu
blas                      1.0                         mkl
ca-certificates           2021.5.25            h06a4308_1
certifi                   2021.5.30        py39h06a4308_0
intel-openmp              2021.2.0           h06a4308_610
ld_impl_linux-64          2.35.1               h7274673_9
libffi                    3.3                  he6710b0_2
libgcc-ng                 9.3.0               h5101ec6_17
libgomp                   9.3.0               h5101ec6_17
libstdcxx-ng              9.3.0               hd4cf53a_17
mkl                       2021.2.0           h06a4308_296
mkl-service               2.3.0            py39h27cfd23_1
mkl_fft                   1.3.0            py39h42c9631_2
mkl_random                1.2.1            py39ha9443f7_2
ncurses                   6.2                  he6710b0_1
numpy                     1.20.2           py39h2d18471_0
numpy-base                1.20.2           py39hfae3a4d_0
openssl                   1.1.1k               h27cfd23_0
pip                       21.1.3           py39h06a4308_0
python                    3.9.5                h12debd9_4
readline                  8.1                  h27cfd23_0
setuptools                52.0.0           py39h06a4308_0
six                       1.16.0             pyhd3eb1b0_0
sqlite                    3.36.0               hc218d9a_0
tk                        8.6.10               hbc83047_0
tzdata                    2021a                h52ac0ba_0
wheel                     0.36.2             pyhd3eb1b0_0
xz                        5.2.5                h7b6447c_0
zlib                      1.2.11               h7b6447c_3
```",2021-07-05 14:49:31,,"SVD fails on large matrices without raising exception (Linux, MKL 2021.2.0)",['component: numpy.linalg']
19405,open,jameslamb,"### Description

As of `numpy` 1.21.0, calling `numpy.unique()` on a `numpy.matrix` raises the following error

> ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()

That error does not occur on `numpy` 1.20.3.

I don't see any notes in https://github.com/numpy/numpy/releases/tag/v1.21.0 that suggest that this is expected, so I think it might be an unexpected breaking change.

Thanks very much for your time and consideration.

### Reproducing code example:

With `numpy` 1.20.3, the following code succeeds.

```python
import numpy as np

# array
arr = np.ones((5, 5))
print(np.unique(arr))

# matrix
mat = np.matrix(arr)
print(np.unique(mat))
# on 1.20.3: [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]
# on 1.21.0: ValueError: The truth value of an array...
```

Running the same code with `numpy` 1.21.0 produces the error mentioned in this issue.

I pinned `numpy` versions using shell commands like the following.

```shell
pip uninstall -y numpy
pip install numpy==1.21.0
```

### Error message:

```text
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-20-72cd9eed9ec9> in <module>
      2 mat = np.matrix(arr)
      3 print(type(mat))
----> 4 np.unique(mat)

<__array_function__ internals> in unique(*args, **kwargs)

~/miniconda3/lib/python3.8/site-packages/numpy/lib/arraysetops.py in unique(ar, return_index, return_inverse, return_counts, axis)
    270     ar = np.asanyarray(ar)
    271     if axis is None:
--> 272         ret = _unique1d(ar, return_index, return_inverse, return_counts)
    273         return _unpack_tuple(ret)
    274 

~/miniconda3/lib/python3.8/site-packages/numpy/lib/arraysetops.py in _unique1d(ar, return_index, return_inverse, return_counts)
    335     mask = np.empty(aux.shape, dtype=np.bool_)
    336     mask[:1] = True
--> 337     if aux.shape[0] > 0 and aux.dtype.kind in ""cfmM"" and np.isnan(aux[-1]):
    338         if aux.dtype.kind == ""c"":  # for complex all NaNs are considered equivalent
    339             aux_firstnan = np.searchsorted(np.isnan(aux), True, side='left')

ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
```

### NumPy/Python version information:

<details><summary>output of 'conda info'</summary>

```text

     active environment : None
       user config file : /Users/jlamb/.condarc
 populated config files : /Users/jlamb/.condarc
          conda version : 4.10.1
    conda-build version : not installed
         python version : 3.8.8.final.0
       virtual packages : __osx=10.14.6=0
                          __unix=0=0
                          __archspec=1=x86_64
       base environment : /Users/jlamb/miniconda3  (writable)
      conda av data dir : /Users/jlamb/miniconda3/etc/conda
  conda av metadata url : https://repo.anaconda.com/pkgs/main
           channel URLs : https://conda.anaconda.org/conda-forge/osx-64
                          https://conda.anaconda.org/conda-forge/noarch
                          https://repo.anaconda.com/pkgs/main/osx-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/r/osx-64
                          https://repo.anaconda.com/pkgs/r/noarch
          package cache : /Users/jlamb/miniconda3/pkgs
                          /Users/jlamb/.conda/pkgs
       envs directories : /Users/jlamb/miniconda3/envs
                          /Users/jlamb/.conda/envs
               platform : osx-64
             user-agent : conda/4.10.1 requests/2.25.1 CPython/3.8.8 Darwin/18.7.0 OSX/10.14.6
                UID:GID : 501:20
             netrc file : None
           offline mode : False
```

</details>",2021-07-05 04:27:31,,error using numpy.unique() on a numpy.matrix as of v1.21.0,"['00 - Bug', 'component: numpy.lib']"
19398,open,seberg,"The functions `isless`, and friends (see https://en.cppreference.com/w/c/numeric/math/isless) are C99 standardized and should be equivalent to the normal math operators but *not* set floating point exceptions.  I expect that using these macros would remove almost all of the places where we currently set `npy_clear_floatstatus_barrier()`.

That would both slightly speed things up (at least if errors did occur `npy_clear_floatstatus_barrier` is pretty slow), and just be slightly more correct not to worry set the flags in the first place.

In the long-run, I am also thinking of chaining (e.g. by numexpr), where a call to `npy_clear_floatstatus_barrier` could silence floating point warnings, if the ufuncs are chained in lowlevel C.

EDIT: https://www.gnu.org/software/libc/manual/html_node/FP-Comparison-Functions.html is probably a nice reference",2021-07-02 21:07:56,,"MAINT,ENH: Use non-error functions `isless`, `isgreater`, etc.","['01 - Enhancement', '17 - Task', '03 - Maintenance']"
19397,open,Stardust1225,"<!-- Please describe the issue in detail here, and fill in the fields below -->
This code is generated by a fuzzing testing tool.
If there exists key 'name' in the input dict, the first element of the corresponding value would have an increasing ref count after the function call. 
From the code, the increase of the ref count has a relationship with the times of calls.
 
### Reproducing code example:
```
import sys
import numpy

arg_list = []
arg_list.append('s')
arg_list.append('off')
arg_list.append('formats')
arg_list.append('i4')
arg_list.append('names')
arg_list.append(-33.70417143346674)
arg_list.append(-57)
arg_list.append(-6.637570701610372)
arg_list.append('drsspunf')
arg_list.append(None)
arg_list.append('sruraabpgf')
arg_list.append(4)
arg_list.append('[[u2rv2t')
arg_list.append(-82.49127767912898)
arg_list.append(True)
arg_list.append((6-99j))
arg_list.append('aaasgei[3rdbfgff1tnaufffoafng')
arg_list.append(True)
arg_list.append('wdnibeap')
arg_list.append(77)
arg_list.append(None)
arg_list.append(-65.26669886064585)
arg_list.append(33.558479525072926)
arg_list.append(None)
arg_list.append(None)
arg_list.append(-80.39813587395895)
arg_list.append(28.8149049806822)
arg_list.append(False)
arg_list.append(-38.529012926417415)
arg_list.append(True)

# check for refcount
arg_bef_10 = [sys.getrefcount(i) for i in arg_list if i != None and i != True and i != False]
for i in range(0, 10):
    try:
        numpy.dtype({arg_list[0] : [arg_list[1]], arg_list[2] : [arg_list[3]], arg_list[4] : [arg_list[5]], arg_list[6] : arg_list[7], arg_list[8] : arg_list[9], arg_list[10] : arg_list[11], arg_list[12] : arg_list[13], arg_list[14] : arg_list[15], arg_list[16] : arg_list[17], arg_list[18] : arg_list[19], arg_list[20] : arg_list[21], arg_list[22] : arg_list[23], arg_list[24] : arg_list[25], arg_list[26] : arg_list[27], arg_list[28] : arg_list[29]})
    except: pass
arg_aft_10 = [sys.getrefcount(i) for i in arg_list if i != None and i != True and i != False]

arg_bef_20 = [sys.getrefcount(i) for i in arg_list if i != None and i != True and i != False]
for i in range(0, 40):
    try:
        numpy.dtype({arg_list[0] : [arg_list[1]], arg_list[2] : [arg_list[3]], arg_list[4] : [arg_list[5]], arg_list[6] : arg_list[7], arg_list[8] : arg_list[9], arg_list[10] : arg_list[11], arg_list[12] : arg_list[13], arg_list[14] : arg_list[15], arg_list[16] : arg_list[17], arg_list[18] : arg_list[19], arg_list[20] : arg_list[21], arg_list[22] : arg_list[23], arg_list[24] : arg_list[25], arg_list[26] : arg_list[27], arg_list[28] : arg_list[29]})
    except: pass
arg_aft_20 = [sys.getrefcount(i) for i in arg_list if i != None and i != True and i != False]


print(arg_bef_10)
print(arg_aft_10)
print(arg_bef_20)
print(arg_aft_20)
```

### Error message:
```
[124, 20, 29, 9, 130, 5, 5, 5, 5, 5, 202, 5, 5, 5, 5, 5, 16, 5, 5, 5, 5, 5]
[124, 20, 29, 9, 130, 15, 5, 5, 5, 5, 202, 5, 5, 5, 5, 5, 16, 5, 5, 5, 5, 5]
[124, 20, 29, 9, 130, 15, 5, 5, 5, 5, 202, 5, 5, 5, 5, 5, 16, 5, 5, 5, 5, 5]
[124, 20, 29, 9, 130, 55, 5, 5, 5, 5, 202, 5, 5, 5, 5, 5, 16, 5, 5, 5, 5, 5]
```

### NumPy/Python version information:
NumPy 1.21.0
",2021-07-02 16:44:56,,Reference count error in function numpy.dtype,['unlabeled']
19382,open,mocquin,"This post follows [this one](https://github.com/numpy/numpy/issues/18902). I figured I should open another issue for `np.random.normal`, but this applies for all distributions I guess.

## Feature

Basicaly, I would like that `numpy.random.*` distributions could trigger an interface when passed instances from custom classes, ""à la"" `__array_ufunc__` or `__array_function__`. Here is an example concept : 

```python
arr = np.arange(10)

# Reference result 
print(np.mean(arr))
print(np.random.normal(arr))

custom_obj = MyArrayLike(arr)
print(np.mean(custom_obj))           # OK : np.mean will trigger __array_function__ interface
print(np.random.normal(custom_obj))  # KO : np.random.normal will ""only"" try to cast the object to float
```

And here is a MWE : 
```python
import numpy as np
np.random.seed(1234)

HANDLED_FUNCTIONS = {}

class NumericalLabeled():
    def __init__(self, value, label=""""):
        self.value = value
        self.label = label
        
    def __repr__(self):
        return ""NumericalLabelled<""+str(self.value) + "","" + self.label+"">""
    
    def __array_function__(self, func, types, args, kwargs):
        if func not in HANDLED_FUNCTIONS:
            return NotImplemented
        return HANDLED_FUNCTIONS[func](*args, **kwargs)
    

def make_numericallabelled(x, label=""""):
    """"""
    Helper function to cast anything into a NumericalLabelled object.
    """"""
    if isinstance(x, NumericalLabeled):
        return x
    else:
        return NumericalLabeled(x, label=label)
    
# Numpy functions            
# Override functions - used with __array_function__
def implements(np_function):
    def decorator(func):
        HANDLED_FUNCTIONS[np_function] = func
        return func
    return decorator    
    

@implements(np.random.normal)
def np_random_normal(loc=0.0, scale=1.0, **kwargs):
    # cast both loc and scale into Numericallabelled
    loc = make_numericallabelled(loc)
    scale = make_numericallabelled(scale)
    # check their label is ""compatible""
    if not loc.label == scale.label:
        raise ValueError
    return NumericalLabeled(np.random.rand(loc=loc.value,
                                           scale=scale.value, **kwargs), 
                            loc.label+scale.label)

@implements(np.mean)
def np_mean(a, *args, **kwargs):
    return NumericalLabeled(np.mean(a.value, *args, **kwargs),
                            a.label)



def main():
    # reference result for standard array
    arr = np.arange(10)
    print(np.mean(arr))
    print(np.random.normal(arr))
    
    # array-like object
    num_labeled = NumericalLabeled(arr, ""toto"")
    print(np.mean(num_labeled))
    try:
        print(np.random.normal(num_labeled))
    except Exception as e:
        print(e)

main()
```
which results in 
```
4.5
[ 0.47143516 -0.19097569  3.43270697  2.6873481   3.27941127  5.88716294
  6.85958841  6.3634765   8.01569637  6.75731505]
NumericalLabelled<4.5,toto> 
float() argument must be a string or a number, not 'NumericalLabeled'
```

Since the distribution functions accept array as input, I would expect them to be wrappable like many other array functions.

### Versions
```
Python : 3.8.5 (default, Sep  4 2020, 02:22:02) 
[Clang 10.0.0 ]
Numpy : 1.21.0
```",2021-06-30 19:27:26,,"Interface for wrapping random distribution functions (__array_function__, __ufunc__, ?)",['unlabeled']
19308,open,Snape3058,"### Reproducing code example:

Static analysis results, no POC.
This static analysis report has been manually reviewed to verify its validity.

### Error message:

The path provided by the static analyzer is as follows.

1. Create module, a new reference is returned from `PyModule_Create` and pointed to by `m`
https://github.com/numpy/numpy/blob/04ab04d93d4d7a4d241fe0ceb725436a8b6c8c2e/numpy/core/src/multiarray/_multiarray_tests.c.src#L2485

2. In macro import_array, the function returns `NULL` directly without decreasing the reference count.
https://github.com/numpy/numpy/blob/04ab04d93d4d7a4d241fe0ceb725436a8b6c8c2e/numpy/core/src/multiarray/_multiarray_tests.c.src#L2489

* The macro defined as:
https://github.com/numpy/numpy/blob/04ab04d93d4d7a4d241fe0ceb725436a8b6c8c2e/numpy/core/code_generators/generate_numpy_api.py#L112

### NumPy/Python version information:

Static analysis carried out on commit 04ab04d.

BTW, I do not know whether you care about such errors in test code. If not, I will not submit the bugs reported in the test code again.",2021-06-23 03:44:16,,Protential memory leak in function PyInit__multiarray_tests (a static analysis report),['unlabeled']
19299,open,charris,"* The parameter type following the `:` is put in  brackets `[...]` with the description and are hard to make out.
* There are unresolved references such as   \`(b-a)\` that are not references.
* Internal references don't go anywhere.
* There is an unrecognized character somewhere in the documentation, maybe generated.
* There seems to be a filename mixup between `index.ind` and `index.idx`.

Some of these problems are likely due to sphinx (4.0.1). The problems show up when running `make dist`.
 ",2021-06-22 16:58:47,,DOC: The PDF documentation does not look good.,['04 - Documentation']
19267,open,shaunc,"<!-- Please describe the issue in detail here, and fill in the fields below -->

`np.allclose` fails when comparing equal arrays with dtype `datetime64[ns]`

### Reproducing code example:
```python
import numpy as np
a = np.zeros(1, dtype='datetime64[ns]')
np.allclose(a, a)
```
fails with:

### Error message:

```
*** TypeError: The DType <class 'numpy._FloatAbstractDType'> could not be promoted by <class 'numpy.dtype[datetime64]'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtype[datetime64]'>, <class 'numpy._FloatAbstractDType'>)
```


### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

1.21.0rc2 3.9.4 | packaged by conda-forge | (default, May 10 2021, 22:13:33) 
[GCC 9.3.0]
",2021-06-17 17:07:59,,np.allclose fails with dtype datetime64,['unlabeled']
19252,open,BvB93,"Back in https://github.com/numpy/numpy/pull/17719 the first steps were taken into introducing static typing support for array dtypes. 

Since the dtype has a substantial effect on the semantics of an array, there is a lot of type-safety 
to be gained if the various function-annotations in numpy can actually utilize this information.
Examples of this would be the rejection of string-arrays for arithmetic operations, or inferring the 
output dtype of mixed float/integer operations.

The Plan
--------

With this in mind I'd ideally like to implement some basic dtype support throughout the main numpy 
namespace (xref https://github.com/numpy/numpy/issues/16546) before the release of 1.22. 

Now, what does ""basic"" mean in this context? Namely, any array-/dtype-like that can be parametrized 
w.r.t. `np.generic`. Notably this excludes builtin scalar types and character codes (literal strings), as the 
only way of implementing the latter two is via excessive use of overloads. 

With this in mind, I realistically only expect dtype-support for builtin scalar types (_e.g._ `func(..., dtype=float)`) 
to-be added with the help of a mypy plugin, _e.g._ via injecting a type-check-only method into the likes of 
`builtins.int` that holds some sort of explicit reference to `np.int_`.

Examples
---------
Two examples wherein the dtype can be automatically inferred:
``` python
from typing import TYPE_CHECKING
import numpy as np

AR_1 = np.array(np.float64(1))
AR_2 = np.array(1, dtype=np.float64)

if TYPE_CHECKING:
    reveal_type(AR_1)  # note: Revealed type is ""numpy.ndarray[Any, numpy.dtype[numpy.floating*[numpy.typing._64Bit*]]]""
    reveal_type(AR_2)  # note: Revealed type is ""numpy.ndarray[Any, numpy.dtype[numpy.floating*[numpy.typing._64Bit*]]]""
```

Three examples wherein dtype-support is substantially more difficult to implement.
``` python
AR_3 = np.array(1.0)
AR_4 = np.array(1, dtype=float)
AR_5 = np.array(1, dtype=""f8"")

if TYPE_CHECKING:
    reveal_type(AR_3)  # note: Revealed type is ""numpy.ndarray[Any, numpy.dtype[Any]]""
    reveal_type(AR_4)  # note: Revealed type is ""numpy.ndarray[Any, numpy.dtype[Any]]""
    reveal_type(AR_5)  # note: Revealed type is ""numpy.ndarray[Any, numpy.dtype[Any]]""
```

In the latter three cases one can always manually declare the dtype of the array:
``` python
import numpy.typing as npt

AR_6: npt.NDArray[np.float64] = np.array(1.0)

if TYPE_CHECKING:
    reveal_type(AR_6)  # note: Revealed type is ""numpy.ndarray[Any, numpy.dtype[numpy.floating*[numpy.typing._64Bit*]]]""
```",2021-06-15 16:33:08,,Add basic dtype typing support to the main numpy namespace,"['01 - Enhancement', 'Static typing']"
19247,open,mataney,"When trying to use `np.load` with smart_open and mmap_mode I'm getting an `os.fspath` TypeError.

```python
>>> import numpy as np
>>> from smart_open import open as sopen
>>> arr = np.array([1,2,3])
>>> fakefile = 'gs://PATHTOFILE'
>>> np.save(sopen(fakefile, 'wb'), arr)
>>> np.load(sopen(fakefile, 'rb'))
array([1, 2, 3])
>>> np.load(sopen(fakefile, 'rb'), mmap_mode='r')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../lib/python3.7/site-packages/numpy/lib/npyio.py"", line 438, in load
    return format.open_memmap(file, mode=mmap_mode)
  File "".../lib/python3.7/site-packages/numpy/lib/format.py"", line 863, in open_memmap
    with open(os_fspath(filename), 'rb') as fp:
TypeError: expected str, bytes or os.PathLike object, not Reader
```

Versions:
python=3.7.3
numpy=1.20.2
smart_open=3.0.0

Thanks!",2021-06-15 07:28:06,,Can't load using smart_open with mmap_mode,['unlabeled']
19243,open,drhpc,"This is a general issue not related to any particular Python code. NumPy links to CBLAS for algebra support. The actual implementation can be the reference from Netlib LAPACK or an optimized library. The NumPy build does not use the cblas.h provided with the library but its own variant npy_cblas.h (with npy_cblas_base.h).

Problem: There are differences between the vendors that are ignored there. One that really affects the API is this
```
#define CBLAS_INDEX size_t  /* this may vary between platforms */
CBLAS_INDEX BLASNAME(cblas_isamax)(const BLASINT N, const float  *X, const BLASINT incX);
CBLAS_INDEX BLASNAME(cblas_idamax)(const BLASINT N, const double *X, const BLASINT incX);
CBLAS_INDEX BLASNAME(cblas_icamax)(const BLASINT N, const void   *X, const BLASINT incX);
CBLAS_INDEX BLASNAME(cblas_izamax)(const BLASINT N, const void   *X, const BLASINT incX);
```
This may vary, as the comment says. Current Netlib does this:
```
#ifdef WeirdNEC
   #define CBLAS_INDEX long
#else
   #define CBLAS_INDEX int
#endif
CBLAS_INDEX cblas_isamax(const CBLAS_INDEX N, const float  *X, const CBLAS_INDEX incX);
CBLAS_INDEX cblas_idamax(const CBLAS_INDEX N, const double *X, const CBLAS_INDEX incX);
CBLAS_INDEX cblas_icamax(const CBLAS_INDEX N, const void   *X, const CBLAS_INDEX incX);
CBLAS_INDEX cblas_izamax(const CBLAS_INDEX N, const void   *X, const CBLAS_INDEX incX);
```
It uses CBLAS_INDEX as index type throughout. No special size_t case as return value.

There is more discussion about this in https://github.com/Reference-LAPACK/lapack/issues/461. You might comment there if you want to yell at them for changing the return value of these functions 5 years ago. Maybe the resolution is to turn the return value into size_t again. In any case, just using the cblas.h the library provides would be best, IMHO.

Would you consider just using the cblas.h or (mkl_cblas.h) the system provides (having CPPFLAGS set up to locate them, or a separate NPY_BLAS_INCLUDES environment variable that mirrors NPY_BLAS_LIBS) so that there are no such mismatch?

How relevant is this statement today?
```
/*
 * This header provides numpy a consistent interface to CBLAS code. It is needed
 * because not all providers of cblas provide cblas.h. For instance, MKL provides
 * mkl_cblas.h and also typedefs the CBLAS_XXX enums.
 */
```
Is there a specific issue with typedef'd enums? You could arrange for the name of mkl_cblas.h … just having your npy_cblas.h as redirector. In my case I will provide a cblas.h wrapper over mkl_cblas.h in the system.

As a simpler workaround: I do not see NumPy ever using `cblas_isamax()` and friends. Maybe you could drop them from the header to avoid possible conflict.",2021-06-14 07:45:50,,npy_cblas.h does not match libcblas (size_t vs int or long for CBLAS_INDEX in Netlib),"['component: numpy._core', 'component: build']"
19205,open,tsuga,"What is the proper way to compare np.nan when dtype is object?
I tried the following two approaches, but both failed

<!-- Please describe the issue in detail here, and fill in the fields below -->
# 1. np.array_equal 
### Reproducing code example:
```python
import numpy as np
np.array_equal(np.array([np.nan], dtype=object), np.array([np.nan], dtype=object), equal_nan=True)
```

### Error message:
```
Traceback (most recent call last):
  File ""c:\Users\xxxx\xxxxx\.venv\lib\site-packages\numpy\core\numeric.py"", line 2455, in array_equal
    a1nan, a2nan = isnan(a1), isnan(a2)
TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
```

# 2. np.testing.assert_array_equal
### Reproducing code example:
```python
import numpy as np
np.testing.assert_array_equal(np.array([np.nan], dtype=object), np.array([np.nan], dtype=object))
```

### Error message:
```
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""c:\Users\xxxx\xxxxx\.venv\lib\site-packages\numpy\testing\_private\utils.py"", line 932, in assert_array_equal
    assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
  File ""c:\Users\xxxx\xxxxx\.venv\lib\site-packages\numpy\testing\_private\utils.py"", line 842, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Arrays are not equal

Mismatched elements: 1 / 1 (100%)
Max absolute difference: nan
Max relative difference: nan
 x: array([nan], dtype=object)
 y: array([nan], dtype=object)
```



### NumPy/Python version information:
1.20.3 3.9.5 (tags/v3.9.5:0a7dcbd, May  3 2021, 17:27:52) [MSC v.1928 64 bit (AMD64)]
<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2021-06-09 12:36:11,,array_equal or testing.assert_array_equal does not compare NaNs as numbers when dtype is object,['33 - Question']
19183,open,rggjan,"Running this code:

`np.testing.assert_allclose(np.array([0], dtype=np.uint8), np.array([1], dtype=np.uint8))`

will output:

```
AssertionError:
Not equal to tolerance rtol=1e-07, atol=0

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 255
Max relative difference: 255.
 x: array([0], dtype=uint8)
 y: array([1], dtype=uint8)
```

even though clearly the absolute difference is only 1 instead of 255.

There seems to be an issue with calculating the absolute difference for unsigned types (wrap-around).

Changing the order works fine:

`np.testing.assert_allclose(np.array([1], dtype=np.uint8), np.array([0], dtype=np.uint8))`

```
AssertionError:
Not equal to tolerance rtol=1e-07, atol=0

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 1
Max relative difference: inf
 x: array([1], dtype=uint8)
 y: array([0], dtype=uint8)
```

```python
import numpy as np
<< your code here >>
```

### NumPy/Python version information:

1.18.1 3.7.10 | packaged by conda-forge | (default, Mar 23 2021, 09:14:10)
[Clang 11.1.0 ]

",2021-06-07 12:58:21,,np.testing.allclose doesn't handle uint8 datatype properly,['unlabeled']
19162,open,2sn,"## f2py interface for external modules and module data

# Background
Recently code I used was changed from using common blocks and include files to f90 code where this information now resided in a module.  With the previous setup, I could just include the include file in a module that then was compiled with f2py.

First a naive question, in case there is a clever solution: is is possible to access variables from an external module not compiled with f2py?

Second, it is possible to create just an interface module for an external fortran module, e.g., already in a library (.a or .so file) to access variables in it, or common blocks?  (it seems to work at link to access common blocks for a pre-compiled library) ",2021-06-03 14:30:11,,f2py - interface for external modules and module data,['component: numpy.f2py']
19157,open,2sn,"## allocatable arrays as function parameters

Some subroutines may need to return variable size arrays based on the properties of the result, in my case a dimension that comes from another module (`jmz`, in my example).   But it seems the current wrapper mechanism in f2py is not really set up for this.  

Or, it may very well be that I am missing something, and workaround suggestion would be very much appreciated.

Irrespective, please allow me to provide my example; it may have a small amount of extra slack, but not too much I hope.
My apologies for that.

```fortran90
subroutine getentropies_(datbuf, jmin, jmax)
  use griddef, only: jmz
  use typedef, only: int32, real64
  implicit none
  save
  integer(kind=int32), intent(in) :: jmin, jmax
  real(kind=real64), intent(out), dimension(:,:), allocatable :: datbuf
  if (.not.allocated(datbuf)) allocate(datbuf(0:jmz,0:5))
  call getentropies(datbuf, jmin, jmax)
end subroutine getentropies_
```
when wrapped it creates the wrapper
```fortran90
!     -*- f90 -*-
!     This file is autogenerated with f2py (version:1.20.3)
!     It contains Fortran 90 wrappers to fortran functions.

      subroutine f2pywrapgetentropies_ (datbuf, jmin, jmax, f2py_datbuf_&
     &d0, f2py_datbuf_d1)
                use griddef, only: jmz
                use typedef, only: int32,real64
      integer(kind=int32) jmin
      integer(kind=int32) jmax
      integer f2py_datbuf_d0
      integer f2py_datbuf_d1
      real(kind=real64) datbuf(f2py_datbuf_d0,f2py_datbuf_d1)
      interface
      
            subroutine getentropies_(datbuf,jmin,jmax) 
                use griddef, only: jmz
                use typedef, only: int32,real64
                real(kind=real64), allocatable,dimension(:,:),intent(out&
     &) :: datbuf
                integer(kind=int32), intent(in) :: jmin
                integer(kind=int32), intent(in) :: jmax
            end subroutine getentropies_
      end interface
      call getentropies_(datbuf, jmin, jmax)
      end
```
resulting in the error message
```
   25 |       call getentropies_(datbuf, jmin, jmax)
      |                         1
Error: Actual argument for ‘datbuf’ must be ALLOCATABLE at (1)
```
(I use a custom `f2py` type map for `int32` and `real64`)
",2021-06-03 07:04:48,,f2py - allocatable arrays as function parameters,['component: numpy.f2py']
19148,open,yosoufe,"<!-- Please describe the issue in detail here, and fill in the fields below -->
After the following calculation with MaskedArray, the shape of the `mask` diverges from the shape of the `data`. While the shape of `data` is correct.

### Reproducing code example:
```python
import numpy as np
import numpy.ma as ma

n_particles = 5
max_n_features = 4
feature_mask = np.random.randint(0,2,size=(max_n_features, n_particles)).astype(bool)

mask_state = feature_mask.reshape((max_n_features, n_particles, 1, 1))
mask_state = np.repeat(mask_state, repeats=3, axis=2) # (4,5,3,1)
mask_covariance = np.repeat(mask_state, repeats=3, axis=3) # (4,5,3,3)

Q = ma.MaskedArray(np.random.uniform(0,1,size=(max_n_features, n_particles,3,3)), mask = mask_covariance)

surprise = ma.MaskedArray(np.random.uniform(0,1,size=(max_n_features, n_particles,3,1)), mask = mask_state)
exp_term = np.exp(-0.5 * np.transpose(surprise, axes=(0, 1, 3, 2)) @ np.linalg.inv(Q) @ surprise)

print(exp_term.data.shape, exp_term.mask.shape) # prints: (4, 5, 1, 1) (4, 5, 3, 3)
print(exp_term.data.shape == exp_term.mask.shape) # prints: False which seems to be wrong.
```

### Error message:
No exception or error message is raised. but the last line `print(exp_term.data.shape == exp_term.mask.shape)`
prints `False`. In this calculation the shape of `exp_term.data` is the correct one. and `exp_term.mask` has wrong shape.

When the shapes are different, it does not allow to reshape or squeeze or even print the MaskedArray.

`exp_term.data.shape` is `(4,5,1,1)`  while `exp_term.mask.shape` is `(4,5,3,3)`. but nice thing about mask is that for every `i` and `j` the `exp_term.mask[i, j, :,:]` are either all True or all False. Or in other words, the following is true 
```py
np.all(   np.any(exp_term.mask,axis=(2,3)) == np.all(exp_term.mask,axis=(2,3))   )
```

So there is a workaround:
```python
new_mask = np.expand_dims(np.all(exp_term.mask,axis=(2,3)),axis=(2,3))
exp_term = ma.MaskedArray(exp_term.data, mask = new_mask) 
```

### NumPy/Python version information:

```bash
numpy version: 1.20.3 

python version: 3.8.5 (default, Feb 16 2021, 00:08:37) 
[GCC 9.3.0]
```

",2021-06-01 05:24:49,,[MaskedArray] mask.shape diverges from data.shape,['component: numpy.ma']
19146,open,jbrockmendel,"I expected to get uint64

```
umax = np.iinfo(np.uint64).max

np.array([umax]).dtype   # <-- uint64, as expected

np.array([0, umax]).dtype  # <-- float64, surprising
```

There seems to be something special going on inference-wise around the int64 bound:

```
imax = np.iinfo(np.int64).max

np.array([imax, umax]).dtype  # <-- float64

np.array([imax+1, umax]).dtype  # <-- uint64
```",2021-05-31 21:47:14,,"BUG/API: np.array([0, max_uint64]) has float64 dtype",['unlabeled']
19145,open,Ark-kun,"<!-- Please describe the issue in detail here, and fill in the fields below -->
Related to https://github.com/vscode-restructuredtext/snooty-parser/issues/24 and https://github.com/networkx/networkx/issues/4857

![image](https://user-images.githubusercontent.com/1829149/120243452-77d12480-c21c-11eb-82ab-a8d7f160e4b9.png)

I expect `import numpy` to commit close to 0 memory.

### Reproducing code example:

```python
import numpy as np
```

### Error message:
```
  File ""C:\Users\Ark\AppData\Local\Programs\Python\Python37\lib\site-packages\numpy\__init__.py"", line 153, in <module>
    from . import ma
  File ""C:\Users\Ark\AppData\Local\Programs\Python\Python37\lib\site-packages\numpy\ma\__init__.py"", line 42, in <module>
    from . import core
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 724, in exec_module
  File ""<frozen importlib._bootstrap_external>"", line 818, in get_code
  File ""<frozen importlib._bootstrap_external>"", line 917, in get_data
MemoryError
```

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/main/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### NumPy/Python version information:

1.19.5 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)]

",2021-05-31 21:32:58,,Just importing numpy makes python process commit 3+GB of memory,"['04 - Documentation', 'component: numpy.linalg']"
19129,open,melissawm,"It would be nice to create a document with a general description of how NumPy treats the GIL.

See #5897 for context.",2021-05-28 17:09:13,,DOC: How does NumPy interact with the GIL?,['04 - Documentation']
19113,open,danielcrane,"When casting a ragged list of `np.ndarray` to an array via `np.array(..., dtype=object)`, if the first dimension of each of the arrays in the list is the same size, a `ValueError` is raised.

When using `np.array(..., dtype=object)` on a ragged list of `np.ndarray` where the first dimension of each of the lists is not equal, the code works as expected and produces an `np.ndarray` of `.shape` `(n,)` where `n` the length of the original list.

### Reproducing code example:

```python
import numpy as np
x = [np.empty((10, 5)), np.empty((9,8))]
print(np.array(x, dtype=object).shape) # This outputs (2,) as expected

x = [np.empty((10, 5)), np.empty((10,8))]
print(np.array(x, dtype=object).shape) # This raises a ValueError
```

### Error message:

```python
      4 x = [np.empty((10, 5)), np.empty((10,8))]
----> 5 print(np.array(x, dtype=object).shape)

ValueError: could not broadcast input array from shape (10,5) into shape (10,)

```

### NumPy/Python version information:

1.20.2 3.8.5 (default, Sep  4 2020, 07:30:14) 
[GCC 7.3.0]

",2021-05-27 02:21:09,,Error when casting ragged list to an array,['unlabeled']
19097,open,langou,"Hi all, 

I am trying the code below where I am computing Frobenius norms of 2x2 matrix and 2-norm of 4x1 vector. 

```python
import numpy

print(numpy.version.version)

# note that (1e150)*(1e150) = 1e300 = does not overflow
# note that (1e200)*(1e200) = 1e400 = overflows = infinity

# answer should be 2e150, answer is 2e150 => good
print(numpy.linalg.norm(numpy.array([[1e150, 1e150],[1e150, 1e150]]),'fro'))

# answer should be 2e200, answer is Infinity => unnecessary overflow
print(numpy.linalg.norm(numpy.array([[1e200, 1e200],[1e200, 1e200]]),'fro'))

# note that (1e-150)*(1e-150) = 1e-300 = does not underflow
# note that (1e-200)*(1e-200) = 1e-400 = underflows = 0.0e+00

# answer should be 2e-150, answer is 2e-150 => good
print(numpy.linalg.norm(numpy.array([[1e-150, 1e-150],[1e-150, 1e-150]]),'fro'))

# answer should be 2e-200, answer is 0.0e+00 => unnecessary underflow
print(numpy.linalg.norm(numpy.array([[1e-200, 1e-200],[1e-200, 1e-200]]),'fro'))
```

The result that I get is
```
1.19.5
2e+150
inf
2e-150
0.0
```
The result that one would expect is
```
x.xx.x
2e+150
2e+200
2e-150
2e-200
```

So numpy.linalg.norm( matrix, Frobenius norm ) causes unnecessary overflow and underflow. (Typically (1e-200) is squared and mapped to zero. And (1e+200) is squared and mapped to Infinity.)

I think 🤔 the code is at: https://github.com/numpy/numpy/blob/ecdba3a140126174a6a4d86fe0fbf2ce2ad680f9/numpy/linalg/linalg.py#L2514
and so one can see that the Frobenius normed is computed by squaring the entries
```python
sqnorm = dot(x, x)            
ret = sqrt(sqnorm)
```
 without scaling.

LAPACK xLANGE computes the Frobenius norm of these matrices without unnecessary overflow and underflow. BLAS xNRM2 computes the 2-norm of a vector without unnecessary overflow and underflow.

So if the input is made of valid floating-point numbers, and if the output is a valid floating-point number, the goal would be to compute the output from the input without overflow/underflow.

I see a few solutions. One solution is to call LAPACK xLANGE. One solution is to write a more complicated algorithm (similar-ish to LAPACK) in python (I assume that we want to use ""dot"" if possible since ""dot"" is probably faster than a loop on scalars). One solution is to do nothing about it. 

Doing nothing is not necessarily bad. Arguably it will be hard to go faster than the current code and the current code returns a correct results for a wide range of input data, so this might be a design choice.

I am not sure I have the bandwidth to propose a PR. I do not know whether the community is aware of this issue, and whether the community considers this an issue. 

Comment: I also see: 
```python
                sqnorm = dot(x.real, x.real) + dot(x.imag, x.imag)
```
And it surprises me that there is not dot for complex vectors. I am not sure how complex vectors are stored in numpy. Real as one vector, and imag as another vector, in which the code above makes a lots of sense. Or a vector of (real,imag), in which case the code above does two passes on the vector, and it might be more efficient (and simpler for user) to have a complex dot.


",2021-05-25 21:14:50,,Frobenius norm of matrix is overflowing,['unlabeled']
19057,open,baist,"<!-- Please describe the issue in detail here, and fill in the fields below -->

The random and argsort operation in AMD cpu is so slow that a random initialization of 10000*10000 matrix needs 30m !!!
For comparison, the same initialization in  Intel CPU needs about 5s. 

When using openblas for acceleration, the time cost is much lower, but still 3x than Intel CPU (about 17s). And the code often stucks in somewhere when performing random initialization or argsort.

CPU: 128  AMD EPYC 7502 32-Core Processor 
Numpy: 1.20.3 


### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy.random as npr
import numpy as np
import time


def print_time(string=''): 
    res = time.strftime(""%Y-%m-%d-%H:%M:%S"", time.localtime())
    res = res + ' | ' + string
    print(res)

np.random.seed(1)
np.__config__.show()

# --- Test 1
N = 1
n = 1000

A = npr.randn(n,n)
B = npr.randn(n,n)

t = time.time()
for i in range(N):
    C = np.dot(A, B)
td = time.time() - t
print_time(""dotted two (%d,%d) matrices in %0.1f ms"" % (n, n, 1e3*td/N))

# --- Test 2
N = 100
n = 4000

A = npr.randn(n)
B = npr.randn(n)

t = time.time()
for i in range(N):
    C = np.dot(A, B)
td = time.time() - t
print_time(""dotted two (%d) vectors in %0.2f us"" % (n, 1e6*td/N))

# --- Test 3
m,n = (2000,1000)

A = npr.randn(m,n)

t = time.time()
[U,s,V] = np.linalg.svd(A, full_matrices=False)
td = time.time() - t
print_time(""SVD of (%d,%d) matrix in %0.3f s"" % (m, n, td))

# --- Test 4
n = 1500
A = npr.randn(n,n)

t = time.time()
w, v = np.linalg.eig(A)
td = time.time() - t
print_time(""Eigendecomp of (%d,%d) matrix in %0.3f s"" % (n, n, td))




for i in range(10):
    print_time(str(i))
    # --- Test 5
    n = 10000
    t = time.time()
    A = npr.randn(n, n)
    td = time.time() - t
    print_time(""Random Initialize (%d,%d) matrix in %0.3f s"" % (n, n, td))


    # --- Test 6
    t = time.time()
    B = np.argsort(A, axis=-1)
    td = time.time() - t
    print_time(""ArgSort (%d,%d) matrix in %0.3f s"" % (n, n, td))

    print(B)
```

### Error message:

```
blas_info:
    libraries = ['cblas', 'blas', 'cblas', 'blas']
    library_dirs = ['/home/username/miniconda3/envs/py38/lib']
    include_dirs = ['/home/username/miniconda3/envs/py38/include']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
blas_opt_info:
    define_macros = [('NO_ATLAS_INFO', 1), ('HAVE_CBLAS', None)]
    libraries = ['cblas', 'blas', 'cblas', 'blas']
    library_dirs = ['/home/username/miniconda3/envs/py38/lib']
    include_dirs = ['/home/username/miniconda3/envs/py38/include']
    language = c
lapack_info:
    libraries = ['lapack', 'blas', 'lapack', 'blas']
    library_dirs = ['/home/username/miniconda3/envs/py38/lib']
    language = f77
lapack_opt_info:
    libraries = ['lapack', 'blas', 'lapack', 'blas', 'cblas', 'blas', 'cblas', 'blas']
    library_dirs = ['/home/username/miniconda3/envs/py38/lib']
    language = c
    define_macros = [('NO_ATLAS_INFO', 1), ('HAVE_CBLAS', None)]
    include_dirs = ['/home/username/miniconda3/envs/py38/include']
dotted two (1000,1000) matrices in 246.3 ms
dotted two (4000) vectors in 22.61 us
SVD of (2000,1000) matrix in 20.998 s
Eigendecomp of (1500,1500) matrix in 128.044 s
Random Initialize matrix (10000,10000) matrix in 1951.663 s
ArgSort matrix (10000,10000) matrix in 1148.212 s
```

#### OpenBLAS
``` 
blas_mkl_info:
  NOT AVAILABLE
blis_info:
  NOT AVAILABLE
openblas_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
blas_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
lapack_mkl_info:
  NOT AVAILABLE
openblas_lapack_info:
  NOT AVAILABLE
openblas_clapack_info:
  NOT AVAILABLE
flame_info:
  NOT AVAILABLE
accelerate_info:
  NOT AVAILABLE
atlas_3_10_threads_info:
  NOT AVAILABLE
atlas_3_10_info:
  NOT AVAILABLE
atlas_threads_info:
  NOT AVAILABLE
atlas_info:
  NOT AVAILABLE
lapack_info:
    libraries = ['openblas', 'lapack']
    library_dirs = ['/usr/local/lib']
    language = f77
lapack_opt_info:
    libraries = ['openblas', 'lapack', 'openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None), ('NO_ATLAS_INFO', 1)]
2021-05-21-15:22:23 | dotted two (1000,1000) matrices in 141.2 ms
2021-05-21-15:22:23 | dotted two (4000) vectors in 6.92 us
2021-05-21-15:22:24 | SVD of (2000,1000) matrix in 0.742 s
2021-05-21-15:22:27 | Eigendecomp of (1500,1500) matrix in 2.571 s
2021-05-21-15:22:27 | 0
2021-05-21-15:22:44 | Random Initialize (10000,10000) matrix in 17.062 s
2021-05-21-15:23:14 | ArgSort (10000,10000) matrix in 29.787 s
2021-05-21-15:23:14 | 1
2021-05-21-15:23:29 | Random Initialize (10000,10000) matrix in 15.164 s
2021-05-21-15:27:31 | ArgSort (10000,10000) matrix in 242.202 s
```

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/main/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### NumPy/Python version information:
Numpy: 1.20.3 
Python: 3.8
<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2021-05-21 07:29:55,,Slow Random Initialization on AMD CPU.,['unlabeled']
19039,open,bhavitvyamalik,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
b = (np.array([1, 2, 3]).astype(np.uint8))
c = (np.array([1, 2, 3]).astype(np.float32))
np.testing.assert_equal(b,c)  # True output
```
I think it would be great if we can check to see if they have same dtype or not.

### NumPy/Python version information:
1.19.1/ 3.6.8",2021-05-19 07:40:22,, numpy.testing.assert_equal does not test for dtype,['unlabeled']
19020,open,shashwatj07,"## Feature

linalg.det should return fraction object when called by passing a matrix of fractions.
<!-- If you're looking to request a new feature or change in functionality, including
adding or changing the meaning of arguments to an existing function, please
post your idea on the [numpy-discussion mailing list]
(https://mail.python.org/mailman/listinfo/numpy-discussion) to explain your
reasoning in addition to opening an issue or pull request. You can also check
out our [Contributor Guide]
(https://github.com/numpy/numpy/blob/main/doc/source/dev/index.rst) if you
need more information. -->
",2021-05-16 09:15:42,,linalg.det for fractions,['unlabeled']
18986,open,asmeurer,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

For some reason, `np.left_shift(np.array([0], dtype=np.int16), np.array([[31, 31, 31], [31, 31, 31], [31, 31, 31]], dtype=np.int16))` gives a floating point warning, but similar function calls do not. 

```python
>>> np.seterr(all='raise')
{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}
>>> np.left_shift(np.array([0], dtype=np.int8), np.array([31], dtype=np.int16))
array([0], dtype=int16)
>>> np.left_shift(np.array([0], dtype=np.int16), np.array([31], dtype=np.int16))
array([0], dtype=int16)
>>> np.left_shift(np.array([0], dtype=np.int16), np.array([[31, 31, 31]], dtype=np.int16))
array([[0, 0, 0]], dtype=int16)
>>> np.left_shift(np.array([0], dtype=np.int16), np.array([[31, 31, 31], [31, 31, 31]], dtype=np.int16))
array([[0, 0, 0],
       [0, 0, 0]], dtype=int16)
>>> np.left_shift(np.array([0], dtype=np.int16), np.array([[31, 31, 31], [31, 31, 31], [31, 31, 31]], dtype=np.int16))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
FloatingPointError: invalid value encountered in left_shift
>>> np.left_shift(np.array([0], dtype=np.int16), np.array([[30, 30, 30], [30, 30, 30], [30, 30, 30]], dtype=np.int16))
array([[0, 0, 0],
       [0, 0, 0],
       [0, 0, 0]], dtype=int16)
```


### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/main/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```
>>> np.__version__
'1.21.0.dev0+1549.g7c4b2482a'
>>> import sys
>>> sys.version
'3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 05:02:20) \n[Clang 11.0.1 ]'
```
",2021-05-10 23:17:04,,"""invalid value encountered in left_shift"" for broadcasted int16 array",['unlabeled']
18981,open,taldcroft,"<!-- Please describe the issue in detail here, and fill in the fields below -->

The `MaskedArray` class has unexpected special-casing of `ndarray` objects within an object array. I expect an object array to be a generic container of whatever is in each cell, and if that element is masked then accessing it should always return `np.ma.masked`. Instead `MaskedArray` appears to turn the `ndarray` into a fully-masked `MaskedArray`.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
>>> m = np.ma.array([None, [1, 2], np.array([1, 2])], dtype=object)
>>> m
masked_array(data=[None, list([1, 2]), array([1, 2])],
             mask=False,
       fill_value='?',
            dtype=object)
>>> m.mask = [True, True, True]
>>> m[0]
masked
>>> m[1]
masked
>>> m[2]  # Accessing the ndarray has auto-magically converted it to masked_array
masked_array(data=[--, --],
             mask=[ True,  True],
       fill_value=999999,
            dtype=int64)
>>> m.data[2]  # The data element under the mask is still the original ndarray
array([1, 2])
```

Expected:
```
>>> m[2]
masked
```

### Error message:
N/A

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/main/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```
1.19.5 3.7.9 (default, Aug 31 2020, 07:22:35) 
[Clang 10.0.0 ]
```
",2021-05-10 13:49:02,,Object-type masked array special-cases elements that are ndarray,"['00 - Bug', 'component: numpy.ma']"
18978,open,GBR-613,"<!-- Please describe the issue in detail here, and fill in the fields below -->
There are two almost equal arrays generated by the same pipeline in different systems. (All differences between respective items are about 1e-16 and apparently caused by differences in hardware.)  
I would expect result of linalg.slogdet() for them very similar, respectively.
However the result for one is about -49.04, and the result for another is -inf.
The npy files are attached.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np


a  = np.load(""a.npy"", allow_pickle=True)
s  = np.load(""s.npy"", allow_pickle=True)
_, det = np.linalg.slogdet(a)
print(det)
_, det = np.linalg.slogdet(s)
print(det)

# The section below illustrates how difference
# between these two arrays is small:
# nothing is printed
for i in range(a.shape[0]):
    for j in range(a.shape[1]):
        d = abs(a[i][j] - s[i][j])
        if d > 0.00000000000001:
            print(""%d, %d"" % (i, j))
            print(a[i][i])
            print(s[i][i])
            print(d)
```

### Error message:
The code below prints the following:
    -inf
    -49.041742650541245
Expected result is about -49.04 in both cases.
(FYI: this inconsistency leads to 20-30% difference in final results in our project on different systems.)
 
<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/main/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### NumPy/Python version information:
1.20.2 3.9.4 (default, May  4 2021, 16:03:03)
[GCC 9.3.0]

The same behaviour is with Python 3.7.10 on both Windows and Linux.
<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2021-05-10 09:15:45,,Inconsistent behavior of linalg.slogdet() ,['component: numpy.linalg']
18926,open,mattip,"## Documentation

Continuation of #18901

The [SIMD](https://numpy.org/devdocs/reference/simd/simd-optimizations.html) documentation could use an overhaul.

- It should be broken up into a number of documents. Each should be one of (based on the [Diataxis framework](https://diataxis.fr/))
  - a reference document (with all the detail about how dispatch works), 
  - a howto page (on how to use the various options in #18901 to control the baseline and dispatch options),  
  - an explanation of the background info about the high-level overview of the general idea behind the framework.
  - a tutorial on how to add new SIMD features or how to use them in ufunc loops
- The content could use an editorial pass to flow better
- The feature listing should be reformatted for easier reading (part of the reference documentation)
- The various pages should link to eachother where appropriate",2021-05-06 04:25:37,,DOC: Improve SIMD documentation,"['04 - Documentation', 'component: SIMD']"
18914,open,rgommers,"This is on a macOS system, but @mattip ran into it yesterday as well during a demo, and that was on Linux I believe. I cannot reproduce it on my macOS or Linux setups, nor on Gitpod. All those use `openblas` from conda-forge, the problem may be coming from another BLAS implementation or be hardware-specific.

```
=================================== FAILURES ===================================
______________________________ TestCond.test_nan _______________________________

self = <numpy.linalg.tests.test_linalg.TestCond object at 0x13d00e670>

    def test_nan(self):
        # nans should be passed through, not converted to infs
        ps = [None, 1, -1, 2, -2, 'fro']
        p_pos = [None, 1, 2, 'fro']
    
        A = np.ones((2, 2))
        A[0,1] = np.nan
        for p in ps:
>           c = linalg.cond(A, p)

A          = array([[ 1., nan],
       [ 1.,  1.]])
p          = None
p_pos      = [None, 1, 2, 'fro']
ps         = [None, 1, -1, 2, -2, 'fro']
self       = <numpy.linalg.tests.test_linalg.TestCond object at 0x13d00e670>

numpy/linalg/tests/test_linalg.py:777: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
<__array_function__ internals>:5: in cond
    ???
        args       = (array([[ 1., nan],
       [ 1.,  1.]]), None)
        kwargs     = {}
        relevant_args = (array([[ 1., nan],
       [ 1.,  1.]]),)
numpy/linalg/linalg.py:1765: in cond
    s = svd(x, compute_uv=False)
        p          = None
        x          = array([[ 1., nan],
       [ 1.,  1.]])
<__array_function__ internals>:5: in svd
    ???
        args       = (array([[ 1., nan],
       [ 1.,  1.]]),)
        kwargs     = {'compute_uv': False}
        relevant_args = (array([[ 1., nan],
       [ 1.,  1.]]),)
numpy/linalg/linalg.py:1672: in svd
    s = gufunc(a, signature=signature, extobj=extobj)
        _nx        = <module 'numpy' from '/Users/username/Documents/GitHub/numpy/build/testenv/lib/python3.9/site-packages/numpy/__init__.py'>
        a          = array([[ 1., nan],
       [ 1.,  1.]])
        compute_uv = False
        extobj     = [8192, 1536, <function _raise_linalgerror_svd_nonconvergence at 0x117e80670>]
        full_matrices = True
        gufunc     = <ufunc 'svd_n'>
        hermitian  = False
        m          = 2
        n          = 2
        result_t   = <class 'numpy.float64'>
        signature  = 'd->d'
        t          = <class 'numpy.float64'>
        wrap       = <built-in method __array_prepare__ of numpy.ndarray object at 0x13c341510>
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

err = 'invalid value', flag = 8

    def _raise_linalgerror_svd_nonconvergence(err, flag):
>       raise LinAlgError(""SVD did not converge"")
E       numpy.linalg.LinAlgError: SVD did not converge

err        = 'invalid value'
flag       = 8

numpy/linalg/linalg.py:97: LinAlgError
=========================== short test summary info ============================
FAILED numpy/linalg/tests/test_linalg.py::TestCond::test_nan - numpy.linalg.L...
= 1 failed, 14429 passed, 611 skipped, 1237 deselected, 18 xfailed, 3 xpassed in 363.18s (0:06:03) =
```",2021-05-05 11:28:28,,TestCond.test_nan test failure for latest OpenBLAS,"['00 - Bug', 'component: numpy.linalg']"
18902,open,mocquin,"It seems that when using numpy's trapz `numpy.trapz`with the `__array_function__` mechanism, only `y` and `x` are caught, but not `dx`, with numpy's signature `numpy.trapz(y, x=None, dx=1.0, axis=-1)`.


### Reproducing code example:

Here is a sample code that creates a numerical-labelled object, basically a value (scalar or array) and a label as a string.
A wrapped version of numpy trapz is created and registered, so that anytime numpy recieves a `NumericalLabelled` in a `trapz` call, it relies on the wrapped version. But it seems that the wrapped version is never called when only `dx` is a `NumericalLabelled`. I would expect it is called as soon as any of the input is of the type NumericalLabelled.

```python
import numpy as np

HANDLED_FUNCTIONS = {}

class NumericalLabeled():
    def __init__(self, value, label=""""):
        self.value = value
        self.label = label
        
    def __repr__(self):
        return ""NumericalLabelled<""+str(self.value) + "","" + self.label+"">""
    
    def __array_function__(self, func, types, args, kwargs):
        print(""Got into array function"")
        if func not in HANDLED_FUNCTIONS:
            return NotImplemented
        return HANDLED_FUNCTIONS[func](*args, **kwargs)
    
def make_numericallabelled(x, label=""""):
    """"""
    Helper function to cast anything into a NumericalLabelled object.
    """"""
    if isinstance(x, NumericalLabeled):
        return x
    else:
        return NumericalLabeled(x, label=label)
    
# Numpy functions            
# Override functions - used with __array_function__
def implements(np_function):
    def decorator(func):
        HANDLED_FUNCTIONS[np_function] = func
        return func
    return decorator    
    
@implements(np.trapz)
def np_trapz(q, x=None, dx=1, **kwargs):
    """"""
    Numpy's trapz wrapper for NumericalLabelled.
    """"""
    # first convert q into a NumericalLabelled to use `q.value` 
    q = make_numericallabelled(q)
    if x is None:    
        # using dx.value and dx.label
        dx = make_numericallabelled(dx, label=""dx"")
        return NumericalLabeled(np.trapz(q.value, dx=dx.value, x=None, **kwargs),
                                q.label + dx.label,
                    )
    else:
        # using x/value and x.label
        x = make_numericallabelled(x, label=""x"")
        return NumericalLabeled(np.trapz(q.value, x=x.value, **kwargs),
                                q.label + x.label,
                    )

def main():
    # create a scalar to use as dx
    half = NumericalLabeled(0.5, ""half"")
    # create an array to use as x
    x = NumericalLabeled(np.arange(5), ""x"")
    # then 
    # this works
    print(np.trapz(NumericalLabeled(np.arange(5), ""a"")))
    # this also works
    print(np.trapz(np.arange(5), x=x))
    # but not this
    print(np.trapz(np.arange(5), dx=half))
    # TypeError: unsupported operand type(s) for *: 'NumericalLabeled' and 'int'
main()
```

### Error message:

```
Got into array function
NumericalLabelled<8.0,adx>
Got into array function
NumericalLabelled<8.0,x>

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-30-3401bee8c135> in <module>
     68     np.trapz(np.arange(5), dx=half)
     69     # TypeError: unsupported operand type(s) for *: 'NumericalLabeled' and 'int'
---> 70 main()

<ipython-input-30-3401bee8c135> in main()
     66     np.trapz(np.arange(5), x=x)
     67     # but not this
---> 68     np.trapz(np.arange(5), dx=half)
     69     # TypeError: unsupported operand type(s) for *: 'NumericalLabeled' and 'int'
     70 main()

<__array_function__ internals> in trapz(*args, **kwargs)

/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/function_base.py in trapz(y, x, dx, axis)
   4161     slice2[axis] = slice(None, -1)
   4162     try:
-> 4163         ret = (d * (y[tuple(slice1)] + y[tuple(slice2)]) / 2.0).sum(axis)
   4164     except ValueError:
   4165         # Operations didn't work, cast to ndarray

TypeError: unsupported operand type(s) for *: 'NumericalLabeled' and 'int'
```

### NumPy/Python version information:

`1.20.0 3.8.5 (default, Sep  4 2020, 02:22:02) 
[Clang 10.0.0 ]`

",2021-05-04 10:06:08,,Trapz dx doesn't trigger array_function mechanism,['00 - Bug']
18901,open,samfux84,"Numpy installation fails due to some SIMD issues. I don't need any avx512, even if the node I am compiling on is supporting it. We are running an HPC cluster with several different hardware generations and I would like to make sure that the installation can run on all different hardware generations. 

Numpy should just take over the optimization flags from the underlying Python installation. In my case this would be

-ftree-vectorize -march=core-avx2 -mavx2

I don't want numpy to optimize for anything else, especially if these optimizations cause the build to fail. I found that you can specify SIMD optimizations:

https://numpy.org/devdocs/reference/simd/simd-optimizations.html

But the entire documentation does not mention a single time where one needs to set the --cpu-baseline and --cpu-dispatch options (are those pip options?). For most programs in general it is sufficient to set

CFLAGS=""-ftree-vectorize -march=core-avx2 -mavx2""

or

CXXFLAGS=""-ftree-vectorize -march=core-avx2 -mavx2""

How can this be done for numpy?

### Reproducing code example:

OPENBLAS=$OPENBLAS_ROOT/lib/libopenblas.so pip install --no-binary :all: numpy

### Error message:
<details>
<summary>build log</summary>

```Bash
(pytorch171cu101) [sfux@lo-s4-009 ~]$ OPENBLAS=$OPENBLAS_ROOT/lib/libopenblas.so pip install --no-binary :all: numpy
Collecting numpy
  Using cached numpy-1.20.2.zip (7.8 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
    Preparing wheel metadata ... done
Building wheels for collected packages: numpy
  Building wheel for numpy (PEP 517) ... error
  ERROR: Command errored out with exit status 1:
   command: /cluster/home/sfux/pytorch171cu101/bin/python /cluster/home/sfux/pytorch171cu101/lib64/python3.7/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /scratch/16050576.tmpdir/tmpavtpfuqb
       cwd: /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc
  Complete output (1373 lines):
  numpy/random/_bounded_integers.pxd.in has not changed
  numpy/random/_mt19937.pyx has not changed
  numpy/random/_generator.pyx has not changed
  numpy/random/_philox.pyx has not changed
  numpy/random/_bounded_integers.pyx.in has not changed
  numpy/random/_common.pyx has not changed
  numpy/random/_sfc64.pyx has not changed
  numpy/random/bit_generator.pyx has not changed
  numpy/random/_pcg64.pyx has not changed
  numpy/random/mtrand.pyx has not changed
  Processing numpy/random/_bounded_integers.pyx
  Cythonizing sources
  blas_opt_info:
  blas_mkl_info:
  customize UnixCCompiler
    libraries mkl_rt not found in ['/cluster/home/sfux/pytorch171cu101/lib', '/usr/local/lib64', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/']
    NOT AVAILABLE

  blis_info:
    libraries blis not found in ['/cluster/home/sfux/pytorch171cu101/lib', '/usr/local/lib64', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/']
    NOT AVAILABLE

  openblas_info:
  Replacing _lib_names[0]=='openblas' with 'openblas'
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  creating /scratch/16050576.tmpdir/tmpo5ixxui4/scratch
  creating /scratch/16050576.tmpdir/tmpo5ixxui4/scratch/16050576.tmpdir
  creating /scratch/16050576.tmpdir/tmpo5ixxui4/scratch/16050576.tmpdir/tmpo5ixxui4
  compile options: '-c'
  gcc: /scratch/16050576.tmpdir/tmpo5ixxui4/source.c
  /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc /scratch/16050576.tmpdir/tmpo5ixxui4/scratch/16050576.tmpdir/tmpo5ixxui4/source.o -L/cluster/apps/gcc-6.3.0/openblas-0.2.19-jusdys62qhv52qjbsoewvthriekuaynw/lib -lopenblas -o /scratch/16050576.tmpdir/tmpo5ixxui4/a.out
  Replacing _lib_names[0]=='openblas' with 'openblas'
  Replacing _lib_names[0]=='openblas' with 'openblas'
  Replacing _lib_names[0]=='openblas' with 'openblas'
    FOUND:
      libraries = ['openblas', 'openblas']
      library_dirs = ['/cluster/apps/gcc-6.3.0/openblas-0.2.19-jusdys62qhv52qjbsoewvthriekuaynw/lib']
      language = c
      define_macros = [('HAVE_CBLAS', None)]
      runtime_library_dirs = ['/cluster/apps/gcc-6.3.0/openblas-0.2.19-jusdys62qhv52qjbsoewvthriekuaynw/lib']

    FOUND:
      libraries = ['openblas', 'openblas']
      library_dirs = ['/cluster/apps/gcc-6.3.0/openblas-0.2.19-jusdys62qhv52qjbsoewvthriekuaynw/lib']
      language = c
      define_macros = [('HAVE_CBLAS', None)]
      runtime_library_dirs = ['/cluster/apps/gcc-6.3.0/openblas-0.2.19-jusdys62qhv52qjbsoewvthriekuaynw/lib']

  non-existing path in 'numpy/distutils': 'site.cfg'
  lapack_opt_info:
  lapack_mkl_info:
    libraries mkl_rt not found in ['/cluster/home/sfux/pytorch171cu101/lib', '/usr/local/lib64', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/']
    NOT AVAILABLE

  openblas_lapack_info:
  Replacing _lib_names[0]=='openblas' with 'openblas'
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  creating /scratch/16050576.tmpdir/tmp31tk2zu4/scratch
  creating /scratch/16050576.tmpdir/tmp31tk2zu4/scratch/16050576.tmpdir
  creating /scratch/16050576.tmpdir/tmp31tk2zu4/scratch/16050576.tmpdir/tmp31tk2zu4
  compile options: '-c'
  gcc: /scratch/16050576.tmpdir/tmp31tk2zu4/source.c
  /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc /scratch/16050576.tmpdir/tmp31tk2zu4/scratch/16050576.tmpdir/tmp31tk2zu4/source.o -L/cluster/apps/gcc-6.3.0/openblas-0.2.19-jusdys62qhv52qjbsoewvthriekuaynw/lib -lopenblas -o /scratch/16050576.tmpdir/tmp31tk2zu4/a.out
  Replacing _lib_names[0]=='openblas' with 'openblas'
  Replacing _lib_names[0]=='openblas' with 'openblas'
  Replacing _lib_names[0]=='openblas' with 'openblas'
    FOUND:
      libraries = ['openblas', 'openblas']
      library_dirs = ['/cluster/apps/gcc-6.3.0/openblas-0.2.19-jusdys62qhv52qjbsoewvthriekuaynw/lib']
      language = c
      define_macros = [('HAVE_CBLAS', None)]
      runtime_library_dirs = ['/cluster/apps/gcc-6.3.0/openblas-0.2.19-jusdys62qhv52qjbsoewvthriekuaynw/lib']

    FOUND:
      libraries = ['openblas', 'openblas']
      library_dirs = ['/cluster/apps/gcc-6.3.0/openblas-0.2.19-jusdys62qhv52qjbsoewvthriekuaynw/lib']
      language = c
      define_macros = [('HAVE_CBLAS', None)]
      runtime_library_dirs = ['/cluster/apps/gcc-6.3.0/openblas-0.2.19-jusdys62qhv52qjbsoewvthriekuaynw/lib']

  running bdist_wheel
  running build
  running config_cc
  unifing config_cc, config, build_clib, build_ext, build commands --compiler options
  running config_fc
  unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options
  running build_src
  build_src
  building py_modules sources
  building library ""npymath"" sources
    adding 'build/src.linux-x86_64-3.7/numpy/core/src/npymath' to include_dirs.
  None - nothing done with h_files = ['build/src.linux-x86_64-3.7/numpy/core/src/npymath/npy_math_internal.h']
  building library ""npyrandom"" sources
  building extension ""numpy.core._multiarray_tests"" sources
  building extension ""numpy.core._multiarray_umath"" sources
    adding 'build/src.linux-x86_64-3.7/numpy/core/src/common' to include_dirs.
    adding 'build/src.linux-x86_64-3.7/numpy/core/src/umath' to include_dirs.
  numpy.core - nothing done with h_files = ['build/src.linux-x86_64-3.7/numpy/core/src/common/npy_sort.h', 'build/src.linux-x86_64-3.7/numpy/core/src/common/npy_partition.h', 'build/src.linux-x86_64-3.7/numpy/core/src/common/npy_binsearch.h', 'build/src.linux-x86_64-3.7/numpy/core/src/umath/funcs.inc', 'build/src.linux-x86_64-3.7/numpy/core/src/umath/simd.inc', 'build/src.linux-x86_64-3.7/numpy/core/src/umath/loops.h', 'build/src.linux-x86_64-3.7/numpy/core/src/umath/matmul.h', 'build/src.linux-x86_64-3.7/numpy/core/src/umath/clip.h', 'build/src.linux-x86_64-3.7/numpy/core/src/common/templ_common.h', 'build/src.linux-x86_64-3.7/numpy/core/include/numpy/config.h', 'build/src.linux-x86_64-3.7/numpy/core/include/numpy/_numpyconfig.h', 'build/src.linux-x86_64-3.7/numpy/core/include/numpy/__multiarray_api.h', 'build/src.linux-x86_64-3.7/numpy/core/include/numpy/__ufunc_api.h']
  building extension ""numpy.core._umath_tests"" sources
  building extension ""numpy.core._rational_tests"" sources
  building extension ""numpy.core._struct_ufunc_tests"" sources
  building extension ""numpy.core._operand_flag_tests"" sources
  building extension ""numpy.core._simd"" sources
    adding 'build/src.linux-x86_64-3.7/numpy/core/src/_simd' to include_dirs.
  numpy.core - nothing done with h_files = ['build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd_inc.h', 'build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd_data.inc']
  building extension ""numpy.fft._pocketfft_internal"" sources
  building extension ""numpy.linalg.lapack_lite"" sources
  building extension ""numpy.linalg._umath_linalg"" sources
  building extension ""numpy.random._mt19937"" sources
  building extension ""numpy.random._philox"" sources
  building extension ""numpy.random._pcg64"" sources
  building extension ""numpy.random._sfc64"" sources
  building extension ""numpy.random._common"" sources
  building extension ""numpy.random.bit_generator"" sources
  building extension ""numpy.random._generator"" sources
  building extension ""numpy.random._bounded_integers"" sources
  building extension ""numpy.random.mtrand"" sources
  building data_files sources
  build_src: building npy-pkg config files
  running build_py
  creating build/lib.linux-x86_64-3.7
  creating build/lib.linux-x86_64-3.7/numpy
  copying numpy/setup.py -> build/lib.linux-x86_64-3.7/numpy
  copying numpy/__init__.py -> build/lib.linux-x86_64-3.7/numpy
  copying numpy/dual.py -> build/lib.linux-x86_64-3.7/numpy
  copying numpy/_globals.py -> build/lib.linux-x86_64-3.7/numpy
  copying numpy/_distributor_init.py -> build/lib.linux-x86_64-3.7/numpy
  copying numpy/conftest.py -> build/lib.linux-x86_64-3.7/numpy
  copying numpy/matlib.py -> build/lib.linux-x86_64-3.7/numpy
  copying numpy/version.py -> build/lib.linux-x86_64-3.7/numpy
  copying numpy/_pytesttester.py -> build/lib.linux-x86_64-3.7/numpy
  copying numpy/ctypeslib.py -> build/lib.linux-x86_64-3.7/numpy
  copying build/src.linux-x86_64-3.7/numpy/__config__.py -> build/lib.linux-x86_64-3.7/numpy
  creating build/lib.linux-x86_64-3.7/numpy/compat
  copying numpy/compat/setup.py -> build/lib.linux-x86_64-3.7/numpy/compat
  copying numpy/compat/__init__.py -> build/lib.linux-x86_64-3.7/numpy/compat
  copying numpy/compat/_inspect.py -> build/lib.linux-x86_64-3.7/numpy/compat
  copying numpy/compat/py3k.py -> build/lib.linux-x86_64-3.7/numpy/compat
  creating build/lib.linux-x86_64-3.7/numpy/compat/tests
  copying numpy/compat/tests/__init__.py -> build/lib.linux-x86_64-3.7/numpy/compat/tests
  copying numpy/compat/tests/test_compat.py -> build/lib.linux-x86_64-3.7/numpy/compat/tests
  creating build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/_add_newdocs_scalars.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/_internal.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/fromnumeric.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/_string_helpers.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/getlimits.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/_add_newdocs.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/defchararray.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/_type_aliases.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/einsumfunc.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/setup.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/_asarray.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/__init__.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/records.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/memmap.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/function_base.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/multiarray.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/arrayprint.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/overrides.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/numeric.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/_dtype_ctypes.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/_dtype.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/cversions.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/umath.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/numerictypes.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/shape_base.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/_exceptions.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/umath_tests.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/_ufunc_config.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/setup_common.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/machar.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/_methods.py -> build/lib.linux-x86_64-3.7/numpy/core
  copying numpy/core/code_generators/generate_numpy_api.py -> build/lib.linux-x86_64-3.7/numpy/core
  creating build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_half.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_extint128.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_umath_accuracy.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_protocols.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_simd_module.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_function_base.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_ufunc.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test__exceptions.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_nditer.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_umath_complex.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/__init__.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_memmap.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_item_selection.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_shape_base.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_mem_overlap.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_records.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_cpu_dispatcher.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_cython.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_array_coercion.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_numerictypes.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_defchararray.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_scalar_methods.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_scalarinherit.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_dtype.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_arrayprint.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_datetime.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_scalarbuffer.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_longdouble.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_numeric.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_einsum.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_abc.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_indexing.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/_locales.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_scalarmath.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_api.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_overrides.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_multiarray.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_simd.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_unicode.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_scalarprint.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_conversion_utils.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_regression.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_casting_unittests.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_deprecations.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_cpu_features.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_print.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_scalar_ctors.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_getlimits.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_machar.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_errstate.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_indexerrors.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  copying numpy/core/tests/test_umath.py -> build/lib.linux-x86_64-3.7/numpy/core/tests
  creating build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/exec_command.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/core.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/npy_pkg_config.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/mingw32ccompiler.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/setup.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/__init__.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/conv_template.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/unixccompiler.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/from_template.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/numpy_distribution.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/misc_util.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/msvc9compiler.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/pathccompiler.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/ccompiler.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/system_info.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/lib2def.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/intelccompiler.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/cpuinfo.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/_shell_utils.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/msvccompiler.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/log.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/line_endings.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/ccompiler_opt.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying numpy/distutils/extension.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  copying build/src.linux-x86_64-3.7/numpy/distutils/__config__.py -> build/lib.linux-x86_64-3.7/numpy/distutils
  creating build/lib.linux-x86_64-3.7/numpy/distutils/command
  copying numpy/distutils/command/sdist.py -> build/lib.linux-x86_64-3.7/numpy/distutils/command
  copying numpy/distutils/command/config_compiler.py -> build/lib.linux-x86_64-3.7/numpy/distutils/command
  copying numpy/distutils/command/build_scripts.py -> build/lib.linux-x86_64-3.7/numpy/distutils/command
  copying numpy/distutils/command/config.py -> build/lib.linux-x86_64-3.7/numpy/distutils/command
  copying numpy/distutils/command/autodist.py -> build/lib.linux-x86_64-3.7/numpy/distutils/command
  copying numpy/distutils/command/build_src.py -> build/lib.linux-x86_64-3.7/numpy/distutils/command
  copying numpy/distutils/command/__init__.py -> build/lib.linux-x86_64-3.7/numpy/distutils/command
  copying numpy/distutils/command/install.py -> build/lib.linux-x86_64-3.7/numpy/distutils/command
  copying numpy/distutils/command/build_py.py -> build/lib.linux-x86_64-3.7/numpy/distutils/command
  copying numpy/distutils/command/install_data.py -> build/lib.linux-x86_64-3.7/numpy/distutils/command
  copying numpy/distutils/command/build_clib.py -> build/lib.linux-x86_64-3.7/numpy/distutils/command
  copying numpy/distutils/command/build_ext.py -> build/lib.linux-x86_64-3.7/numpy/distutils/command
  copying numpy/distutils/command/bdist_rpm.py -> build/lib.linux-x86_64-3.7/numpy/distutils/command
  copying numpy/distutils/command/install_headers.py -> build/lib.linux-x86_64-3.7/numpy/distutils/command
  copying numpy/distutils/command/install_clib.py -> build/lib.linux-x86_64-3.7/numpy/distutils/command
  copying numpy/distutils/command/egg_info.py -> build/lib.linux-x86_64-3.7/numpy/distutils/command
  copying numpy/distutils/command/build.py -> build/lib.linux-x86_64-3.7/numpy/distutils/command
  copying numpy/distutils/command/develop.py -> build/lib.linux-x86_64-3.7/numpy/distutils/command
  creating build/lib.linux-x86_64-3.7/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/nag.py -> build/lib.linux-x86_64-3.7/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/pathf95.py -> build/lib.linux-x86_64-3.7/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/intel.py -> build/lib.linux-x86_64-3.7/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/none.py -> build/lib.linux-x86_64-3.7/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/__init__.py -> build/lib.linux-x86_64-3.7/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/vast.py -> build/lib.linux-x86_64-3.7/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/lahey.py -> build/lib.linux-x86_64-3.7/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/mips.py -> build/lib.linux-x86_64-3.7/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/hpux.py -> build/lib.linux-x86_64-3.7/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/nv.py -> build/lib.linux-x86_64-3.7/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/sun.py -> build/lib.linux-x86_64-3.7/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/environment.py -> build/lib.linux-x86_64-3.7/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/pg.py -> build/lib.linux-x86_64-3.7/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/gnu.py -> build/lib.linux-x86_64-3.7/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/g95.py -> build/lib.linux-x86_64-3.7/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/compaq.py -> build/lib.linux-x86_64-3.7/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/fujitsu.py -> build/lib.linux-x86_64-3.7/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/absoft.py -> build/lib.linux-x86_64-3.7/numpy/distutils/fcompiler
  copying numpy/distutils/fcompiler/ibm.py -> build/lib.linux-x86_64-3.7/numpy/distutils/fcompiler
  creating build/lib.linux-x86_64-3.7/numpy/distutils/tests
  copying numpy/distutils/tests/test_shell_utils.py -> build/lib.linux-x86_64-3.7/numpy/distutils/tests
  copying numpy/distutils/tests/test_from_template.py -> build/lib.linux-x86_64-3.7/numpy/distutils/tests
  copying numpy/distutils/tests/test_ccompiler_opt_conf.py -> build/lib.linux-x86_64-3.7/numpy/distutils/tests
  copying numpy/distutils/tests/test_fcompiler_nagfor.py -> build/lib.linux-x86_64-3.7/numpy/distutils/tests
  copying numpy/distutils/tests/test_misc_util.py -> build/lib.linux-x86_64-3.7/numpy/distutils/tests
  copying numpy/distutils/tests/__init__.py -> build/lib.linux-x86_64-3.7/numpy/distutils/tests
  copying numpy/distutils/tests/test_fcompiler.py -> build/lib.linux-x86_64-3.7/numpy/distutils/tests
  copying numpy/distutils/tests/test_fcompiler_intel.py -> build/lib.linux-x86_64-3.7/numpy/distutils/tests
  copying numpy/distutils/tests/test_fcompiler_gnu.py -> build/lib.linux-x86_64-3.7/numpy/distutils/tests
  copying numpy/distutils/tests/test_npy_pkg_config.py -> build/lib.linux-x86_64-3.7/numpy/distutils/tests
  copying numpy/distutils/tests/test_exec_command.py -> build/lib.linux-x86_64-3.7/numpy/distutils/tests
  copying numpy/distutils/tests/test_system_info.py -> build/lib.linux-x86_64-3.7/numpy/distutils/tests
  copying numpy/distutils/tests/test_mingw32ccompiler.py -> build/lib.linux-x86_64-3.7/numpy/distutils/tests
  copying numpy/distutils/tests/test_build_ext.py -> build/lib.linux-x86_64-3.7/numpy/distutils/tests
  copying numpy/distutils/tests/test_ccompiler_opt.py -> build/lib.linux-x86_64-3.7/numpy/distutils/tests
  creating build/lib.linux-x86_64-3.7/numpy/doc
  copying numpy/doc/__init__.py -> build/lib.linux-x86_64-3.7/numpy/doc
  copying numpy/doc/constants.py -> build/lib.linux-x86_64-3.7/numpy/doc
  copying numpy/doc/ufuncs.py -> build/lib.linux-x86_64-3.7/numpy/doc
  creating build/lib.linux-x86_64-3.7/numpy/f2py
  copying numpy/f2py/f2py_testing.py -> build/lib.linux-x86_64-3.7/numpy/f2py
  copying numpy/f2py/setup.py -> build/lib.linux-x86_64-3.7/numpy/f2py
  copying numpy/f2py/__init__.py -> build/lib.linux-x86_64-3.7/numpy/f2py
  copying numpy/f2py/crackfortran.py -> build/lib.linux-x86_64-3.7/numpy/f2py
  copying numpy/f2py/rules.py -> build/lib.linux-x86_64-3.7/numpy/f2py
  copying numpy/f2py/f2py2e.py -> build/lib.linux-x86_64-3.7/numpy/f2py
  copying numpy/f2py/diagnose.py -> build/lib.linux-x86_64-3.7/numpy/f2py
  copying numpy/f2py/cb_rules.py -> build/lib.linux-x86_64-3.7/numpy/f2py
  copying numpy/f2py/cfuncs.py -> build/lib.linux-x86_64-3.7/numpy/f2py
  copying numpy/f2py/f90mod_rules.py -> build/lib.linux-x86_64-3.7/numpy/f2py
  copying numpy/f2py/auxfuncs.py -> build/lib.linux-x86_64-3.7/numpy/f2py
  copying numpy/f2py/__main__.py -> build/lib.linux-x86_64-3.7/numpy/f2py
  copying numpy/f2py/func2subr.py -> build/lib.linux-x86_64-3.7/numpy/f2py
  copying numpy/f2py/use_rules.py -> build/lib.linux-x86_64-3.7/numpy/f2py
  copying numpy/f2py/__version__.py -> build/lib.linux-x86_64-3.7/numpy/f2py
  copying numpy/f2py/capi_maps.py -> build/lib.linux-x86_64-3.7/numpy/f2py
  copying numpy/f2py/common_rules.py -> build/lib.linux-x86_64-3.7/numpy/f2py
  creating build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_return_character.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_string.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_semicolon_split.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/__init__.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_callback.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_module_doc.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_return_real.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_array_from_pyobj.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_return_integer.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_return_logical.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_block_docstring.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_return_complex.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_crackfortran.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_size.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_kind.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_quoted_character.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/util.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_regression.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_assumed_shape.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_parameter.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_common.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_compile_function.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  copying numpy/f2py/tests/test_mixed.py -> build/lib.linux-x86_64-3.7/numpy/f2py/tests
  creating build/lib.linux-x86_64-3.7/numpy/fft
  copying numpy/fft/_pocketfft.py -> build/lib.linux-x86_64-3.7/numpy/fft
  copying numpy/fft/setup.py -> build/lib.linux-x86_64-3.7/numpy/fft
  copying numpy/fft/__init__.py -> build/lib.linux-x86_64-3.7/numpy/fft
  copying numpy/fft/helper.py -> build/lib.linux-x86_64-3.7/numpy/fft
  creating build/lib.linux-x86_64-3.7/numpy/fft/tests
  copying numpy/fft/tests/__init__.py -> build/lib.linux-x86_64-3.7/numpy/fft/tests
  copying numpy/fft/tests/test_helper.py -> build/lib.linux-x86_64-3.7/numpy/fft/tests
  copying numpy/fft/tests/test_pocketfft.py -> build/lib.linux-x86_64-3.7/numpy/fft/tests
  creating build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/_datasource.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/arrayterator.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/utils.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/_version.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/scimath.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/ufunclike.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/setup.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/__init__.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/nanfunctions.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/_iotools.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/function_base.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/npyio.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/polynomial.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/user_array.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/arraysetops.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/twodim_base.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/index_tricks.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/shape_base.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/stride_tricks.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/type_check.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/histograms.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/arraypad.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/format.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/mixins.py -> build/lib.linux-x86_64-3.7/numpy/lib
  copying numpy/lib/recfunctions.py -> build/lib.linux-x86_64-3.7/numpy/lib
  creating build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_histograms.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_arrayterator.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_ufunclike.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_financial_expired.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_function_base.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_nanfunctions.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_arraypad.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_recfunctions.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/__init__.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_index_tricks.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_shape_base.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_utils.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_mixins.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_twodim_base.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test__version.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_io.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test__datasource.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_type_check.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_regression.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_packbits.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test__iotools.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_stride_tricks.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_arraysetops.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_polynomial.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  copying numpy/lib/tests/test_format.py -> build/lib.linux-x86_64-3.7/numpy/lib/tests
  creating build/lib.linux-x86_64-3.7/numpy/linalg
  copying numpy/linalg/setup.py -> build/lib.linux-x86_64-3.7/numpy/linalg
  copying numpy/linalg/__init__.py -> build/lib.linux-x86_64-3.7/numpy/linalg
  copying numpy/linalg/linalg.py -> build/lib.linux-x86_64-3.7/numpy/linalg
  creating build/lib.linux-x86_64-3.7/numpy/linalg/tests
  copying numpy/linalg/tests/__init__.py -> build/lib.linux-x86_64-3.7/numpy/linalg/tests
  copying numpy/linalg/tests/test_build.py -> build/lib.linux-x86_64-3.7/numpy/linalg/tests
  copying numpy/linalg/tests/test_regression.py -> build/lib.linux-x86_64-3.7/numpy/linalg/tests
  copying numpy/linalg/tests/test_deprecations.py -> build/lib.linux-x86_64-3.7/numpy/linalg/tests
  copying numpy/linalg/tests/test_linalg.py -> build/lib.linux-x86_64-3.7/numpy/linalg/tests
  creating build/lib.linux-x86_64-3.7/numpy/ma
  copying numpy/ma/core.py -> build/lib.linux-x86_64-3.7/numpy/ma
  copying numpy/ma/timer_comparison.py -> build/lib.linux-x86_64-3.7/numpy/ma
  copying numpy/ma/setup.py -> build/lib.linux-x86_64-3.7/numpy/ma
  copying numpy/ma/testutils.py -> build/lib.linux-x86_64-3.7/numpy/ma
  copying numpy/ma/bench.py -> build/lib.linux-x86_64-3.7/numpy/ma
  copying numpy/ma/__init__.py -> build/lib.linux-x86_64-3.7/numpy/ma
  copying numpy/ma/extras.py -> build/lib.linux-x86_64-3.7/numpy/ma
  copying numpy/ma/mrecords.py -> build/lib.linux-x86_64-3.7/numpy/ma
  creating build/lib.linux-x86_64-3.7/numpy/ma/tests
  copying numpy/ma/tests/test_core.py -> build/lib.linux-x86_64-3.7/numpy/ma/tests
  copying numpy/ma/tests/test_extras.py -> build/lib.linux-x86_64-3.7/numpy/ma/tests
  copying numpy/ma/tests/__init__.py -> build/lib.linux-x86_64-3.7/numpy/ma/tests
  copying numpy/ma/tests/test_old_ma.py -> build/lib.linux-x86_64-3.7/numpy/ma/tests
  copying numpy/ma/tests/test_subclassing.py -> build/lib.linux-x86_64-3.7/numpy/ma/tests
  copying numpy/ma/tests/test_mrecords.py -> build/lib.linux-x86_64-3.7/numpy/ma/tests
  copying numpy/ma/tests/test_regression.py -> build/lib.linux-x86_64-3.7/numpy/ma/tests
  copying numpy/ma/tests/test_deprecations.py -> build/lib.linux-x86_64-3.7/numpy/ma/tests
  creating build/lib.linux-x86_64-3.7/numpy/matrixlib
  copying numpy/matrixlib/setup.py -> build/lib.linux-x86_64-3.7/numpy/matrixlib
  copying numpy/matrixlib/__init__.py -> build/lib.linux-x86_64-3.7/numpy/matrixlib
  copying numpy/matrixlib/defmatrix.py -> build/lib.linux-x86_64-3.7/numpy/matrixlib
  creating build/lib.linux-x86_64-3.7/numpy/matrixlib/tests
  copying numpy/matrixlib/tests/__init__.py -> build/lib.linux-x86_64-3.7/numpy/matrixlib/tests
  copying numpy/matrixlib/tests/test_defmatrix.py -> build/lib.linux-x86_64-3.7/numpy/matrixlib/tests
  copying numpy/matrixlib/tests/test_numeric.py -> build/lib.linux-x86_64-3.7/numpy/matrixlib/tests
  copying numpy/matrixlib/tests/test_masked_matrix.py -> build/lib.linux-x86_64-3.7/numpy/matrixlib/tests
  copying numpy/matrixlib/tests/test_multiarray.py -> build/lib.linux-x86_64-3.7/numpy/matrixlib/tests
  copying numpy/matrixlib/tests/test_matrix_linalg.py -> build/lib.linux-x86_64-3.7/numpy/matrixlib/tests
  copying numpy/matrixlib/tests/test_regression.py -> build/lib.linux-x86_64-3.7/numpy/matrixlib/tests
  copying numpy/matrixlib/tests/test_interaction.py -> build/lib.linux-x86_64-3.7/numpy/matrixlib/tests
  creating build/lib.linux-x86_64-3.7/numpy/polynomial
  copying numpy/polynomial/hermite.py -> build/lib.linux-x86_64-3.7/numpy/polynomial
  copying numpy/polynomial/setup.py -> build/lib.linux-x86_64-3.7/numpy/polynomial
  copying numpy/polynomial/__init__.py -> build/lib.linux-x86_64-3.7/numpy/polynomial
  copying numpy/polynomial/laguerre.py -> build/lib.linux-x86_64-3.7/numpy/polynomial
  copying numpy/polynomial/legendre.py -> build/lib.linux-x86_64-3.7/numpy/polynomial
  copying numpy/polynomial/polynomial.py -> build/lib.linux-x86_64-3.7/numpy/polynomial
  copying numpy/polynomial/chebyshev.py -> build/lib.linux-x86_64-3.7/numpy/polynomial
  copying numpy/polynomial/polyutils.py -> build/lib.linux-x86_64-3.7/numpy/polynomial
  copying numpy/polynomial/hermite_e.py -> build/lib.linux-x86_64-3.7/numpy/polynomial
  copying numpy/polynomial/_polybase.py -> build/lib.linux-x86_64-3.7/numpy/polynomial
  creating build/lib.linux-x86_64-3.7/numpy/polynomial/tests
  copying numpy/polynomial/tests/test_classes.py -> build/lib.linux-x86_64-3.7/numpy/polynomial/tests
  copying numpy/polynomial/tests/test_legendre.py -> build/lib.linux-x86_64-3.7/numpy/polynomial/tests
  copying numpy/polynomial/tests/__init__.py -> build/lib.linux-x86_64-3.7/numpy/polynomial/tests
  copying numpy/polynomial/tests/test_hermite.py -> build/lib.linux-x86_64-3.7/numpy/polynomial/tests
  copying numpy/polynomial/tests/test_chebyshev.py -> build/lib.linux-x86_64-3.7/numpy/polynomial/tests
  copying numpy/polynomial/tests/test_printing.py -> build/lib.linux-x86_64-3.7/numpy/polynomial/tests
  copying numpy/polynomial/tests/test_hermite_e.py -> build/lib.linux-x86_64-3.7/numpy/polynomial/tests
  copying numpy/polynomial/tests/test_polyutils.py -> build/lib.linux-x86_64-3.7/numpy/polynomial/tests
  copying numpy/polynomial/tests/test_laguerre.py -> build/lib.linux-x86_64-3.7/numpy/polynomial/tests
  copying numpy/polynomial/tests/test_polynomial.py -> build/lib.linux-x86_64-3.7/numpy/polynomial/tests
  creating build/lib.linux-x86_64-3.7/numpy/random
  copying numpy/random/setup.py -> build/lib.linux-x86_64-3.7/numpy/random
  copying numpy/random/__init__.py -> build/lib.linux-x86_64-3.7/numpy/random
  copying numpy/random/_pickle.py -> build/lib.linux-x86_64-3.7/numpy/random
  creating build/lib.linux-x86_64-3.7/numpy/random/tests
  copying numpy/random/tests/test_generator_mt19937_regressions.py -> build/lib.linux-x86_64-3.7/numpy/random/tests
  copying numpy/random/tests/test_extending.py -> build/lib.linux-x86_64-3.7/numpy/random/tests
  copying numpy/random/tests/test_seed_sequence.py -> build/lib.linux-x86_64-3.7/numpy/random/tests
  copying numpy/random/tests/__init__.py -> build/lib.linux-x86_64-3.7/numpy/random/tests
  copying numpy/random/tests/test_randomstate.py -> build/lib.linux-x86_64-3.7/numpy/random/tests
  copying numpy/random/tests/test_smoke.py -> build/lib.linux-x86_64-3.7/numpy/random/tests
  copying numpy/random/tests/test_generator_mt19937.py -> build/lib.linux-x86_64-3.7/numpy/random/tests
  copying numpy/random/tests/test_randomstate_regression.py -> build/lib.linux-x86_64-3.7/numpy/random/tests
  copying numpy/random/tests/test_regression.py -> build/lib.linux-x86_64-3.7/numpy/random/tests
  copying numpy/random/tests/test_direct.py -> build/lib.linux-x86_64-3.7/numpy/random/tests
  copying numpy/random/tests/test_random.py -> build/lib.linux-x86_64-3.7/numpy/random/tests
  creating build/lib.linux-x86_64-3.7/numpy/testing
  copying numpy/testing/utils.py -> build/lib.linux-x86_64-3.7/numpy/testing
  copying numpy/testing/setup.py -> build/lib.linux-x86_64-3.7/numpy/testing
  copying numpy/testing/__init__.py -> build/lib.linux-x86_64-3.7/numpy/testing
  copying numpy/testing/print_coercion_tables.py -> build/lib.linux-x86_64-3.7/numpy/testing
  creating build/lib.linux-x86_64-3.7/numpy/testing/_private
  copying numpy/testing/_private/utils.py -> build/lib.linux-x86_64-3.7/numpy/testing/_private
  copying numpy/testing/_private/__init__.py -> build/lib.linux-x86_64-3.7/numpy/testing/_private
  copying numpy/testing/_private/nosetester.py -> build/lib.linux-x86_64-3.7/numpy/testing/_private
  copying numpy/testing/_private/noseclasses.py -> build/lib.linux-x86_64-3.7/numpy/testing/_private
  copying numpy/testing/_private/decorators.py -> build/lib.linux-x86_64-3.7/numpy/testing/_private
  copying numpy/testing/_private/parameterized.py -> build/lib.linux-x86_64-3.7/numpy/testing/_private
  creating build/lib.linux-x86_64-3.7/numpy/testing/tests
  copying numpy/testing/tests/test_decorators.py -> build/lib.linux-x86_64-3.7/numpy/testing/tests
  copying numpy/testing/tests/__init__.py -> build/lib.linux-x86_64-3.7/numpy/testing/tests
  copying numpy/testing/tests/test_doctesting.py -> build/lib.linux-x86_64-3.7/numpy/testing/tests
  copying numpy/testing/tests/test_utils.py -> build/lib.linux-x86_64-3.7/numpy/testing/tests
  creating build/lib.linux-x86_64-3.7/numpy/typing
  copying numpy/typing/_array_like.py -> build/lib.linux-x86_64-3.7/numpy/typing
  copying numpy/typing/_add_docstring.py -> build/lib.linux-x86_64-3.7/numpy/typing
  copying numpy/typing/setup.py -> build/lib.linux-x86_64-3.7/numpy/typing
  copying numpy/typing/__init__.py -> build/lib.linux-x86_64-3.7/numpy/typing
  copying numpy/typing/_dtype_like.py -> build/lib.linux-x86_64-3.7/numpy/typing
  copying numpy/typing/_shape.py -> build/lib.linux-x86_64-3.7/numpy/typing
  copying numpy/typing/_scalars.py -> build/lib.linux-x86_64-3.7/numpy/typing
  copying numpy/typing/_callable.py -> build/lib.linux-x86_64-3.7/numpy/typing
  creating build/lib.linux-x86_64-3.7/numpy/typing/tests
  copying numpy/typing/tests/__init__.py -> build/lib.linux-x86_64-3.7/numpy/typing/tests
  copying numpy/typing/tests/test_typing.py -> build/lib.linux-x86_64-3.7/numpy/typing/tests
  copying numpy/typing/tests/test_isfile.py -> build/lib.linux-x86_64-3.7/numpy/typing/tests
  creating build/lib.linux-x86_64-3.7/numpy/tests
  copying numpy/tests/test_scripts.py -> build/lib.linux-x86_64-3.7/numpy/tests
  copying numpy/tests/__init__.py -> build/lib.linux-x86_64-3.7/numpy/tests
  copying numpy/tests/test_matlib.py -> build/lib.linux-x86_64-3.7/numpy/tests
  copying numpy/tests/test_numpy_version.py -> build/lib.linux-x86_64-3.7/numpy/tests
  copying numpy/tests/test_reloading.py -> build/lib.linux-x86_64-3.7/numpy/tests
  copying numpy/tests/test_public_api.py -> build/lib.linux-x86_64-3.7/numpy/tests
  copying numpy/tests/test_ctypeslib.py -> build/lib.linux-x86_64-3.7/numpy/tests
  copying numpy/tests/test_warnings.py -> build/lib.linux-x86_64-3.7/numpy/tests
  running build_clib
  customize UnixCCompiler
  customize UnixCCompiler using new_build_clib
  CCompilerOpt.cc_test_flags[999] : testing flags (-march=native)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  creating /scratch/16050576.tmpdir/tmpavv6wga5/scratch
  creating /scratch/16050576.tmpdir/tmpavv6wga5/scratch/16050576.tmpdir
  creating /scratch/16050576.tmpdir/tmpavv6wga5/scratch/16050576.tmpdir/pip-install-3tn44a7e
  creating /scratch/16050576.tmpdir/tmpavv6wga5/scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc
  creating /scratch/16050576.tmpdir/tmpavv6wga5/scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy
  creating /scratch/16050576.tmpdir/tmpavv6wga5/scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils
  creating /scratch/16050576.tmpdir/tmpavv6wga5/scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks
  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-march=native'
  CCompilerOpt.cc_test_flags[999] : testing flags (-O3)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-O3'
  CCompilerOpt.cc_test_flags[999] : testing flags (-Werror)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-Werror'
  CCompilerOpt.__init__[1674] : check requested baseline
  CCompilerOpt.cc_test_flags[999] : testing flags (-msse)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse'
  CCompilerOpt.cc_test_flags[999] : testing flags (-msse2)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse2'
  CCompilerOpt.feature_test[1446] : testing feature 'SSE' with flags (-msse -msse2)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -Werror'
  CCompilerOpt.feature_test[1446] : testing feature 'SSE2' with flags (-msse -msse2)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -Werror'
  CCompilerOpt.cc_test_flags[999] : testing flags (-msse3)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse3'
  CCompilerOpt.feature_test[1446] : testing feature 'SSE3' with flags (-msse -msse2 -msse3)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3 -Werror'
  CCompilerOpt.__init__[1683] : check requested dispatch-able features
  CCompilerOpt.cc_test_flags[999] : testing flags (-mssse3)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-mssse3'
  CCompilerOpt.cc_test_flags[999] : testing flags (-msse4.1)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse4.1'
  CCompilerOpt.cc_test_flags[999] : testing flags (-mpopcnt)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-mpopcnt'
  CCompilerOpt.cc_test_flags[999] : testing flags (-msse4.2)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse4.2'
  CCompilerOpt.cc_test_flags[999] : testing flags (-mavx)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-mavx'
  CCompilerOpt.feature_test[1446] : testing feature 'AVX' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -Werror'
  CCompilerOpt.feature_test[1446] : testing feature 'SSE41' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -Werror'
  CCompilerOpt.feature_test[1446] : testing feature 'SSSE3' with flags (-msse -msse2 -msse3 -mssse3)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3 -mssse3 -Werror'
  CCompilerOpt.feature_test[1446] : testing feature 'POPCNT' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -Werror'
  CCompilerOpt.feature_test[1446] : testing feature 'SSE42' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -Werror'
  CCompilerOpt.cc_test_flags[999] : testing flags (-mf16c)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-mf16c'
  CCompilerOpt.feature_test[1446] : testing feature 'F16C' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -Werror'
  CCompilerOpt.cc_test_flags[999] : testing flags (-mfma)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-mfma'
  CCompilerOpt.feature_test[1446] : testing feature 'FMA3' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -Werror'
  CCompilerOpt.cc_test_flags[999] : testing flags (-mavx2)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-mavx2'
  CCompilerOpt.cc_test_flags[999] : testing flags (-mavx512f)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-mavx512f'
  CCompilerOpt.feature_test[1446] : testing feature 'AVX512F' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -Werror'
  CCompilerOpt.feature_test[1446] : testing feature 'AVX2' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mavx2)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mavx2 -Werror'
  CCompilerOpt.cc_test_flags[999] : testing flags (-mavx512cd)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-mavx512cd'
  CCompilerOpt.feature_test[1446] : testing feature 'AVX512CD' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -Werror'
  CCompilerOpt.cc_test_flags[999] : testing flags (-mavx512vl -mavx512bw -mavx512dq)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-mavx512vl -mavx512bw -mavx512dq'
  CCompilerOpt.cc_test_flags[999] : testing flags (-mavx512ifma -mavx512vbmi)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-mavx512ifma -mavx512vbmi'
  CCompilerOpt.feature_test[1446] : testing feature 'AVX512_CNL' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq -mavx512ifma -mavx512vbmi)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq -mavx512ifma -mavx512vbmi -Werror'
  CCompilerOpt.feature_test[1446] : testing feature 'AVX512_SKX' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq -Werror'
  CCompilerOpt.cc_test_flags[999] : testing flags (-mavx512vnni)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-mavx512vnni'
  CCompilerOpt.dist_test[576] : CCompilerOpt._dist_test_spawn[713] : Command (/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/test_flags.c -o /scratch/16050576.tmpdir/tmpavv6wga5/scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/test_flags.o -MMD -MF /scratch/16050576.tmpdir/tmpavv6wga5/scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/test_flags.o.d -mavx512vnni) failed with exit status 1 output ->
  gcc: error: unrecognized command line option ‘-mavx512vnni’; did you mean ‘-mavx512vbmi’?

  CCompilerOpt.cc_test_flags[1003] : testing failed
  CCompilerOpt.feature_test[1446] : testing feature 'AVX512_CLX' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq -Werror'
  CCompilerOpt.dist_test[576] : CCompilerOpt._dist_test_spawn[713] : Command (/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/cpu_avx512_clx.c -o /scratch/16050576.tmpdir/tmpavv6wga5/scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/cpu_avx512_clx.o -MMD -MF /scratch/16050576.tmpdir/tmpavv6wga5/scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/cpu_avx512_clx.o.d -msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq -Werror) failed with exit status 1 output ->
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/cpu_avx512_clx.c: In function ‘main’:
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/cpu_avx512_clx.c:6:17: error: implicit declaration of function ‘_mm512_dpbusd_epi32’ [-Werror=implicit-function-declaration]
       __m512i a = _mm512_dpbusd_epi32(_mm512_setzero_si512(), _mm512_setzero_si512(), _mm512_setzero_si512());
                   ^~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/cpu_avx512_clx.c:6:17: error: incompatible types when initializing type ‘__m512i {aka __vector(8) long long int}’ using type ‘int’
  cc1: all warnings being treated as errors

  CCompilerOpt.feature_test[1458] : testing failed
  CCompilerOpt.cc_test_flags[999] : testing flags (-mavx512er -mavx512pf)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-mavx512er -mavx512pf'
  CCompilerOpt.feature_test[1446] : testing feature 'AVX512_KNL' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512er -mavx512pf)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512er -mavx512pf -Werror'
  CCompilerOpt.cc_test_flags[999] : testing flags (-mavx5124fmaps -mavx5124vnniw -mavx512vpopcntdq)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-mavx5124fmaps -mavx5124vnniw -mavx512vpopcntdq'
  CCompilerOpt.dist_test[576] : CCompilerOpt._dist_test_spawn[713] : Command (/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/test_flags.c -o /scratch/16050576.tmpdir/tmpavv6wga5/scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/test_flags.o -MMD -MF /scratch/16050576.tmpdir/tmpavv6wga5/scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/test_flags.o.d -mavx5124fmaps -mavx5124vnniw -mavx512vpopcntdq) failed with exit status 1 output ->
  gcc: error: unrecognized command line option ‘-mavx5124fmaps’; did you mean ‘-mavx512ifma’?
  gcc: error: unrecognized command line option ‘-mavx5124vnniw’; did you mean ‘-mavx512vbmi’?
  gcc: error: unrecognized command line option ‘-mavx512vpopcntdq’; did you mean ‘-mavx512cd’?

  CCompilerOpt.cc_test_flags[1003] : testing failed
  CCompilerOpt.feature_test[1446] : testing feature 'AVX512_KNM' with flags (-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512er -mavx512pf)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512er -mavx512pf -Werror'
  CCompilerOpt.dist_test[576] : CCompilerOpt._dist_test_spawn[713] : Command (/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/cpu_avx512_knm.c -o /scratch/16050576.tmpdir/tmpavv6wga5/scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/cpu_avx512_knm.o -MMD -MF /scratch/16050576.tmpdir/tmpavv6wga5/scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/cpu_avx512_knm.o.d -msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512er -mavx512pf -Werror) failed with exit status 1 output ->
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/cpu_avx512_knm.c: In function ‘main’:
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/cpu_avx512_knm.c:9:9: error: implicit declaration of function ‘_mm512_4fmadd_ps’ [-Werror=implicit-function-declaration]
       b = _mm512_4fmadd_ps(b, b, b, b, b, NULL);
           ^~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/cpu_avx512_knm.c:9:7: error: incompatible types when assigning to type ‘__m512 {aka __vector(16) float}’ from type ‘int’
       b = _mm512_4fmadd_ps(b, b, b, b, b, NULL);
         ^
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/cpu_avx512_knm.c:11:9: error: implicit declaration of function ‘_mm512_4dpwssd_epi32’ [-Werror=implicit-function-declaration]
       a = _mm512_4dpwssd_epi32(a, a, a, a, a, NULL);
           ^~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/cpu_avx512_knm.c:11:7: error: incompatible types when assigning to type ‘__m512i {aka __vector(8) long long int}’ from type ‘int’
       a = _mm512_4dpwssd_epi32(a, a, a, a, a, NULL);
         ^
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/cpu_avx512_knm.c:13:9: error: implicit declaration of function ‘_mm512_popcnt_epi64’ [-Werror=implicit-function-declaration]
       a = _mm512_popcnt_epi64(a);
           ^~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/cpu_avx512_knm.c:13:7: error: incompatible types when assigning to type ‘__m512i {aka __vector(8) long long int}’ from type ‘int’
       a = _mm512_popcnt_epi64(a);
         ^
  cc1: all warnings being treated as errors

  CCompilerOpt.feature_test[1458] : testing failed
  CCompilerOpt.__init__[1696] : skip features (SSE3 SSE SSE2) since its part of baseline
  CCompilerOpt.__init__[1699] : initialize targets groups
  CCompilerOpt.__init__[1701] : parse target group simd_test
  CCompilerOpt._parse_target_tokens[1912] : skip targets (VSX3 NEON ASIMD VSX2 XOP FMA4 VSX) not part of baseline or dispatch-able features
  CCompilerOpt._parse_policy_not_keepbase[2022] : skip baseline features (SSE2)
  CCompilerOpt.generate_dispatch_header[2236] : generate CPU dispatch header: (build/src.linux-x86_64-3.7/numpy/distutils/include/npy_cpu_dispatch_config.h)
  CCompilerOpt.generate_dispatch_header[2247] : dispatch header dir build/src.linux-x86_64-3.7/numpy/distutils/include does not exist, creating it
  CCompilerOpt.feature_extra_checks[1519] : Testing extra checks for feature 'AVX512F' (AVX512F_REDUCE)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -Werror'
  CCompilerOpt.dist_test[576] : CCompilerOpt._dist_test_spawn[713] : Command (/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c -o /scratch/16050576.tmpdir/tmpavv6wga5/scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.o -MMD -MF /scratch/16050576.tmpdir/tmpavv6wga5/scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.o.d -msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -Werror) failed with exit status 1 output ->
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c: In function ‘main’:
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:13:21: error: implicit declaration of function ‘_mm512_reduce_add_ps’ [-Werror=implicit-function-declaration]
       float sum_ps  = _mm512_reduce_add_ps(one_ps);
                       ^~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:14:21: error: implicit declaration of function ‘_mm512_reduce_add_pd’ [-Werror=implicit-function-declaration]
       double sum_pd = _mm512_reduce_add_pd(one_pd);
                       ^~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:15:26: error: implicit declaration of function ‘_mm512_reduce_add_epi64’ [-Werror=implicit-function-declaration]
       int sum_int   = (int)_mm512_reduce_add_epi64(one_i64);
                            ^~~~~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:16:26: error: implicit declaration of function ‘_mm512_reduce_add_epi32’ [-Werror=implicit-function-declaration]
           sum_int  += (int)_mm512_reduce_add_epi32(one_i64);
                            ^~~~~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:18:16: error: implicit declaration of function ‘_mm512_reduce_mul_ps’ [-Werror=implicit-function-declaration]
       sum_ps  += _mm512_reduce_mul_ps(one_ps);
                  ^~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:19:16: error: implicit declaration of function ‘_mm512_reduce_mul_pd’ [-Werror=implicit-function-declaration]
       sum_pd  += _mm512_reduce_mul_pd(one_pd);
                  ^~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:20:21: error: implicit declaration of function ‘_mm512_reduce_mul_epi64’ [-Werror=implicit-function-declaration]
       sum_int += (int)_mm512_reduce_mul_epi64(one_i64);
                       ^~~~~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:21:21: error: implicit declaration of function ‘_mm512_reduce_mul_epi32’ [-Werror=implicit-function-declaration]
       sum_int += (int)_mm512_reduce_mul_epi32(one_i64);
                       ^~~~~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:23:16: error: implicit declaration of function ‘_mm512_reduce_min_ps’ [-Werror=implicit-function-declaration]
       sum_ps  += _mm512_reduce_min_ps(one_ps);
                  ^~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:24:16: error: implicit declaration of function ‘_mm512_reduce_min_pd’ [-Werror=implicit-function-declaration]
       sum_pd  += _mm512_reduce_min_pd(one_pd);
                  ^~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:25:21: error: implicit declaration of function ‘_mm512_reduce_min_epi32’ [-Werror=implicit-function-declaration]
       sum_int += (int)_mm512_reduce_min_epi32(one_i64);
                       ^~~~~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:26:21: error: implicit declaration of function ‘_mm512_reduce_min_epu32’ [-Werror=implicit-function-declaration]
       sum_int += (int)_mm512_reduce_min_epu32(one_i64);
                       ^~~~~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:27:21: error: implicit declaration of function ‘_mm512_reduce_min_epi64’ [-Werror=implicit-function-declaration]
       sum_int += (int)_mm512_reduce_min_epi64(one_i64);
                       ^~~~~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:29:16: error: implicit declaration of function ‘_mm512_reduce_max_ps’ [-Werror=implicit-function-declaration]
       sum_ps  += _mm512_reduce_max_ps(one_ps);
                  ^~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:30:16: error: implicit declaration of function ‘_mm512_reduce_max_pd’ [-Werror=implicit-function-declaration]
       sum_pd  += _mm512_reduce_max_pd(one_pd);
                  ^~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:31:21: error: implicit declaration of function ‘_mm512_reduce_max_epi32’ [-Werror=implicit-function-declaration]
       sum_int += (int)_mm512_reduce_max_epi32(one_i64);
                       ^~~~~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:32:21: error: implicit declaration of function ‘_mm512_reduce_max_epu32’ [-Werror=implicit-function-declaration]
       sum_int += (int)_mm512_reduce_max_epu32(one_i64);
                       ^~~~~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:33:21: error: implicit declaration of function ‘_mm512_reduce_max_epi64’ [-Werror=implicit-function-declaration]
       sum_int += (int)_mm512_reduce_max_epi64(one_i64);
                       ^~~~~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:35:21: error: implicit declaration of function ‘_mm512_reduce_and_epi32’ [-Werror=implicit-function-declaration]
       sum_int += (int)_mm512_reduce_and_epi32(one_i64);
                       ^~~~~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:36:21: error: implicit declaration of function ‘_mm512_reduce_and_epi64’ [-Werror=implicit-function-declaration]
       sum_int += (int)_mm512_reduce_and_epi64(one_i64);
                       ^~~~~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:38:21: error: implicit declaration of function ‘_mm512_reduce_or_epi32’ [-Werror=implicit-function-declaration]
       sum_int += (int)_mm512_reduce_or_epi32(one_i64);
                       ^~~~~~~~~~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512f_reduce.c:39:21: error: implicit declaration of function ‘_mm512_reduce_or_epi64’ [-Werror=implicit-function-declaration]
       sum_int += (int)_mm512_reduce_or_epi64(one_i64);
                       ^~~~~~~~~~~~~~~~~~~~~~
  cc1: all warnings being treated as errors

  CCompilerOpt.feature_extra_checks[1537] : testing failed for checks (AVX512F_REDUCE)
  CCompilerOpt.feature_extra_checks[1519] : Testing extra checks for feature 'AVX512_SKX' (AVX512BW_MASK AVX512DQ_MASK)
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq -Werror'
  CCompilerOpt.dist_test[576] : CCompilerOpt._dist_test_spawn[713] : Command (/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512bw_mask.c -o /scratch/16050576.tmpdir/tmpavv6wga5/scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512bw_mask.o -MMD -MF /scratch/16050576.tmpdir/tmpavv6wga5/scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512bw_mask.o.d -msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq -Werror) failed with exit status 1 output ->
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512bw_mask.c: In function ‘main’:
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512bw_mask.c:12:11: error: implicit declaration of function ‘_kor_mask64’ [-Werror=implicit-function-declaration]
       m64 = _kor_mask64(m64, m64);
             ^~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512bw_mask.c:13:11: error: implicit declaration of function ‘_kxor_mask64’ [-Werror=implicit-function-declaration]
       m64 = _kxor_mask64(m64, m64);
             ^~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512bw_mask.c:14:11: error: implicit declaration of function ‘_cvtu64_mask64’ [-Werror=implicit-function-declaration]
       m64 = _cvtu64_mask64(_cvtmask64_u64(m64));
             ^~~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512bw_mask.c:14:26: error: implicit declaration of function ‘_cvtmask64_u64’ [-Werror=implicit-function-declaration]
       m64 = _cvtu64_mask64(_cvtmask64_u64(m64));
                            ^~~~~~~~~~~~~~
  cc1: all warnings being treated as errors

  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq -Werror'
  CCompilerOpt.dist_test[576] : CCompilerOpt._dist_test_spawn[713] : Command (/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512dq_mask.c -o /scratch/16050576.tmpdir/tmpavv6wga5/scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512dq_mask.o -MMD -MF /scratch/16050576.tmpdir/tmpavv6wga5/scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512dq_mask.o.d -msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq -Werror) failed with exit status 1 output ->
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512dq_mask.c: In function ‘main’:
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512dq_mask.c:12:10: error: implicit declaration of function ‘_kor_mask8’ [-Werror=implicit-function-declaration]
       m8 = _kor_mask8(m8, m8);
            ^~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512dq_mask.c:13:10: error: implicit declaration of function ‘_kxor_mask8’ [-Werror=implicit-function-declaration]
       m8 = _kxor_mask8(m8, m8);
            ^~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512dq_mask.c:14:10: error: implicit declaration of function ‘_cvtu32_mask8’ [-Werror=implicit-function-declaration]
       m8 = _cvtu32_mask8(_cvtmask8_u32(m8));
            ^~~~~~~~~~~~~
  /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/numpy/distutils/checks/extra_avx512dq_mask.c:14:24: error: implicit declaration of function ‘_cvtmask8_u32’ [-Werror=implicit-function-declaration]
       m8 = _cvtu32_mask8(_cvtmask8_u32(m8));
                          ^~~~~~~~~~~~~
  cc1: all warnings being treated as errors

  CCompilerOpt.feature_extra_checks[1537] : testing failed for checks (AVX512BW_MASK AVX512DQ_MASK)
  Detected changes on compiler optimizations, force rebuilding
  building 'npymath' library
  compiling C sources
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  creating build/temp.linux-x86_64-3.7
  creating build/temp.linux-x86_64-3.7/numpy
  creating build/temp.linux-x86_64-3.7/numpy/core
  creating build/temp.linux-x86_64-3.7/numpy/core/src
  creating build/temp.linux-x86_64-3.7/numpy/core/src/npymath
  creating build/temp.linux-x86_64-3.7/build
  creating build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7
  creating build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy
  creating build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core
  creating build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src
  creating build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/npymath
  compile options: '-Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -Inumpy/core/include -Ibuild/src.linux-x86_64-3.7/numpy/core/include/numpy -Ibuild/src.linux-x86_64-3.7/numpy/distutils/include -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3'
  gcc: numpy/core/src/npymath/npy_math.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/npymath/ieee754.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/npymath/npy_math_complex.c
  gcc: numpy/core/src/npymath/halffloat.c
  ar: adding 4 object files to build/temp.linux-x86_64-3.7/libnpymath.a
  building 'npyrandom' library
  compiling C sources
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  creating build/temp.linux-x86_64-3.7/numpy/random
  creating build/temp.linux-x86_64-3.7/numpy/random/src
  creating build/temp.linux-x86_64-3.7/numpy/random/src/distributions
  compile options: '-Inumpy/core/include -Ibuild/src.linux-x86_64-3.7/numpy/core/include/numpy -Ibuild/src.linux-x86_64-3.7/numpy/distutils/include -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3'
  gcc: numpy/random/src/distributions/logfactorial.c
  gcc: numpy/random/src/distributions/distributions.c
  gcc: numpy/random/src/distributions/random_mvhg_count.c
  gcc: numpy/random/src/distributions/random_mvhg_marginals.c
  gcc: numpy/random/src/distributions/random_hypergeometric.c
  ar: adding 5 object files to build/temp.linux-x86_64-3.7/libnpyrandom.a
  running build_ext
  customize UnixCCompiler
  customize UnixCCompiler using new_build_ext
  CCompilerOpt.__init__[781] : hit the memory cache
  CCompilerOpt.generate_dispatch_header[2236] : generate CPU dispatch header: (build/src.linux-x86_64-3.7/numpy/distutils/include/npy_cpu_dispatch_config.h)
  Detected changes on compiler optimizations, force rebuilding
  building 'numpy.core._multiarray_tests' extension
  compiling C sources
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  creating build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/multiarray
  creating build/temp.linux-x86_64-3.7/numpy/core/src/common
  compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.linux-x86_64-3.7/numpy/core/include/numpy -Ibuild/src.linux-x86_64-3.7/numpy/distutils/include -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3'
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/multiarray/_multiarray_tests.c
  gcc: numpy/core/src/common/mem_overlap.c
  /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -pthread -shared build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/multiarray/_multiarray_tests.o build/temp.linux-x86_64-3.7/numpy/core/src/common/mem_overlap.o -L/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/lib64 -Lbuild/temp.linux-x86_64-3.7 -lnpymath -lpython3.7m -o build/lib.linux-x86_64-3.7/numpy/core/_multiarray_tests.cpython-37m-x86_64-linux-gnu.so
  building 'numpy.core._multiarray_umath' extension
  compiling C dispatch-able sources
  CCompilerOpt.parse_targets[1741] : looking for '@targets' inside ->  build/src.linux-x86_64-3.7/numpy/core/src/umath/loops_unary_fp.dispatch.c
  CCompilerOpt._parse_target_tokens[1912] : skip targets (VSX2 NEON) not part of baseline or dispatch-able features
  CCompilerOpt._parse_policy_not_keepbase[2022] : skip baseline features (SSE2)
  CCompilerOpt._parse_target_tokens[1934] : policy 'MAXOPT' is ON
  CCompilerOpt._generate_config[2469] : generate dispatched config ->  build/src.linux-x86_64-3.7/numpy/core/src/umath/loops_unary_fp.dispatch.h
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  creating build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/umath
  compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DHAVE_CBLAS -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/umath -Inumpy/core/include -Ibuild/src.linux-x86_64-3.7/numpy/core/include/numpy -Ibuild/src.linux-x86_64-3.7/numpy/distutils/include -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-O3 -msse -msse2 -msse3'
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/umath/loops_unary_fp.dispatch.c
  compiling C sources
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  creating build/temp.linux-x86_64-3.7/numpy/core/src/multiarray
  creating build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/npysort
  creating build/temp.linux-x86_64-3.7/numpy/core/src/umath
  creating build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/common
  compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DHAVE_CBLAS -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/umath -Inumpy/core/include -Ibuild/src.linux-x86_64-3.7/numpy/core/include/numpy -Ibuild/src.linux-x86_64-3.7/numpy/distutils/include -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3'
  gcc: numpy/core/src/multiarray/abstractdtypes.c
  gcc: numpy/core/src/multiarray/calculation.c
  gcc: numpy/core/src/multiarray/alloc.c
  gcc: numpy/core/src/multiarray/arrayobject.c
  gcc: numpy/core/src/multiarray/compiled_base.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/multiarray/arraytypes.c
  gcc: numpy/core/src/multiarray/common.c
  gcc: numpy/core/src/multiarray/convert.c
  gcc: numpy/core/src/multiarray/convert_datatype.c
  gcc: numpy/core/src/multiarray/conversion_utils.c
  gcc: numpy/core/src/multiarray/ctors.c
  gcc: numpy/core/src/multiarray/datetime.c
  gcc: numpy/core/src/multiarray/datetime_strings.c
  gcc: numpy/core/src/multiarray/datetime_busday.c
  gcc: numpy/core/src/multiarray/datetime_busdaycal.c
  gcc: numpy/core/src/multiarray/descriptor.c
  gcc: numpy/core/src/multiarray/dtypemeta.c
  gcc: numpy/core/src/multiarray/dragon4.c
  gcc: numpy/core/src/multiarray/dtype_transfer.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/multiarray/einsum.c
  gcc: numpy/core/src/multiarray/array_coercion.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/multiarray/einsum_sumprod.c
  gcc: numpy/core/src/multiarray/array_method.c
  gcc: numpy/core/src/multiarray/array_assign_scalar.c
  gcc: numpy/core/src/multiarray/array_assign_array.c
  gcc: numpy/core/src/multiarray/arrayfunction_override.c
  gcc: numpy/core/src/multiarray/buffer.c
  gcc: numpy/core/src/multiarray/item_selection.c
  gcc: numpy/core/src/multiarray/iterators.c
  gcc: numpy/core/src/multiarray/legacy_dtype_implementation.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/multiarray/lowlevel_strided_loops.c
  gcc: numpy/core/src/multiarray/flagsobject.c
  gcc: numpy/core/src/multiarray/getset.c
  gcc: numpy/core/src/multiarray/hashdescr.c
  gcc: numpy/core/src/multiarray/nditer_pywrap.c
  gcc: numpy/core/src/multiarray/number.c
  gcc: numpy/core/src/multiarray/refcount.c
  gcc: numpy/core/src/multiarray/sequence.c
  gcc: numpy/core/src/multiarray/shape.c
  gcc: numpy/core/src/multiarray/scalarapi.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/multiarray/scalartypes.c
  gcc: numpy/core/src/multiarray/strfuncs.c
  gcc: numpy/core/src/multiarray/temp_elide.c
  gcc: numpy/core/src/multiarray/typeinfo.c
  gcc: numpy/core/src/multiarray/usertypes.c
  gcc: numpy/core/src/multiarray/vdot.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/npysort/quicksort.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/npysort/mergesort.c
  gcc: numpy/core/src/multiarray/mapping.c
  gcc: numpy/core/src/multiarray/methods.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/npysort/timsort.c
  gcc: numpy/core/src/multiarray/multiarraymodule.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/multiarray/nditer_templ.c
  gcc: numpy/core/src/multiarray/nditer_api.c
  gcc: numpy/core/src/multiarray/nditer_constr.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/npysort/heapsort.c
  gcc: numpy/core/src/umath/reduction.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/umath/loops.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/npysort/radixsort.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/npysort/selection.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/npysort/binsearch.c
  gcc: numpy/core/src/umath/umathmodule.c
  gcc: numpy/core/src/common/mem_overlap.c
  gcc: numpy/core/src/common/npy_longdouble.c
  gcc: numpy/core/src/common/ucsnarrow.c
  gcc: numpy/core/src/common/ufunc_override.c
  gcc: numpy/core/src/common/numpyos.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/common/npy_cpu_features.c
  gcc: numpy/core/src/common/cblasfuncs.c
  gcc: numpy/core/src/common/python_xerbla.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/umath/matmul.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/umath/clip.c
  gcc: numpy/core/src/umath/ufunc_object.c
  gcc: numpy/core/src/umath/extobj.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/umath/scalarmath.c
  gcc: numpy/core/src/umath/ufunc_type_resolution.c
  gcc: numpy/core/src/umath/override.c
  gcc: numpy/core/src/common/array_assign.c
  /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -pthread -shared build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/umath/loops_unary_fp.dispatch.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/abstractdtypes.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/alloc.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/arrayobject.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/multiarray/arraytypes.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/array_coercion.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/array_method.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/array_assign_scalar.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/array_assign_array.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/arrayfunction_override.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/buffer.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/calculation.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/compiled_base.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/common.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/convert.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/convert_datatype.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/conversion_utils.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/ctors.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/datetime.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/datetime_strings.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/datetime_busday.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/datetime_busdaycal.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/descriptor.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/dtypemeta.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/dragon4.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/dtype_transfer.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/multiarray/einsum.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/multiarray/einsum_sumprod.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/flagsobject.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/getset.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/hashdescr.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/item_selection.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/iterators.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/legacy_dtype_implementation.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/multiarray/lowlevel_strided_loops.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/mapping.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/methods.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/multiarraymodule.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/multiarray/nditer_templ.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/nditer_api.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/nditer_constr.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/nditer_pywrap.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/number.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/refcount.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/sequence.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/shape.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/scalarapi.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/multiarray/scalartypes.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/strfuncs.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/temp_elide.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/typeinfo.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/usertypes.o build/temp.linux-x86_64-3.7/numpy/core/src/multiarray/vdot.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/npysort/quicksort.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/npysort/mergesort.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/npysort/timsort.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/npysort/heapsort.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/npysort/radixsort.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/npysort/selection.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/npysort/binsearch.o build/temp.linux-x86_64-3.7/numpy/core/src/umath/umathmodule.o build/temp.linux-x86_64-3.7/numpy/core/src/umath/reduction.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/umath/loops.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/umath/matmul.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/umath/clip.o build/temp.linux-x86_64-3.7/numpy/core/src/umath/ufunc_object.o build/temp.linux-x86_64-3.7/numpy/core/src/umath/extobj.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/umath/scalarmath.o build/temp.linux-x86_64-3.7/numpy/core/src/umath/ufunc_type_resolution.o build/temp.linux-x86_64-3.7/numpy/core/src/umath/override.o build/temp.linux-x86_64-3.7/numpy/core/src/common/array_assign.o build/temp.linux-x86_64-3.7/numpy/core/src/common/mem_overlap.o build/temp.linux-x86_64-3.7/numpy/core/src/common/npy_longdouble.o build/temp.linux-x86_64-3.7/numpy/core/src/common/ucsnarrow.o build/temp.linux-x86_64-3.7/numpy/core/src/common/ufunc_override.o build/temp.linux-x86_64-3.7/numpy/core/src/common/numpyos.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/common/npy_cpu_features.o build/temp.linux-x86_64-3.7/numpy/core/src/common/cblasfuncs.o build/temp.linux-x86_64-3.7/numpy/core/src/common/python_xerbla.o -L/cluster/apps/gcc-6.3.0/openblas-0.2.19-jusdys62qhv52qjbsoewvthriekuaynw/lib -L/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/lib64 -Lbuild/temp.linux-x86_64-3.7 -Wl,--enable-new-dtags,-R/cluster/apps/gcc-6.3.0/openblas-0.2.19-jusdys62qhv52qjbsoewvthriekuaynw/lib -lnpymath -lopenblas -lopenblas -lm -lpython3.7m -o build/lib.linux-x86_64-3.7/numpy/core/_multiarray_umath.cpython-37m-x86_64-linux-gnu.so
  building 'numpy.core._umath_tests' extension
  compiling C dispatch-able sources
  CCompilerOpt.parse_targets[1741] : looking for '@targets' inside ->  numpy/core/src/umath/_umath_tests.dispatch.c
  CCompilerOpt._parse_target_tokens[1912] : skip targets (VSX3 VSX NEON VSX2 ASIMD ASIMDHP) not part of baseline or dispatch-able features
  CCompilerOpt._parse_policy_not_keepbase[2022] : skip baseline features (SSE2)
  CCompilerOpt._parse_target_tokens[1934] : policy 'WERROR' is ON
  CCompilerOpt._parse_policy_werror[2066] : compiler warnings are treated as errors
  CCompilerOpt._generate_config[2469] : generate dispatched config ->  build/src.linux-x86_64-3.7/numpy/core/src/umath/_umath_tests.dispatch.h
  CCompilerOpt._wrap_target[2432] : wrap dispatch-able target ->  build/src.linux-x86_64-3.7/numpy/core/src/umath/_umath_tests.dispatch.avx2.c
  CCompilerOpt._wrap_target[2432] : wrap dispatch-able target ->  build/src.linux-x86_64-3.7/numpy/core/src/umath/_umath_tests.dispatch.sse41.c
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.linux-x86_64-3.7/numpy/core/include/numpy -Ibuild/src.linux-x86_64-3.7/numpy/distutils/include -Ibuild/src.linux-x86_64-3.7/numpy/core/src/umath -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-Werror -msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mavx2'
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/umath/_umath_tests.dispatch.avx2.c
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.linux-x86_64-3.7/numpy/core/include/numpy -Ibuild/src.linux-x86_64-3.7/numpy/distutils/include -Ibuild/src.linux-x86_64-3.7/numpy/core/src/umath -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-Werror -msse -msse2 -msse3 -mssse3 -msse4.1'
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/umath/_umath_tests.dispatch.sse41.c
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.linux-x86_64-3.7/numpy/core/include/numpy -Ibuild/src.linux-x86_64-3.7/numpy/distutils/include -Ibuild/src.linux-x86_64-3.7/numpy/core/src/umath -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-Werror -msse -msse2 -msse3'
  gcc: numpy/core/src/umath/_umath_tests.dispatch.c
  compiling C sources
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.linux-x86_64-3.7/numpy/core/include/numpy -Ibuild/src.linux-x86_64-3.7/numpy/distutils/include -Ibuild/src.linux-x86_64-3.7/numpy/core/src/umath -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3'
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/umath/_umath_tests.c
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/common/npy_cpu_features.c
  /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -pthread -shared build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/umath/_umath_tests.dispatch.avx2.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/umath/_umath_tests.dispatch.sse41.o build/temp.linux-x86_64-3.7/numpy/core/src/umath/_umath_tests.dispatch.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/umath/_umath_tests.o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/common/npy_cpu_features.o -L/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/lib64 -Lbuild/temp.linux-x86_64-3.7 -lpython3.7m -o build/lib.linux-x86_64-3.7/numpy/core/_umath_tests.cpython-37m-x86_64-linux-gnu.so
  building 'numpy.core._rational_tests' extension
  compiling C sources
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.linux-x86_64-3.7/numpy/core/include/numpy -Ibuild/src.linux-x86_64-3.7/numpy/distutils/include -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3'
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/umath/_rational_tests.c
  /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -pthread -shared build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/umath/_rational_tests.o -L/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/lib64 -Lbuild/temp.linux-x86_64-3.7 -lpython3.7m -o build/lib.linux-x86_64-3.7/numpy/core/_rational_tests.cpython-37m-x86_64-linux-gnu.so
  building 'numpy.core._struct_ufunc_tests' extension
  compiling C sources
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.linux-x86_64-3.7/numpy/core/include/numpy -Ibuild/src.linux-x86_64-3.7/numpy/distutils/include -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3'
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/umath/_struct_ufunc_tests.c
  /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -pthread -shared build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/umath/_struct_ufunc_tests.o -L/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/lib64 -Lbuild/temp.linux-x86_64-3.7 -lpython3.7m -o build/lib.linux-x86_64-3.7/numpy/core/_struct_ufunc_tests.cpython-37m-x86_64-linux-gnu.so
  building 'numpy.core._operand_flag_tests' extension
  compiling C sources
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.linux-x86_64-3.7/numpy/core/include/numpy -Ibuild/src.linux-x86_64-3.7/numpy/distutils/include -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-msse -msse2 -msse3'
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/umath/_operand_flag_tests.c
  /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -pthread -shared build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/umath/_operand_flag_tests.o -L/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/lib64 -Lbuild/temp.linux-x86_64-3.7 -lpython3.7m -o build/lib.linux-x86_64-3.7/numpy/core/_operand_flag_tests.cpython-37m-x86_64-linux-gnu.so
  building 'numpy.core._simd' extension
  compiling C dispatch-able sources
  CCompilerOpt.parse_targets[1741] : looking for '@targets' inside ->  build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.c
  CCompilerOpt._parse_target_tokens[1934] : policy 'WERROR' is ON
  CCompilerOpt._parse_policy_werror[2066] : compiler warnings are treated as errors
  CCompilerOpt._generate_config[2469] : generate dispatched config ->  build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.h
  CCompilerOpt._wrap_target[2432] : wrap dispatch-able target ->  build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.c
  CCompilerOpt._wrap_target[2432] : wrap dispatch-able target ->  build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512f.c
  CCompilerOpt._wrap_target[2432] : wrap dispatch-able target ->  build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.fma3.avx2.c
  CCompilerOpt._wrap_target[2432] : wrap dispatch-able target ->  build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.sse42.c
  C compiler: /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC

  creating build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/_simd
  compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Ibuild/src.linux-x86_64-3.7/numpy/core/src/_simd -Inumpy/core/include -Ibuild/src.linux-x86_64-3.7/numpy/core/include/numpy -Ibuild/src.linux-x86_64-3.7/numpy/distutils/include -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c'
  extra options: '-Werror -msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq'
  gcc: build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.c
  In file included from numpy/core/src/common/simd/avx512/avx512.h:69:0,
                   from numpy/core/src/common/simd/simd.h:31,
                   from numpy/core/src/_simd/_simd_inc.h.src:5,
                   from numpy/core/src/_simd/_simd.dispatch.c.src:3,
                   from build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.c:21:
  numpy/core/src/common/simd/avx512/memory.h: In function ‘npyv_loadn_u32’:
  numpy/core/src/common/simd/avx512/memory.h:104:40: error: passing argument 2 of ‘_mm512_i32gather_epi32’ from incompatible pointer type [-Werror=incompatible-pointer-types]
       return _mm512_i32gather_epi32(idx, (const __m512i*)ptr, 4);
                                          ^
  In file included from /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/immintrin.h:45:0,
                   from build/src.linux-x86_64-3.7/numpy/distutils/include/npy_cpu_dispatch_config.h:62,
                   from numpy/core/src/common/npy_cpu_dispatch.h:36,
                   from numpy/core/src/common/npy_config.h:6,
                   from numpy/core/include/numpy/npy_common.h:10,
                   from numpy/core/src/_simd/_simd.h:16,
                   from numpy/core/src/_simd/_simd.dispatch.c.src:2,
                   from build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.c:21:
  /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/avx512fintrin.h:9308:1: note: expected ‘const int *’ but argument is of type ‘const __vector(8) long long int *’
   _mm512_i32gather_epi32 (__m512i __index, int const *__addr, int __scale)
   ^~~~~~~~~~~~~~~~~~~~~~
  In file included from numpy/core/src/common/simd/avx512/avx512.h:69:0,
                   from numpy/core/src/common/simd/simd.h:31,
                   from numpy/core/src/_simd/_simd_inc.h.src:5,
                   from numpy/core/src/_simd/_simd.dispatch.c.src:3,
                   from build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.c:21:
  numpy/core/src/common/simd/avx512/memory.h: In function ‘npyv_loadn_u64’:
  numpy/core/src/common/simd/avx512/memory.h:117:40: error: passing argument 2 of ‘_mm512_i64gather_epi64’ from incompatible pointer type [-Werror=incompatible-pointer-types]
       return _mm512_i64gather_epi64(idx, (const __m512i*)ptr, 8);
                                          ^
  In file included from /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/immintrin.h:45:0,
                   from build/src.linux-x86_64-3.7/numpy/distutils/include/npy_cpu_dispatch_config.h:62,
                   from numpy/core/src/common/npy_cpu_dispatch.h:36,
                   from numpy/core/src/common/npy_config.h:6,
                   from numpy/core/include/numpy/npy_common.h:10,
                   from numpy/core/src/_simd/_simd.h:16,
                   from numpy/core/src/_simd/_simd.dispatch.c.src:2,
                   from build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.c:21:
  /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/avx512fintrin.h:9381:1: note: expected ‘const long long int *’ but argument is of type ‘const __vector(8) long long int *’
   _mm512_i64gather_epi64 (__m512i __index, long long const *__addr, int __scale)
   ^~~~~~~~~~~~~~~~~~~~~~
  In file included from numpy/core/src/common/simd/avx512/avx512.h:69:0,
                   from numpy/core/src/common/simd/simd.h:31,
                   from numpy/core/src/_simd/_simd_inc.h.src:5,
                   from numpy/core/src/_simd/_simd.dispatch.c.src:3,
                   from build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.c:21:
  numpy/core/src/common/simd/avx512/memory.h: In function ‘npyv_storen_u32’:
  numpy/core/src/common/simd/avx512/memory.h:134:29: error: passing argument 1 of ‘_mm512_i32scatter_epi32’ from incompatible pointer type [-Werror=incompatible-pointer-types]
       _mm512_i32scatter_epi32((__m512i*)ptr, idx, a, 4);
                               ^
  In file included from /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/immintrin.h:45:0,
                   from build/src.linux-x86_64-3.7/numpy/distutils/include/npy_cpu_dispatch_config.h:62,
                   from numpy/core/src/common/npy_cpu_dispatch.h:36,
                   from numpy/core/src/common/npy_config.h:6,
                   from numpy/core/include/numpy/npy_common.h:10,
                   from numpy/core/src/_simd/_simd.h:16,
                   from numpy/core/src/_simd/_simd.dispatch.c.src:2,
                   from build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.c:21:
  /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/avx512fintrin.h:9476:1: note: expected ‘int *’ but argument is of type ‘__vector(8) long long int *’
   _mm512_i32scatter_epi32 (int *__addr, __m512i __index,
   ^~~~~~~~~~~~~~~~~~~~~~~
  In file included from numpy/core/src/common/simd/avx512/avx512.h:69:0,
                   from numpy/core/src/common/simd/simd.h:31,
                   from numpy/core/src/_simd/_simd_inc.h.src:5,
                   from numpy/core/src/_simd/_simd.dispatch.c.src:3,
                   from build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.c:21:
  numpy/core/src/common/simd/avx512/memory.h: In function ‘npyv_storen_u64’:
  numpy/core/src/common/simd/avx512/memory.h:147:29: error: passing argument 1 of ‘_mm512_i64scatter_epi64’ from incompatible pointer type [-Werror=incompatible-pointer-types]
       _mm512_i64scatter_epi64((__m512i*)ptr, idx, a, 8);
                               ^
  In file included from /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/immintrin.h:45:0,
                   from build/src.linux-x86_64-3.7/numpy/distutils/include/npy_cpu_dispatch_config.h:62,
                   from numpy/core/src/common/npy_cpu_dispatch.h:36,
                   from numpy/core/src/common/npy_config.h:6,
                   from numpy/core/include/numpy/npy_common.h:10,
                   from numpy/core/src/_simd/_simd.h:16,
                   from numpy/core/src/_simd/_simd.dispatch.c.src:2,
                   from build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.c:21:
  /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/avx512fintrin.h:9530:1: note: expected ‘long long int *’ but argument is of type ‘__vector(8) long long int *’
   _mm512_i64scatter_epi64 (long long *__addr, __m512i __index,
   ^~~~~~~~~~~~~~~~~~~~~~~
  In file included from numpy/core/src/common/simd/avx512/avx512.h:69:0,
                   from numpy/core/src/common/simd/simd.h:31,
                   from numpy/core/src/_simd/_simd_inc.h.src:5,
                   from numpy/core/src/_simd/_simd.dispatch.c.src:3,
                   from build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.c:21:
  numpy/core/src/common/simd/avx512/memory.h: In function ‘npyv_loadn_till_s32’:
  numpy/core/src/common/simd/avx512/memory.h:202:58: error: passing argument 4 of ‘_mm512_mask_i32gather_epi32’ from incompatible pointer type [-Werror=incompatible-pointer-types]
       return _mm512_mask_i32gather_epi32(vfill, mask, idx, (const __m512i*)ptr, 4);
                                                            ^
  In file included from /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/immintrin.h:45:0,
                   from build/src.linux-x86_64-3.7/numpy/distutils/include/npy_cpu_dispatch_config.h:62,
                   from numpy/core/src/common/npy_cpu_dispatch.h:36,
                   from numpy/core/src/common/npy_config.h:6,
                   from numpy/core/include/numpy/npy_common.h:10,
                   from numpy/core/src/_simd/_simd.h:16,
                   from numpy/core/src/_simd/_simd.dispatch.c.src:2,
                   from build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.c:21:
  /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/avx512fintrin.h:9321:1: note: expected ‘const int *’ but argument is of type ‘const __vector(8) long long int *’
   _mm512_mask_i32gather_epi32 (__m512i __v1_old, __mmask16 __mask,
   ^~~~~~~~~~~~~~~~~~~~~~~~~~~
  In file included from numpy/core/src/common/simd/avx512/avx512.h:69:0,
                   from numpy/core/src/common/simd/simd.h:31,
                   from numpy/core/src/_simd/_simd_inc.h.src:5,
                   from numpy/core/src/_simd/_simd.dispatch.c.src:3,
                   from build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.c:21:
  numpy/core/src/common/simd/avx512/memory.h: In function ‘npyv_loadn_till_s64’:
  numpy/core/src/common/simd/avx512/memory.h:219:58: error: passing argument 4 of ‘_mm512_mask_i64gather_epi64’ from incompatible pointer type [-Werror=incompatible-pointer-types]
       return _mm512_mask_i64gather_epi64(vfill, mask, idx, (const __m512i*)ptr, 8);
                                                            ^
  In file included from /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/immintrin.h:45:0,
                   from build/src.linux-x86_64-3.7/numpy/distutils/include/npy_cpu_dispatch_config.h:62,
                   from numpy/core/src/common/npy_cpu_dispatch.h:36,
                   from numpy/core/src/common/npy_config.h:6,
                   from numpy/core/include/numpy/npy_common.h:10,
                   from numpy/core/src/_simd/_simd.h:16,
                   from numpy/core/src/_simd/_simd.dispatch.c.src:2,
                   from build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.c:21:
  /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/avx512fintrin.h:9394:1: note: expected ‘const long long int *’ but argument is of type ‘const __vector(8) long long int *’
   _mm512_mask_i64gather_epi64 (__m512i __v1_old, __mmask8 __mask,
   ^~~~~~~~~~~~~~~~~~~~~~~~~~~
  In file included from numpy/core/src/common/simd/avx512/avx512.h:69:0,
                   from numpy/core/src/common/simd/simd.h:31,
                   from numpy/core/src/_simd/_simd_inc.h.src:5,
                   from numpy/core/src/_simd/_simd.dispatch.c.src:3,
                   from build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.c:21:
  numpy/core/src/common/simd/avx512/memory.h: In function ‘npyv_storen_till_s32’:
  numpy/core/src/common/simd/avx512/memory.h:255:34: error: passing argument 1 of ‘_mm512_mask_i32scatter_epi32’ from incompatible pointer type [-Werror=incompatible-pointer-types]
       _mm512_mask_i32scatter_epi32((__m512i*)ptr, mask, idx, a, 4);
                                    ^
  In file included from /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/immintrin.h:45:0,
                   from build/src.linux-x86_64-3.7/numpy/distutils/include/npy_cpu_dispatch_config.h:62,
                   from numpy/core/src/common/npy_cpu_dispatch.h:36,
                   from numpy/core/src/common/npy_config.h:6,
                   from numpy/core/include/numpy/npy_common.h:10,
                   from numpy/core/src/_simd/_simd.h:16,
                   from numpy/core/src/_simd/_simd.dispatch.c.src:2,
                   from build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.c:21:
  /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/avx512fintrin.h:9485:1: note: expected ‘int *’ but argument is of type ‘__vector(8) long long int *’
   _mm512_mask_i32scatter_epi32 (int *__addr, __mmask16 __mask,
   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
  In file included from numpy/core/src/common/simd/avx512/avx512.h:69:0,
                   from numpy/core/src/common/simd/simd.h:31,
                   from numpy/core/src/_simd/_simd_inc.h.src:5,
                   from numpy/core/src/_simd/_simd.dispatch.c.src:3,
                   from build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.c:21:
  numpy/core/src/common/simd/avx512/memory.h: In function ‘npyv_storen_till_s64’:
  numpy/core/src/common/simd/avx512/memory.h:266:34: error: passing argument 1 of ‘_mm512_mask_i64scatter_epi64’ from incompatible pointer type [-Werror=incompatible-pointer-types]
       _mm512_mask_i64scatter_epi64((__m512i*)ptr, mask, idx, a, 8);
                                    ^
  In file included from /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/immintrin.h:45:0,
                   from build/src.linux-x86_64-3.7/numpy/distutils/include/npy_cpu_dispatch_config.h:62,
                   from numpy/core/src/common/npy_cpu_dispatch.h:36,
                   from numpy/core/src/common/npy_config.h:6,
                   from numpy/core/include/numpy/npy_common.h:10,
                   from numpy/core/src/_simd/_simd.h:16,
                   from numpy/core/src/_simd/_simd.dispatch.c.src:2,
                   from build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.c:21:
  /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/avx512fintrin.h:9539:1: note: expected ‘long long int *’ but argument is of type ‘__vector(8) long long int *’
   _mm512_mask_i64scatter_epi64 (long long *__addr, __mmask8 __mask,
   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
  cc1: all warnings being treated as errors
  Running from numpy source directory.
  /cluster/apps/python/3.7.4_gpu_gcc630/x86_64/lib64/python3.7/distutils/dist.py:274: UserWarning: Unknown distribution option: 'define_macros'
    warnings.warn(msg)

  ########### EXT COMPILER OPTIMIZATION ###########
  Platform      :
    Architecture: x64
    Compiler    : gcc

  CPU baseline  :
    Requested   : 'min'
    Enabled     : SSE SSE2 SSE3
    Flags       : -msse -msse2 -msse3
    Extra checks: none

  CPU dispatch  :
    Requested   : 'max -xop -fma4'
    Enabled     : SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512F AVX512CD AVX512_KNL AVX512_SKX AVX512_CNL
    Generated   :
                :
    SSE41       : SSE SSE2 SSE3 SSSE3
    Flags       : -msse -msse2 -msse3 -mssse3 -msse4.1
    Extra checks: none
    Detect      : SSE SSE2 SSE3 SSSE3 SSE41
                : numpy/core/src/umath/_umath_tests.dispatch.c
                :
    SSE42       : SSE SSE2 SSE3 SSSE3 SSE41 POPCNT
    Flags       : -msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2
    Extra checks: none
    Detect      : SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42
                : build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.c
                :
    AVX2        : SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C
    Flags       : -msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mavx2
    Extra checks: none
    Detect      : AVX F16C AVX2
                : numpy/core/src/umath/_umath_tests.dispatch.c
                :
    (FMA3 AVX2) : SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C
    Flags       : -msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2
    Extra checks: none
    Detect      : AVX F16C FMA3 AVX2
                : build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.c
                :
    AVX512F     : SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2
    Flags       : -msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f
    Extra checks: none
    Detect      : AVX512F
                : build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.c
                :
    AVX512_SKX  : SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512F AVX512CD
    Flags       : -msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq
    Extra checks: none
    Detect      : AVX512_SKX
                : build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.c
  CCompilerOpt._cache_write[796] : write cache to path -> /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/build/temp.linux-x86_64-3.7/ccompiler_opt_cache_ext.py

  ########### CLIB COMPILER OPTIMIZATION ###########
  Platform      :
    Architecture: x64
    Compiler    : gcc

  CPU baseline  :
    Requested   : 'min'
    Enabled     : SSE SSE2 SSE3
    Flags       : -msse -msse2 -msse3
    Extra checks: none

  CPU dispatch  :
    Requested   : 'max -xop -fma4'
    Enabled     : SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512F AVX512CD AVX512_KNL AVX512_SKX AVX512_CNL
    Generated   : none
  CCompilerOpt._cache_write[796] : write cache to path -> /scratch/16050576.tmpdir/pip-install-3tn44a7e/numpy_7e1ab146f6fd41929bbb08a457339cfc/build/temp.linux-x86_64-3.7/ccompiler_opt_cache_clib.py
  error: Command ""/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-d7fey5ouwb3cw26w5zqafqcse7q3fs6d/bin/gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -ftree-vectorize -march=core-avx2 -mavx2 -ftree-vectorize -march=core-avx2 -mavx2 -fPIC -DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Ibuild/src.linux-x86_64-3.7/numpy/core/src/_simd -Inumpy/core/include -Ibuild/src.linux-x86_64-3.7/numpy/core/include/numpy -Ibuild/src.linux-x86_64-3.7/numpy/distutils/include -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -Inumpy/core/src/_simd -I/cluster/home/sfux/pytorch171cu101/include -I/cluster/apps/python/3.7.4_gpu_gcc630/x86_64/include/python3.7m -Ibuild/src.linux-x86_64-3.7/numpy/core/src/common -Ibuild/src.linux-x86_64-3.7/numpy/core/src/npymath -c build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.c -o build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.o -MMD -MF build/temp.linux-x86_64-3.7/build/src.linux-x86_64-3.7/numpy/core/src/_simd/_simd.dispatch.avx512_skx.o.d -Werror -msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq"" failed with exit status 1
  ----------------------------------------
  ERROR: Failed building wheel for numpy
Failed to build numpy
ERROR: Could not build wheels for numpy which use PEP 517 and cannot be installed directly
(pytorch171cu101) [sfux@lo-s4-009 ~]$
```
</details>

### NumPy/Python version information:
Python: 3.7.4
GCC: 6.3.0
OS: CentOS 7.9
CPU: Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz
Supported SIMD instructions:
fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 invpcid_single intel_ppin intel_pt ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts spec_ctrl intel_stibp flush_l1d

",2021-05-04 08:38:43,,DOC: improve documentation on build flags related to SIMD (`--cpu-baseline` & co),"['04 - Documentation', 'component: SIMD']"
18898,open,iramazanli,"In the following example, we can see 

```
>>> import numpy as np
>>> np.gradient(np.array([[1, 2], [3, 4]]), 1+2j)
[array([[0.4, 0.4],
       [0.4, 0.4]]), array([[0.2, 0.2],
       [0.2, 0.2]])]
```

However,  naturally we would expect that resulting gradient values to be complex.",2021-05-03 23:51:37,,numpy.gradient fails type promotion for real input array and complex spacing,['unlabeled']
18894,open,Templarrr,"## Feature

I've encountered quite a few cases in my work when it was necessary to count number of results of some ufunc that evaluates to True on the input array without knowing where. 
E.g. if mask_func = np.isnan I will get the number of NaNs in array, if np.isinf - number of infinities etc.

Currently the most runtime efficient way is `np.count_nonzero(mask_func(x))` but it creates bool array as a middle step that doesn't seems necessary in this usecase - not only it spends runtime constructing the array only to discard it right away, it also consumes memory which can be a problematic when you deal with huge arrays in memory.

Is it possible to implement this counter in a more memory efficient way - instead of all the hassle of creating intermittent array it will just increment int var for each calculated True and return the result.

When implemented outside the numpy in Python iteration over each element with np.nditer is much slower, so currently the most memory efficient way to do it without sacrificing performance  for me is chunking.

<!-- If you're looking to request a new feature or change in functionality, including
adding or changing the meaning of arguments to an existing function, please
post your idea on the [numpy-discussion mailing list]
(https://mail.python.org/mailman/listinfo/numpy-discussion) to explain your
reasoning in addition to opening an issue or pull request. You can also check
out our [Contributor Guide]
(https://github.com/numpy/numpy/blob/main/doc/source/dev/index.rst) if you
need more information. -->
",2021-05-03 14:55:18,,Memory-efficient alternative of np.count_nonzero(mask_func(x)),['unlabeled']
18881,open,asmeurer,"```py
>>> np.linspace(-9007199254740993, 0, 1, dtype=np.int64)
array([-9007199254740992])
>>> np.linspace(0, 9007199254740993, 2, dtype=np.uint64, endpoint=True)
array([               0, 9007199254740992], dtype=uint64)
>>> np.__version__
'1.21.0.dev0+1420.gc2dd42fda'
```

(notice that the last digit has been changed from a 3 to a 2)

It's not clear to me if this should be considered a bug, or if this is expected from the way linspace inherently has to do rounding. 

I found https://github.com/numpy/numpy/issues/16813 which seems related. ",2021-04-30 22:42:20,,linspace with int dtype sometimes doesn't include endpoints,"['00 - Bug', 'component: numpy._core']"
18879,open,timhoffm,"I noticed that numpy uses a shpinx config setting

https://github.com/numpy/numpy/blob/05b12102f93f18c0a8bbd86aa2e4a6fcbb554ab4/doc/source/conf.py#L125

This is poor documented at sphinx, but seems to be deprecated:
https://www.sphinx-doc.org/en/master/search.html?q=autolink

and is scheduled for removal in Sphinx 4.0.",2021-04-30 14:46:52,,"Default sphinx role ""autolink"" appears to be deprecated",['04 - Documentation']
18854,open,Sola85,"<!-- Please describe the issue in detail here, and fill in the fields below -->
When used in implicit mode, einsum sorts the output axis alphabetically. If the total number of indices exceeds 26, capital letters are used in addition to lower-case letters as indices. These capital-letter indices are sorted before lower-case indices, which can lead to inconsistent behaviour. Eg:

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
n = 27 # Change n here  
x = np.random.random((2,)*n)
print(np.einsum_path(x, range(n))[1])
```

For n <= 26 the einsum call corresponding to the above einsum_path does nothing. For n=27 the last 'input index' is assigned the letter 'A' which einsum then sorts as the first index. Thus for n>26 einsum permutes the axes.
This sudden change in behaviour is completely intransparent when indexing array axes using integers as above, rather than strings, and results in an absurd ordering of indices 27 < 28 < 29 < 1 < 2 < ... < 25 < 26

The above example is an extreme case, as 27-dimensional arrays will rarely be used. However, inconsistencies resulting from the same einsum behaviour can result even with much smaller arrays, simply if the total number of indices exceeds 26.
 
My suggestions would be either:
a) start indexing with 'A-Z' rather than 'a-z' or
b) manually sort capital indices after lower-case indices

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.18.5 3.8.5 (default, Jan 27 2021, 15:41:15) 
[GCC 9.3.0]

",2021-04-26 15:43:54,,BUG: Einsum implicit mode: alphabetical output axis sorting can lead to inconsistent behaviour for high-dimensional arrays,"['00 - Bug', 'component: numpy.einsum', 'sprintable']"
18840,open,bashtage,"## Feature

<!-- If you're looking to request a new feature or change in functionality, including
adding or changing the meaning of arguments to an existing function, please
post your idea on the [numpy-discussion mailing list]
(https://mail.python.org/mailman/listinfo/numpy-discussion) to explain your
reasoning in addition to opening an issue or pull request. You can also check
out our [Contributor Guide]
(https://github.com/numpy/numpy/blob/main/doc/source/dev/index.rst) if you
need more information. -->

Standard uniform random variables underly many other random algorithms. For example, `choice` makes use of standard uniforms when sampling with non-uniform probabilities. `Generator.random` already supports the `dtype` argument but is limited to `float32` and `float64`.  It would be useful for anyone working in higher precision to have support for `longdouble` and other extended precision values available on specific platform.

```
import numpy as np
gen = np.random.default_rng() 
gen.random(1, dtype=np.longdouble)
```
",2021-04-23 11:00:18,,ENH: Add support for extended precision standard uniform variables,"['01 - Enhancement', 'component: numpy.random']"
18832,open,jorisvandenbossche,"### Reproducing code example:

```python
import numpy as np
import pandas as pd
import datetime

>>> np.array([1.0, 2.0, 3.0]) + datetime.datetime.now()
...
TypeError: unsupported operand type(s) for +: 'float' and 'datetime.datetime'

>>> np.array([1.0, 2.0, 3.0]) + pd.Timestamp(""2021-01-01"")
...
TypeError: Concatenation operation is not implemented for NumPy arrays, use np.concatenate() instead. Please do not rely on this error; it may not be given on all Python implementations.
```

So for `datetime.datetime` object, this gives the expected TypeError. But for `pd.Timestamp` it gives a confusing error message.

I suppose the difference could stem from the fact that `pd.Timestamp` defines an `__array_priority__`. However, in this case it just returns `NotImplemented` when passed a float ndarray, so I would still expect to get the normal ""unsupported operand type"" TypeError.

(opening the issue here in numpy because the error message comes from numpy; of course it might also be that we need to change something in `Timestamp.__add__`, although as said above, it's returning `NotImplemented` in this case)

### NumPy/Python version information:

```
In [6]: import sys, numpy; print(numpy.__version__, sys.version)
1.20.1 3.8.6 | packaged by conda-forge | (default, Nov 27 2020, 19:31:52) 
[GCC 9.3.0]
```
",2021-04-22 11:43:38,,"Wrong error message for ""+"" (add) operation about ""use np.concontenate() instead"" for scalar with array priority",['00 - Bug']
18821,open,david-cortes,"From:
https://github.com/scikit-learn/scikit-learn/issues/19926

Using `np.ma.median` in a pandas DataFrame takes a lot longer than `np.nanmedian`. The issue seems to be about using a full sort rather than a partial argsort.

Example:
```python
import numpy as np, pandas as pd

rng = np.random.default_rng(seed=1)
nrows = int(1e6)
ncols = 10
X = pd.DataFrame(rng.normal(size=(nrows, ncols)))
```
```python
%%timeit
np.nanmedian(X, axis=0)
```
```
191 ms ± 3.98 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```
```python
%%timeit
np.ma.median(X, axis=0)
```
```
949 ms ± 4.99 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```


The problem does not arise when the input is a numpy array:
```python
X2 = rng.normal(size=(nrows, ncols))
```
```python
%%timeit
np.nanmedian(X2, axis=0)
```
```
188 ms ± 2.62 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```
```python
%%timeit
np.ma.median(X2, axis=0)
```
```
244 ms ± 6.83 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```",2021-04-20 15:28:44,,np.ma.median is inefficient with DataFrames,"['01 - Enhancement', 'component: numpy.ma']"
18798,open,alan-isaac,"In current numpy (1.20.2),  `np.choose` does not accept a `dtype` argument. This makes sense if the `choices` have an evident dtype, but they may not. Please consider accepting a `dtype` argument.",2021-04-17 15:31:24,,accept a dtype argument in np.choose,"['01 - Enhancement', 'component: numpy._core']"
18774,open,ivirshup,"`arr.astype(None)` converts `arr` to `float64`. I'd expect this to either error, or do nothing. Doc string says:

```
dtype : str or dtype
    Typecode or data-type to which the array is cast.
```

Which makes it seem like it should error.

### Reproducing code example:

```python
import numpy as np

assert np.ones(3, dtype=np.float32).astype(None).dtype != np.float64
```

### Error message:

```pytb
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-17-115497a92e9f> in <module>
----> 1 assert np.ones(3, dtype=np.float32).astype(None).dtype != np.float64

AssertionError: 
```

### NumPy/Python version _information:_

```
1.20.2 3.8.6 (default, Nov 20 2020, 18:29:40) 
[Clang 12.0.0 (clang-1200.0.32.27)]
```
",2021-04-14 04:51:56,,arr.astype(None) converts to float64,['unlabeled']
18767,open,pearu,"The issue has been brought up in several places:
- https://github.com/numpy/numpy/issues/18431#issuecomment-780303107
- https://github.com/numpy/numpy/pull/18759#discussion_r611998081
",2021-04-13 17:10:37,,f2py should use npy_intp instead of int for buffer size types.,"['component: numpy.f2py', '03 - Maintenance']"
18744,open,jpkrooney,"Under certain circumstances dividing a masked array by regular array with zeros seems to unexpactantly screen out `nan` and `inf` answers.

### Reproducing code example:
```python
import numpy as np
from numpy import ma

# Make masked and regular array
x = np.array([ 0.,  1., 0.,  1.])
xm = ma.masked_equal(x, -1)
y = np.array([ 0.,  0., 0.,  0.])
```
If we divide x by y we get:
```python
x/y
Out[239]: array([nan, inf, nan, inf])
```
If we divide xm by y we get:
```
xm/y
Out[240]: 
masked_array(data=[--, --, --, --],
             mask=[ True,  True,  True,  True],
       fill_value=-1.0,
            dtype=float64)
```
...it has masked the `nan` and `inf` values even though they are not -1

If we divide xm by y and get just the data we get:
```python
(xm/y).data
Out[242]: array([0., 1., 0., 1.])
```
🤯.. now we have data where I would expect `nan` and `inf`. I'm not very experienced in Python but this looks like an unexpected result and I thought I should report it (I spent alot of time tracing unexpected results from a function that turns out to be due to this).

Edit: Where I found this in the wild is even more insidious because there was no specific `.data` step - it happened silently as follows:

Suppose our calculation was done in a function:
```
def somefunc3(a,b):
    c = a / b
    return c

somefunc3(xm, y)
Out[76]: 
masked_array(data=[--, --, --, --],
             mask=[ True,  True,  True,  True],
       fill_value=-1.0,
            dtype=float64)
```
It output the masked array without nan and inf.

Now suppose we were stuffing the result of `somefunc` into a larger array:
```
d = np.zeros((4, 2))

d[:,0] = somefunc3(xm, y)
d
Out[77]: 
array([[0., 0.],
       [1., 0.],
       [0., 0.],
       [1., 0.]])
```
Now it silently converted the masked array back to a regular array and put in 1 or 0 when it should be `nan` or `inf`. Note that when I ran this on my machine I got a divide by zero warning only one time, but all other times I ran it I did not (I have no idea why).


### NumPy/Python version information:
1.18.1 3.7.6 (default, Jan  8 2020, 13:42:34) 
[Clang 4.0.1 (tags/RELEASE_401/final)]
Edit: same behaviour on my other machine with versions:
1.19.2 3.8.5
[Clang 10.0.0]",2021-04-08 08:08:32,,BUG (Possible): masked array divide by zero array seems to screen out nan and inf,['component: numpy.ma']
18700,open,sklam,"**EDIT (seberg): The original behavior change described here seems OK.  However, another issue was identified: https://github.com/numpy/numpy/issues/18700#issuecomment-810618183**

When the `out` parameter on ufunc is given, numpy 1.19 (and older versions) would always pass the values of the `out` array to the kernel. In numpy 1.20, that changed and a fresh uninitialized array is given when the type does not match exactly.

In other words... given a no-op ufunc (kernel does nothing) that takes 1 input and 1 output. And, it is called as:

```python
out = np.arange(10)
no_op_ufunc(inp, out=out)
```
Is the output guaranteed to retain the original values?


### Reproducing code example:

Reproducer from https://github.com/numba/numba/issues/6864

```python
import numba
import numpy as np

@numba.guvectorize([""void(float64[:], uint8[:])""], ""(n)->(n)"", nopython=True)
def func(x, out):
    
    for i in range(x.size):
        # set every fourth element to 1
        if i % 4 == 0:
            out[i] = 1

x = np.random.rand(150,150)
out = np.zeros_like(x, dtype=np.int8)  # dtype does not match expected

func(x, out)
```

with Numba 0.53 and Numpy 1.20.1, the result is:

```python
array([[  1,  49,  29, ...,  96,   1,   2],
       [  1,   0, -80, ..., -38,   1,  97],
       [  1,   2,   0, ...,   0,   1,   0],
       ...,
       [  1,  -1,  -1, ...,   0,   1,   0],
       [  1,   0,   0, ...,   0,   1,   0],
       [  1,   0,   0, ..., -34,   1,  97]], dtype=int8)
```
In np1.20, the skipped slots are containing random values.

with Numba 0.53 and Numpy 1.19.5, the result is:
```
array([[1, 0, 0, ..., 0, 1, 0],
       [1, 0, 0, ..., 0, 1, 0],
       [1, 0, 0, ..., 0, 1, 0],
       ...,
       [1, 0, 0, ..., 0, 1, 0],
       [1, 0, 0, ..., 0, 1, 0],
       [1, 0, 0, ..., 0, 1, 0]], dtype=int8)
```
In np1.19, the skipped slots are retaining the original zero values.

### Error message:

No error message. The problem is a change in behavior.

### NumPy/Python version information:

```python
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.20.1 3.9.2 (default, Mar  3 2021, 11:58:52)
[Clang 10.0.0 ]
```
",2021-03-30 17:08:47,,BUG: `where=` in gufuncs is slightly broken with `out=` and casts,['unlabeled']
18699,open,MarkusKoebis,"## Documentation

<!-- If this is an issue with the current documentation for NumPy (e.g.
incomplete/inaccurate docstring, unclear explanation in any part of the
documentation), make sure to leave a reference to the document/code you're
referring to. You can also check the development version of the documentation
and see if this issue has already been addressed: https://numpy.org/devdocs/
-->
If used with ""period is not None"", numpy.interpolate() does not bother whether the function is actually periodic. In my opinion, this can lead to difficult-to-track errors and the docstring should be adapted such that users are made aware of this.

A minimal example:
`numpy.interp([0, 1], [0,1], [0,1], period=1.0)# returns numpy.array([1., 1.]) (in numpy 1.18.5) which is okay if you know what to expect but you could as well expect to get the ""given values""`

An alternative could be to raise a warning/exception if such a situation occurs but then (I think) you open a Pandora box as the complexity of what counts as ""wrong input"" is difficult to foresee.

<!-- If this is an idea or a request for content, please describe as clearly as
possible what topics you think are missing from the current documentation. Make
sure to check https://github.com/numpy/numpy-tutorials and see if this issue
might be more appropriate there. -->
",2021-03-30 12:38:49,,numpy.interpolate does not check for periodicity,['04 - Documentation']
18696,open,RashiqAzhan,"## Feature

I would like this feature to be added since I think it can very useful when there is a need to process data that cannot be included in uint8. One of my personal requirements is modifying a 10-bit, per channel, images held in a NumPy array but I cannot do that using the specified functions. `numpy.packbits` and `numpy.unpackbits` are eloquent solution and works well with the with NumPy functions as long as the data is uint8.
",2021-03-29 18:27:25,,ENH: Expanding the scope of numpy.unpackbits and numpy.packbits to include more than uint8 type,['23 - Wish List']
18675,open,jypeter,"This is obviously more a feature than a bug, otherwise it would have been corrected (I'm using numpy 1.20.1). But it has been bothering me for a very long while, and been the **indirect source of several bugs** in my (and other colleagues') scripts

There must be some logic behind it, but I have not found it in the documentation. The closest issue I have found is #8881 (an open issue from 4 years ago)

I have a masked array. If I work on it with `np.ma` functions, things will be fine, but the equivalent **function straight from `np` will silently ignore and remove the mask**!

The example below is with `hstack`, but I get the same problem with `vstack`, `repeat`, and probably many other numpy functions
```
$ conda list | grep numpy
numpy                     1.20.1           py38h18fd61f_0    conda-forge
numpydoc                  1.1.0                      py_1    conda-forge

$ python
Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27)
[GCC 9.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy as np
>>> a = np.ma.arange(4)
>>> a[2] = np.ma.masked

>>> a
masked_array(data=[0, 1, --, 3],
             mask=[False, False,  True, False],
       fill_value=999999)

>>> b_ma = np.ma.hstack((a, a))
>>> b_ma
masked_array(data=[0, 1, --, 3, 0, 1, --, 3],
             mask=[False, False,  True, False, False, False,  True, False],
       fill_value=999999)

>>> b_NOma = np.hstack((a, a))
>>> b_NOma
masked_array(data=[0, 1, 2, 3, 0, 1, 2, 3],
             mask=False,
       fill_value=999999)
>>>
```

On the other hand, some functions fortunately use and keep the mask, regardless of being taken from `np` or `np.ma`

```
>>> np.exp(b_ma)
masked_array(data=[1.0, 2.718281828459045, --, 20.085536923187668, 1.0,
                   2.718281828459045, --, 20.085536923187668],
             mask=[False, False,  True, False, False, False,  True, False],
       fill_value=999999)
>>> np.ma.exp(b_ma)
masked_array(data=[1.0, 2.718281828459045, --, 20.085536923187668, 1.0,
                   2.718281828459045, --, 20.085536923187668],
             mask=[False, False,  True, False, False, False,  True, False],
       fill_value=999999)
```

So, **is this a bug or a feature**? What is the logic (so that I can tell our students), and where is it clearly (for beginners) explained?

Even if this is not a bug, I think **it would be much safer if numpy functions working on masked array would always use the mask and return a masked array**",2021-03-24 13:31:56,,Why/when does np.something remove the mask of a np.ma array ?,"['04 - Documentation', 'component: numpy.ma']"
18672,open,seberg,"The recent buffer changes mean that `view->obj` may be set even on error when exporting a buffer.  Previously, we probably never set `view->obj` if an error occurred (I did not double check).

Technically, the Python documentation says we should `NULL` object, but de-facto not even Python does it, and any consumer should just not make assumption if the buffer export fails!

Python also has one (or two) code places that actually rely on `view->len` not being initialized on error, I think those should probably be fixed, but if someone wants to be particularly clean, we could just leave `view->len` uninitialized until after all possible errors are handled.  (In most places handling errors up-front is easier anyway)

Note: I may look into some time (and it would be good to fix), but its also not super high priority, since we never `NULL`'ed `object` it seems very unlikely anyone relies on us not touching it...  See also: https://bugs.python.org/issue43608

(I stumbled upon this randomly, because I missed an actual, absolutely harmless, reference count leak on our side...)",2021-03-24 01:54:08,,BUG: (probably) Should ensure that `view->obj` is set to NULL or at least unmodified,['unlabeled']
18669,open,bmerry,"For 2D inputs, `np.matmul` and `np.dot` are semantically the same, but I've found that in some cases `matmul` can be much slower even though the documentation for `np.dot` says `matmul` is preferred for this case. I haven't messed around much with the matrix sizes, but I have found that this is happening when using an `out=` parameter and the output array is uninitialised and hence not yet faulted in. When it has been pre-faulted, the performance difference reverses, and `matmul` is much faster than `dot`.

Specifically, I have a 1000×1024×32 array and a 1000×32×1024 array, both C-contiguous and complex64, and for each element of the outer dimension I multiply the 1024×32 and 32×1024 matrices together, and collect all the outputs in a 1000×1024×1024 array.

I'm assuming that manually faulting the memory first helps because it prevents OpenBLAS's multiple threads from all causing page faults at the same time and contending on locks in the kernel. But I haven't done any profiling yet. I'm prepared to do some further digging, but it would help to know what differences there are in handling `out` parameters between `matmul` and `dot`. I'm pretty sure they're both using OpenBLAS because in both cases setting `OPENBLAS_NUM_THREADS` affects performance.

### Reproducing code example:

Run the below with no arguments to use `np.matmul` or with `--func=dot` to use `np.dot` instead. One can also use `--func=matmul` to pass 3D arrays to a single `matmul` call instead of looping over the outer dimension. To prefault the output memory, use `--prefault`.

NB: it'll need at least 8GB of RAM. If that's too much, reduce `size`.

```python
#!/usr/bin/env python3

import argparse
import time

import numpy as np


parser = argparse.ArgumentParser()
parser.add_argument('--func', choices=('matmul', 'matmul_loop', 'dot'), default='matmul_loop')
parser.add_argument('--prefault', action='store_true')
args = parser.parse_args()

n = 1024
m = 1024
k = 32
size = 1000
dtype = np.complex64
a = np.ones((size, n, k), dtype=dtype)
b = np.ones((size, k, m), dtype=dtype)
c = np.empty((size, n, m), dtype=dtype)
if args.prefault:
    c.ravel()[::(4096//c.dtype.itemsize)] = 0

start = time.monotonic()
if args.func == 'matmul':
    np.matmul(a, b, out=c)
elif args.func == 'matmul_loop':
    for i in range(size):
        np.matmul(a[i], b[i], out=c[i])
else:
    for i in range(size):
        np.dot(a[i], b[i], out=c[i])
stop = time.monotonic()
elapsed = stop - start
print(f'{elapsed:.3f} s')
print(f'{n * m * k * 8 * size / elapsed / 1e9:.1f} GFlops')
```

My results:
- `./gemm.py`: 36.0 GFlops
- `./gemm.py --func=dot`: 87.7 GFlops
- `./gemm.py --prefault`: 213.9 GFlops
- `./gemm.py --func=dot --prefault`: 135.7 GFlops

### NumPy/Python version information:

1.20.1 3.8.5 (default, Jan 27 2021, 15:41:15) 
[GCC 9.3.0]

numpy is installed using `pip` from the Linux binary wheel on Ubuntu 20.04.",2021-03-23 14:22:18,,np.matmul with `out` parameter slower than np.dot in some cases,['57 - Close?']
18644,open,adeak,"I tried building the numpy docs today, and it took me a very long time to figure out why `make html` was complaining about a difference between installed numpy version and current numpy version.

### Reproducing (shell) code example:

```bash
# create and activate a fresh venv (actually ~/ins/Python-3.8.7./python in my case)
python3.8 -m venv numpy_tmpenv
. numpy_tmpenv/bin/activate
# clone numpy main
git clone https://github.com/numpy/numpy numpy_tmprepo
cd numpy_tmprepo
```
At this point there's no numpy installed in the active venv. Yet if I try to build the doc:
```
(numpy_tmpenv) $ cd doc
(numpy_tmpenv) $ make html
installed numpy 91118b3363 != current repo git version 'ddbff082e0'
use ""make dist"" or ""GITVER=91118b3363 make html ...""
make: *** [Makefile:94: version-check] Error 1
```

I spent a lot of time rebuilding and `git clean`ing and `make clean`ing and playing with `python setup.py install` and `pip install .`, before I realized that the error persists even if I don't have any numpy installed!

So I finally looked at the `doc/Makefile` and found:
```makefile
PYVER:=$(shell python3 -c 'from sys import version_info as v; print(""{0}.{1}"".format(v[0], v[1]))')
PYTHON = python$(PYVER)
[...]
NUMPYVER:=$(shell $(PYTHON) -c ""import numpy; print(numpy.version.git_revision[:10])"" 2>/dev/null)
GITVER ?= $(shell cd ..; $(PYTHON) -c ""import versioneer as v; print(v.get_versions()['full-revisionid'][:10])"")
```
and then these two versions are reported in the error. So the problem is this: I'm using Python 3.8, so `$(PYTHON)` ends up being defined as `python3.8`. However my venv only defines `python` and `python3`, so `python3.8` refers to the (system) Python 3.8 env which has numpy installed (either from pip or from apt).

Am I using the wrong workflow here, or is this something that should be fixed? As a temporary workaround I've edited the Makefile so that it always uses `python` (which is obviously not a solution, but it allows me to build the documentation for now).

I'm using Python 3.8.7 on debian.",2021-03-18 21:32:06,,Possible dev workflow (doc building) bug with virtualenvs,['component: documentation']
18632,open,qria,"### Description

`np.isin` only sometimes fails when comparing integer array into string array.

Specifically, if one intended to compare to integer arrays with `np.isin` but second array was converted to string for some reason, it bugs out only on specific conditions relating to the length of the supplied arrays.

In my case this happened somewhere in our data pipeline missing data was converted to an empty string (`''`), causing the array to be converted to a string type.

I think I've figured out why it happens and already worked around the issue, but I am submitting an issue for documentation's sake.

I understand that it can be the case that `np.isin` working on string converted array is unintentional and should be avoided, however I believe this issue is something that can easily be encountered in the wild, and should be documented.

### Reproducing code example:

```python
import numpy as np

print(np.isin([1], [1] * 9 + ['']))  # array([True])  # works
print(np.isin([1], [1] * 8 + ['']))  # array([False])  # bugs out
```

### Error message:

Not an error, but a warning:

{my venv folder}/lib/python3.8/site-packages/numpy/lib/arraysetops.py:583: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  mask |= (ar1 == a)

### NumPy/Python version information:

1.20.0 3.8.7 (default, Mar  6 2021, 18:53:50) 
[Clang 12.0.0 (clang-1200.0.32.29)]


### What I think is happening

I believe this issue happens because behaviour is different from when optimization in `in1d` happens or not.

If optimization happens, the comparison becomes `np.array([0]) == '0'` which is `False`.

https://github.com/numpy/numpy/blob/d7aa4085623b222058edb0ff38392c38c5e00c54/numpy/lib/arraysetops.py#L575-L584


If optimization does not happen, the type is silently converted to string while doing `np.concatenate((ar1, ar2))`, and all values are treated as string. Therefore it returns `True`.

https://github.com/numpy/numpy/blob/d7aa4085623b222058edb0ff38392c38c5e00c54/numpy/lib/arraysetops.py#L591
",2021-03-17 10:13:13,,`np.isin` for int array to string array fails based on length.,['unlabeled']
18620,open,josh146,"When clicking an internal page link on the NumPy docs, Sphinx will scroll down to the location of the object docstring.

However, the top navigation bar is not taken into account, resulting in the first couple of lines of the docstring being obscured. This can be confusing, especially on pages with a lot of functions.

See the following gif for an example:

![ezgif com-crop](https://user-images.githubusercontent.com/2959003/111193072-bd565c00-85f4-11eb-8136-9bec34f17ede.gif)

(My web browser is Firefox 86.0 (64-bit), Windows 10)
",2021-03-15 17:13:31,,Internal links on the NumPy documentation do not take into account the top navigation bar when scrolling,['04 - Documentation']
18604,open,asmeurer,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
>>> np.array([b''], dtype='S5')[0].dtype
dtype('S')
```

The `5` information in the dtype is lost. Note that if a shape () array is used instead, this issue does not occur:

```py
>>> np.array([b''], dtype='S5')[0, ...]
array(b'', dtype='|S5')
>>> np.array([b''], dtype='S5').reshape(())
array(b'', dtype='|S5'
```

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```
1.21.0.dev0+20210202221217_33dc7be 3.8.6 | packaged by conda-forge | (default, Oct  7 2020, 18:42:56)
[Clang 10.0.1 ]
```
",2021-03-11 23:49:03,,Array scalar for fixed-length string loses length information,['unlabeled']
18592,open,mattip,"There are a number of issues around SWIG and the files in `tools/swig`, including the `numpy.i` interface file. We should have tests that cover this, and ship the parts of the interface needed for users to conveniently use SWIG. I asked on the [swig mailing list](https://sourceforge.net/p/swig/mailman/swig-user/?viewmonth=202103) (scroll to the bottom) how to best do this, and got this reply about tests:

> There are a pile of tests in SWIG you could copy, such as this simple interface file, https://github.com/swig/swig/blob/master/Examples/test-suite/li_std_containers_int.i. The runtime tests are in https://github.com/swig/swig/blob/master/Examples/test-suite/python/li_std_containers_int_runme.py and are fairly comprehensive. They test Python list functionality with C++ std::vector<int> wrappers.

We should do something like this in our CI.

How best to ship the interface is still not clear.",2021-03-10 22:00:14,,Add tests for swig and package the swig interface file,['component: swig']
18586,open,basilgello,"I found this issue to be a root cause of:

https://github.com/xbmc/xbmc/issues/19324
https://github.com/jurialmunkey/plugin.video.themoviedb.helper/issues/445
https://github.com/python-pillow/Pillow/issues/5320

### Steps to reproduce:

1. Install Kodi from Arch Linux or Kodi from Debian cleanly
2. Add jurialmunkey's plugin.video.themoviedb.helper
3. Try starting an addon and experience Kodi segfault

### Error message:

The error message is quite obscure at a first glance:

https://github.com/xbmc/xbmc/issues/19324#issuecomment-792191296
https://github.com/xbmc/xbmc/issues/19324#issuecomment-792312572

but digging it [here](https://github.com/jurialmunkey/plugin.video.themoviedb.helper/issues/445#issuecomment-792612672) revealed that the issue is the static members:

https://github.com/numpy/numpy/blob/cb557b79fa0ce467c881830f8e8e042c484ccfaa/numpy/core/src/multiarray/typeinfo.c#L18

Running the following GDB script:
[numpy.gdb.txt](https://github.com/numpy/numpy/files/6114257/numpy.gdb.txt)
illustrates the issue:
[gdb.log.txt](https://github.com/numpy/numpy/files/6114260/gdb.log.txt)

1. Here in `gdb.log.txt`, ""Thread 34"" is the first sub-interpreter thread that performs NumPy import. 
2. Then, a new sub-interpreter is kicked in ""Thread 56"".
3. Since ""Thread 56"" has no `numpy` variable declared in its scope, the re-import occurs.
4. The following code returns -1:

https://github.com/numpy/numpy/blob/cb557b79fa0ce467c881830f8e8e042c484ccfaa/numpy/core/src/multiarray/typeinfo.c#L116

and the `../Objects/structseq.c:401: bad argument to internal function',)` exception is thrown.

5. Later, the `Py_XIncRef` tries incrementing the wrong memory chunk and throws SIGSEGV.

I tried to make `PyArray_typeinfoType` and `PyArray_typeinforangedType` sub-intepreter compatible, but it seems all C++ modules declare [module context block size](https://docs.python.org/3/c-api/module.html#c.PyModuleDef.m_size) as `-1`,
indicating they are all not sub-interpreter compatible.

To make `multiarray` sub-interpreter compatible, one needs to move all static members like `PyArray_typeinfoType` into that block allocated on module definiton and ensure it gets freed on module unload.",2021-03-10 08:16:21,,Static PyArray_typeinfoType* breaks sub-interpreter numpy load,['Embedded']
18573,open,ycopin,"When using the numpy function `count_nonzero` onto a masked array, it does not account for the mask to count non-zero elements. 

### Reproducing code example:

```python
import numpy as np

marr = np.ma.masked_array((np.arange(4*5).reshape(4,5) % 2).astype(bool), mask=np.arange(4*5) % 3)
print(np.count_nonzero(marr), np.count_nonzero(marr.data))  # 10, 10, both wrong
print(np.count_nonzero(marr.compressed()))  # 3, as expected
```

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

```
1.19.1 3.6.11 (default, Jun 29 2020, 05:15:03) 
[GCC 5.4.0 20160609]
```",2021-03-07 21:28:47,,Bug in np.count_nonzero when applied on masked array,"['01 - Enhancement', 'component: numpy.ma']"
18527,open,toslunar,"<!-- Please describe the issue in detail here, and fill in the fields below -->

`np.linalg.pinv` keeps `dtype` of the input, if the input is a (stack of) empty matrix.  Should `np.linalg.pinv` always return an array of `np.inexact` type (`_commonType(a)[1]`), like `np.linalg.inv` does?

https://github.com/numpy/numpy/blob/d7aa4085623b222058edb0ff38392c38c5e00c54/numpy/linalg/linalg.py#L1997-L2000

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
np.linalg.pinv(np.empty((3, 0), np.int32))  # => array([], shape=(0, 3), dtype=int32)
```

This seems incompatible with

```python
np.linalg.inv(np.empty((0, 0), np.int32))  # => array([], shape=(0, 0), dtype=float64)
```

`TypeError` could be raised at
```python
np.linalg.pinv(np.array([[]], object))  # => array([], shape=(0, 1), dtype=object)
```

The issue is not about an empty batch
```python
np.linalg.pinv(np.empty((0, 3, 7), np.int32))  # => array([], shape=(0, 7, 3), dtype=float64)
```



### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

```
1.20.1 3.9.0 (default, Oct 15 2020, 16:29:12)
[Clang 11.0.0 (clang-1100.0.33.16)]
```",2021-03-02 10:35:22,,linalg.pinv of empty int matrix,"['00 - Bug', 'component: numpy.linalg']"
18519,open,nschloe,"NumPy is too strict when it comes to reshaping arrays of size 0. MWE:
```python
import numpy as np

b = np.empty((0, 3))
b.reshape(0, -1)  # ValueError: cannot reshape array of size 0 into shape (0,newaxis)
```
While it's not possible to deduce the newaxis shape without info, one can well deduce it from the shape of the input array.",2021-03-01 11:46:11,,"cannot reshape array of size 0 into shape (0,newaxis)",['unlabeled']
18516,open,victor-zou,"## Feature

<!-- If you're looking to request a new feature or change in functionality, including
adding or changing the meaning of arguments to an existing function, please
post your idea on the [numpy-discussion mailing list]
(https://mail.python.org/mailman/listinfo/numpy-discussion) to explain your
reasoning in addition to opening an issue or pull request. You can also check
out our [Contributor Guide]
(https://github.com/numpy/numpy/blob/master/doc/source/dev/index.rst) if you
need more information. -->
`np.where` can be regarded as a ternary ufuncs composed of `cond?x:y`, so it is natural
for `np.where` to have kwargs like `out`, `dtype` etc. Supporting these kwargs can save
efforts for allocating memory, type casting, etc. In addtion, library `numexpr` does support
this kind of syntax. ",2021-03-01 08:42:52,,"ENH: add ufuncs additional kwargs like `out`, `dtype` etc.. for `np.where` (`out` is needed most)",['unlabeled']
18512,open,cbassa,"<!-- Please describe the issue in detail here, and fill in the fields below -->

It appears `numpy.argmax` performs extremely slow for certain array sizes.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

Example of slow performance:
```python
import numpy as np

nx, ny, nz = 1280, 960, 100
z = np.random.normal(60, 5, nx * ny * nz).reshape(nz, ny, nx)

%timeit np.argmax(z, axis=0)
10.1 s ± 43.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

Incrementing the array by 1 drastically improves performance drastically:
```python
import numpy as np

nx, ny, nz = 1281, 960, 100
z = np.random.normal(60, 5, nx * ny * nz).reshape(nz, ny, nx)

%timeit np.argmax(z, axis=0)
330 ms ± 689 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
```
### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

```
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.20.0 3.9.1 (default, Feb  6 2021, 06:49:13) 
[GCC 10.2.0]

```",2021-02-28 15:21:59,,ENH: Iterator does not block and NumPy has no transposed copy fast-paths,"['component: numpy._core', '28 - Benchmark']"
18511,open,yxdragon,"a = lut[arr] would melloc new memory, is there a method like this:
np.lookup(arr, lut, out=a) ?",2021-02-28 14:07:52,,"array as index with out parameter: np.lookup(arr, lut, out=xx)",['unlabeled']
18506,open,CharlesAverill,"# Documentation

Hi there, I’m looking for the formula that numpy uses to calculate the inner product of multidimensional arrays. I found the C code that generates `np.dot` but I’m not as competent with C so I’m having a hard time deciphering it. Is there a well-known or at least named algorithm that numpy is using to calculate the inner product of any two given arrays?

Thank you!

<!-- If this is an issue with the current documentation for NumPy (e.g.
incomplete/inaccurate docstring, unclear explanation in any part of the
documentation), make sure to leave a reference to the document/code you're
referring to. You can also check the development version of the documentation
and see if this issue has already been addressed: https://numpy.org/devdocs/
-->

<!-- If this is an idea or a request for content, please describe as clearly as
possible what topics you think are missing from the current documentation. Make
sure to check https://github.com/numpy/numpy-tutorials and see if this issue
might be more appropriate there. -->
",2021-02-27 20:23:10,,DOC: Formula used by np.dot for multidimensional arrays,"['04 - Documentation', '33 - Question']"
18501,open,MarsBarLee,"Hi all, this is a tracking issue for adding and updating the graphics in the Numpy documentation. If you see any graphics you think deserve an update, let us know here!

Graphical elements that need an update:

- Outdated information, such as deprecated functions
- Hard-to-read text, such as small, blurry or handwritten text
- Cluttered or messy elements

Here's an example of a graphic we think deserve an update.
[A flowchart for Array types for scalars](https://numpy.org/devdocs/reference/arrays.scalars.html#numpy.int_)
![Untitled (1)](https://user-images.githubusercontent.com/46167686/109349675-59861200-7844-11eb-9a6d-429a625a2525.png)
It does the job right now, but some text is cut off and some content is outdated. 

If you're working on a documentation contribution, consider adding graphics! You can post here with your issue or PR. @melissawm and I can suggest new diagrams or digitize your existing hand-drawn diagram. We can also add diagrams after merging if you prefer that workflow.

Here's an example of an update I did with a [contributor's PR](https://github.com/numpy/numpy-tutorials/pull/37).
![circuit diagram update 2](https://user-images.githubusercontent.com/46167686/109349710-6440a700-7844-11eb-9f0c-34e0feb97c40.png)

We are also looking to standardize the file format and style of the graphics to make it more accessible for readers and easier for contributors to edit in the future. 
Let us know if you have any thoughts or suggestions!

**File format:** SVG (scalable vector graphics)
- Small, scalable and text can be edited without needing graphic software.
- SVG text can also be read by screen-readers, unlike PNG or JPG

**Design principle**: [Material Design](https://material.io/)
- Clean, easy to understand, and fits with rest of the Numpy website design

**Editing SVGs**:
- Edit the SVG code directly, which is in the [XML format](https://developer.mozilla.org/en-US/docs/Web/SVG/Tutorial/Getting_Started), similar to HTML
- Use open-source graphic software such as [Boxy SVG Editor](https://boxy-svg.com/), [Inkscape](https://inkscape.org/) or [GIMP](https://www.gimp.org/)
- Programatically create SVGs with [Graphviz](https://graphviz.org/) or [Flowchart.js](https://flowchart.js.org/)",2021-02-26 20:11:22,,Doc: Tracking Issue for Graphics,['04 - Documentation']
18494,open,alanhdu,"I am trying to share a `np.memmap` array across two processes by sharing the underlying `mmap`-ed file. To fully replicate this across processes, I need to replicate the filename (to share the buffer) and the numpy metadata. Unfortunately, the `np.memmap` constructor does *not* take a `strides` argument, which means that I can't directly construct the appropriate array.

I can currently achieve my aim by doing:
```python
memmap = np.memmap(filename, dtype=..., mode=""r"", offset=...)
memmap = np.lib.stride_tricks.as_strided(memmap, shape=..., strides=..., subok=True)
```
but I think it'd be nice for `np.memmap` to directly supported the `strides` argument.

Implementationally, I think we just need to update the `num_bytes` calculation to take into acount striding and to pass `strides` into the `ndarray.__new__` call at https://github.com/numpy/numpy/blob/a14c41264855e44ebd6187d7541b5b8d59bb32cb/numpy/core/memmap.py#L269-L270
",2021-02-25 19:21:45,,ENH: np.memmap should take a `strides` arguemnt in the construtor,['unlabeled']
18484,open,browshanravan,"This perhaps stems from my poor understanding of data and apologies if this has been answered but is there a specific reason as to why `numpy.corrcoef()` parameter `rowvar` is set to `True` by default? In vast majority of cases, variables are columns not rows so it is not very intuitive to have `rowvar=True`. Shouldn't it be `rowvar=False` by default instead?

Many thanks in advance",2021-02-24 23:47:00,,Reason for numpy.corrcoef parameter rowvar=True by default,['unlabeled']
18483,open,seberg,"We have quite a few issues that request things like `minmax`, `sincos` or maybe gufuncs that ""chain"" very common operations, such as `all_greater`.

I _do_ think there is merit in such functions but we are not likely to add them into the NumPy main namespace.  (There was an old PR or branch by Julian I think where he added a bunch of similar ones such as multiply-4-numbers-and-add (which I guess is a SIMD intrinsic).  (Seems I probably remembered [this PR](https://github.com/numpy/numpy/pull/2954) for linalg, where the initial version included a few fused ops)

I see two options for this:

1. If anyone wants to create these functions, I think we can *definitely* add them into their own repository/project in the NumPy organization.
2. It could make sense to have a `np.lib.performance`, at least for a subset of this type of functions.

There is little chance of adding a new functions such as this in the main namespace though, so this is to encourage anyone interesting in it to create a small additional package and then it will be easier to discuss about option 2. as well. That is my current angle on it.  ",2021-02-24 21:29:37,,"ENH: Create a place for ""optimization"" ufuncs","['01 - Enhancement', 'component: numpy.ufunc']"
18468,open,codethief,"### Reproducing code example:

I installed Numpy through `pip3 install --user numpy==1.19.5` on an ARM64 device (Nvidia Jetson Nano) running Ubuntu 20.04 and cannot import it in Python. Interestingly, Numpy 1.16.1 and 1.19.4 work.


### Error message:
```
$ python3
Python 3.6.9 (default, Oct  8 2020, 12:12:24) 
[GCC 8.4.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy
Illegal instruction (core dumped)
```

### NumPy/Python version information:
- Python 3.6.9
- Numpy 1.19.5


### Related issues:
- https://github.com/numpy/numpy/issues/17674 (segmentation fault in Numpy 1.19.3; was fixed in 1.19.4)
- https://github.com/numpy/numpy/issues/17834 (segmentation fault in Numpy 1.19.4; looks related but Numpy 1.19.4 does work for me)
",2021-02-22 17:12:54,,"Numpy 1.19.5, Python 3.6.9: Import results in segmentation fault",['unlabeled']
18445,open,BvB93,"Despite booleans being castable to timedelta, the `np.true_divide` and `np.floor_divide` ufuncs 
currently do not support divisions between `np.timedelta64` and `np.bool_`. 
Note that `timedelta / integer` operations _do_ work as expected.

This is inconsistent w.r.t. the rest of the ufuncs, which support either no timedelta casting or the full range of casts:
* `np.add` and `np.multiply` support the full range of timedelta casts, 
  _i.e._  both `timedelta + timedelta`, `timedelta + integer` and `timedelta + bool` operations are accepted.
* `np.remainder` (https://github.com/numpy/numpy/issues/16195) and `np.nat` do not support timedelta casting at all.

### Reproducing code example:

Division:
``` python
In [1]: import numpy as np

In [2]: b = np.array([True])
   ...: u = np.array([1], dtype=np.uint)
   ...: i = np.array([1])
   ...: m = np.array([1], dtype=""timedelta64[s]"")

In [3]: m / i
Out[3]: array([1], dtype='timedelta64[s]')

In [4]: m / u
Out[4]: array([1], dtype='timedelta64[s]')

In [5]: m / b
---------------------------------------------------------------------------
UFuncTypeError: ufunc 'true_divide' cannot use operands with types dtype('<m8[s]') and dtype('bool')
```

Floor division:
``` python
In [6]: m // i
Out[6]: array([1], dtype='timedelta64[s]')

In [7]: m // u
Out[7]: array([1], dtype='timedelta64[s]')

In [8]: m // b
---------------------------------------------------------------------------
UFuncTypeError: ufunc 'floor_divide' cannot use operands with types dtype('<m8[s]') and dtype('bool')
```

### NumPy/Python version information:

``` python
In [9]: import sys, numpy; print(numpy.__version__, sys.version)
1.21.0.dev0+807.g121b6097a 3.8.5 (default, Sep  4 2020, 02:22:02) 
[Clang 10.0.0 ]
```",2021-02-19 15:13:04,,BUG: `timedelta64` does not support boolean (floor-)division,"['00 - Bug', 'component: numpy.datetime64']"
18435,open,nschloe,"```python
import numpy as np

arr = np.fromstring(""\n"", sep="" "")
print(arr)
```
```
array([-1.])
```",2021-02-17 17:48:59,,"np.fromstring(""\n"", sep="" "") returns non-empty array","['00 - Bug', 'component: numpy._core']"
18434,open,paris0120,"output: True
version: '1.18.5'
System: Windows 10",2021-02-17 15:21:49,,DEP: np.dtype('float64') == None and `np.dtype(None)` should be deprecated,"['07 - Deprecation', 'component: numpy.dtype']"
18425,open,taldcroft,"<!-- Please describe the issue in detail here, and fill in the fields below -->

#18116 caused the `astropy` regression tests to start failing with numpy-dev, see https://github.com/astropy/astropy/issues/11291.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
>>> import numpy as np
>>> np.ma.array(['val', np.ma.masked])
/Users/aldcroft/git/astropy/.tox/test-devdeps/lib/python3.7/site-packages/numpy/ma/core.py:2826: FutureWarning: Promotion of numbers and bools to strings is deprecated. In the future, code such as `np.concatenate((['string'], [0]))` will raise an error, while `np.asarray(['string', 0])` will return an array with `dtype=object`.  To avoid the warning while retaining a string result use `dtype='U'` (or 'S').  To get an array of Python objects use `dtype=object`. (Warning added in NumPy 1.21)
  order=order, subok=True, ndmin=ndmin)
masked_array(data=['val', --],
             mask=[False,  True],
       fill_value='N/A',
            dtype='<U32')
```
The expected result is the same masked array but with no `FutureWarning` (from numpy 1.18.5 in this case):
```
>>> import numpy as np
>>> np.ma.array(['val', np.ma.masked])
masked_array(data=['val', --],
             mask=[False,  True],
       fill_value='N/A',
            dtype='<U32')
```

Things get a little confusing for tests that are run in `pytest` because when warnings are turned into errors then there appears to be a try/except within numpy which triggers immediate conversion to `object`:

```
(test-devdeps) (astropy) ➜  astropy git:(master) python -Werror                                         
Python 3.7.9 (default, Aug 31 2020, 07:22:35) 
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy as np
>>> np.ma.array(['val', np.ma.masked])
masked_array(data=['val', --],
             mask=[False,  True],
       fill_value='?',
            dtype=object)
```

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.21.0.dev0+759.g7a18e4ac4 3.7.9 (default, Aug 31 2020, 07:22:35) 
[Clang 10.0.0 ]
```",2021-02-16 11:28:09,,"np.ma.arrray(['string', np.ma.masked]) gives FutureWarning after #18116 (breaks astropy)","['00 - Bug', '06 - Regression', 'component: numpy.ma']"
18412,open,Ra-works,"```
xyz =np.array( [[[612.        ,  0.8679449]],
                [[612.        ,  0.7679449]],

 [[206.,          0.338741 ]],

 [[62.,          2.338741 ]]])
xyx = np.copy(xyz)
np.delete(xyx, np.where([[False], [True],[True],[ True]]), axis=0)

```
Output:
array([], shape=(0, 1, 2), dtype=float64)


```
`xyz =np.array( [[[612.        ,  0.8679449]],
                [[612.        ,  0.7679449]],

 [[206.,          0.338741 ]],

 [[62.,          2.338741 ]]])
xyx = np.copy(xyz)
np.delete(xyx, np.where([[False], [False],[False],[ True]]), axis=0)`
```

output
array([[[6.120000e+02, 7.679449e-01]],

       [[2.060000e+02, 3.387410e-01]]])


```
xyz =np.array( [[[612.        ,  0.8679449]],
                [[612.        ,  0.7679449]],

 [[206.,          0.338741 ]],

 [[62.,          2.338741 ]]])
xyx = np.copy(xyz)
np.delete(xyx, np.where([[False], [False],[False],[ False]]), axis=0)
```
but this works fine as output below.
array([[[6.120000e+02, 8.679449e-01]],

       [[6.120000e+02, 7.679449e-01]],

       [[2.060000e+02, 3.387410e-01]],

       [[6.200000e+01, 2.338741e+00]]])

Issue is observed- if there is atleast one True in where condition, then it is also deleting the first row.

numpy version 1.20.1",2021-02-14 21:34:20,,numpy delete where is giving wrong result i.e. always deleting the first element if there is atleast one True,['unlabeled']
18391,open,pwschaedler,"## Documentation

In the basics documentation for genfromtxt, under ""Choosing the data type"" [(see here)](https://numpy.org/doc/stable/user/basics.io.genfromtxt.html#choosing-the-data-type), one of the suggested options is:
> a sequence of types, such as dtype=(int, float, float).

However, this does not apply for 2-tuples, I'm assuming because a 2-tuple has an alternate meaning for the dtype constructor. Ideally I think it'd be nice if this worked for 2-tuples, but at the very least the current documentation is misleading. See the following code snippet for an example.

(Python 3.9.1, Numpy 1.20.1)
```python
In [1]: from io import StringIO; import numpy as np

In [2]: data = '1 2 3\n4 5 6'

In [3]: np.genfromtxt(StringIO(data), dtype=(int, float, float))
Out[3]:
array([(1, 2., 3.), (4, 5., 6.)],
      dtype=[('f0', '<i8'), ('f1', '<f8'), ('f2', '<f8')])

In [4]: data = '1 2\n3 4\n5 6'

In [5]: np.genfromtxt(StringIO(data), dtype=(int, float))
Out[5]:
array([[1, 2],
       [3, 4],
       [5, 6]])

In [6]: np.genfromtxt(StringIO(data), dtype=(int, float)).dtype
Out[6]: dtype('int64')
```",2021-02-11 01:56:15,,"genfromtxt suggests dtype tuple allowed, but not for 2-tuples",['04 - Documentation']
18388,open,bnavigator,"While building and testing NumPy 1.20.0 for openSUSE Tumbleweed on i586 architecture:

```
[ 1044s] _________________ TestRemainder.test_float_remainder_overflow __________________
[ 1044s] [gw7] linux -- Python 3.8.7 /usr/bin/python3.8
[ 1044s] 
[ 1044s] self = <numpy.core.tests.test_umath.TestRemainder object at 0xf075f448>
[ 1044s] 
[ 1044s]     def test_float_remainder_overflow(self):
[ 1044s]         a = np.finfo(np.float64).tiny
[ 1044s]         with np.errstate(over='ignore', invalid='ignore'):
[ 1044s]             div, mod = np.divmod(4, a)
[ 1044s]             np.isinf(div)
[ 1044s]             assert_(mod == 0)
[ 1044s]         with np.errstate(over='raise', invalid='ignore'):
[ 1044s]             assert_raises(FloatingPointError, np.divmod, 4, a)
[ 1044s]         with np.errstate(invalid='raise', over='ignore'):
[ 1044s] >           assert_raises(FloatingPointError, np.divmod, 4, a)
[ 1044s] 
[ 1044s] ../../../BUILDROOT/python-numpy-1.20.0-90.1.i386/usr/lib/python3.8/site-packages/numpy/core/tests/test_umath.py:449: 
[ 1044s] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[ 1044s] /usr/lib/python3.8/unittest/case.py:816: in assertRaises
[ 1044s]     return context.handle('assertRaises', args, kwargs)
[ 1044s] /usr/lib/python3.8/unittest/case.py:202: in handle
[ 1044s]     callable_obj(*args, **kwargs)
[ 1044s] /usr/lib/python3.8/unittest/case.py:224: in __exit__
[ 1044s]     self._raiseFailure(""{} not raised by {}"".format(exc_name,
[ 1044s] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[ 1044s] 
[ 1044s] self = <unittest.case._AssertRaisesContext object at 0xf075f778>
[ 1044s] standardMsg = 'FloatingPointError not raised by divmod'
[ 1044s] 
[ 1044s]     def _raiseFailure(self, standardMsg):
[ 1044s]         msg = self.test_case._formatMessage(self.msg, standardMsg)
[ 1044s] >       raise self.test_case.failureException(msg)
[ 1044s] E       AssertionError: FloatingPointError not raised by divmod
[ 1044s] 
[ 1044s] /usr/lib/python3.8/unittest/case.py:164: AssertionError
```

```
(Pdb) p a
2.2250738585072014e-308
(Pdb) np.divmod(4,a)
*** RuntimeWarning: overflow encountered in divmod
(Pdb) with np.errstate(over='raise', invalid='ignore'): np.divmod(4,a)
*** FloatingPointError: overflow encountered in divmod
(Pdb) with np.errstate(over='ignore', invalid='raise'): np.divmod(4,a)
(inf, 0.0)
(Pdb) p np.__version__
'1.20.0'
(Pdb) sys.version
'3.8.7 (default, Dec 22 2020, 08:33:13) [GCC]'
```",2021-02-10 14:05:09,,"test_float_remainder_overflow fails on intel 32-bit because divmod(_, tiny) does not raise invalid value",['unlabeled']
18387,open,bnavigator,"Hi, 
NumPy 1.20.0 on openSUSE Tumbleweed 32-bit looks even worse than Ubuntu some years ago (#430):

```
[ 1044s] __________________________ TestRandomDist.test_pareto __________________________
[ 1044s] [gw4] linux -- Python 3.8.7 /usr/bin/python3.8
[ 1044s] 
[ 1044s] self = <numpy.random.tests.test_generator_mt19937.TestRandomDist object at 0xf06d9bc8>
[ 1044s] 
[ 1044s]     def test_pareto(self):
[ 1044s]         random = Generator(MT19937(self.seed))
[ 1044s]         actual = random.pareto(a=.123456789, size=(3, 2))
[ 1044s]         desired = np.array([[1.0394926776069018e+00, 7.7142534343505773e+04],
[ 1044s]                             [7.2640150889064703e-01, 3.4650454783825594e+05],
[ 1044s]                             [4.5852344481994740e+04, 6.5851383009539105e+07]])
[ 1044s]         # For some reason on 32-bit x86 Ubuntu 12.10 the [1, 0] entry in this
[ 1044s]         # matrix differs by 24 nulps. Discussion:
[ 1044s]         #   https://mail.python.org/pipermail/numpy-discussion/2012-September/063801.html
[ 1044s]         # Consensus is that this is probably some gcc quirk that affects
[ 1044s]         # rounding but not in any important way, so we just use a looser
[ 1044s]         # tolerance on this test:
[ 1044s] >       np.testing.assert_array_almost_equal_nulp(actual, desired, nulp=30)
[ 1044s] E       AssertionError: X and Y are not equal to 30 ULP (max is 31)
[ 1044s] 
[ 1044s] ../../../BUILDROOT/python-numpy-1.20.0-90.1.i386/usr/lib/python3.8/site-packages/numpy/random/tests/test_generator_mt19937.py:1520: AssertionError
```
```
(Pdb) (actual-desired)
array([[ 4.44089210e-16,  0.00000000e+00],
       [-2.22044605e-16,  0.00000000e+00],
       [ 0.00000000e+00, -2.30967999e-07]])
(Pdb) np.spacing(max(abs(actual).max(), abs(desired).max()))*30
2.2351741790771484e-07
```
",2021-02-10 13:53:06,,test_pareto on 32-bit got even worse,['component: numpy.random']
18385,open,pearu,"## Current state

Fortran routines may call user-defined `external` functions that implementations must be given by users, usually in the form of a Fortran source code that is compiled and linked in to the application. When using f2py, users can provide such user-defined external functions as Python functions that will be called from Fortran code. For example, consider

```
! foo.f90
subroutine primary()
  external callback
  !f2py    intent(callback, hide) :: callback
  call callback(1, 2)
end subroutine primary

subroutine secondary()
  external callback
  call callback(3, 4)
end subroutine secondary
```
Notice that when wrapping `primary` subroutine, the `intent(callback, hide) :: callback` statement is interpreted by f2py to generate an auxiliary wrapper function that is called from Fortran as `external callback`, and this wrapper function implements a callback to a Python function provided by users via setting the `callback` attribute of f2py generated extensions module:
```
$ f2py -c foo.f90 -m foo
$ python
>>> import foo
>>> def callback(a, b):
...     print(f'Python callback: {a=} {b=}')
... 
>>> foo.callback = callback  # expose Python callback as Fortran external callback
>>> foo.primary()
Python callback: a=1 b=2
```

## Unintended behavior

However, the `secondary` subroutine calls the same `external callback` but it will fail:
```
>>> foo.secondary()
capi_return is NULL
Call-back cb_callback_in_primary__user__routines failed.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: callback() missing 2 required positional arguments: 'a' and 'b'
```
The expected behavior would be the output:
```
Python callback: a=3 b=4
```

One can still make the secondary call work by defining the Python callback using keyword arguments:
```
>>> def callback2(a=None, b=None):
...     print(f'Python callback2: {a=} {b=}')
... 
>>> foo.callback = callback2
>>> foo.secondary()
Python callback2: a=None b=None
```
This behavior is unintended but apparently existing applications make use of this (see e.g. #18341 ).

## Feature request

There are two options to make the hidden callbacks work from secondary functions:
1. Use `!f2py    intent(callback, hide) :: callback` in the `secondary` subroutine. Currently this leads to a compilation error: `redefinition of 'callback_'`
2. Extend the generated auxiliary wrapper function to correctly handle the user-defined hidden callbacks. Internally, the wrapper function must call `create_cb_arglist`.

## Backward compatibility concerns

The option 1. does not expose BG issues because using `!f2py    intent(callback, hide) :: callback` in `secondary` subroutine must remain optional.

Re option 2: If existing applications rely on the current behavior, say, these use
```
def callback(a=None, b=None):
    if a is not None:
        # do smth
    else:
        # do smth else
```
then for BC, the extension of the auxiliary wrapper function must treat the functions with keyword arguments in BC way.

## Plan

Support both options as these represent different use cases.",2021-02-10 08:57:41,,ENH: support hidden callbacks from secondary functions,['component: numpy.f2py']
18363,open,feefladder,"<!-- Please describe the issue in detail here, and fill in the fields below -->
When converting nanosecond precision `np.datetime64` to `datetime.datetime`, an `int` is returned, whereas a `datetime.datetime` would be desireable/expected. 


### Reproducing code example:





<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

from [this SO thread](https://stackoverflow.com/a/58174729/14681457). I also explained it in [this xarray issue](https://github.com/pydata/xarray/issues/4880)

```python
import numpy as np

print('second precision')
t = np.datetime64('2000-01-01 00:00:00') 
print(t)
print(t.astype(datetime.datetime))
print(t.item())

print('microsecond precision')
t = np.datetime64('2000-01-01 00:00:00.0000') 
print(t)
print(t.astype(datetime.datetime))
print(t.item())

print('nanosecond precision')
t = np.datetime64('2000-01-01 00:00:00.0000000') 
print(t)
print(t.astype(datetime.datetime))
print(t.item())
import pandas as pd 
print(pd.to_datetime(str(t)))


second precision
2000-01-01T00:00:00
2000-01-01 00:00:00
2000-01-01 00:00:00
microsecond precision
2000-01-01T00:00:00.000000
2000-01-01 00:00:00
2000-01-01 00:00:00
nanosecond precision
2000-01-01T00:00:00.000000000
946684800000000000
946684800000000000
2000-01-01 00:00:00
```

### Error message:
none, 
<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
numpy 1.20.0
sys:
3.7.9 | packaged by conda-forge | (default, Dec  9 2020, 21:08:20) 
[GCC 9.3.0]
",2021-02-07 23:21:48,,nanosecond precsion datetime converts to int in stead of datetime,['component: numpy.datetime64']
18360,open,syagev,"<!-- Please describe the issue in detail here, and fill in the fields below -->
If numpy is installed in a venv which is hosted on a network share on Windows, it seems `distutils.misc_util.get_info` returns the path with double-backslashes as a path separator. This breaks modules that depend on this output in order to set include and lib paths during compilation (e.g. `hmmlearn`).

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
from numpy.distutils.misc_util import get_info
print(get_info(""npymath""))

>>> {'include_dirs': ['\\\\\\\\ilabs-htcfs\\\\Shared\\\\styagev\\\\.virtualenvs\\\\OfflineSystem-B.38\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\include'], 'library_dirs': ['\\\\\\\\ilabs-htcfs\\\\Shared\\\\styagev\\\\.virtualenvs\\\\OfflineSystem-B.38\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\lib'], 'libraries': ['npymath'], 'define_macros': []}
```
The correct path for the npymath library is `\\ilabs-htcfs\Shared\styagev\.virtualenvs\OfflineSystem-B.38\lib\site-packages\numpy\core\lib`


### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.20.0 3.8.5 (tags/v3.8.5:580fbb0, Jul 20 2020, 15:57:54) [MSC v.1924 64 bit (AMD64)]
",2021-02-07 19:35:28,,"distutils.misc_util.get_info(""npymath"") returns double backslashes in path when venv is on a network share","['00 - Bug', 'component: numpy.distutils']"
18309,open,JimBiardVexcel,"## Documentation

The new documentation removed the single most useful part for me — the index. I used the index constantly. And no, searching is not the same as having the index. Please restore the index!",2021-02-03 01:07:43,,Please restore the index!,['04 - Documentation']
18304,open,kurtamohler,"When `linalg.cond` is given a complex input, the output dtype is either the downgraded value type or the complex type, depending on which order value `p` is given. In contrast, `linalg.norm` always returns the downgraded value type, for all possible order values.

To improve `linalg.cond`'s consistency with itself and with `linalg.norm`, it would probably be best if `linalg.cond` always returns the downgraded value type.

When `cond` returns a complex value, its imaginary part is always 0 (as far as I understand), so downgrading the type shouldn't affect value correctness.

### Reproducing code example:

Behavior of `cond`:

```python
>>> import numpy as np
>>> a = np.random.randn(10, 10) + 1j * np.random.randn(10, 10)
>>> np.linalg.cond(a, p=None)
72.2881023589724
>>> np.linalg.cond(a, p=2)
72.2881023589724
>>> np.linalg.cond(a, p=-2)
0.013833535082082288
>>> np.linalg.cond(a, p=1)
(253.11513101087314+0j)
>>> np.linalg.cond(a, p=-1)
(15.869620616376949+0j)
>>> np.linalg.cond(a, p=float('inf'))
(200.2689864179095+0j)
>>> np.linalg.cond(a, p=-float('inf'))
(24.578874030599213+0j)
>>> np.linalg.cond(a, p='fro')
(126.26418916571008+0j)
>>> np.linalg.cond(a, p='nuc')
(452.1421038722304+0j)
```

Behavior of `norm`:
```python
>>> import numpy as np
>>> a = np.random.randn(10, 10) + 1j * np.random.randn(10, 10)
>>> np.linalg.norm(a, ord=None)
14.445562956889793
>>> np.linalg.norm(a, ord=2)
8.343686487155832
>>> np.linalg.norm(a, ord=-2)
0.11542267973396612
>>> np.linalg.norm(a, ord=1)
16.439210053387864
>>> np.linalg.norm(a, ord=-1)
9.113381379488501
>>> np.linalg.norm(a, ord=float('inf'))
15.027415916886657
>>> np.linalg.norm(a, ord=-float('inf'))
8.36772188424056
>>> np.linalg.norm(a, ord='fro')
14.445562956889793
>>> np.linalg.norm(a, ord='nuc')
39.26235633671831
```

### NumPy/Python version information:

```python
>>> print(np.__version__, sys.version)
1.19.1 3.7.7 (default, Mar 26 2020, 15:48:22) 
[GCC 7.3.0]
```",2021-02-02 18:47:32,,`linalg.cond` complex dtype is inconsistent depending on `p`,"['01 - Enhancement', 'component: numpy.linalg', '56 - Needs Release Note.']"
18298,open,anntzer,"<!-- Please describe the issue in detail here, and fill in the fields below -->

For arrays up to ~1e6 elements, calling median() is slower that calling partition(..., n//2) followed by getting the (n//2)th element.  In fact, there seems to be so much overhead in median() that it's even slower than sorting the whole array and getting the (n//2)th element for arrays up to ~1e3 elements.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

(I cheated a bit and only test odd-size arrays here, but for even-size arrays, the overhead for partition() would be no more than ~1.5x (partition to get the (n//2)th element and then the min of all elements above that to get the (n//2+1)th.)

```python
In [1]: for n in [11, 101, 1_001, 10_001, 100_001, 1_000_001, 10_000_001]:
   ...:     print(n)
   ...:     x = np.random.rand(n)
   ...:     assert np.median(x) == np.sort(x)[n//2] == np.partition(x, n//2)[n//2]
   ...:     %timeit np.median(x)
   ...:     %timeit np.sort(x)[n//2]
   ...:     %timeit np.partition(x, n//2)[n//2]
   ...: 
11
74.6 µs ± 5.06 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
5.76 µs ± 397 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
6.68 µs ± 244 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
101
88.4 µs ± 4.72 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
7.32 µs ± 385 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
7.95 µs ± 386 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
1001
103 µs ± 7.13 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
61.2 µs ± 2.45 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
15 µs ± 720 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
10001
398 µs ± 32 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
1.05 ms ± 31.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
243 µs ± 14.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
100001
2.61 ms ± 172 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
13.2 ms ± 376 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
2.36 ms ± 53.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
1000001
25.1 ms ± 1.91 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
164 ms ± 3.96 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
21.6 ms ± 1.91 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
10000001
349 ms ± 33.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
2.06 s ± 95.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
375 ms ± 54.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```
1.20.0 3.9.1 (default, Dec 13 2020, 11:55:53) 
[GCC 10.2.0]
```
",2021-02-02 09:33:54,,"median() is slower than partition(..., n//2)[n//2]",['unlabeled']
18279,open,k1o0,"NB: I'm reporting this under documentation, however it's also an [argument in favour](http://numpy-discussion.10968.n7.nabble.com/datetime64-Remove-deprecation-warning-when-constructing-with-timezone-td48635.html#a48800) of keeping support for constructing a datetime64 with a timezone.
## Documentation
Instantiating datetime64 objects with a timezone has been deprecated since version 1.11.0. There is currently little guidance on what to do about loading ISO 8601 timestamps from a text file if they contain timezone information.  

We use numpy.genfromtxt to load an array with UTC offset timestamps e.g. `2020-08-19T12:42:57.7903616-04:00`:
```
ssv_params = dict(names=('bonsai', 'camera'), dtype='<M8[ns],<u4', delimiter=' ')
ssv_times = np.genfromtxt(file, **ssv_params)  # np.loadtxt is slower for some reason
```
If loading this array takes 0.0352s without having to convert it, it now takes 0.8615s with the following converter:
```
lambda x: dateutil.parser.parse(x).astimezone(timezone.utc).replace(tzinfo=None)
```
That's a huge performance hit to do something that should be considered a standard operation, namely loading ISO compliant data.  If there is a more efficient way to load timestamps than using the above lambda, perhaps it could be included beside the deprecation note [in the documentation](https://numpy.org/doc/stable/reference/arrays.datetime.html#basic-datetimes).",2021-02-01 08:16:30,,Lack of guidance on deprecation of datetime64 with timezones,['04 - Documentation']
18244,open,tomerh2001,"1. Writing `np.lib.stride_tricks.sliding_window_view` is a bit of a mouthful, I think it would be better if it was placed inside the main module. (also doing `import from` is a bit inconvenient)
2. I suggest adding a `step_size` parameter to the function, an example of its use would be:

```
x = np.arange(10)
sliding_window_view(x, 3, step_size=2) 

>> array([[0, 1, 2],
          [2, 3, 4],
          [4, 5, 6],
          [6, 7, 8]])
```",2021-01-27 21:43:00,,Move `sliding_window_view` to the main module and add `step_size`,['unlabeled']
18240,open,hdkire,"The return value of np.max is always numpy.datetime64('NaT') for masked_array of datetime64 containing masked values.
This can be seen when using np.max, np.argmax and their counterparts from np.ma. It seems that np.min, np.argmin, etc. behaves correctly.

At least np.ma.max should return the largest non-masked value, not NaT.

### Reproducing code example:
```
# Some masked, max returns numpy.datetime64('NaT')
>>> import numpy as np
>>> a = np.ma.masked_array(['2021-01-27T10:33:15', '2021-01-27T10:33:16'], mask=[True, False], dtype='datetime64[us]')
>>> np.max(a)
numpy.datetime64('NaT')
>>> np.min(a)
numpy.datetime64('2021-01-27T10:33:16.000000')

# All masked, masked is returned
>>> a = np.ma.masked_array(['2021-01-27T10:33:15', '2021-01-27T10:33:16'], mask=[True, True], dtype='datetime64[us]')
>>> np.max(a)
masked
>>> np.min(a)
masked

# Masked float column, non-masked value is returned
>>> a = np.ma.masked_array([0.1, 0.2], mask=[True, False])
>>> np.max(a)
0.2
>>> np.min(a)
0.2
```

### NumPy/Python version information:

1.19.5 3.8.6 (default, Sep 25 2020, 09:36:53) 

",2021-01-27 10:25:40,,"np.max, np.ma.max returns NAT for masked_array with datetime64","['component: numpy.ma', 'component: numpy.datetime64']"
18209,open,jackmitch,"## Feature

When compiling programs such as scikit-learn which leverage numpy, quite often you will find them including the numpy include paths by using something similar to:

```
     config.add_extension(""_barnes_hut_tsne"",
                          sources=[""_barnes_hut_tsne.pyx""],
                          include_dirs=[numpy.get_include()],
                          libraries=libraries,
                          extra_compile_args=['-O3'])

```

This causes a problem when cross compiling as get_include returns the include path for the host system numpy installation which can cause issues specifically around type casting, for example integer pointers when compiling on a 64bit host for a 32bit target.

It would be great to have a way to override or prefix the path which get_include returns. This has been done elsewhere recently here:

https://github.com/numpy/numpy/commit/153fc148eec60e5cbec0e80617f75a3a5dd2a3f8

Which allows an alternative path for customised npy-pkg-config files, however this does not seem to address the get_include path problem. I would propose somehow switching the numpy get_include function to either accept a prefixed sysroot variable or for the numpy get_include function to look at the npy-pkg-config file in order to build it's include paths which would then take into account a customised version with a modified sysroot.

",2021-01-22 10:44:24,,allow get_include to point to a cross-compile compatible sysroot,"['component: numpy.distutils', '38 - Cross compilation']"
18198,open,Carreau,"## Feature

For reasons related to documentation and showing it in IPython/Jupyter I have some questions/suggestions wrt fully qualified names of object and improvement.

For example if a user does:

```
import <whatever> as x
x?
```

I want to be able to say ""view online docs at https://numpy.org/doc/stable/reference/generated/{fully_qual}.html"".format(full_qual(x))"" with roughly:

```
def full_qual(x):
    return x.__module__+'.'+x.__qualname__
```

As well as the ""inverse"" operation: from a str get the object back:
```
parts = full_qual_name.split('.')
target = parts[0]
for k in parts[1:]:
    target = getattr(target, k)
```

And I have some issues with a few object, the mostly fell in 2 categories:

1) Object with no __module__
```
# full_qual(np.exp) # AttrError , no attribute __module__, (no __qualname__, but have __name__) same with sin, cos...
```

Could it be possible to add a __module__ to those ? 

```
full_qual(np.memmap) # 'numpy.memmap' is Ok
full_qual(np.memmap.flush)  # 'numpy.core.memmap.memmap.flush' is problematic:

target = numpy
for k in ['core', 'memmap', 'memmap', 'flush']:
    target = getattr(target, k) # fails with AttributeError: type object 'memmap' has no attribute 'memmap'
```

For this second one this is due to  `numpy.core.memmap` being ambiguous as `core/__init__.py` contain `from .memmap import memmap` so the class shadow the module in which it's defined.

Yes I know I can access `.flush` via `np.memmap.flush` it's just one example.

Would it be ok to have a consistent way to go from objects to fully_qualified name and vice versa, and should I open issues when I find such functions/methods/class ?





",2021-01-20 21:37:54,,ENH: Improve object fully qualified names.,['unlabeled']
18186,open,jbrockmendel,"xref https://github.com/pandas-dev/pandas/pull/38068

There are two pain points I've found in implementing `__array_function__`.

1)  AFAICT there is no nice way to implement it piecemeal, i.e. implement only a handful of functions (out of scope for this issue, but I want to make sure this is written down somewhere)

2) There isn't a clear way to test that an implementation is correct.

The proposal/request here is to implement a test suite (analogous to pandas/tests/extension/base for ExtensionArrays) that downstream libraries can use to test their implementations of `__array_function__`",2021-01-18 18:16:30,,ENH: __array_function__ test suite for downstream libraries,"['01 - Enhancement', 'component: numpy.testing']"
18158,open,mdhaber,"I'm working on closing some old issues with `scipy.stats.scoreatpercentile`, and I noticed that when performing `percentile` along an axis of zero length, it raises an `IndexError`, whereas other functions, like `nanpercentile`, `mean`, and `std`, return NaN. I am not personally bothered by inconsistencies in these sorts of edge cases, but it would help if this were fixed here so I don't need to patch it downstream.

### Reproducing code example:

```python
import numpy as np
a = []
np.mean(a) # nan
np.std(a) # nan
np.nanpercentile(a, 50) # nan
np.percentile(a, 50) # error

a = np.zeros(1, 0)
np.mean(a, axis=1) # [nan]
np.std(a, axis=1) # [nan]
np.nanpercentile(a, 50, axis=1) # [nan]
np.percentile(a, 50, axis=1) # error
```

### Error message:
```
np.percentile([], 50)
Traceback (most recent call last):

  File ""<ipython-input-167-c47e9204ebd3>"", line 1, in <module>
    np.percentile([], 50)

  File ""<__array_function__ internals>"", line 6, in percentile

  File ""/opt/anaconda3/envs/scipydev/lib/python3.7/site-packages/numpy/lib/function_base.py"", line 3706, in percentile
    a, q, axis, out, overwrite_input, interpolation, keepdims)

  File ""/opt/anaconda3/envs/scipydev/lib/python3.7/site-packages/numpy/lib/function_base.py"", line 3826, in _quantile_unchecked
    interpolation=interpolation)

  File ""/opt/anaconda3/envs/scipydev/lib/python3.7/site-packages/numpy/lib/function_base.py"", line 3403, in _ureduce
    r = func(a, **kwargs)

  File ""/opt/anaconda3/envs/scipydev/lib/python3.7/site-packages/numpy/lib/function_base.py"", line 3941, in _quantile_ureduce_func
    x1 = take(ap, indices_below, axis=axis) * weights_below

  File ""<__array_function__ internals>"", line 6, in take

  File ""/opt/anaconda3/envs/scipydev/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 194, in take
    return _wrapfunc(a, 'take', indices, axis=axis, out=out, mode=mode)

  File ""/opt/anaconda3/envs/scipydev/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 61, in _wrapfunc
    return bound(*args, **kwds)

IndexError: cannot do a non-empty take from an empty axes.
```

### NumPy/Python version information:
```
1.18.1 3.7.7 (default, Mar 26 2020, 10:32:53) 
[Clang 4.0.1 (tags/RELEASE_401/final)]
```

---

Also, I'm surprised by the shape of the array `nanpercentile` returns when multiple percentiles are requested. When the size of the first input (`a`) array is nonzero, the first dimension of the output always has length equal to that of the second input (`q`) , but that breaks down when the size of `a` is zero.
```
np.nanpercentile([], [50, 99])  # nan, seems wrong
np.nanpercentile([np.nan], [50, 99])  # [nan, nan], seems right
np.nanpercentile(np.zeros((0, 1)), [50, 99], axis=1).shape  # (0,)
np.nanpercentile(np.zeros((1, 1)), [50, 99], axis=1).shape  # (2, 1)
```
Should I post a separate issue about this?
",2021-01-13 01:28:05,,`percentile` and `nanpercentile` taken along axis of zero length behave differently,"['00 - Bug', 'component: numpy.lib']"
18151,open,cosama,"## Feature

This feature request is a follow up of https://github.com/numba/numba/issues/6617.

I am not familiar enough with `ufunc` to understand the feasibility or even if this is already possible when implementing ""pure"" `ufunc`s. However, a similar use case is the `numpy.vectorize` class which takes a `signature` argument.

What I would like to do is to perform simple arithmetic operations in these signatures. For example a use case that comes up often in my work with histograms is the need for a signature of the form:

`(n)->(n-1)`.

The input array contains edges, thus is one element longer than the output array with the bin values.

In general I think it would be useful to support some very simple arithmetic operations in the signature (i.e. addition, multiplication, etc) of given inputs. For example I can imagine this one also to be quite useful:

`(n),(m)->(m*n)`

My understanding is, that if this would be implemented in NumPy a package such as `numba` could directly use this and allow for more flexibility in their own `ufunc` translator as well.

Thanks.",2021-01-11 18:36:46,,ENH: Add support for more complex signatures to ufunc,"['01 - Enhancement', '15 - Discussion', 'component: numpy.ufunc']"
18147,open,jrfiedler,"<!-- Please describe the issue in detail here, and fill in the fields below -->

There are issues with `numpy.unique` when an array has dtype ""object""
1. when there are incomparable types in the array, an exception is raised
2. when the types are all numeric, but the array contains `NaN`, the result is incorrect.

Both of those issues seem to be caused by issues with `sort`.

Are `unique` and `sort` known to not work when the dtype is ""object""? I didn't see any indication in the documentation for either function.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

Here, `np.unique` works as expected:

```python
In [1]: import numpy as np

In [2]: arr = np.array([0.0, 1, np.nan, 1.0, 0])

In [3]: np.sort(arr)
Out[3]: array([ 0.,  0.,  1.,  1., nan])

In [4]: np.unique(arr)
Out[4]: array([ 0.,  1., nan])
```
Here, mixed type inside of `dtype=""object""` causes an exception:
```python
In [5]: arr = np.array([""a"", ""b"", 1.0, ""b"", ""a""], dtype=""object"")

In [6]: np.sort(arr)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-6-3b220bbb055a> in <module>
----> 1 np.sort(arr)

<__array_function__ internals> in sort(*args, **kwargs)

~/anaconda3/envs/tabnet/lib/python3.8/site-packages/numpy/core/fromnumeric.py in sort(a, axis, kind, order)
    989     else:
    990         a = asanyarray(a).copy(order=""K"")
--> 991     a.sort(axis=axis, kind=kind, order=order)
    992     return a
    993 

TypeError: '<' not supported between instances of 'float' and 'str'
```
Here, no exception is raised, but the results are incorrect:
```python
In [7]: arr = np.array([0.0, 1, np.nan, 1.0, 0], dtype=""object"")

In [8]: np.sort(arr)
Out[8]: array([0.0, 1, nan, 0, 1.0], dtype=object)

In [9]: np.unique(arr)
Out[9]: array([0.0, 1, nan, 0, 1.0], dtype=object)

In [10]: arr = np.array([0.0, 1.0, np.nan, 1.0, 0.0], dtype=""object"")

In [11]: np.sort(arr)
Out[11]: array([0.0, 1.0, nan, 0.0, 1.0], dtype=object)

In [12]: np.unique(arr)
Out[12]: array([0.0, 1.0, nan, 0.0, 1.0], dtype=object)
```
The documentation for `sort` says ""In numpy versions >= 1.4.0 nan values are sorted to the end.""


### NumPy/Python version information:

Seen in NumPy versions 1.19.5, Python version 3.8.5

",2021-01-11 04:32:16,,"Issues with np.unique when dtype=""object""",['unlabeled']
18134,open,maciej-jedynak,"To the contrary ```numpy.save()``` seems safe, because when executed on a Masked Array, it raises the NotImplementedError, which is good, because the user does not fall in the trap of unconsciously loosing masks.
Nevertheless, it is possible with ```numpy.savez()```.


### Reproducing code example:
```
In [1]: import numpy as np

In [2]: np.__version__
Out[2]: '1.19.2'

In [3]: import numpy.ma as ma

In [4]: am = ma.masked_array([1, 2, 3], mask=[False, True, False])

In [5]: np.save('am.np', am)
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-5-acde106107e0> in <module>
----> 1 np.save('am.np', am)

<__array_function__ internals> in save(*args, **kwargs)

~/anaconda3/envs/numpy119/lib/python3.8/site-packages/numpy/lib/npyio.py in save(file, arr, allow_pickle, fix_imports)
    526     with file_ctx as fid:
    527         arr = np.asanyarray(arr)
--> 528         format.write_array(fid, arr, allow_pickle=allow_pickle,
    529                            pickle_kwargs=dict(fix_imports=fix_imports))
    530 

~/anaconda3/envs/numpy119/lib/python3.8/site-packages/numpy/lib/format.py in write_array(fp, array, version, allow_pickle, pickle_kwargs)
    673     else:
    674         if isfileobj(fp):
--> 675             array.tofile(fp)
    676         else:
    677             for chunk in numpy.nditer(

~/anaconda3/envs/numpy119/lib/python3.8/site-packages/numpy/ma/core.py in tofile(self, fid, sep, format)
   6114 
   6115         """"""
-> 6116         raise NotImplementedError(""MaskedArray.tofile() not implemented yet."")
   6117 
   6118     def toflex(self):

NotImplementedError: MaskedArray.tofile() not implemented yet.

In [6]: np.savez('am.npz', am)

In [7]: am_loaded = np.load('am.npz')

In [8]: am_loaded
Out[8]: <numpy.lib.npyio.NpzFile at 0x7f08a7157a90>

In [9]: am_loaded['arr_0']
Out[9]: array([1, 2, 3])

```

### NumPy/Python version information:

```In [10]:  import sys, numpy; print(numpy.__version__, sys.version)```
1.19.2 3.8.5 (default, Sep  4 2020, 07:30:14) 
[GCC 7.3.0]



",2021-01-07 16:30:22,,"BUG: Masked array can be saved to file with numpy.savez(), but after loading it, the mask is gone.","['00 - Bug', 'component: numpy.ma']"
18112,open,Shaurya2525,"I'm trying to use the numpy.lcm function in my code, but after a 19 digit number output, the answers start shifting from positive to negative and back. Can you guys help me with my problem? Thanks.

### Reproducing code example:
>>> import numpy as np
>>> np.lcm(219060189739591200, 43)
-9027155914907130016

### Error message:

No specific error, but the answer is negative when it should clearly not be.

### NumPy/Python version information:
numpy              1.8.1

",2021-01-04 23:55:04,,numpy.lcm() function can't handle large numbers,['unlabeled']
18099,open,ghobart,"I apologize if this is my bad.
I encountered an issue with numpy.arrange(), I believe that I isolated the problem to the dtype arg giving different results between dtype =np.float and dtype =np.float32. In the case dtype =float32 , numpy.arange  with large start and end values overrided to a step of two.  Here's snippet to demonstrate:
```python
#np_ar_bug.py
import numpy as np
ns= 3635
stline =5682
num_lines=571
n_elements=ns*num_lines

ar_low_val_f32_hack = np.arange(n_elements ,dtype =np.float32) + stline*ns
ar_high_val_f32 = np.arange(  stline*ns,n_elements + stline*ns,1,dtype =np.float32) #overrides to step=2
ar_high_val_f = np.arange(  stline*ns,n_elements + stline*ns,1,dtype =np.float)

print (ar_low_val_f32_hack[:5])
print (ar_high_val_f32[:5])
print (ar_high_val_f[:5])
```
```
[20654070. 20654071. 20654072. 20654073. 20654074.]
[20654070. 20654072. 20654074. 20654076. 20654078.]
[20654070. 20654071. 20654072. 20654073. 20654074.]
```",2020-12-31 00:17:59,,numpy.arange issue with large numbers,['04 - Documentation']
18087,open,xuejianma,"<!-- Please describe the issue in detail here, and fill in the fields below -->
In the following snippet, I would expect problematicArray in line 3 gives the same error message as errorArray in line 6 for users to debug. However, line 4 prints correctArray and problematicArray without any error or warning and it shows
```
[6.123234e-17 1.000000e+00] 6.123234e-17
``` 
Only line 6 gives error as below.
### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
correctArray = np.array([np.cos(np.pi/2), np.sin(np.pi/2)])
problematicArray = np.array(np.cos(np.pi/2), np.sin(np.pi/2))
print(correctArray, problematicArray)

errorArray = np.array(np.cos(np.pi/2),1.0)
```

### Error message:
```
TypeError                                 Traceback (most recent call last)
<ipython-input-23-b77a50730289> in <module>
      4 print(correctArray, problematicArray)
      5 
----> 6 errorArray = np.array(np.cos(np.pi/2), 1.0)

TypeError: data type not understood
```

It turns out the second argument in line 3 is taken as a type in ```numpy.array(object, dtype=None, *, copy=True, order='K', subok=False, ndmin=0)```, though it appears to be a float instance. Line 6 instead captured that 1.0 is a float rather than a type and yields the error message as above. They seem not consistent. After some tests, I find that when the second argument is an instance of float or np.float, it yields the error, while when that's an instance of np.float32 or np.float62, the code doesn't yield the error and takes the second argument as a type (though it shouldn't be to users) accordingly:
```
### Lines yielding errors:
error1 = np.array(0.0, float(1.0))
error2 = np.array(0.0, np.float(1.0))
### Lines not yielding errors:
good1 = np.array(0.0, np.float32(1.0))
good2 = np.array(0.0, np.float64(1.0))
```

And more bizarrely, though having the difference as above, the np.float64(1.0) appears as an instance of python float object as:
```
isinstance(np.float64(1.0), float)
Out[1]: True
```
Same issues occur for np.asarray as well. Overall, it may be confusing for users to debug in some specific cases.
<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### NumPy/Python version information:
```
np.__version__
Out[1]: '1.17.0'
```
<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2020-12-29 03:18:06,,BUG? np.array unexpectedly taking np.float32/64 instance as dtype,['07 - Deprecation']
18071,open,v1gnesh,"The doco says that input and output has to be same dimensions.
Say I want to work on 3 different categories of a dataframe, I initially do something like

```
df1 = df[df['zoop'] == 'a']
df2 = df[df['zoop'] == 'b']
df3 = df[df['zoop'] == 'c']
```

Can something like this be implemented?

`np.piecewise(df, [condlist with above 3 as conds], [funclist for each of the above 3 conds])`

Essentially, what I want to do is 

- pass in a few columns from a DF (or maybe even the whole DF)
- condlist selecting specific rows from the DF
- funclist to do different functions to different categories / subsets from the DF
- funcs return a new columns/arrays to be slapped on the side of the DF

`piecewise` seems to be an elegant way of handling this conditional execution.
I understand that we can do `df.to_numpy()` and then pass it in, but I don't think it's possible to `out` a bigger array with higher dimensions (ex: in = 100x6, out=100x9).

Any thoughts/comments on this?",2020-12-25 13:11:24,,piecewise support for different shaped 'x' and 'out',['unlabeled']
18069,open,mattip,"Just a heads up that there is a [flaky win32 test](https://dev.azure.com/numpy/numpy/_build/results?buildId=14756&view=logs&j=0a8c3230-d1e3-529e-137b-f006b8eee956&t=4931b323-55c2-5014-af60-4597ac5180e3&l=216) from the last post-merge CI run: 

<details>

```
___________________ TestMatmul.test_vector_matrix_values _____________________

self = <numpy.core.tests.test_multiarray.TestMatmul object at 0x16138CB8>

        mat1 = np.array([[1, 2], [3, 4]])
        mat2 = np.stack([mat1]*2, axis=0)
        tgt1 = np.array([7, 10])
        tgt2 = np.stack([tgt1]*2, axis=0)
        for dt in self.types[1:]:
            v = vec.astype(dt)
            m1 = mat1.astype(dt)
            m2 = mat2.astype(dt)
>           res = self.matmul(v, m1)
E           RuntimeWarning: invalid value encountered in matmul

dt         = 'F'
m1         = array([[1.+0.j, 2.+0.j],
       [3.+0.j, 4.+0.j]], dtype=complex64)
m2         = array([[[1.+0.j, 2.+0.j],
        [3.+0.j, 4.+0.j]],

       [[1.+0.j, 2.+0.j],
        [3.+0.j, 4.+0.j]]], dtype=complex64)
mat1       = array([[1, 2],
       [3, 4]])
mat2       = array([[[1, 2],
        [3, 4]],

       [[1, 2],
        [3, 4]]])
res        = array([[ 7., 10.],
       [ 7., 10.]], dtype=float64)

```",2020-12-24 21:44:15,,Occasional failure in win32 complex matmul,['unlabeled']
18028,open,tdimitri,"## Feature

<!-- If you're looking to request a new feature or change in functionality, including
adding or changing the meaning of arguments to an existing function, please
post your idea on the [numpy-discussion mailing list]
(https://mail.python.org/mailman/listinfo/numpy-discussion) to explain your
reasoning in addition to opening an issue or pull request. You can also check
out our [Contributor Guide]
(https://github.com/numpy/numpy/blob/master/doc/source/dev/index.rst) if you
need more information. -->
These are 5 groups of feature hook requests from the [pnumpy](https://github.com/Quansight/numpy-threading-extensions) project.  @mattip request I file issues here with attention to @seberg.
We desire to make numpy work even better by adding pnumpy which can distribute work across large arrays to different CPU cores.  Due to the nature of CPU L1/L2/L3 caching, especially on the AMD which uses [MOESI](https://en.wikipedia.org/wiki/MOESI_protocol), it is beneficial to hook **all popular array loops**.  If the routines are not hooked, the ""pnumpy"" project will perform poorly only because we lack the hooks from the parent project ""numpy"", even though the code and concepts work well.

**Hook 1**: conversion functions
When an array is converted from one dtype to another, such as int32 to float64, we would like to hook this loop.  This is necessary to speed operations like np.true_divide(a,3)  where a is int32.  Although we can hook ""true_divide"" there is a behind the scenes conversion of the int32 array (in this example) which is single threaded, and destroys the cache on another core for large arrays.

**Hook 2**: reduce functions
Although we can hook functions like np.sum, setbufsize gets in the way.  pnumpy's low level loops can internally upcast float32 to float64 and does not require setbufsize upcasting or additional conversions.  With this change, a parallel sum will run 10x faster on a large array.  Further functions like nansum, argmax, nanargmax, var, std, and others are missing ufunc style hooks.  For example ""std"" can be sped up 20x for large arrays with a hook in place.

**Hook 3**: array initialization functions
Because of how pnumpy works with caches across cores, it is benefical to hook functions which are often used to initalize arrays.  Common functions include zeros, ones, arange, and random number generation.

**Hook 4**: how to hook a call signature that does not yet exist?
For pnumpy we have some additional loops that can do things like: compare int64 to uint64 without upcasting.  In numpy today, such a comparison results in upcasting to float64, with mantissa loss and thus an imperfect comparison.  We would like to via PyUFunc_ReplaceLoopBySignature the ""compare_"" functions with signatures like (int64, uint64, bool) but I am not sure how.
I am also not sure how to hook true_divide with a signature like (int32, int32, float64).  Really I prefer (int32, any, float64) and (any, int32, float64) because inside the true_divide loop, a conversion can be passed as a parameter.  Inside the true_divide loop, it will convert int32-> float64 on the stack, such that it fits in the L1 cache.  Then it will internally upcast the other array (i.e., int32) and divide with the converted result still on the stack, and lastly place the result in the output array.  This will make true_divide run much faster when dividing integers.

**Hook 5**: allow for user callback arg
There currently is no user arg callback in the ufunc routine.  Because of this, thousands of stubs have to be written to feed back into to a common function.  See [stubs.h](https://github.com/Quansight/numpy-threading-extensions/blob/main/src/pnumpy/stubs.h).  This entire file goes away if we can pass a ""void *"" callback argument in the function PyUFunc_ReplaceLoopBySignature.

Thank you
",2020-12-18 21:59:03,,"pnumpy: request ufunc like hooks for conversion functions, reduce funcs, better ufunc hooking with callback",['unlabeled']
18020,open,ehchen,"The example given [here](https://github.com/numpy/numpy/blob/5b63f260933672b7182daf4fb15ffcd15bae68bf/numpy/lib/polynomial.py#L1012) with the following polynomial:

3x^2 + 5x + 2 / (2x + 1) 

Should have the following code to be consistent with numpy >1.4, as documented [here](https://numpy.org/devdocs/reference/routines.polynomials.package.html#module-numpy.polynomial):
```
x = np.array([2.0, 5.0, 3.0])
y = np.array([1.0, 2.0])
np.polydiv(x, y)
(array([1.75 , 1.5]), array([0.25]))
```",2020-12-18 04:36:46,,polydiv example incorrect,['04 - Documentation']
18009,open,prehensilecode,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
np.test()
```

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->
```
NumPy version 1.19.4
NumPy relaxed strides checking option: True
...
=================================== FAILURES ===================================
_____________________________ TestArgmax.test_all ______________________________

self = <numpy.core.tests.test_multiarray.TestArgmax object at 0x1552f387a4c0>

    def test_all(self):
        a = np.random.normal(0, 1, (4, 5, 6, 7, 8))
        for i in range(a.ndim):
            amax = a.max(i)
            aargmax = a.argmax(i)
            axes = list(range(a.ndim))
            axes.remove(i)
>           assert_(np.all(amax == aargmax.choose(*a.transpose(i,*axes))))
E           AssertionError

a          = array([[[[[ 1.38877940e+00, -6.61344243e-01,  3.03085711e+00, ...,
           -5.11884476e-02, -7.25597119e-01, -8.677...20441223e+00, -2.10748923e-01, -1.13366514e+00, ...,
           -1.15429538e+00, -1.31669311e+00, -1.87520944e+00]]]]])
aargmax    = array([[[[0, 2, 0, 2, 0, 3, 2, 1],
         [1, 6, 4, 5, 5, 6, 0, 3],
         [0, 6, 1, 5, 5, 1, 0, 1],
         [5, ..., 1, 6, 1],
         [1, 3, 2, 3, 2, 2, 5, 1],
         [1, 4, 4, 2, 5, 1, 2, 6],
         [4, 2, 3, 6, 6, 0, 4, 5]]]])
amax       = array([[[[ 1.3887794 ,  1.15528789,  3.03085711,  1.12232832,
           0.65458015,  1.27852808,  1.45142926,  1.1926... [ 1.73858465,  0.75702112,  1.47586912,  1.56265184,
           2.0087944 ,  0.96147764,  1.5981996 ,  0.45579891]]]])
axes       = [0, 1, 2, 4]
i          = 3
self       = <numpy.core.tests.test_multiarray.TestArgmax object at 0x1552f387a4c0>

/somewhere/python/gcc/3.8.6/lib/python3.8/site-packages/numpy/core/tests/test_multiarray.py:4126: AssertionError
_____________________________ TestArgmin.test_all ______________________________

self = <numpy.core.tests.test_multiarray.TestArgmin object at 0x15523afaf100>

    def test_all(self):
        a = np.random.normal(0, 1, (4, 5, 6, 7, 8))
        for i in range(a.ndim):
            amin = a.min(i)
            aargmin = a.argmin(i)
            axes = list(range(a.ndim))
            axes.remove(i)
>           assert_(np.all(amin == aargmin.choose(*a.transpose(i,*axes))))
E           AssertionError

a          = array([[[[[-1.20189124e+00, -2.64069726e+00, -1.05673589e+00, ...,
           -1.92950056e-01,  1.61903824e+00, -1.154...32679837e-01, -1.27060434e+00,  9.74533236e-01, ...,
            9.90226982e-02,  2.92480583e-01,  1.46520893e+00]]]]])
aargmin    = array([[[[0, 0, 0, 0, 4, 3, 1, 6],
         [4, 5, 3, 4, 6, 0, 2, 3],
         [0, 2, 1, 3, 2, 4, 1, 2],
         [4, ..., 2, 6, 1],
         [4, 5, 4, 1, 6, 5, 4, 1],
         [4, 6, 6, 3, 2, 3, 6, 0],
         [3, 4, 4, 1, 5, 5, 3, 1]]]])
amin       = array([[[[-1.20189124, -2.64069726, -1.05673589, -0.98774648,
          -1.31636155, -1.76966134, -0.88461579, -1.5217... [-1.05422176, -2.2090923 , -1.09069519, -0.767583  ,
          -2.06558049, -2.30822806, -0.1176568 , -1.08836093]]]])
axes       = [0, 1, 2, 4]
i          = 3
self       = <numpy.core.tests.test_multiarray.TestArgmin object at 0x15523afaf100>

/somewhere/python/gcc/3.8.6/lib/python3.8/site-packages/numpy/core/tests/test_multiarray.py:4261: AssertionError
________________________ TestSelect.test_many_arguments ________________________

self = <numpy.lib.tests.test_function_base.TestSelect object at 0x1552ca7c3cd0>

    def test_many_arguments(self):
        # This used to be limited by NPY_MAXARGS == 32
        conditions = [np.array([False])] * 100
        choices = [np.array([1])] * 100
>       select(conditions, choices)

choices    = [array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), ...]
conditions = [array([False]), array([False]), array([False]), array([False]), array([False]), array([False]), ...]
self       = <numpy.lib.tests.test_function_base.TestSelect object at 0x1552ca7c3cd0>

/somewhere/python/gcc/3.8.6/lib/python3.8/site-packages/numpy/lib/tests/test_function_base.py:444:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<__array_function__ internals>:5: in select
    ???
        args       = ([array([False]), array([False]), array([False]), array([False]), array([False]), array([False]), ...], [array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), ...])
        kwargs     = {}
        relevant_args = <generator object _select_dispatcher at 0x1552ca8baba0>
/somewhere/python/gcc/3.8.6/lib/python3.8/site-packages/numpy/lib/function_base.py:682: in select
    choicelist = np.broadcast_arrays(*choicelist)
        choicelist = [array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), ...]
        condlist   = [array([False]), array([False]), array([False]), array([False]), array([False]), array([False]), ...]
        default    = 0
        dtype      = dtype('int64')
<__array_function__ internals>:5: in broadcast_arrays
    ???
        args       = (array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), ...)
        kwargs     = {}
        relevant_args = (array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), ...)
/somewhere/python/gcc/3.8.6/lib/python3.8/site-packages/numpy/lib/stride_tricks.py:264: in broadcast_arrays
    return [_broadcast_to(array, shape, subok=subok, readonly=False)
        args       = [array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), ...]
        shape      = ()
        subok      = False
/somewhere/python/gcc/3.8.6/lib/python3.8/site-packages/numpy/lib/stride_tricks.py:264: in <listcomp>
    return [_broadcast_to(array, shape, subok=subok, readonly=False)
        .0         = <list_iterator object at 0x1552c26d0070>
        array      = array([1])
        shape      = ()
        subok      = False
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

array = array([1]), shape = (), subok = False, readonly = False

    def _broadcast_to(array, shape, subok, readonly):
        shape = tuple(shape) if np.iterable(shape) else (shape,)
        array = np.array(array, copy=False, subok=subok)
        if not shape and array.shape:
>           raise ValueError('cannot broadcast a non-scalar to a scalar array')
E           ValueError: cannot broadcast a non-scalar to a scalar array

array      = array([1])
readonly   = False
shape      = ()
subok      = False

/somewhere/python/gcc/3.8.6/lib/python3.8/site-packages/numpy/lib/stride_tricks.py:118: ValueError
_____________________________ test_broadcast_shape _____________________________

    def test_broadcast_shape():
        # broadcast_shape is already exercized indirectly by broadcast_arrays
        assert_equal(_broadcast_shape(), ())
        assert_equal(_broadcast_shape([1, 2]), (2,))
        assert_equal(_broadcast_shape(np.ones((1, 1))), (1, 1))
        assert_equal(_broadcast_shape(np.ones((1, 1)), np.ones((3, 4))), (3, 4))
>       assert_equal(_broadcast_shape(*([np.ones((1, 2))] * 32)), (1, 2))
E       AssertionError:
E       Items are not equal:
E        ACTUAL: 0
E        DESIRED: 2

/somewhere/python/gcc/3.8.6/lib/python3.8/site-packages/numpy/lib/tests/test_stride_tricks.py:282: AssertionError
=============================== warnings summary ===============================
../../../somewhere/python/gcc/3.8.6/lib/python3.8/site-packages/_pytest/config/__init__.py:1114
  /somewhere/python/gcc/3.8.6/lib/python3.8/site-packages/_pytest/config/__init__.py:1114: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: hypothesis
    self._mark_plugins_for_rewrite(hook)

-- Docs: https://docs.pytest.org/en/stable/warnings.html
=========================== short test summary info ============================
FAILED core/tests/test_multiarray.py::TestArgmax::test_all - AssertionError
FAILED core/tests/test_multiarray.py::TestArgmin::test_all - AssertionError
FAILED lib/tests/test_function_base.py::TestSelect::test_many_arguments - Val...
FAILED lib/tests/test_stride_tricks.py::test_broadcast_shape - AssertionError:
4 failed, 10897 passed, 60 skipped, 108 deselected, 18 xfailed, 3 xpassed, 1 warning in 76.63s (0:01:16)
```
### NumPy/Python version information:
<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.19.4 3.8.6 (default, Dec 15 2020, 21:10:40)
[GCC 9.2.0]

Linked against OpenBLAS 0.3.12",2020-12-16 17:24:58,,"TST: Failed tests - test_multiarray, test_function_base, test_stride_tricks",['05 - Testing']
18004,open,inducer,"### Reproducing code example:

```python
>>> import numpy as np
>>> np.__version__
'1.19.4'
>>> a = np.array([1., 2.], dtype=object)
>>> np.exp(a)
AttributeError: 'float' object has no attribute 'exp'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: loop of ufunc does not support argument 0 of type float which has no callable exp method
```
or even
```python
>>> a = np.array([np.float64(1), np.float64(2)], dtype=object)
>>> np.exp(a)
AttributeError: 'numpy.float64' object has no attribute 'exp'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: loop of ufunc does not support argument 0 of type numpy.float64 which has no callable exp method
```

It seems odd that this doesn't work, and it feels like it should... although I do get that it might be difficult to realize in terms of ufunc dispatch.

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```
1.19.4 3.9.1 (default, Dec  8 2020, 07:51:42) 
[GCC 10.2.0]
```

cc @mtcam",2020-12-15 22:41:21,,np.exp doesn't like object arrays of (numpy) scalars,"['15 - Discussion', '57 - Close?']"
17983,open,khippytch,"<!-- Please describe the issue in detail here, and fill in the fields below -->
When used with pandas.Series() of pandas.Timedelta() objects, where() returns list of integers instead of the list of pandas.Timedelta

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
import pandas as pd

deltas = [
    pd.Timedelta(""1s"")
]
serDeltas = pd.Series(deltas)

# OK - array of Timedelta`s
print(np.where([True], deltas[0:1], pd.Timedelta(0)))

# error - array of int`s
print(np.where([True], serDeltas[0:1], pd.Timedelta(0)))

# error - array of int`s
print(np.where([True], serDeltas[0:1].values, pd.Timedelta(0)))


```

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.18.5 3.7.9 (default, Aug 31 2020, 17:10:11) [MSC v.1916 64 bit (AMD64)]
",2020-12-11 17:01:36,,numpy.where() incorrectly works with pandas.Series() of pandas.Timedelta,['unlabeled']
17982,open,Socob,"## Documentation
This is basically a request to document an answer to [this (thus-far unanswered) Stackoverflow question](https://stackoverflow.com/q/65219228).

On my machine, I can observe the following behavior for integer data types:

    >>> import numpy
    >>> numpy.uint8(-1)
    255
    >>> numpy.int8(128)
    -128
    >>> numpy.int8(-129)
    127
    >>> numpy.int8(127) + numpy.int8(1)
    # RuntimeWarning: overflow encountered in byte_scalars
    -128
    >>> numpy.int8(-128) - numpy.int8(1)
    # RuntimeWarning: overflow encountered in byte_scalars
    127

So in all cases, when creating a value of a given type, the input overflows or underflows by wrapping around to the other end of the range of representable values (without a warning), as for unsigned integers. When an overflow or underflow occurs during an operation like addition, the behavior is the same, but a `RuntimeWarning` is emitted.

However, it is unclear/undocumented whether this behavior can be relied on, or whether it can vary depending on the underlying C compiler, since signed integer overflow is undefined behavior in C, and the result of casting can depend on the underlying binary representation of signed integers. The relevant part of the documentation seems to be [the “Data types” page, specifically this section on overflow errors][1], which does give an example involving a signed integer type, but does not explicitly specify what the rules are for casts and overflows and whether the behavior can be expected to be the same across all platforms.


  [1]: https://numpy.org/doc/stable/user/basics.types.html#overflow-errors",2020-12-11 16:50:27,,Document behavior of casts and overflows for signed integer types,['04 - Documentation']
17926,open,ganesh-k13,"## Documentation

During NumPy office-hours meeting, @melissawm, @MaiaKaplan and @a-elhag shared a [YouTube video](https://youtu.be/lHJqOE5j6xE?t=3005) of @mattip showing how we can debug C part of NumPy using GDB. This was very helpful and will be nice if it's part of the official documentation for newcomers in particular.

Some additional links for anyone picking this up:
1. https://scipy-lectures.org/advanced/debugging/index.html
2. https://serge-m.github.io/debugging-numpy-any-c-code-of-python.html

cc: @seberg 
<!-- If this is an issue with the current documentation for NumPy (e.g.
incomplete/inaccurate docstring, unclear explanation in any part of the
documentation), make sure to leave a reference to the document/code you're
referring to. You can also check the development version of the documentation
and see if this issue has already been addressed: https://numpy.org/devdocs/
-->

<!-- If this is an idea or a request for content, please describe as clearly as
possible what topics you think are missing from the current documentation. Make
sure to check https://github.com/numpy/numpy-tutorials and see if this issue
might be more appropriate there. -->
",2020-12-05 06:52:40,,DOC: GDB for NumPy,['04 - Documentation']
17903,open,Verma-Rajat,"```python
import numpy as np
a = np.random.randint(-2045, 35678,(536870912),dtype=np.int32) # this consumes 2GB
b = np.random.randint(-2045, 35678,(536870912),dtype=np.int32) # this consumes another 2GB

# this api call consumes huge amount of memory, on my system, this consume almost all of available memory, close to 14GB.
np.testing.assert_array_equal(a,b)
```

While running these commands in terminal, observe memory usage in another terminal using:
`watch -n1 free -h`

System info:
Linux machine15 4.15.0-106-generic #107-Ubuntu SMP Thu Jun 4 11:27:52 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
Python 3.6.8
numpy==1.16.6

Note: I can't check on latest python, but I upgraded numpy to v1.19.4 and observed same behavior.
",2020-12-03 10:28:09,,numpy testing methods consume huge memory,['unlabeled']
17864,open,theXYZT,"It seems that `np.dtype(...)` does not produce consistent hashes and does not equal the hash of the contained dtype. I feel like these should be consistent with each other so that the following will return true:

```
np.empty(1).dtype in {np.float32, np.float64}  # False
```

### Reproducing code example:

```
In [1]: np.float64 == np.dtype(np.float64)
Out[1]: True

In [2]: np.float64 in {np.dtype(np.float64)}
Out[2]: False

In [3]: hash(np.float64)
Out[3]: 8793996338852

In [4]: hash(np.dtype(np.float64))
Out[4]: 3855163183283070272
```

`hash(np.dtype(np.float64))` returns a different value when called in different runtimes too.

---

Would this be worth fixing?",2020-11-28 06:36:15,,np.dtype object does not produce consistent hashes.,['unlabeled']
17850,open,nschloe,"I have 5 arrays of the same data type and the same length `n` (can be quite large). I need to regularly mask all of those array with the same mask, and I thought about what would be the fastest way. Fitting the data into one `(n, 5)` array works, and masking is equally fast with a `(5, n)` array.

I'd love to use a [structured array](https://numpy.org/doc/stable/user/basics.rec.html) so I can properly name the data in the rest of the code (e.g., `arr[""density""]` instead of `arr[:, 3]`). Unfortunately, structured arrays mask way slower than the other two options:

![out](https://user-images.githubusercontent.com/181628/100277704-e7abdd00-2f63-11eb-8b21-e786ae723597.png)

Code to reproduce the plot:

```python
import numpy
import perfplot
import random


def setup(n):
    a = numpy.random.rand(n, 5)
    b = numpy.ascontiguousarray(a.T)
    c = a.copy()
    c.dtype = [(f""col{k}"", c.dtype) for k in range(a.shape[1])]

    idx = random.sample(range(0, n), 25)
    mask = numpy.ones(n, dtype=bool)
    mask[idx] = False
    return a, b, c, mask


def mask_rows(data):
    a, _, _, mask = data
    return a[mask]


def mask_named(data):
    _, _, c, mask = data
    return c[mask]


def mask_cols(data):
    _, b, _, mask = data
    return b[:, mask]


perfplot.show(
    setup=setup,
    kernels=[mask_rows, mask_cols, mask_named],
    n_range=[2 ** k for k in range(5, 20)],
    equality_check=None,
)
```",2020-11-25 20:24:43,,structured arrays slow to mask,['unlabeled']
17846,open,samredai,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

```python
import pandas as pd  # v1.0.5
import numpy as np   # v1.19.4
df = pd.DataFrame({""a"":[1,2,3],""b"":[4,5,6]})
np.isclose(df.a,df.b)
# output: array([False, False, False])
df.a = df.a.astype(pd.Int64Dtype())
df.b = df.b.astype(pd.Int64Dtype())
np.isclose(df.a,df.b)
```

### Error message:
```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-22-fe7c48482d30> in <module>
----> 1 np.isclose(df.a,df.b)

<__array_function__ internals> in isclose(*args, **kwargs)

/apps/python3/lib/python3.7/site-packages/numpy/core/numeric.py in isclose(a, b, rtol, atol, equal_nan)
   2285     y = array(y, dtype=dt, copy=False, subok=True)
   2286 
-> 2287     xfin = isfinite(x)
   2288     yfin = isfinite(y)
   2289     if all(xfin) and all(yfin):

TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
```
### NumPy/Python version information:

```python
import sys, numpy; print(numpy.__version__, sys.version)
# 1.19.4 3.7.9 (default, Nov  9 2020, 18:54:35) 
# [GCC 7.5.0]
```
",2020-11-25 17:52:34,,np.isclose() raises an exception when used with pandas 1.0 Int64Dtype objects,['unlabeled']
17839,open,Qiyu8,"

Fast Fourier transforms are widely used for [applications](https://en.wikipedia.org/wiki/Discrete_Fourier_transform#Applications) in engineering, music, science, and mathematics.The current fft implementation was introduced two years ago([#11885](https://github.com/numpy/numpy/issues/11885), [#11888](https://github.com/numpy/numpy/pull/11888) ), which is not the best implementation as far as I know, The drawbacks includes:

- Don't supports single  precision transforms.
- Don't support for multi-D transforms.
- Don't  make use of vector instructions for FFTs, The main reason that It's performance is not good enough(At least better than [fftpack](https://www.netlib.org/fftpack/)).

Although the author published the C++ based [pypocketfft](https://gitlab.mpcdf.mpg.de/mtr/pypocketfft) later, which has overcame these shortcomings, but It's was not compatible with C based numpy,  I found out that there has several backend projects that worth considering.

| project                                                      | worth introducing?                                           |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [fftw3](https://github.com/FFTW/fftw3)                       | Most popular FFT library, which nowadays is the ""gold standard"" for FFT implementations. It's also the default fft algorithm of [matlab](https://nl.mathworks.com/help/matlab/ref/fft.html#moreabout), there has a python wrapper [pyfftw](https://github.com/pyFFTW/pyFFTW), can't integrated due to it's GPL Licence. |
| [Intel MKL](https://software.intel.com/en-us/mkl/features/fft)/[IPP](https://software.intel.com/en-us/intel-ipp) | Significantly faster than FFTW with intel processors, already integrated as a third party lib. |
| [KFR](https://github.com/kfrlib/kfr)                         | Claims to be faster than FFTW, can't integrated due to it's commecial Licence. |
| [FFTS](https://github.com/anthonix/ffts)                     | reported to be faster than FFTW because the use of SIMD instructions, at least in some cases.  **worth considering**. |
| [FFTE](http://www.ffte.jp/)                                  | reported to be faster than FFTW, but the source code is Fortran-based. |
| [Ooura FFT](http://www.kurims.kyoto-u.ac.jp/~ooura/fft.html) | provide C and Fortran version implementation, but It hasn't been updated in a long time. |
| [muFFT](https://github.com/Themaister/muFFT)/[pffft](https://bitbucket.org/jpommier/pffft)/[PGFFT](https://www.shoup.net/PGFFT/) | have performance comparable to FFTW.depends strongly on the SIMD instructions. but with limited features. |
| [KissFFT](https://github.com/mborgerding/kissfft)/[PocketFFT](https://gitlab.mpcdf.mpg.de/mtr/pocketfft) | The simplest but also the slowest one here,  which is the current solution. |

Now we have two options:

- choose a new backend system such as FFTS, which is painful because of the trival adaptation process.
- optimize the current algorithm based on universal intrinsics, which is more practical and operable.",2020-11-24 12:47:27,,ENH: Optimize the FFT implementation,['unlabeled']
17835,open,efwilliams,"Feeding all nans into linalg.lstsq doesn't get caught at the Numpy level, and returns different MKL errors depending on the version. Numpy 1.19.4 kind of catches it in that it throws a ValueError, but it actually gives a more confusing message since while it is clear that matrix A is the 4th parameter of DGELSD, I have no idea what CFROM, the 4th parameter of DLASCL, is or where/why it is called. 

This may be related to issue #7644, and I have found a couple examples of this same problem reported on other websites and not as an issue here. 

```python
import numpy as np
N = 2
A = np.empty((N,N))
b = np.empty(N)
A[:]= np.nan
b[:]= np.nan
x = np.linalg.lstsq(A,b,rcond=None)[0]
```

For numpy version 1.14.6:
```
Intel MKL ERROR: Parameter 4 was incorrect on entry to DGELSD.
```

For numpy version 1.19.4:
```
Traceback (most recent call last):
  File ""/home/efwillia/Desktop/research/Belgium/interferometry/Gibraltar_tomo/tomo_tomo/bug_ex.py"", line 14, in <module>
    x = np.linalg.lstsq(A,b,rcond=None)[0]
  File ""<__array_function__ internals>"", line 5, in lstsq
  File ""/home/efwillia/anaconda3/envs/newnumpy/lib/python3.9/site-packages/numpy/linalg/linalg.py"", line 2306, in lstsq
    x, resids, rank, s = gufunc(a, b, rcond, signature=signature, extobj=extobj)
ValueError: On entry to DLASCL parameter number 4 had an illegal value
```


",2020-11-24 07:20:20,,"linalg.lstsq nan error not caught by numpy, get MKL error message",['component: numpy.linalg']
17831,open,ganesh-k13,"I would like to take this as a follow-up PR as we need to make two major changes:
For both timedeltas, we need to do this: https://github.com/numpy/numpy/blob/000dfda49a012bd38efc6fbb89ece58279895bad/numpy/core/src/umath/loops.c.src#L1446-L1448

But as I mentioned in https://github.com/numpy/numpy/pull/17727#issuecomment-726084647, we are truncating instead of flooring. Second being the setting of invalid division.

_Originally posted by @ganesh-k13 in https://github.com/numpy/numpy/pull/17727#discussion_r528838830_

Refers: #17727, #16458
CC: @seberg , @mattip, @eric-wieser ",2020-11-23 17:03:16,,"BUG, ENH: Refine Nat checks and division by 0 setting in timedelta division code","['00 - Bug', 'component: numpy.ufunc', 'component: numpy.datetime64']"
17803,open,munael,"## Feature

- Provide some API for getting both a sorted version of some array _and_ the argsort of the input.
- The main point here vs the naive solution is to have a more efficient low-level call.",2020-11-19 08:36:26,,Combined `sort`/`argsort` function,['unlabeled']
17760,open,nschloe,"The docs of `bincount` say

> Count number of occurrences of each value in array of non-negative ints.

but doesn't work with an input array of dtype `numpy.uint64`.
```python
import numpy

a = numpy.array([0, 1, 2], dtype=numpy.uint64)
numpy.bincount(a)
```
```
  File ""<__array_function__ internals>"", line 5, in bincount
TypeError: Cannot cast array data from dtype('uint64') to dtype('int64') according to the rule 'safe'
```
This is with
```
python3 -c ""import numpy; print(numpy.__version__)""
```
```
1.19.4
```
on Linux.",2020-11-12 12:16:08,,"BUG, ENH: make bincount work with uint64",['unlabeled']
17752,open,jonashaag,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

Unfortunately I don't have a small reproducing example, but I can reproduce it most of the times in my code.

I call into `librosa.feature.melspectrogram` which calls into `np.dot`, which hangs.

It only happens after a while of doing these calculations. With 2 threads started, I get 3 cores maxed out when it happens.

When I let it run in that 100% CPU state for a very long time, I get exceptions saying stuff about Python thread state. Sorry that I don't have the output anymore. I will follow up if I can get my hands on it again.

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

```
1.18.5 3.8.5 (default, Sep  5 2020, 10:50:12)
[GCC 10.2.0]
```

On Arch Linux, related libs:

```
local/blas 3.9.0-3
    Basic Linear Algebra Subprograms
local/cblas 3.9.0-3
    C interface to BLAS
local/openmpi 4.0.4-1
    High performance message passing library (MPI)
```",2020-11-10 20:52:07,,np.dot infinite hang if used with multiple threads,['unlabeled']
17744,open,EricCousineau-TRI,"## Documentation

Things can get confusing when you see an array of size 1 but dimension 0, e.g.:
```
>>> import numpy as np
>>> np.__version__
'1.19.2'
>>> x = np.array(1.0)
>>> x.shape
()
>>> x
array(1.)
>>> x.reshape(-1)
array([1.])
```

Is this mentioned anywhere in the docs with a more succinct name?

## Background

I skimmed through the docs here:
https://numpy.org/devdocs/user/index.html
As well as for the version that I've been using on Ubuntu 18.04:
https://numpy.org/doc/1.13/user/quickstart.html

The closest ""nuance"" I can see about this is array scalars / `np.generic` / `ndarray.item`:
https://numpy.org/devdocs/reference/arrays.scalars.html

So this all is great when dealing with different `dtype`s, etc., esp. for `dtype=object` for our use, but doesn't really shed light on scalar-sized arrays (or scalar arrays).",2020-11-09 19:22:20,,"DOC: ""Scalar arrays"" (or 0-dimensional arrays)?",['04 - Documentation']
17724,open,seberg,"We should maybe have a test for this type of thing somewhere - verifying that only objects actually in the `np.` namespace have `__module__` set this way.

_Originally posted by @eric-wieser in https://github.com/numpy/numpy/pull/17723#issuecomment-723126822_

---

This might overlap to some degree with the fact that we should probably move the sanity checks in the `__array_function__` wrappers into a test rather than running them at import time.",2020-11-06 16:00:29,,TST: Create tests for correct `__module__` overriding,"['17 - Task', '05 - Testing', 'sprintable']"
17718,open,BvB93,"From https://github.com/numpy/numpy/pull/17717#issuecomment-722441407:
> It would be nice if we could provide a bash command to find these automatically in the git ignore - the ones in question are simply outputs of .c.src and .h.src files, so a one-liner that finds all .src files, strips that extension, and spits out the names suitable for inclusion in a gitignore would be great (as a comment in the git-ignore)",2020-11-05 18:00:47,,ENH: Add a tool for automatically identifying `.gitgnore`-able files,"['01 - Enhancement', '23 - Wish List']"
17713,open,mattip,"In pr gh-17102 we make a hard requirement on the ""baseline profile"" in order to use the binary package compiled with default settings. We should document that the minimum baseline is a hard requirement for using the NumPy source code as-is, otherwise compilation is required with a lower `--cpu-baseline`. Our installation documentation lives at numpy/numpy.org. Perhaps a sentence on this page https://numpy.org/install/ about minimum hardware requirements would be appropriate, with a comment in the numpy source code at the point where [we define the `min` baseline](https://github.com/numpy/numpy/blob/8829b807a841911ce18e79b308fee9fb92fb91b6/numpy/distutils/ccompiler_opt.py#L214) that links to https://numpy.org/install/. Note also that OpenBLAS has a [similar requirement](https://github.com/MacPython/openblas-libs/blob/master/travis-ci/build_steps.sh#L88)

We might want to revisit the `RuntimeError` that could be raised when trying to import `_multiarray_ufunc` due to a non-conformant hardware platform, and make sure the [error message](https://github.com/numpy/numpy/blob/459991afcddfce6fbbae3fa83467fe607f2de7d0/numpy/core/src/common/npy_cpu_features.c.src#L164) provides enough information for a naive user to proceed. In many cases, they will be unable to compile NumPy on such weak hardware.",2020-11-05 06:40:06,,DOC: document the minimum required hardware,['04 - Documentation']
17704,open,chenzhaiyu,"<!-- Please describe the issue in detail here, and fill in the fields below -->
The `numpy.array2string()` method seems very inefficient when comparing with an ordinary float-to-string cast followed by a string concatenation.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
import time

num = 100000
precision = 10  # this actually should give array2string an advantage
disp_limit = 17

myarray = np.random.rand(num)

# an ordinary float-to-string cast
time1_start = time.time()
mystr1 = ''
for i in myarray:
    mystr1 += str(i) + '\n'
time1_end = time.time()

# array2string method
time2_start = time.time()
mystr2 = np.array2string(myarray, threshold=num, precision=precision, separator='\n')
time2_end = time.time()


print('myarray:', myarray[0])
print('mystr1:', mystr1[:disp_limit])
print('mystr2:', mystr2[1:disp_limit])  # to get rid of '[' at index 0
print('time1:', time1_end-time1_start)
print('time2:', time2_end-time2_start)

# output (when num==100000):
# myarray: 0.9384956966968542
# mystr1: 0.938495696696854
# mystr2: 9.3849569670e-01
# time1: 0.18199920654296875
# time2: 5.781404972076416

# output (when num==200000):
# myarray: 0.07377634223264928
# mystr1: 0.073776342232649
# mystr2: 7.3776342233e-02
# time1: 0.6239731311798096
# time2: 22.97590446472168

# output (when num==500000):
# myarray: 0.2910482709854372
# mystr1: 0.291048270985437
# mystr2: 2.9104827099e-01
# time1: 3.205667734146118
# time2: 152.17857217788696
```

<!-- Full error message, if any (starting from line Traceback: ...) -->

### NumPy/Python version information:
1.19.2 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]
<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2020-11-03 17:20:45,,array2string very inefficient,['01 - Enhancement']
17698,open,mattip,"We are slowly getting to a best-of-breed set of CPU inner loop functions using Unversal Intrinsics. It would be helpful to other projects if we could refactor these into a stand-alone C/C++ library, so they could be re-used in the growing number of NumPy-like libraries.",2020-11-03 11:38:46,,ENH: refactor our loop implementations into a stand-alone library,"['01 - Enhancement', '23 - Wish List', 'component: numpy.ufunc', 'component: SIMD']"
17697,open,CastleStar14654,"If User accessed `MaskedArray`'s default `fill_value` before assignment of it, an overflow might occur.

### Reproducing code example:

```python
import numpy as np
a = np.ma.MaskedArray([], dtype='i8') # int64
print(a.fill_value)
a.set_fill_value(2**40) # 2**40 is of course not overflow for 'i8'
```

### Error message:

```
Traceback (most recent call last):
  File ""tmp.py"", line 4, in <module>
    a.set_fill_value(2**40) # 2**40 is of course not overflow for 'i8'
  File ""C:\Program Files\Python38\lib\site-packages\numpy\ma\core.py"", line 3724, in fill_value
    _fill_value[()] = target
OverflowError: Python int too large to convert to C long
```

### NumPy/Python version information:

`1.19.2 3.8.5 (tags/v3.8.5:580fbb0, Jul 20 2020, 15:57:54) [MSC v.1924 64 bit (AMD64)]`

### Probable Cause of this bug

Default `fill_value`s for **all integer dtype** are `999999`, which is defined by
https://github.com/numpy/numpy/blob/2a267e6a49ed68da01761c92deb7c90be207660d/numpy/ma/core.py#L428-L438
and when it is returned, no `dtype` is assigned,
https://github.com/numpy/numpy/blob/2a267e6a49ed68da01761c92deb7c90be207660d/numpy/ma/core.py#L469
and **default `fill_value` for `int64` array would be `int32`**.

Replace `return np.array(fill_value)` as `return np.array(fill_value, dtype=ndtype)` could possibly fix it. ",2020-11-02 17:22:02,,"BUG of 1.19.2, numpy.ma.MaskedArray: unexpected overflow of fill_value","['00 - Bug', 'component: numpy.ma']"
17676,open,aminnj,"I noticed that `histogram2d` (which uses `histogramdd`) takes the same time for bins that are uniform by
construction (specified by number of bins and a range) as for bins that may not
be uniform (specified as an array of edges). This is due to
https://github.com/numpy/numpy/blob/8829b807a841911ce18e79b308fee9fb92fb91b6/numpy/lib/histograms.py#L1058
which internally makes an array of edges to later use with `searchsorted`.
Is there a reason to not directly calculate the bin index for the case of uniform binning (as it looks like is done for the 1D `histogram`)?
Locally, I've been testing an implementation [here](https://github.com/aminnj/numpy/commit/6601776e7e1d84047f203e62ca5f3d2e43ca1da4), which does exactly this, and passes unit testing.
In my use-case, I get a 4-5x speedup. More generally, I observe the following speedups for a 2D gaussian distribution where both dimensions have uniform binning.

![image](https://user-images.githubusercontent.com/5760027/97626580-4757ac80-19e7-11eb-9ae5-e8ace7727caa.png)


### Reproducing code example:

```python
import numpy as np
import time

np.random.seed(42)
xy = np.random.normal(0, 1, (int(5e6), 2))

# warmup
_ = np.histogram2d(xy[:, 0], xy[:, 1], bins=[100, 100], range=[[-3, 3], [-3, 3]])

# bins are uniform by construction
t0 = time.time()
h1 = np.histogram2d(xy[:, 0], xy[:, 1], bins=[100, 100], range=[[-3, 3], [-3, 3]])
print(time.time() - t0)

# bins may not be uniform
t0 = time.time()
edges = np.linspace(-3, 3, 100+1)
h1 = np.histogram2d(xy[:, 0], xy[:, 1], bins=[edges, edges])
print(time.time() - t0)
```

### NumPy/Python version information:

1.19.0
3.7.3 (default, Mar 27 2019, 09:23:39) [Clang 10.0.0 (clang-1000.11.45.5)]",2020-10-29 20:11:03,,Slow histogramdd with uniform binning,['component: numpy.lib']
17669,open,WarrenWeckesser,"This issue is motivated by ongoing work in SciPy to improve the existing multivariate distributions and add new ones.  We'd like to be sure that, if possible, enhancements that we make remain consistent with the current and future behavior of NumPy's multivariate distributions.

The specific issue is how the `size` parameter interacts with broadcasting of the distribution parameters, some of which are 1-d or 2-d arrays rather than scalars.

I'm going to refer to gufunc broadcasting, so it will be helpful to have the gufunc signatures of the existing multivariate distributions (even though they are not actually implemented as gufuncs):

    Name                         Parameters      Gufunc signature
    ---------------------------  --------------  ----------------
    dirichlet                    alpha           (n)       -> (n)
    multinomial                  n, pvals        (),(n)    -> (n)
    multivariate_normal          mean, cov       (n),(n,n) -> (n)
    multivariate_hypergeometric  colors, nsample (n),()    -> (n)


@bashtage has a [pull request to add broadcasting to `multinomial`](https://github.com/numpy/numpy/pull/16740).  As an example, suppose I have four values of `n`, say 2, 5, 10 and 20, and I have two discrete probability distributions of length 3, say `[0.1, 0.2, 0.7]` and `[0.8, 0.2, 0.0]`, and I want to generate 10 multinomial variates for each combination of those inputs.  With the code in @bashtage's PR, we can write:

```
In [85]: n = np.array([2, 5, 10, 20]).reshape(-1, 1)

In [86]: pvals = np.array([[0.1, 0.2, 0.7], [0.8, 0.2, 0.0]])

In [87]: rng = np.random.default_rng()

In [88]: x = rng.multinomial(n, pvals, size=(10, 4, 2))

In [89]: x.shape
Out[89]: (10, 4, 2, 3)
```

The shape of the output has three sources: (10,) is the number of variates that we wanted for each combination of input parameters; (4, 2) is the broadcast shape of the input parameters, without the dimension associated with the gufunc signature; (3,) is the dimension associated with each variate.

Note that `size` is not the shape of the output array--`size` does not include the dimension associated with the variates themselves.

The question is how to adapt this to the other multivariate distribution methods.  The following is my attempt to generalize the behavior of @bashtage's PR, and document how the `size` parameter must be set when broadcasting the distribution parameters.

*Edit*: the rest of this comment is obsolete; I'll keep the text here to preserve the context of existing responses, but the latest version of this is in a comment below.

<strike>
*How to set the `size` parameter of the multivariate distribution methods*

1. Let S be the shape of the result of broadcasting the distribution parameters according to gufunc broadcasting.  (In the example above, this is (4, 2, 3); (4, 2) comes from the ""actual"" broadcasting (i.e. forming all the pairwise combinations),  and (3,) is the dimension associated with the elementary operation of the gufunc.)
2. If we want just one variate per broadcast combination of the distribution parameters, we can use `size=None`, and the shape of the output will be S.
3. Otherwise, drop from S the trailing dimension associated with the output term of the gufunc signature, and call this shape S1. (Continuing the example, this gives S1 = (4, 2).)
4. Prepend to S1 the number of variates desired for each broadcast combination, say `m`, to get the `size`: `size = (m,) + S1`.  (Finishing the example: we wanted 10 variates per broadcast combination, so we have `size=(10, 4, 2)`.)  (Note that the desired number of variates could be a tuple of integers instead of an integer. In the example, if we want the 10 variates arranged as a 2-d array with shape (2, 5), we could use `size=(2, 5, 4, 2)`.)


Here's how that looks for an example of broadcasting applied to the `multivariate_normal` method.  Suppose I want to generate random variates from the multivariate normal distribution.  I have four different means, say [0, 0, 0], [1, -1, 1], [0, 0, 5] and [3, 3, 2], and two covariance matrices, each with shape (3, 3).  For each combination of mean and covariance matrix, I want to generate 10 variates.  Let's say I define `mean` and `cov` as follows:

```
In [55]: mean = np.array([[0, 0, 0], [1, -1, 1], [0, 0, 5], [3, 3, 2]])

In [56]: cov = np.array([np.eye(3), 4*np.eye(3)])
```

so `mean` has shape (4, 3) and `cov` has shape (2, 3, 3).  These variables are not compatible for broadcasting in a gufunc with signature (n),(n,n)->(n), so we'll reshape `mean` to have shape (4, 1, 3) when we call `multivariate_normal`.  The gufunc broadcasting then gives a shape of (4, 2, 3).  To get the correct `size` parameter, we drop the final (3,), and prepend (10,), to get `size=(10, 4, 2)`.  The output will have shape (10, 4, 2, 3).

The call to `multivariate_normal` would be

    x = rng.multivariate_normal(mean[:, None, :], cov, size=(10, 4, 2))


Now the question is, is this the API that we want? If not, what alternatives should be considered? [0]

I think it is essential to have the desired API explicitly defined before proceeding with @bashtage's PR for the `multinomial` method.
</strike>

-----

[0] Edit: Ignore this. <strike>I have a radically different idea, but it would mean changing the API of *all* the existing distribution methods, both univariate and multivariate. The idea is to add a new parameter, say `num_variates`, that gives the number of variates to generate for each broadcast combination. (The code could be written so that if `num_variates` is given, `size` is ignored.)  The function can figure out the rest of the output shape from the shapes of the distribution parameters.  With this API, the `multinomial` example shown above becomes

    x = rng.multinomial(n, pvals, num_variates=10)

and the `multivariate_normal` example becomes

    x = rng.multivariate_normal(mean[:, None, :], cov, num_variates=10)

I think this makes using broadcasting and setting the size much simpler, but I'm not sure there is interest in such a significant change.</strike>
",2020-10-29 08:04:22,,Extending broadcasting to the multivariate distributions in `random`,"['01 - Enhancement', '15 - Discussion', 'component: numpy.random', '30 - API']"
17667,open,mattip,The new tests in PR gh-16782 (soon to be merged) could use some cleanup and refactoring. Breaking this out into a follow-on issue in order to unblock the PR.,2020-10-29 05:51:14,,"ENH, TST: refactor tests from PR 16782 (new _simd testing module)","['01 - Enhancement', '05 - Testing', '03 - Maintenance']"
17665,open,DStauffman,"### Reproducing code example:

```python
import numpy as np
import datetime

t1 = datetime.datetime(2020, 10, 28, 12, 34, 56, tzinfo=None)
t2 = np.datetime64('2020-10-28T12:34:56')
t3 = np.datetime64('2020-10-28T12:34:56', 'ms')
t4 = np.datetime64('2020-10-28T12:34:56', 'ns')
```

When I compare the values, I get what I expect until comparing the nanosecond version of datetime64 to the datetime object.

```python
>>> print(t1 == t2)
True

>>> print(t2 == t3)
True

>>> print(t3 == t4)
True

>>> print(t1 == t3)
True

>>> print(t1 == t4)
False  # bug?
```


### NumPy/Python version information:
Numpy: 1.19.1
Python: 3.8.5
",2020-10-28 23:03:48,,Datetime64 comparison to datetime fails for datetime64[ns],['component: numpy.datetime64']
17612,open,mwaskom,"Standard arrays can do (some?) numeric operations on object-typed arrays that contain only numeric entries; but masked arrays encounter an error.

The relevant context involves pandas nullable integer series and conversions to numpy objects that occur within matplotlib: https://github.com/matplotlib/matplotlib/issues/18788

<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:



```python
import numpy as np
a = np.array([1, 2, 3], dtype=np.object)
print(a.min())

a_masked = np.ma.masked_array(a)
print(a_masked.min())
```

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

```python-traceback
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-155-e7c8a9bbb3e9> in <module>
      4 
      5 a_masked = np.ma.masked_array(a)
----> 6 print(a_masked.min())

~/miniconda3/envs/seaborn-py38-latest/lib/python3.8/site-packages/numpy/ma/core.py in min(self, axis, out, fill_value, keepdims)
   5698         # No explicit output
   5699         if out is None:
-> 5700             result = self.filled(fill_value).min(
   5701                 axis=axis, out=out, **kwargs).view(type(self))
   5702             if result.ndim:

AttributeError: 'int' object has no attribute 'view'
```

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

```
1.19.1 3.8.5 (default, Aug  5 2020, 03:39:04) 
[Clang 10.0.0 ]
```
",2020-10-22 14:52:58,,Inconsistent failure to do numeric operations on object-typed masked arrays,['component: numpy.ma']
17605,open,cdfredrick,"The numpy.ma.masked_where does not broadcast the masking condition over the array to mask. 

### Reproducing code example:
```python
import numpy as np

condition = np.array([False, True, False])
a = np.random.randn(3, 100)
np.ma.masked_where(condition[:, np.newaxis], a) #throws IndexError

condition_br = np.broadcast_to(condition[:, np.newaxis], a.shape)
np.ma.masked_where(condition_br, a) #works as expected
```

### NumPy/Python version information:
1.19.1 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]

",2020-10-21 17:05:49,,Condition in numpy.ma.masked_where does not broadcast,['component: numpy.ma']
17602,open,antalszava,"<!-- Please describe the issue in detail here, and fill in the fields below -->

When using `numpy<=1.18.4`, an example `np.complex128` array multiplied with itself returns the identity (up to precision) as expected. With `numpy>=1.19.0`, however, the complex part of certain elements are still there (and lead to an eventual deviation in computations).

Perhaps related in some way to [`Deprecation of round for np.complexfloating scalars`](https://numpy.org/devdocs/release/1.19.0-notes.html#deprecation-of-round-for-np-complexfloating-scalars).

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np

m = 1
n = 2
BS = np.identity(4, dtype=np.complex128)
BS[m, m] = 1.0 / np.sqrt(2)
BS[m, n] = 1.0j / np.sqrt(2)
BS[n, m] = 1.0j / np.sqrt(2)
BS[n, n] = 1.0 / np.sqrt(2)
np.set_printoptions(precision=25)
print(BS @ BS)
```

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

The code example outputs (with version information):

*1.18.4 3.7.6 (default, Jan  8 2020, 19:59:22) [GCC 7.3.0]*
```
[[1.+0.j                 0.+0.j                 0.+0.j
  0.+0.j                ]
 [0.+0.j                 0.+0.j                 0.+0.9999999999999998j
  0.+0.j                ]
 [0.+0.j                 0.+0.9999999999999998j 0.+0.j
  0.+0.j                ]
 [0.+0.j                 0.+0.j                 0.+0.j
  1.+0.j                ]]
```
*1.19.2 3.8.1 (default, Jan  8 2020, 22:29:32) [GCC 7.3.0]*
```
[[ 1.0000000000000000e+00+0.j
   0.0000000000000000e+00+0.j
   0.0000000000000000e+00+0.j
   0.0000000000000000e+00+0.j                ]
 [ 0.0000000000000000e+00+0.j
  -2.2371143170757382e-17+0.j
   0.0000000000000000e+00+0.9999999999999998j
   0.0000000000000000e+00+0.j                ]
 [ 0.0000000000000000e+00+0.j
   0.0000000000000000e+00+0.9999999999999998j
   2.2371143170757382e-17+0.j
   0.0000000000000000e+00+0.j                ]
 [ 0.0000000000000000e+00+0.j
   0.0000000000000000e+00+0.j
   0.0000000000000000e+00+0.j
   1.0000000000000000e+00+0.j                ]]
```

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
Included in the previous section.",2020-10-21 14:42:16,,BUG: rounding error for np.complex128 array multiplication from numpy v1.19.0,"['00 - Bug', '33 - Question']"
17600,open,zklaus,"## Feature

`MaskedArray` doesn't provide specialized `partition` and `argpartition` functions. As a consequence, the corresponding `ndarray` functions are used, that are not equipped to deal with the masks.

I suggest adding these functions in the spirit of the already existing `sort` and `argsort` functions.

This issue is related to #4356, but that issue is more comprehensive and I am not sure a PR addressing all of its parts would be suitably small.
",2020-10-21 14:03:57,,Add `partition` and `argpartition` to `MaskedArray`,"['01 - Enhancement', 'component: numpy.ma']"
17593,open,loupdhiver,"After running my code on a new computer I had an issue with polyfit, which raises an error that should not happen.
Initially I ran my code on a Mac, with numpy 1.15.1.
The new computer is a laptop running on windows 10.
I tried several versions of numpy (1.19.1, 1.18.5, 1.17.5) but it didn't help and I could not dowlod 1.16.6 or 1.15.1 (because they are too old I guess)
 My initial code was  ;
```python
import numpy as np
...
p0=np.polyfit(omega[nInit:nFin]-CentOmega,phi1[nInit:nFin],2,w=Spectrum[nInit:nFin])
```
omega, phi1 and Spectrum are numpy arrays (shape (4096,) ), nInit and nFin are within range and CentOmega is a scalar. I checked that there is no NaN values or infinites and that spectrum is positive.
### Error message:

<!-- Full error message, if any (starting from line Traceback: ...) -->
LinAlgError                               Traceback (most recent call last)
<ipython-input-5-08715d79f34e> in calcCS2()
    285     #print(np.min(phi1[nInit:nFin]))
    286     #Here we remove the GDD to find the pulse shape with only higher order disp
--> 287     p0=np.polyfit(omega[nInit:nFin]-CentOmega,phi1[nInit:nFin],2,w=Spectrum[nInit:nFin]) #fitting
    288     while True:
    289         try:

<__array_function__ internals> in polyfit(*args, **kwargs)

c:\users\pierre-marc\appdata\local\programs\python\python38\lib\site-packages\numpy\lib\polynomial.py in polyfit(x, y, deg, rcond, full, w, cov)
    629     scale = NX.sqrt((lhs*lhs).sum(axis=0))
    630     lhs /= scale
--> 631     c, resids, rank, s = lstsq(lhs, rhs, rcond)
    632     c = (c.T/scale).T  # broadcast scale coefficients
    633 

<__array_function__ internals> in lstsq(*args, **kwargs)

c:\users\pierre-marc\appdata\local\programs\python\python38\lib\site-packages\numpy\linalg\linalg.py in lstsq(a, b, rcond)
   2266         # lapack can't handle n_rhs = 0 - so allocate the array one larger in that axis
   2267         b = zeros(b.shape[:-2] + (m, n_rhs + 1), dtype=b.dtype)
-> 2268     x, resids, rank, s = gufunc(a, b, rcond, signature=signature, extobj=extobj)
   2269     if m == 0:
   2270         x[...] = 0

c:\users\pierre-marc\appdata\local\programs\python\python38\lib\site-packages\numpy\linalg\linalg.py in _raise_linalgerror_lstsq(err, flag)
    107 
    108 def _raise_linalgerror_lstsq(err, flag):
--> 109     raise LinAlgError(""SVD did not converge in Linear Least Squares"")
    110 
    111 def get_linalg_error_extobj(callback):

LinAlgError: SVD did not converge in Linear Least Squares

### NumPy 1.17.5 / Python 3.8.6 

**I found a way to avoid this error by doing the following:**
```python
while True: 
        try: 
            p0=np.polyfit(omega[nInit:nFin]-CentOmega,phi1[nInit:nFin],2,w=Spectrum[nInit:nFin]) #fitting 
            break 
        except: 
            continue
```

I saw this on stackoverflow for people who had the same issue. They noticed running the same code a second time works.
This is really a stupid code, and should not work (but it does). All it says is keep doing the same thing until you don't get an error
 I don't understand why successive calls to polyfit with the same parameters would give a different result.

I didn't find this bug in your list of known issues but maybe someone else reported it?

I hope you can fix this in later versions.

Best regards,

Pierre-Marc Dansette


",2020-10-20 11:03:48,,polyfit LinAlgError: SVD did not converge in Linear Least Squares,['unlabeled']
17588,open,eric-wieser,"```python
>>> import numpy as np
>>> import inspect
>>> inspect.isabstract(np.generic)
False
>>> inspect.isabstract(np.number)
False
# etc
```

We might be able to fix this by setting the undocumented `TPFLAGS_IS_ABSTRACT` flag in C. We may also need to set the `__abstractmethods__` member to `True`, which I can do with `type.__dict__[""__abstractmethods__""].__set__(np.generic, True)`.

If we need the latter solution, it would make sense to do it in the same place as where we register ourselves with the `abc` module.

",2020-10-19 14:59:27,,inspect.isabstract does not understand numpy abstract types,['00 - Bug']
17573,open,Moe82,"I hope I'm posting in the right place. I'm looking to make my first contribution to an open source project and I believe I found a something that would improve the NumPy library. Wanted to get some opinions on it first before I dive into it. 

The numpy.roll() method rolls an array along a given axis without padding. It might be helpful if it accepted an argument that allows for padding.

For example,

numpy.roll(a, 1, axis=None, pad=0)

would shift the array 1 place to the right and change the first element to 0 instead of re-introducing the element that rolled beyond the last position at the first. One instance where this would helpful is image augmentation. 

I believe that padding the array with numpy.pad() at the start of the method would result in a clean solution. Any feedback would be greatly appreciated.",2020-10-15 17:12:57,,Padding functionality for numpy.roll(),['01 - Enhancement']
17567,open,janosdaru,"Dear Experts,

could you provide me a ""math"" reference for the 
numpy.polynomial.chebyshev.Chebyshev.fromroots
function? It is a bit complicated to recover from the source, and I would like to understand the underlying equations.


Best,

J.",2020-10-14 19:47:27,,Chebyshev from roots,['33 - Question']
17563,open,MarkWieczorek,"## Declaration of fortran types in pyf signtature file using ""use iso_fortran_env""

When calling fortran functions in python it is necessary to create both a wrapped function as well as a python signature file (pyf). In the fortran wrapper function, most people make use of the `iso_fortran_env` module to define their types, like this:

```
 # wrapper function
   subroutine pyfunc(i, x)
        use iso_fortran_env, only: sp=>real32, dp=>real64, qp=>real128
        use iso_fortran_env, only: int8, int16, int32, int64
        implicit none
        integer(int32),intent(in) :: i
        real(dp),intent(out) :: x
        call func(i, x)
    end subroutine pyfunc
```

However, in the python signature file, it is not possible to use `iso_fortran_env`. I have tried to use this module, but f2py simply ignores it. According to the documentation [online](https://numpy.org/devdocs/f2py/signature-file.html): ""Currently F2PY uses use statement only for linking call-back modules and external arguments (call-back functions)"", and this suggests to me that using arbitrary modules is not implemented.

Since it is not possible to use `iso_fortran_env` in the signature file, one possibility is to instead use intrinsic functions like `selected_real_kind`

```
# python signature file
python module sig
    interface

        subroutine func(i, x)
            fortranname pyfunc
            integer, parameter :: dp = selected_real_kind(p=15)
            integer, parameter :: int32 = selected_int_kind(9)
            integer(int32),intent(in) :: i
            real(dp),intent(out) :: x
        end subroutine func

    end interface
end python module sig
```
This works, but it is inconsistent, and could conceivably fail with some compilers/architectures. In particular, in the wrapper function we are defining the kind by the number of bits, but in the signature file by the precision of the number.

I note that there is a very short discussion [here](https://numpy.org/devdocs/f2py/advanced.html#dealing-with-kind-specifiers) on how to modify the kind declarations to be compatible with C-types. However, if there is an easy way to do this, it is not obvious from the extremely terse and undocumented nature of these features.

In conclusion: For simplicity, the types defined in the `iso_fortran_env` module should be made accessible to the python signature file.",2020-10-14 13:43:06,,"F2PY: Allow the use of ""iso_fortran_env"" in pyf signature declarations","['01 - Enhancement', 'component: numpy.f2py']"
17562,open,BvB93,"The return type of [`np.count_nonzero()`](https://numpy.org/doc/stable/reference/generated/numpy.count_nonzero.html) is currently somewhat inconsistent:
* A builtin integer is returned if `axis is None and not keepdims`.
* An `np.intp` (or an array) is returned otherwise.

The former case is handled by the an identically named function in the `np.core._multiarray_umath` 
module (which apparently always returns an `int`) while the latter is effectively a wrapper around 
`np.ndarray.sum()` with its dtype explicitly set to `np.intp`:

https://github.com/numpy/numpy/blob/4ccfbe69b7a8c511d262a9759e576bb87ec119cc/numpy/core/numeric.py#L484-L495

### Reproducing code example:
MacOS 10.15.6; Python 3.8.5; NumPy 1.20.0.dev0+eb2c751

```python
In [1]: import numpy as np

In [2]: ar = np.arange(10)

In [3]: np.count_nonzero(ar).__class__
Out[3]: int

In [4]: np.count_nonzero(ar, axis=0).__class__
Out[4]: numpy.int64  # i.e. np.intp
```",2020-10-14 12:49:40,,`count_nonzero` can return either a builtin `int` or numpy's `intp`,"['04 - Documentation', 'component: numpy._core']"
17554,open,pradghos,"while running `numpy` test cases on Linux-s390x ( Big Endian ) platform, I have observed below failures - 

```
========================================================================== FAILURES ==========================================================================
__________________________________________________ TestComplexFunctions.test_loss_of_precision[complex256] ___________________________________________________

self = <numpy.core.tests.test_umath.TestComplexFunctions object at 0x3ff8d5797c8>, dtype = <class 'numpy.complex256'>

    @pytest.mark.parametrize('dtype', [np.complex64, np.complex_, np.longcomplex])
    @pytest.mark.xfail(condition=platform.machine().startswith(""ppc64""),
                       reason=""poor precision on older ppc64le glibc"")
    def test_loss_of_precision(self, dtype):
        """"""Check loss of precision in complex arc* functions""""""

        # Check against known-good functions

        info = np.finfo(dtype)
        real_dtype = dtype(0.).real.dtype
        eps = info.eps

        def check(x, rtol):
            x = x.astype(real_dtype)

            #pdb.set_trace();
            z = x.astype(dtype)
            d = np.absolute(np.arcsinh(x)/np.arcsinh(z).real - 1)
            assert_(np.all(d < rtol), (np.argmax(d), x[np.argmax(d)], d.max(),
                                      'arcsinh'))

            z = (1j*x).astype(dtype)
            d = np.absolute(np.arcsinh(x)/np.arcsin(z).imag - 1)
            assert_(np.all(d < rtol), (np.argmax(d), x[np.argmax(d)], d.max(),
                                      'arcsin'))

            z = x.astype(dtype)
            d = np.absolute(np.arctanh(x)/np.arctanh(z).real - 1)
            assert_(np.all(d < rtol), (np.argmax(d), x[np.argmax(d)], d.max(),
                                      'arctanh'))

            z = (1j*x).astype(dtype)
            d = np.absolute(np.arctanh(x)/np.arctan(z).imag - 1)
            assert_(np.all(d < rtol), (np.argmax(d), x[np.argmax(d)], d.max(),
                                      'arctan'))

        # The switchover was chosen as 1e-3; hence there can be up to
        # ~eps/1e-3 of relative cancellation error before it

        x_series = np.logspace(-20, -3.001, 200)
        x_basic = np.logspace(-2.999, 0, 10, endpoint=False)

        if dtype is np.longcomplex:
            # It's not guaranteed that the system-provided arc functions
            # are accurate down to a few epsilons. (Eg. on Linux 64-bit)
            # So, give more leeway for long complex tests here:
            # Can use 2.1 for > Ubuntu LTS Trusty (2014), glibc = 2.19.
>           check(x_series, 50*eps)

test_umath.py:2697:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

x = array([1.00000000e-20, 1.21736864e-20, 1.48198641e-20, 1.80412378e-20,
       2.19628372e-20, 2.67368693e-20, 3.254862...5156e-04, 4.54267386e-04,
       5.53010871e-04, 6.73218092e-04, 8.19554595e-04, 9.97700064e-04],
      dtype=float128)
rtol = 9.629649721936179265279889712924637e-33

    def check(x, rtol):
        x = x.astype(real_dtype)
        z = x.astype(dtype)
        d = np.absolute(np.arcsinh(x)/np.arcsinh(z).real - 1)
        assert_(np.all(d < rtol), (np.argmax(d), x[np.argmax(d)], d.max(),
>                                 'arcsinh'))
E       AssertionError: (120, 1.7809563130653488765574048895753284e-10, 5.286342315078822882670006871837315e-21, 'arcsinh')

test_umath.py:2669: AssertionError

```

Other information : 
1. numpy version : 1.17.4

2. Epsilon information : 

```
(Pdb) np.finfo(np.float64).eps
2.220446049250313e-16
(Pdb) np.finfo(np.float32).eps
1.1920929e-07
(Pdb) np.finfo(np.float64).eps
2.220446049250313e-16
(Pdb) np.finfo(np.float128).eps
1.9259299443872358530559779425849273e-34
(Pdb)
```

Any suggestions would really help. Thanks in advance !  ",2020-10-14 09:02:53,,test_loss_of_precision[complex256] test failure for s390x platform,['unlabeled']
17551,open,mwelinder,"Refer to https://github.com/numpy/numpy/blob/master/numpy/core/src/umath/simd.inc.src
line 3214 and onwards.

The code in AVX512F_log_DOUBLE has issues.

1. The description as well as Tang1991 use 65 different c_k values, not 64 as in the code.
2. The code falls back to npy_log for values [1,1 + 0x1.09p-4].  This more or less fits with
what Tang1991 does, but hurts performance badly.
3. Unlike Tang1991, the code does not do anything special for values just above 2, just above 4, etc.
This is actually fine.
4. For values just below 1, the code falls back to npy_log.  Tang1991 does not do that.  This hurts
performance a lot and, with a little care to be discussed, is not necessary.
5. The code picks the wrong c_k in half the cases.  Tang1991 rounds, the code floors.

1+5 mean that the error bounds analysis goes out the window.  The difference to c_k can be as large as 1/64,
not 1/128, and the polynomial is 200x less precise at range 1/64.  The fallback in 2+3 is unnecessary and only
there for performance in the sequential world of 1991.  4 is clearly there because the code otherwise suffers
large cancellation errors, but see below.

So, what to do?  Suggestions:

1. Never fall back to npy_log.
2. Do not range reduce numbers in [1-1/128;1].  The code works just fine for this range and
with no range reduction the fallback is not necessary anymore.
3. I don't believe it is necessary to fix the c_k picking (because of 2)
4. I don't believe 65 table entries are needed (because of 2).  If need be, map [2-1/128;2] down into [1-1/256;1].
5. Document the differences to Tang1991.
6. Verify the accuracy against, say, crlibm's log_rn (""correctly-rounded log"") function.

The critical values to test for this function are values near 1 and values just below 2 and 0.5.

This is a 3x speed-up for the case where most of the numbers are near 1.  The sequential fallback absolutely
kills performance.

Unfortunately I cannot share code for this.
",2020-10-13 17:07:56,,AVX512F_log_DOUBLE is much slower than it needs to be,"['01 - Enhancement', 'component: SIMD']"
17544,open,tkzv,"<!-- Please describe the issue in detail here, and fill in the fields below -->
When I apply numpy.cumsum() to a big-endian array, the result is little-endian. 
Observed on AMD64 machine running 64-bit Linux.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
c = np.array([1, 2, 3], dtype='>i4')
u = np.cumsum(c, dtype='>i4')
print(c, c.dtype, c.view('u1'), sep='\t')
print(u, u.dtype, u.view('u1'), sep='\t')
```

The output is:
```
[1 2 3] >i4     [0 0 0 1 0 0 0 2 0 0 0 3]
[1 3 6] int32   [1 0 0 0 3 0 0 0 6 0 0 0]
```
instead of
```
[1 2 3] >i4     [0 0 0 1 0 0 0 2 0 0 0 3]
[1 3 6] >i4     [0 0 0 1 0 0 0 3 0 0 0 6]
```


### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.19.1 3.6.9 (default, Jul 17 2020, 12:50:27) 
[GCC 8.4.0]

---

Note by seberg 2022-11: The issue is now `cumsum` and `cumprod` specific.  `np.ufunc.accumulate` does correctly reject the dtype.",2020-10-13 00:02:46,,cumsum() changes endianness,"['00 - Bug', 'component: numpy._core', '07 - Deprecation', 'component: numpy.dtype']"
17532,open,WarrenWeckesser,"Unlike the other scalar types, `numpy.float64` silently ignores unknown keyword arguments:

```
In [17]: np.__version__
Out[17]: '1.20.0.dev0+f0171ba'

In [18]: np.float64(123, foobar=""plate of shrimp"")
Out[18]: 123.0
```
The other types raise the exception `TypeError: function takes at most 1 argument (2 given)`:

```
In [19]: np.float32(123, foobar=""plate of shrimp"")
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-19-2f7cf4afa2cb> in <module>
----> 1 np.float32(123, foobar=""plate of shrimp"")

TypeError: function takes at most 1 argument (2 given)

In [20]: np.uint32(123, foobar=""plate of shrimp"")
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-20-8b32fef69395> in <module>
----> 1 np.uint32(123, foobar=""plate of shrimp"")

TypeError: function takes at most 1 argument (2 given)
```
",2020-10-10 16:55:18,,`numpy.float64` ignores unknown keyword arguments.,['00 - Bug']
17517,open,bjnath,"Improve navigation of the [I/O how-to](https://numpy.org/devdocs/user/how-to-io.html) by prepending  a decision tree/flowchart using an image or graphviz, as suggested by @mattip.",2020-10-09 11:42:23,,DOC: ENH: I/O how-to needs a visual aid,['04 - Documentation']
17495,open,seberg,"This is a followup to review the behaviour set in gh-17410, which includes that:

```
np.array(np.float64(np.nan), dtype=np.int64)
```
does *not* raise an error, while the following operations do:
```
np.array([np.float64(np.nan)], dtype=np.int64)
arr1d_int64[()] = np.float64(np.nan)
np.array(np.array(np.nan), dtype=np.int64)
```

The error is inconsistent with how a zero dimensional array would behave, and the long term goal should probably be to make it work the same. Changing the behaviour in master, however, caused issues for pandas: https://github.com/pandas-dev/pandas/issues/35481

We should expect this issue to be fixed in pandas in the future, so the decision to retain the old style (pre 1.20) behaviour as much as possible should be reviewed, and the behaviour likely be changed in the long run.
A final decision will have to be made as to whether:

* this should always raise an error
* should never raise an error
* should give a warning.

or another solution be found.  Creating a 1.21 milestone so that this is reviewed at some point.",2020-10-07 19:19:44,,"TASK: Review coercion to numpy integers e.g. in `np.array([...], dtype=np.int64)`","['54 - Needs decision', 'component: numpy.dtype', '62 - Python API']"
17471,open,seberg,"@mattip noticed that buffering is always used in reductions.  For example the following code (for reductions, normal ufuncs are similar):

```
arr = np.arange(10000)
res = np.array(arr.sum())

it = np.nditer((res, arr), op_axes=[(None,), (0,)], op_flags=[[""readwrite""], ['readonly']], flags=[""reduce_ok"", ""growinner"", ""buffered""])

it.debug_print()
```
Shows that a buffer is used, (as requested), but the buffer is not actually necessary (all transfer functions are NUL and both operands have `BUFNEVER` flagged).

This also happens for typical ufuncs, but since they have contiguous and 1-D strided fast paths, it probably rarely hits.

There are two possible solutions to this:

1. We could not pass the ""buffered"" flag. At least for ufuncs we already know buffering is not necessary.
2. We could ignore the buffered flag in NpyIter if all operands have the `BUFNEVER` flag set.

My feeling is that option 2 is better. But potentially a lot more complicated.  I am not sure if there may be any bad effects, so it may be we need to use a new flag for ""use buffer only if necessary"". ",2020-10-06 18:37:01,,"BUG,PERF: UFuncs use unnecessary buffering losing unnecessary performance","['00 - Bug', '01 - Enhancement']"
17463,open,ivirshup,"<!-- Please describe the issue in detail here, and fill in the fields below -->

`np.split` and `np.array_split` are generally very similar. However, given a `list` or `tuple` first argument, `np.split` will raise an `AttributeError` if an integer section is passed, while `np.array_split` does not. This doesn't seem intentional to me. I would propose that either both of these cases work or neither do. 

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np

# These are equivalent
assert np.array_equal(
    np.array_split([1, 2, 3, 4], [2]),
    np.split([1, 2, 3, 4], [2])
)

# This works
np.array_split([1, 2, 3, 4], 2)
# [array([1, 2]), array([3, 4])]

# This does not
np.split([1, 2, 3, 4], 2)
```

### Error message:

```pytb
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.8/site-packages/numpy/lib/shape_base.py in split(ary, indices_or_sections, axis)
    866     try:
--> 867         len(indices_or_sections)
    868     except TypeError:

TypeError: object of type 'int' has no len()

During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)
<ipython-input-7-574c4074b926> in <module>
----> 1 np.split([1, 2, 3, 4], 2)

<__array_function__ internals> in split(*args, **kwargs)

/usr/local/lib/python3.8/site-packages/numpy/lib/shape_base.py in split(ary, indices_or_sections, axis)
    868     except TypeError:
    869         sections = indices_or_sections
--> 870         N = ary.shape[axis]
    871         if N % sections:
    872             raise ValueError(

AttributeError: 'list' object has no attribute 'shape'
```

### NumPy/Python version information:


```
1.19.2 3.8.5 (default, Jul 23 2020, 15:50:11) 
[Clang 11.0.3 (clang-1103.0.32.62)]
```

",2020-10-06 04:15:40,,"np.split throws AttributeError for list/ tuple `ary` and int sections, np.array_split does not","['00 - Bug', 'component: numpy.lib']"
17428,open,isVoid,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

```python
import numpy as np
np.find_common_type([np.dtype(""datetime64[ms]""), np.dtype(""datetime64[ms]"")], [])
```

### Expected Result:
`dtype('<M8[ms]')`

### Actual Result:
`dtype('O')`

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.19.1 3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 02:23:50) 
[GCC 7.5.0]
",2020-10-03 00:17:30,,Faulty common type for `datetime64[ms]`,['component: numpy.datetime64']
17425,open,kurtamohler,"`numpy.reciprocal(a)` and `1 / a` give different results with `a = numpy.array(0 + 0j)`. `reciprocal` gives `nan+nanj`, and `1 / a` gives `inf+nanj`.

It seems like neither of these results is correct, but I could certainly be wrong. I feel like the result in both cases should be `inf+0j`, since we get `inf` when we use real numbers: `1 / numpy.array(0.0)` and `numpy.reciprocal(numpy.array(0.0))`.


### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->


```
>>> import numpy

>>> a = numpy.array(0 + 0j)
__main__:1: RuntimeWarning: invalid value encountered in reciprocal
>>> numpy.reciprocal(a)
(nan+nanj)
>>> 1 / a
(inf+nanj)

>>> 1 / numpy.array(0.0)
inf
>>> numpy.reciprocal(numpy.array(0.0))
__main__:1: RuntimeWarning: divide by zero encountered in reciprocal
inf
```


### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

```
>>> print(numpy.__version__, sys.version)
1.18.1 3.7.4 (default, Aug 13 2019, 20:35:49) 
[GCC 7.3.0]
```

### OS

Distributor ID:	Ubuntu
Description:	Ubuntu 18.04.4 LTS
Release:	18.04
Codename:	bionic",2020-10-02 16:27:51,,Reciprocal of complex 0 and `1 / numpy.array(0+0j)` give different results,['unlabeled']
17408,open,Joueswant,"Hi there, 

I am used to work with datetime.datetime and datetime.timedelta object, but I'd like to switch to np.datetime64 (which looks pretty cool btw!)
Despite all, I have seen an unexpected behaviour (at least, for me) when transforming an array of floats to a np.timedelta64. Now assume that 65.5 is a **timedelta in days** with a *reference data* as **days since 1800-1-1**

### datetime.timedelta:
The expected ouput is the following:

```python
from datetime import datetime, timedelta
td = timedelta(days = 65.5)
print(td)
-------------
datetime.timedelta(days=65, seconds=43200)
```

### np.timedelta64:
But when I do the same with timedelta64

```python
import numpy as np
td = np.timedelta64(65.5, 'D') #or np.asarray(65.5).astype('timedelta64[D]')
print(td)
-------------
array(65, dtype='timedelta64[D]')
```

### np.datetime64 (sum):
Furthermore, if I try to sum the a the float with a datetime64 i get a message error

```python
import numpy as np
td = np.datetime64(time_ref, 'D') + 65.5
-------------
UFuncTypeError
UFuncTypeError: ufunc 'add' cannot use operands with types dtype('<M8[D]') and dtype('float64')
```
What I expect to get is a decimal timedelta in days, or automatically a timedelta in hours, minutes, us. But never a ""rounded down"" timedelta, since I am loosing valuable information.

Is this something you are already aware of? Do you have a way to bypass it I might have missed?

My numpy version is '1.19.1'

Thanks a lot!
Best",2020-09-30 21:18:03,,np.timedelta64 not working as expected with decimal floats,"['component: numpy.datetime64', '33 - Question']"
17403,open,rossbar,"As of cb107d1c, the following statement can be found in the `numpy.lib.arraysetops` module docstring:

 > To do: Optionally return indices analogously to unique for all functions.

Moving this to the issue tracker as I propose removing this from the docstring in #17402 ",2020-09-30 03:22:51,,Return indices from array set operations,"['01 - Enhancement', 'component: numpy.lib']"
17359,open,seberg,"I am mostly creating this issue for the hope of a short discussion. Maybe @ahaldane you have an opinion on this?

Currently, we have the `alignment` on dtypes, which works fine. But the dtype copy code actually uses the full itemsize sometimes. This is relevant only for complex data types as far as I can tell, because `np.complex64` has a 32-bit alignment, since that is the alignment of the embedded floats.
The copy code in some cases will use ``uint64`` to copy a ``complex64`` currently, which is convenient, because we can reuse this for all types. But, it breaks alignment and requires us to check against both alignments (although in most cases they match).

It would be nice to solve this in the long run. My main issue is that it is confusing that alignment is usually 32-bit but sometimes 64-bit here. Possible solutions:

1. Define the alignment as 64-bit for this complex dtype, even though that is much stricter for than necessary usually.
2. Specialize complex copy code so that it cannot have any alignment issues (does not use `uint64` internally).
3. Signal the alignment specific to a certain functionality. This is possible, but annoying since we need the alignment requirement before getting the final inner-loop function. Because we would like to set up buffering first.

There might be one other ""middle"" ground: Require complex to provide a 32bit aligned copy function, but actually use the current `uint64` copy function as a fast-path (because at that point, we already know that just copying the data is sufficient). ",2020-09-21 17:28:21,,MAINT: Figure out alignment for complex loops,"['15 - Discussion', 'component: numpy._core', 'component: numpy.ufunc', 'component: numpy.dtype']"
17351,open,finagle29,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

on macOS
```python
>>>import numpy as np
>>>type((np.ones(1, dtype=np.int32) | 0)[0])
numpy.int32
>>>(np.ones(1, dtype=np.int32) | 0)[0].dtype
dtype('int32')
```

on Windows
```python
>>>import numpy as np
>>>type((np.ones(1, dtype=np.int32) | 0)[0])
numpy.intc
>>>(np.ones(1, dtype=np.int32) | 0)[0].dtype
dtype('int32')
```

#### Expected Behavior
```python
>>>import numpy as np
>>>type((np.ones(1, dtype=np.int32) | 0)[0])
numpy.int32
>>>(np.ones(1, dtype=np.int32) | 0)[0].dtype
dtype('int32')
```
on both platforms

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
On macOS
1.19.1 3.7.8 | packaged by conda-forge | (default, Jul 23 2020, 03:39:37)
[Clang 10.0.0 ]
and
1.19.1 3.8.5 | packaged by conda-forge | (default, Sep 16 2020, 17:43:11)
[Clang 10.0.1 ]

On Windows
1.19.1 3.7.7 (default, May  6 2020, 11:45:54) [MSC v.1916 64 bit (AMD64)]
and
1.19.1 3.8.5 | packaged by conda-forge | (default, Sep 16 2020, 17:19:16) [MSC v.1916 64 bit (AMD64)]",2020-09-18 22:45:38,,"Bitwise Operations on numpy.int32 and int have different result types in macOS vs Windows, but same dtype __repr__",['00 - Bug']
17347,open,anands-repo,"<!-- Please describe the issue in detail here, and fill in the fields below -->

I am using a POWER8 machine with a CentOS docker container. I do not use the system gcc but the compiler that comes with the IBM Advance Toolchain. It is common to use this alternative compiler for IBM POWER systems.

The advance tool chain's gcc libraries do not contain the xlocale.h header, but the system gcc contains the header. In this case, the OPTIONAL_HEADER checks described in https://github.com/numpy/numpy/pull/8367 come out successful for xlocale.h. However this causes conflicts between some definitions in the advance tool chain's gcc library and the system libraries.

To get a successful build, I needed to comment xlocale.h in numpy/core/setup_common.py. I have two concerns:
1. The build comes out fine, and some numpy functionality works, but it would be great to know whether this is an appropriate way to deal with this.
2. Is there a cleaner way to do the same thing? I am not sure this is a fair request since this probably occurs due to having an alternative library. However I am also wondering whether this is a common case for a subset of users who need to use a different compiler from the standard system compiler.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```
CC=/opt/at11.0/bin/gcc python setup.py install
```

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->


<!-- Full error message, if any (starting from line Traceback: ...) -->

```
/usr/include/xlocale.h:27:16: error: redefinition of 'struct __locale_struct'
 typedef struct __locale_struct
                ^~~~~~~~~~~~~~~
In file included from /opt/at11.0/include/bits/types/locale_t.h:22:0,
                 from /opt/at11.0/include/string.h:152,
                 from /root/install/include/python3.6m/Python.h:30,
                 from numpy/core/src/common/numpyos.c:2:
/opt/at11.0/include/bits/types/__locale_t.h:28:8: note: originally defined here
 struct __locale_struct
        ^~~~~~~~~~~~~~~
In file included from numpy/core/src/common/numpyos.c:23:0:
/usr/include/xlocale.h:39:4: error: conflicting types for '__locale_t'
 } *__locale_t;
    ^~~~~~~~~~
```

### NumPy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
Version is 1.19.2
",2020-09-18 03:36:55,,xlocale.h definition conflicts with IBM advanced toolchain,['unlabeled']
17335,open,RoyLarson,"I am working with a symmetric matrix that should not be invertible without dropping a  row and column.  
The general form of the is -1 for all off diagonal elements, and -1*sum of the off diagonal elements.   
I found that this starts at specific size in the general form of the array 7X7.

```python
import numpy as np

def create_dependent_array(size):
    """"""Make a dependent square matrix of size.
    general form = 
    [
        [size-1, -1...]
        , ... ,
        [-1..., size-1]
    ]
    """"""
    a = -1*np.ones(shape=(size, size))+size*np.eye(size)

    return a

for i in range(2, 10):
    a = create_dependent_array(i)
    print(a)
    assert (a.sum(axis=0)==0).all() # proves the rows are dependent
    try:
        inverse = np.linalg.inv(a)
    except np.linalg.LinAlgError:
        print(f""correct solution for size {i}"")
    else:
        print(f""incorrect solution for size {i}\n{inverse}"")
        np.testing.assert_almost_equal(inverse@a, np.eye(i))
```

This will get to i=7 and fail



### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.19.1 3.8.5 (default, Aug  5 2020, 09:44:06) [MSC v.1916 64 bit (AMD64)]

",2020-09-16 15:03:27,,Error in numpy.linalg.inv,['unlabeled']
17328,open,eric-wieser,"Looking at https://numpy.org/devdocs/reference/generated/numpy.dtype.html#numpy.dtype, I notice:

* The docstring for `np.dtype` is not shown, even though `help(np.dtype)` shows it
* The `type` attribute is missing its docstring, even though `help(np.dtype.type)` shows it
* (minor) Attributes are shown as an indented block, methods are shown as a table",2020-09-16 11:41:06,,Documentation page for `np.dtype` is malformed,"['component: documentation', 'component: numpy.dtype']"
17324,open,ZisIsNotZis,"I was doing some experiments on code optimization and found some `ufunc`s have ""unexpected"" performance change when changing `dtype` and inplace-ness.

### Reproducing code example:

```python
import numpy as np
for i in dir(np):
    i = getattr(np, i)
    if isinstance(i, np.ufunc):
        print(i)
        try:
            %timeit i(np.random.rand(1048576))
            %timeit i(*[np.random.rand(1048576)]*2) # i.e. out=
            %timeit i(np.random.rand(1048576).astype('f'))
            %timeit i(*[np.random.rand(1048576).astype('f')]*2) # i.e. out=
        except:
            pass
```

The followings are functions that I found ""special"". These functions are only slow in fp64 out-of-place mode:

```python
<ufunc 'expm1'>
20 ms ± 109 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
16.3 ms ± 94 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
16.9 ms ± 89.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
16.9 ms ± 87.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
<ufunc 'log1p'>
20.2 ms ± 75.9 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
17.7 ms ± 66.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
17.4 ms ± 57 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
17.4 ms ± 105 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
<ufunc 'reciprocal'>
9.06 ms ± 23.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
7.46 ms ± 17.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
7.32 ms ± 30 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
7.49 ms ± 24.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
<ufunc 'tanh'>
26.2 ms ± 141 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
22.6 ms ± 63.5 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
22.7 ms ± 220 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
22.7 ms ± 79.7 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

These functions have significant performance boost in fp32 mode: (maybe need documentation compared to consistent functions?)

```python
<ufunc 'arccos'>
23.6 ms ± 107 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
21.7 ms ± 493 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
16.1 ms ± 51.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
16.3 ms ± 95.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
<ufunc 'arcsin'>
23.5 ms ± 544 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
21.4 ms ± 96.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
15.5 ms ± 22.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
15.5 ms ± 50.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
<ufunc 'arcsinh'>
32.7 ms ± 181 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
32.6 ms ± 116 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
28.8 ms ± 179 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
29.1 ms ± 532 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
<ufunc 'arctan'>
24.8 ms ± 110 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
24.4 ms ± 208 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
17.8 ms ± 39 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
17.9 ms ± 38.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
<ufunc 'tan'>
28.9 ms ± 623 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
27.3 ms ± 46.7 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
19.8 ms ± 39 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
19.9 ms ± 93 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
<ufunc 'cosh'>
23 ms ± 118 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
21.7 ms ± 45.6 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
18 ms ± 95.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
18.1 ms ± 39.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
<ufunc 'exp2'>
18 ms ± 124 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
17.8 ms ± 38.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
12.2 ms ± 40.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
12.2 ms ± 73 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
<ufunc 'log10'>
27.8 ms ± 170 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
28.1 ms ± 507 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
15 ms ± 51.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
15 ms ± 41 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
<ufunc 'log2'>
20.3 ms ± 149 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
15.3 ms ± 84.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
11.5 ms ± 73.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
11.5 ms ± 60.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

These functions are faster than fp64 in fp32 inplace mode, but slower in fp32 out-of-place mode:

```python
<ufunc 'cos'>
19.3 ms ± 66 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
17.4 ms ± 78.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
8.36 ms ± 26.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
35 ms ± 119 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
<ufunc 'exp'>
19.6 ms ± 84.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
18.6 ms ± 54.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
8.33 ms ± 22.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
47.3 ms ± 171 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
<ufunc 'log'>
27 ms ± 201 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
27.4 ms ± 611 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
8.58 ms ± 22.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
47.1 ms ± 118 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
<ufunc 'sin'>
20.5 ms ± 123 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
19 ms ± 114 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
8.34 ms ± 48.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
34.8 ms ± 99 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

These performance inconsistencies might not be a serious problem, but they tend to lead to difficulty in optimizing code without a clear understanding of which mode runs fastest.

I believe it's better to either ""fix"" or document them somewhere. The first and last category probably should be ""fixed"" since they make no sense. The second category might need better documentation.

### Numpy/Python version information:

```python
1.19.1 3.8.5 (default, Sep  4 2020, 07:30:14)
[GCC 7.3.0]
```",2020-09-16 08:13:44,,ufunc performance inconsistency regarding fp32/fp64 inplace/out-of-place,['unlabeled']
17310,open,redkuul,"<!-- Please describe the issue in detail here, and fill in the fields below -->
When I have a masked array where the mask is all True (unfortunately this sometimes happens), and I take the nanmean, I get a ValueError. 

### Reproducing code example:
```python
import numpy as np
import numpy.ma as ma
np.nanmean([np.nan,np.nan]) #returns np.nan, expected
mx=ma.array([1.0,2.0,3.0],mask=[True,True,False], fill_value=np.nan)
np.nanmean(mx) #returns 3, expected
mx=ma.array([1.0,2.0,3.0],mask=[True,True,True], fill_value=np.nan)
np.nanmean(mx) #returns ValueError: output array is read-only, expected np.nan

```

<!-- Remove these sections for a feature request -->

### Error message:
ValueError: output array is read-only, expected np.nan

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.18.5 3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]

",2020-09-14 13:03:18,,nanmean of all masked values returns value error,"['00 - Bug', 'component: numpy.ma']"
17305,open,brunobeltran,"<!-- Please describe the issue in detail here, and fill in the fields below -->
Probably not related, but this was previously #2612. As in the title, trying to concatenate the output of `dict.values` fails, not recognizing that it is a `Sequence`.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
test = {i: np.zeros((3,2 )) for i in range(5)}
np.concatenate(test.values())
```


<!-- Remove these sections for a feature request -->

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<__array_function__ internals>"", line 5, in concatenate
TypeError: The first input argument needs to be a sequence
```

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.18.4 3.8.2 | packaged by conda-forge | (default, Mar  5 2020, 17:11:00)
[GCC 7.3.0]

This failure occurs on latest 64-bit Arch running latest numpy (1.18.4) from PIP on Python 3.8.2.

The code also fails on `master` (`numpy-1.20.0.dev0+3ad444b`).
",2020-09-13 15:47:27,,`concatenate` doesn't understand `dict_values`,['unlabeled']
17303,open,mruberry,"Currently NumPy has a function, `hanning`, that is documented as ""return[ing] the Hanning window"". See here:

https://numpy.org/doc/stable/reference/generated/numpy.hanning.html?highlight=hanning#numpy.hanning

This function is actually a misnomer for the [Hann window](https://en.wikipedia.org/wiki/Hann_function). The window is named after Julius von Hann. Its Wikipedia article even mentions this confusion over its name:

""However, the erroneous 'Hanning' function is also heard of on occasion, derived from the paper in which it was named, where the term 'hanning a signal' was used to mean applying the Hann window to it. The confusion arose from the similar Hamming function, named after Richard Hamming.""

The NumPy documentation also mentions the discrepancy:

""The Hanning was named for Julius von Hann, an Austrian meteorologist. It is also known as the Cosine Bell. Some authors prefer that it be called a Hann window, to help avoid confusion with the very similar Hamming window.""

I don't know what the basis for this latter claim is. [matlab](https://www.mathworks.com/help/signal/ref/hann.html), [SciPy](https://scipy.github.io/devdocs/generated/scipy.signal.windows.hann.html), [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/signal/hann_window) and [PyTorch](https://pytorch.org/docs/master/generated/torch.hann_window.html?highlight=hann#torch.hann_window) use the name `hann` or `hann_window`.  

I suggest adding an alias to satisfy existing NumPy users and those would prefer the correct name. 

",2020-09-13 09:06:06,,"Feature Request: add hann alias for incorrectly named hanning function, update hanning documentation","['01 - Enhancement', '04 - Documentation', '54 - Needs decision', '62 - Python API']"
17286,open,seberg,"The old PR gh-5978 wanted to add a `maxlag` functionality to our correlate/convolve function, so that correlations with a smaller shift can be conveniently calculated.

In general, this is probably a good feature to add, but the PR is outdated and always had a few outstanding issues.  We had discussed this briefly, and one option may be to implement this, but potentially start in SciPy since has the more full featured versions and any API additions in NumPy should probably end up in SciPy anyway.",2020-09-10 15:10:36,,ENH: Implement a maxlag like feature for `np.correlate`,"['01 - Enhancement', 'component: numpy._core']"
17269,open,WRKampi,"the legweight code is (numpy version 1.18.1):
```
def legweight(x):
    w = x*0.0 + 1.0
    return w
```
it returns an array like x with all values equal to 1. however outside the integration domain [-1, 1] it should return 0. So an option to correct this is:
```
def legweight(x):
    inside = np.abs(x) <= 1
    w = x*0.0 + 1.0*inside
    return w
```
This would make the legweight function more useful.",2020-09-08 09:57:37,,numpy.polynomial.legendre.legweight returns 1. outside the integration domain where it should return 0.,['unlabeled']
17236,open,redkuul,"<!-- Please describe the issue in detail here, and fill in the fields below -->
The default print behavior of ma is not what I expected. 
I expected that when precision is set, this would also be reflected in print(mx)
This seems to work as expected with print(mx.data) or print(mx.filled()) 

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
import numpy.ma as ma
mx=ma.array(data=[1.12345,2.12345,3.12345,-1.12345,5.12345],mask=[False, False, False, True, False], fill_value=np.nan)
np.set_printoptions(precision=2,floatmode='fixed')

print(mx) #expected output: [1.12, 2.12, 3.12,--,5.12] -> but I get: [1.12345,2.12345,3.12345,--,5.12345]
print(mx.data) # expected output [1.12, 2.12, 3.12,-1.12,5.12] -> OK
print(mx.filled()) # expected output [1.12, 2.12, 3.12,nan,5.12] -> OK
```

<!-- Remove these sections for a feature request -->

### Numpy/Python version information:
1.18.5 3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]

",2020-09-03 12:09:14,,print(ma.array) neglects print precision,"['00 - Bug', 'component: numpy.ma']"
17228,open,ZisIsNotZis,"I use `argsort` as an alternative to `sort` when the sorted array is too big to fit into memory or even hard drive. This is the case when the input is ""strided"" (like a rolling window of long sequence). It's expected that `argsort` should use `len*8/2**30` gigabyte of memory since the output is `int64`, but it seems that this is not the case

### Reproducing code example:

```python
import numpy as np
a = np.memmap('dat/small.256', mode='r') # 3.6G
n = 256
np.lib.stride_tricks.as_strided(a[:0].view(('V',n)), [len(a)-n+1], [1]).argsort()
```

### Error message:

It should use ~28.5G of memory, while I have 32G of memory and 128G of swap

```python
MemoryError                               Traceback (most recent call last)
<ipython-input-9-acf25f26501a> in <module>
      2 a = np.memmap('dat/small.256', mode='r') # 3.6G
      3 n = 256
----> 4 np.lib.stride_tricks.as_strided(a[:0].view(('V',n)), [len(a)-n+1], [1]).argsort()

MemoryError: 
```

### Reproducing code example 2:

```python
import numpy as np
a = np.memmap('dat/small.256', mode='r') # 3.6G
n = 16
np.lib.stride_tricks.as_strided(a[:0].view(('V',n)), [len(a)-n+1], [1]).argsort()
```

### Error message:

It's using should use ~28.5G of memory, while actually using 97.3G of memory+swap. This value indicates that it's trying to keep a sorted copy in memory?

```bash
top - 01:34:43 up 17:11,  0 users,  load average: 1.84, 1.40, 0.65
Tasks: 249 total,   1 running, 158 sleeping,   0 stopped,   0 zombie
%Cpu(s):  1.7 us,  2.0 sy,  0.0 ni, 94.7 id,  1.5 wa,  0.0 hi,  0.1 si,  0.0 st
KiB Mem : 32720944 total,   277108 free, 32332820 used,   111016 buff/cache
KiB Swap: 13421772+total, 67026940 free, 67190784 used.    62000 avail Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 7378 a         20   0 97.322g 0.030t      0 D  39.1 97.1   1:10.76 python
```

### Numpy/Python version information:

1.19.1 3.8.5 (default, Aug  5 2020, 08:36:46) 
[GCC 7.3.0]
From anaconda",2020-09-03 01:38:30,,argsort uses more memory than expected,['unlabeled']
17224,open,eric-wieser,"This behavior is a little confusing:
```python
>>> np.asarray([np.zeros((10, 3)), np.zeros(10)])
VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating 
ValueError: could not broadcast input array from shape (10,3) into shape (10)
```
since I still get an error even if I provide `dtype=object`:
```python
>>> np.asarray([np.zeros((10, 3)), np.zeros(10)], dtype=object)
ValueError: could not broadcast input array from shape (10,3) into shape (10)
```
~~In my mind, the behavior  most consistent with other ragged arrrays here would be to return an array of shape `(2, 10)` of dtype object, where some element are `np.float64` and others are objects arrays.~~

At any rate, it would be nice to skip the deprecation here and/or emit a more helpful message.",2020-09-02 13:09:42,,Creating object arrays of ragged depth fails,['component: numpy.dtype']
17202,open,blee3014,"<!-- Please describe the issue in detail here, and fill in the fields below -->
So polyfit seems weird in a for loop. I get a SVD convergence error on the second loop. If I run polyfit individually (e.g. by putting if i == 1, then run polyfit, etc in the for loop) then it works! In other words, I know it isn't NaN or inf problem because it outputs correctly with the if statement. Any ideas?


### Reproducing code example:

```python
import numpy as np
import glob

file_name = 'tt_64_mod*.npz'
file_list = glob.glob(file_name)

for i, f in enumerate(file_list):
    # if i == 1:    # for example, if I select the data manually and run it, it fits perfectly, but in a consecutive run, it raises SVD convergence error. E.g., it works okay for i == 0, then error on i == 1 run. But if I let it fit only on i == 1, then it works.

    data = np.load(f)
    power = data['power']
    bias = data['bias']
    data.close()

    pfit = np.polyfit(bias, power, deg=10)
```

### Error message:
<!-- Full error message, if any (starting from line Traceback: ...) -->
Traceback (most recent call last):
  File ""D:/My Documents/Google Drive/LAB/Projects/high_yield/Project/Figures/Figure4_yield_vs_data_rate/dirac_data/new_folder/plot_data.py"", line 14, in <module>
    pfit = np.polyfit(b, power, deg=10)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\numpy\lib\polynomial.py"", line 580, in polyfit
    c, resids, rank, s = lstsq(lhs, rhs, rcond)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\numpy\linalg\linalg.py"", line 2156, in lstsq
    x, resids, rank, s = gufunc(a, b, rcond, signature=signature, extobj=extobj)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\numpy\linalg\linalg.py"", line 101, in _raise_linalgerror_lstsq
    raise LinAlgError(""SVD did not converge in Linear Least Squares"")
numpy.linalg.linalg.LinAlgError: SVD did not converge in Linear Least Squares

### Numpy/Python version information:
numpy: 1.15.1
sys: 3.6.4

",2020-08-31 16:03:37,,Issue with running polyfit in a for loop?,['50 - Duplicate']
17192,open,tbenst,"First of all, thank you so much for the work on Numpy--an incredible contribution to science and progress.

### Reproducing code example:

Numpy has excellent support for functional programming patterns, and the hundreds of `np.*` functions I regularly use act as pure functions with no side effects.

The current behavior of `np.negative` breaks this pattern, and is not deterministic:

```python
>>> import numpy as np
>>> np.negative(np.arange(5), where=np.array([0,0,1,1,0],dtype=bool))
array([     94871006724704,        171798691860,                  -2,
                        -3, 4285579895102272626])
>>> np.negative(np.arange(5), where=np.array([0,0,1,1,0],dtype=bool))
array([ 94871006750048, 140541164903200,              -2,              -3,
                    48])
>>> x = np.arange(5)
>>> np.negative(x,out=x, where=np.array([0,0,1,1,0],dtype=bool))
array([ 0,  1, -2, -3,  4])
```

Thus, np.negative does not act as a mathematical function, which is confusing and error-prone. It's hard to imagine that a user would ever want uninitialized values in the returned values. 

 A quick search on github reveals that there are thousands (if not hundreds of thousands) of instances where code introduces uninitialized values due to calls to `np.negative`: https://github.com/search?q=%22np.negative%22&type=Code

Based on reading #7158, I believe this issue applies to all `ufunc`. I understand in https://github.com/numpy/numpy/pull/11086 that the documentation was improved, but I wanted to open this issue to highlight both the danger of uninitialized values being returned, as well as how frequently in the wild these uninitialized values are silently corrupting data.

### Numpy/Python version information:

```python
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.19.1 3.8.5 (default, Aug  5 2020, 08:36:46) 
[GCC 7.3.0]
```

### (Edit) Proposed default behavior
```python
>>> def wrapped_negative(x, out=None, where=True):
...  if (out is None):
...    temp = x[()]
...  else:
...    temp = out
...  return np.negative(x, out=temp, where=where)
>>> wrapped_negative(np.arange(5), where=np.array([0,0,1,1,0],dtype=bool))
array([ 0,  1, -2, -3,  4])
```
",2020-08-31 04:33:03,,ENH: change ufuncs when using `where` without `out` to initialize all values,"['01 - Enhancement', 'component: numpy.ufunc']"
17177,open,gerritholl,"I would like for `np.datetime64` to support the format-specification language using the `__format__` method, so that it can be used in formatting strings.  I frequently want to format plot labels using datetimes extracted from time coordinates in `xarray.DataArray` objects.  Those have dtype `M8[ns]`.  To format the date in a string directly, I need to convert these to `datetime.datetime` — due to the precision (see #12550) I need to manually cast to a lower time-resolution first.  So rather than `f'{d:%Y}'` I need to write `f'{d.astype(""M8[s]"").astype(datetime.datetime):%Y}`, which is somewhat cumbersome.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
d = np.datetime64(1234567890987654321, ""ns"")
print(f""{d:%Y}"")
```

### Error message:

```python
Traceback (most recent call last):
  File ""mwe84.py"", line 3, in <module>
    print(f""{d:%Y}"")
ValueError: Invalid format specifier
```

### Numpy/Python version information:

Numpy 1.19.1, Python 3.8.5.

",2020-08-28 09:00:55,,add datetime64.__format__ similar to datetime.__format__,"['01 - Enhancement', 'component: numpy.datetime64']"
17175,open,BvB93,"While attempting to address https://github.com/numpy/numpy/issues/17113 I stumbled upon an issue with `flatiter` and boolean indexing: 
It appears that the latter _only_ works as intended if a boolean array is passed. 
If, for example, a list of booleans is passed instead then they're treated as normal integers.
As can be seen below, things get even more ""creative"" when dealing with plain booleans.

Note that the `flatiter` documentation does claim that advanced indexing is supported ([docs](https://numpy.org/doc/stable/reference/generated/numpy.flatiter.html#numpy.flatiter)).

### Reproducing code example:

``` python
In [1]: import sys; import numpy as np

In [2]: sys.version
Out[2]: '3.8.3 | packaged by conda-forge | (default, Jun  1 2020, 17:21:09) \n[Clang 9.0.1 ]'

In [3]: np.__version__
Out[3]: '1.20.0.dev0+32b3f82'

In [4]: a: np.flatiter = np.arange(4).flat

# Behaves as expected
In [5]: a[np.array([True, True, True, True])]
Out[5]: array([0, 1, 2, 3])

# The booleans are treated as normal integers
In [6]: a[[True, True, True, True]]
Out[6]: array([1, 1, 1, 1])

# Not even sure what's going on at this point
In [7]: a[True]                                                                                            
Out[7]: 0

In [8]: a[False]                                                                                           
Out[8]: array([], dtype=int64)

In [9]: a[np.bool_(True)]                                                                                  
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-31-b696e95f8475> in <module>
----> 1 a[np.bool_(True)]

IndexError: unsupported iterator index
```",2020-08-28 01:51:09,,BUG: Boolean indexing broken in `np.flatiter`,"['00 - Bug', 'component: numpy._core']"
17166,open,bhaveshshrimali,"`numpy.linalg.inv` seems to be significantly slower than a hard-coded version for a simple test case, a `(1000,4,3,3,)` array.
Any pointers as to what could be going off and possible remedies, if any, to make it faster. See the following MWE: 

### Reproducing code example:

```python3
from numpy.linalg import inv as npinv
from numpy import zeros_like, einsum, random
from timeit import timeit

# Helper function for determinant
def vdet(A):
    detA = zeros_like(A[0, 0])
    detA = A[0, 0] * (A[1, 1] * A[2, 2] - A[1, 2] * A[2, 1]) -\
           A[0, 1] * (A[2, 2] * A[1, 0] - A[2, 0] * A[1, 2]) +\
        A[0, 2] * (A[1, 0] * A[2, 1] - A[2, 0] * A[1, 1])
    return detA

# Another function for computing inverse
# using the Cayley-Hamilton corollary
def finv(A):
    detA = vdet(A)
    I1 = einsum(""ii..."", A)
    I2 = -0.5 * (einsum(""ik...,ki..."", A, A) - I1**2)
    Asq = einsum(""ik...,kj...->ij..."", A, A)
    eye = zeros_like(A)
    eye[0, 0] = 1.
    eye[1, 1] = 1.
    eye[2,2] = 1.
    return 1./detA * (Asq - I1 * A + I2 * eye)

# Hard coded inverse
def hdinv(A):
    invA = zeros_like(A)
    detA = vdet(A)

    invA[0, 0] = (-A[1, 2] * A[2, 1] +
                  A[1, 1] * A[2, 2]) / detA
    invA[1, 0] = (A[1, 2] * A[2, 0] -
                  A[1, 0] * A[2, 2]) / detA
    invA[2, 0] = (-A[1, 1] * A[2, 0] +
                  A[1, 0] * A[2, 1]) / detA
    invA[0, 1] = (A[0, 2] * A[2, 1] -
                  A[0, 1] * A[2, 2]) / detA
    invA[1, 1] = (-A[0, 2] * A[2, 0] +
                  A[0, 0] * A[2, 2]) / detA
    invA[2, 1] = (A[0, 1] * A[2, 0] -
                  A[0, 0] * A[2, 1]) / detA
    invA[0, 2] = (-A[0, 2] * A[1, 1] +
                  A[0, 1] * A[1, 2]) / detA
    invA[1, 2] = (A[0, 2] * A[1, 0] -
                  A[0, 0] * A[1, 2]) / detA
    invA[2, 2] = (-A[0, 1] * A[1, 0] +
                  A[0, 0] * A[1, 1]) / detA
    return invA
```

## Timings 
```python3
F = random.random((3,3,1000,4))
F2 = einsum(""ij...->...ij"", F)

%timeit hdinv(F) # 371 µs ± 27 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
%timeit finv(F) # 5.35 ms ± 661 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
%timeit npinv(F2) # 79.2 ms ± 12.9 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)


```

## Numpy/Python version information:

### python version
`3.7.6 | packaged by conda-forge | (default, Jun  1 2020, 18:57:50) \n[GCC 7.5.0]`

### numpy version
`1.19.0`
",2020-08-26 14:37:54,,numpy.linalg.inv very slow for a stack of 3x3 arrays,['component: numpy.linalg']
17155,open,PhilippThoelke,"<!-- Please describe the issue in detail here, and fill in the fields below -->
When calling `np.linspace()` with very large stop values (e.g. `np.iinfo(np.int64).max`) and `dtype=np.int64`, an overflow occurs in the cast from the intermediate float64 array to the result int64 array.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
print(np.linspace(0, np.iinfo(np.int64).max, 2, dtype=np.int64))
```
This produces `[                   0 -9223372036854775808]` while the expected endpoint is expected to be `9223372036854775807` (i.e. `np.iinfo(np.int64).max`).

<!-- Remove these sections for a feature request -->

### Error message:
None.

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.19.1 3.7.8 (default, Jul  9 2020, 07:05:14)
[GCC 10.1.0]
",2020-08-24 17:27:52,,BUG: linspace on int64 overflows for large stop values,"['00 - Bug', 'component: numpy._core']"
17136,open,ijoseph,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->
#### Actual behavior
```python
>>> import numpy as np
>>> np.allclose([False, True], [0, 1])
True
```
#### Expected behavior: 

```python
>>> import numpy as np
>>> np.allclose([False, True], [0, 1])
TypeError: Cannot compare types 'bool' and 'int' without allow_cast=True
```

```python
>>> import numpy as np
>>> np.allclose([False, True], [0, 1], allow_cast=True)
True
```

<!-- Remove these sections for a feature request -->

### Numpy/Python version information:
1.19.1 / 3.x
<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

### Comments
Raising an exception might be overly-dramatic, but it's at least worth warning about type differences. It should not cast silently; having done so greatly weakens unit tests that use this. 

",2020-08-21 21:49:25,,allclose should at least warn about type mismatches,['01 - Enhancement']
17134,open,cjdoris,"I have a custom C-Python type which exposes the buffer protocol, the array interface (both `__array_struct__` and `__array_interface__`) and has an `__array__` method.

If `x` is an object of this type, then `numpy.array(x)` uses the buffer protocol to construct the resulting array, when I was expecting it to use the array interface or `__array__` (as per the documentation for `numpy.array` which says that `x` should be `""An array, any object exposing the array interface, an object whose __array__ method returns an array, or any (nested) sequence.""`).

If I disable the buffer protocol on my type, then `numpy.array(x)` does indeed use the array interface (and if I disable that it uses `__array__`).

Is this a bug or an undocumented feature? It's certainly surprising.

It's problematic because the array interface contains more information about the type than the buffer protocol, namely the names of each field.

Numpy `1.18.5`
Python `3.6.10 |Anaconda, Inc.| (default, May  7 2020, 19:46:08) [MSC v.1916 64 bit (AMD64)]`",2020-08-21 16:55:37,,numpy.array(x) prefers buffers over arrays,['04 - Documentation']
17130,open,Intro1997,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import random
import time
# import matplotlib
import numpy as np 
from scipy.fftpack import fft
# import cv2

coordinate = np.ndarray((1920, 1080, 2))
print(coordinate.shape)

for i in range(1920):
    for j in range(1080):
      coordinate[i][j] = (random.randint(0, 1920), random.randint(0, 1080))

print(coordinate[0][0])
# test = ([2.+4.j, 3.+3.j, 4.+4.j, 5.+3.j, 4.+2.j, 3.+1.j, 2.+2.j, 1.+3.j])
start_time = time.time();
res = np.fft.fft(coordinate)
end_time = time.time();
print(""spend time: "", (end_time - start_time));
```

<!-- Remove these sections for a feature request -->

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->
The result is :
```
% python2 ./test.py
(1920, 1080, 2)
[ 1022.   435.]
('spend time: ', 0.06316184997558594)
% python3 ./test.py
(1920, 1080, 2)
[1143.  678.]
spend time:  0.18218493461608887
```

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:
The pythons' numpy version are both 1.19.1
Python2: 2.7.16
Python3: 3.7.3

macOS: 10.15.6
<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2020-08-21 10:02:05,,fft with 1.16.6 is faster than with 1.18.0,['component: numpy.fft']
17124,open,mruberry,"`np.conj` returns a new array, but `ndarray.conj` returns `self` for non-complex arrays and a new array for complex arrays. Example:

```
a = np.array((1, 2, 3), dtype=np.int64)

# np.conj returns a copy of a
fn_result = np.conj(a)
fn_result[0] = 0
a
: array([1, 2, 3])

# ndarray.conj returns a itself
method_result = a.conj()
method_result[0] = 0
a
: array([0, 2, 3])

# ndarray.conj returns a copy for complex arrays
a = np.array((1, 2, 3), dtype=np.complex64)
complex_method_result = a.conj()
complex_method_result[0] = 100
a
: array([1.+0.j, 2.+0.j, 3.+0.j], dtype=complex64)
```

The divergence between function and method and different dtypes seems very odd. ",2020-08-20 22:18:35,,np.conj has divergent behavior from ndarray.conj,['unlabeled']
17118,open,Barry1,"<!-- Please describe the issue in detail here, and fill in the fields below -->

If using an archive, opening with the module `zipfile` and trying to read data from one element of the archive, it is not working. For example reading xml-files with bs4 is working.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy
import zipfile
# PREPARATION
numpy.array([1, 2, numpy.pi]).tofile('numpyarray.bin')
with zipfile.ZipFile('Archive.zip', mode='x') as archivefile:
    archivefile.write('numpyarray.bin')
# REPRODUCTION OF ERROR
with zipfile.ZipFile('Archive.zip') as archivefile:
    binarrayPath = zipfile.Path(archivefile, 'numpyarray.bin')
    try:
        data=numpy.fromfile(binarrayPath,dtype=numpy.float64)
    except AttributeError as expectederror:
        print(f""The prefered way resulted in the expected AttributeError -->{expectederror}<--"")
        tempbuf = binarrayPath.read_bytes()
        data = numpy.frombuffer(tempbuf)
print(data)
```
<!-- Remove these sections for a feature request -->

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->
Traceback (most recent call last):
  File ""reproduce.py"", line 10, in <module>
    data=numpy.fromfile(binarrayPath,dtype=numpy.float64)
AttributeError: 'Path' object has no attribute 'flush'
### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.19.1 3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019, 19:37:50) [MSC v.1916 64 bit (AMD64)]
also tried on
1.19.1 3.8.5 (tags/v3.8.5:580fbb0, Jul 20 2020, 15:57:54) [MSC v.1924 64 bit (AMD64)]",2020-08-20 12:47:48,,zipfile with inner binary file could not be read numpy.fromfile,['component: numpy._core']
17100,open,xuhdev,"Both `around` and `round` supports complex input, but not `trunc`. This looks a bit abnormal.

### Reproducing code example:

```python
In [1]: import numpy as np

In [2]: a=np.array([complex(1+1j)]*3)

In [3]: np.around(a)
Out[3]: array([1.+1.j, 1.+1.j, 1.+1.j])

In [4]: np.round(a)
Out[4]: array([1.+1.j, 1.+1.j, 1.+1.j])

In [5]: np.trunc(a)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-5-17494546b8df> in <module>
----> 1 np.trunc(a)

TypeError: ufunc 'trunc' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
```

### Numpy/Python version information:

```
1.19.1 3.7.3 (default, Jul 25 2020, 13:03:44) 
[GCC 8.3.0]
```
",2020-08-18 23:58:04,,"np.around and np.round supports complex numbers, but not np.trunc","['00 - Bug', '07 - Deprecation']"
17095,open,seberg,"We had discussed removing the `numpy.doc` namespace in the [community meeting](https://github.com/numpy/archive/blob/master/status_meetings/status_2020-08-05.md):

* It doesn't seem to serve much purpose
* Most of the docs are severely outdated and duplicate newer versions in the sphinx docs
* Debian does not even ship `numpy.doc`, indicating that it is not really used
* The main entry point is the little (or unused) used `np.info` command, probably

There are a few things that might be salvaged in those doc-strings, but in general the consensus at that meeting was that that the namespace does not serve a purpose anymore at this time.",2020-08-18 14:42:18,,"DOC,MAINT: Remove `numpy.doc` namespace and salvage usable documentation","['17 - Task', '04 - Documentation']"
17088,open,shtse8,"```py
import numpy as np

a = np.array([0.1, 0.2, 0.4, 0.3])
masked_a = np.ma.masked_array(a, [0, 0, 1, 1])
counter = np.zeros(len(a) + 1)
while True:
    action = np.random.choice(len(a), p=masked_a)
    counter[action] += 1
    print(counter / counter.sum())
    
# [0.09931198 0.20076697 0.         0.         0.69992105]
```

It shouldn't returns an index > 3, but when the last element of masked array is true, it may return index = 4",2020-08-15 09:55:01,,np.random.choice returns out of range index on masked array prob.,"['00 - Bug', 'component: numpy.random', 'component: numpy.ma']"
17085,open,mruberry,"Today in NumPy there's `transpose`, which ""reverses or permutes"" an array's axes. A ""transposition,"" however, is typically a swap of two elements, like what `swapaxes` does. This issue proposes a new function, `permute`, which is equivalent to `transpose` except it requires the permutation be specified. There are two reasons for this proposal:

- It is the correct mathematical name for the operation
- It would be helpful to provide library writers a mechanism to permute both NumPy-like arrays and PyTorch tensors. PyTorch uses `transpose` for transpositions and `permute` for permutations. It plans to implement `swapaxes` as an alternative transposition mechanism, so `swapaxes` and `permute` would work on both PyTorch tensors and NumPy-like arrays (and make PyTorch tensors more NumPy-like). 

When considering new names I think natural questions are:

- Would a user expect a function with this name to do what it does?
- Does this name conflict with any existing functions?

In this case I think `permute`, as proposed, would do exactly what a user expects:  return the specified permutation of the array. The name also does not conflict with any names in NumPy or SciPy. There is `numpy.random.permutation`, which returns a permutation, but that seems natural in context:


```
np.permute(a, numpy.random.permutation(a.ndim))
```

would randomly permute the axes of an array.

I could implement this if the proposal is accepted.",2020-08-14 18:29:19,,"Feature request: ""np.permute""",['unlabeled']
17042,open,HagaiHargil,"<!-- Please describe the issue in detail here, and fill in the fields below -->
When creating a `flatiter` object, the values are returned correctly, but the attributes are wrong.


### Reproducing code example:
```python
import numpy as np

arr = np.arange(9).reshape((3, 3))
fl = arr.flat
for item in fl:
    print(item)  # correct
    print(fl.index)  # wrong, starts from 1
    print(fl.coords)  # wrong, offset by 1 and will end at (3, 0) instead of (2, 2)
 ```

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.19.1 3.7.7 (default, May  7 2020, 21:25:33)
[GCC 7.3.0]
[Tested on 3.8 as well with the same results]",2020-08-10 11:32:52,,DEP: Deprecate flatiter attributes (which produce confusing results),"['component: numpy._core', '07 - Deprecation']"
17026,open,ZiaWang,"I tried to use np.dot to process two identity matrices in the python3 environment of machine A, but the result was not an identity matrix. Then I switched to python2 environment and it became correct.

```python
import numpy as np
import pandas as pd

a = np.diag(np.ones(10))
np.dot(a, a)
```

the bad result is:
```
array([[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
       [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],
       [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],
       [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],
       [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],
       [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])
```

the correct result should be：
```
array([[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
       [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
       [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
       [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
       [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],
       [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],
       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],
       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],
       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],
       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])
```

Numpy/Python version information:

```
PY3 environment:  '1.14.6'

PY2 environment: '1.9.3'
```




machine A‘s cpuinfo:
```
cat /proc/cpuinfo 

processor	: 0
vendor_id	: GenuineIntel
cpu family	: 6
model		: 85
model name	: Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz
stepping	: 4
microcode	: 0x1
cpu MHz		: 2494.136
cache size	: 33792 KB
physical id	: 0
siblings	: 32
core id		: 0
cpu cores	: 16
apicid		: 0
initial apicid	: 0
fpu		: yes
fpu_exception	: yes
cpuid level	: 13
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl cpuid pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 arat
bugs		: cpu_meltdown spectre_v1 spectre_v2
bogomips	: 4988.27
clflush size	: 64
cache_alignment	: 64
address sizes	: 46 bits physical, 48 bits virtual
power management:

```



machine B‘s cpuinfo:
```
cat /proc/cpuinfo 
processor	: 0
vendor_id	: GenuineIntel
cpu family	: 6
model		: 63
model name	: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
stepping	: 2
microcode	: 0x1
cpu MHz		: 2494.224
cache size	: 30720 KB
physical id	: 0
siblings	: 32
core id		: 0
cpu cores	: 16
apicid		: 0
initial apicid	: 0
fpu		: yes
fpu_exception	: yes
cpuid level	: 13
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl cpuid pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single pti fsgsbase bmi1 avx2 smep bmi2 erms invpcid xsaveopt
bugs		: cpu_meltdown spectre_v1 spectre_v2
bogomips	: 4988.44
clflush size	: 64
cache_alignment	: 64
address sizes	: 46 bits physical, 48 bits virtual
power management:

```


Both machine A and machine B have the same python2 and python3 environment. They are managed by conda, and the numpy version in each python environment is also the same.",2020-08-07 04:26:08,,python3 unit matrix calculation error on some machines ,['component: numpy.linalg']
17023,open,mdikovsky,"<!-- Please describe the issue in detail here, and fill in the fields below -->

Advanced indexing can swap axes in certain situations that are not obvious to an average Numpy user.
The proposal is to create a separate indexing scheme for such cases. Perhaps through "".take()"" or '.idx[]""

It could be also useful to disallow using regular indexing syntax '[]' for such cases, but of course that
would break backward compatibility, and so that might need to wait. 

Background:
Advanced indexing can lead to counterintuitive results, see below.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
adv_index = [0] * 10
a = np.zeros((1, 2, 3, 4, 5))

a[:, :, 1, adv_index, :].shape   # all dimensions are at predictable places
> (1, 2, 10, 5)

a[:, 1, :, adv_index, :].shape  # in this case the dimensions are reordered in a non-trivial way.
> (10, 1, 3, 5)

# This is currently an intended behavior, see
#   https://numpy.org/doc/stable/reference/arrays.indexing.html#advanced-indexing
# Two advanced indexers are not adjacent ('1' is considered advanced indexer in this context),
# and this triggers a special behavior: all advanced indexers go first, followed by the remaining slices.
# It is this special behavior that is the subject of this report.
```

",2020-08-06 19:10:24,,Feature request: make a separate syntax for the advanced indexing that reorders the dimensions.,['unlabeled']
17012,open,jakobjakobson13,"I ran an automated code analysis by https://www.deepcode.ai/ and the AI found the following bugs. I'm not sure if all of them are serious or important but if two or three code improvements can be made, that's something at least.

 - [x] 1. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/core/code_generators/genapi.py#L309 ""Use binary mode in open [:309] (current mode is w [:309] ) to avoid encoding-related issues for written file, on Windows or with Python 3.""

   * See gh-17054 (withdrawn)

- [x] 2. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/lib/polynomial.py#L162 ~""Use sorted(). sort [:162] sorts in-place and returns None. == [:162] expects a value.""~

    * `np.sort` returns the sorted array and the comparison works elementwise. The `sorted` builtin applied to a 1D array returns a list and errors for multidimensional arrays.

- [x] 3. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/core/src/umath/umathmodule.c#L203 ~""The result of malloc [:202] , which may return null flows [:202, :203] to the first argument [:203] of strcpy [:203] . This could result in undefined behavior. Consider adding a check for nullness.""~

   * Fixed in gh-17014

- [ ] 4. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/core/src/multiarray/nditer_api.c#L2228 ""This division [:2228] may result in a division by zero error because 0 [:2046] is a possible divisor.""

   * @seberg: I can't see this actually happening. If this would be 0, the buffer and thus the iteration size should be 0, in which case it is invalid to actually iterate.

- [x] 5. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/linalg/lapack_lite/f2c_lapack.c#L708 ~~""If statement has identical branches.""~~

   * @charris: Generated code
- [x] 6. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/linalg/lapack_lite/f2c_lapack.c#L1071 ~~""The expression will always evaluate to true because both sides always hold the same value.""~~

   * @charris: Generated code

- [x] 7. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/random/src/pcg64/pcg64.h#L234 ""Applying unary minus to an unsigned expression. Consider casting the operand to a signed expression before applying minus [:234] .""
- [x] 8. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/benchmarks/benchmarks/common.py#L28-34 ~~""Accessing attribute append [:32] on possibly None value. None flows [:32] as determined by the following check [:31] . Consider adding a check.""~~
   * @eric-wieser: this is clearly incorrect analysis when looking at the full context
- [x] 9. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/distutils/fcompiler/gnu.py#L402 "" hashlib.sha1 [:402] is insecure. Consider changing it to a secure hashing algorithm (e.g. SHA256).""
- [ ] 10. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/f2py/crackfortran.py#L917 ""Trying to store [:917] a value in element of immutable type str [:182] (from str literal """" [:182] ) will lead to a crash.""
- [ ] 11. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/polynomial/chebyshev.py#L682 ""Trying to call len [:682] on an object of type 0 (0) will lead to a TypeError.""
- [x] 12. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/distutils/command/build_ext.py#L562 ""There is no need to call list [:562] on an object [:562] that is already constructed as a list [:557] .""

    * See gh-17052

- [x] 13. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/distutils/misc_util.py#L1903 ""Missing close for open [:1903] , add close or use a with block.""

    * See gh-17051

- [x] 14. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/distutils/fcompiler/gnu.py#L26 ""If statement has identical branches.""

    * See gh-17050

- [x] 15. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/distutils/system_info.py#L718 ""Use comprehensions instead of list [:719] ( map [:718] (lambda: x ...))""

    * See gh-17055

- [x] 16. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/lib/polynomial.py#L1020 ""Duplicate expressions on both sides of a logical operator [:1020] is probably a mistake.""

    * See gh-17053

- [ ] 17. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/polynomial/hermite_e.py#L426 ""Trying to load attribute dtype [:426] on an object of primitive type int [:485] (from int literal 0 [:485] ) will lead to an AttributeError.""
- [x] 18. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/tools/openblas_support.py#L227 ""Use os.makedirs instead of os.mkdir [:227] because the given path may require creating the parent directories.""

    * See gh-17061 (withdrawn)

- [x] 19. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/tools/refguide_check.py#L391 ""There is no need to call set [:391] on an object [:391] that is already constructed as a set [:319] ""

    * See  gh-17063

- [x] 20. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/core/src/multiarray/iterators.c#L517 ~""Use Py_XDECREF instead of Py_DECREF [:517] ""~

    * As pointed out by Eric, obviously cannot be NULL, its checked a few lines earlier.

- [x] 21. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/core/src/multiarray/methods.c#L2603 "" PyString_FromString [:2603] does not work with Python 3.5, use PyUnicode_FromString instead""

    * See #17068 for fixes.

- [x] 22. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/core/src/multiarray/strfuncs.c#L89 ""Use PyBytes_AsString instead of PyString_AsString [:89] for Python 3 compatibility""
  
    * See gh-17141 for fix.

- [ ] 23. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/f2py/src/fortranobject.c#L268 ""Numerical operation lessThan [:268] used with boolean operand equals [:267] . Consider making the conversion from bool to integer explicit.""
- [x] 24. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/core/src/multiarray/nditer_constr.c#L829 ""Be aware that an optimizing compiler might be removing this call to memset [:829] , since this axes_dupcheck array [:829] does not seem to be used after the call to memset [:829].""
- [x] 25. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/random/src/legacy/legacy-distributions.c#L41 ""Equals comparison of floating point numbers. Floating point numbers are only considered equal if their bit representation is equal. Consider using an epsilon comparison instead.""
- [x] 26. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/f2py/tests/src/array_from_pyobj/wrapmodule.c#L152  ""PyString_FromString [:152] does not work with Python 3.5, use PyUnicode_FromString instead""

    * See #17068 for fixes.

- [x] 27. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/f2py/tests/src/array_from_pyobj/wrapmodule.c#L59 ""Use Py_XDECREF instead of Py_DECREF [:59] ""
- [ ] 28. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/lib/_datasource.py#L267 ""No need to call keys [:267] , directly check with the in operator.""

    * See gh-17064 (needs refactoring/rewriting)

- [x] 29. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/core/tests/test_array_coercion.py#L494 ""Use len() instead of calling __len__ [:494] directly""
- [x] 30. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/core/tests/test_numeric.py#L1376 ""Use predictable random with seed or secure random.""
- [x] 31. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/core/tests/test_scalarmath.py#L496 ""Use the is operator instead of == [:496] to compare to None [:496] , because == [:496] calls __eq__ which is slower and error-prone.""
- [x] 32. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/core/tests/test_shape_base.py#L158 ""Use comprehensions instead of map [:158] .""

    * See gh-17065 (withdrawn)

- [x] 33. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/lib/_datasource.py#L580 ""Use del statement instead of calling __del__ [:580] directly""

    * @charris The current code is recommended when both the base and derived class define the `__del__` method. That said, it isn't clear that an explicit `__del__` is needed, the inherited version should serve.

      > Note del x doesn’t directly call x.__del__() — the former decrements the reference count
        for x by one, and the latter is only called when x’s reference count reaches zero.

- [x] 34. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/lib/tests/test__datasource.py#L350 ""Use del statement instead of calling __del__ [:350] directly""

    * @charris Calling the function directly insures that it is immediately run at that spot in the test rather than possibly 
      waiting for the reference count to go to zero. Both options  should work here, but I think the direct call is better 
      because it is more explicit.

 - [ ] 35. https://github.com/numpy/numpy/blob/e3c52130add2ed5f7f0b9442b3b2742c09eb2de0/numpy/lib/tests/test_io.py#L2581 ""No need to call keys [:2581] , directly check with the in operator.""
",2020-08-04 22:35:17,, List of possible bugs found by automated code analysis,"['17 - Task', '03 - Maintenance']"
16985,open,ghost,"Hi,

I unexpectedly came across this fact that iterating over a python array.array() is much faster than iterating over a numpy array.
Is there a specific reason why, given that the data on both arrays is held in contiguous memory?

The tests I ran can be found (and reproduced) by cloning this repo: 
https://github.com/xvxvxvxvxv/python_vs_numpy_arrays_iteration

But here's a simple overview of what I mean:
![measurements](https://user-images.githubusercontent.com/68772868/89100161-d8b98180-d3ec-11ea-84bb-339369d99271.png)

If the error is on my side I apologize for raising this issue and I would appreciate for clarification on what I am doing wrong.

Thank you.",2020-08-01 09:40:18,,Request for clarification - Iteration over python arrays is much faster compared to numpy arrays.,['33 - Question']
16983,open,vyacheslav-zhukov,"In reply to:
> ... If someone wants `inner1d` exposed, please open a new issue for that. I will close this one. ...

_Originally posted by @seberg in https://github.com/numpy/numpy/issues/13568#issuecomment-492718737_

Please expose `inner1d` and other generalized universal functions from `numpy.core.umath_tests`.  

Except `matrix_multiply` has been [re-implemented](https://numpy.org/devdocs/release/1.10.0-notes.html#support-for-the-operator-in-python-3-5) as `matmul` in NumPy 1.10.  Perhaps move the `umath_tests` module next to `matmul` module under a new name?  

Originally, these functions were placed in `umath_tests` module for no particular reason.  They are not test functions by any means, in fact, they are very useful functions sometimes.  They make corresponding operations on ""stacked arrays"" faster and/or using less memory.

It seems like gufuncs have been gradually gaining acceptance.  `numpy.linalg` was [upgraded](https://numpy.org/doc/stable/release/1.8.0-notes.html?highlight=numpy%20sum#support-for-linear-algebra-on-stacked-arrays) to gufuncs in  NumPy1.8.  And #11492 ""Use gufuncs for partition, sort, etc., so they can be overridden with `__array_ufunc__`"" is on the road map.  

Why remove several useful gufuncs?  Perhaps even improve `inner1d` to use BLAS, like `matmul`?


",2020-07-31 18:11:54,,Expose inner1d and other generalized universal functions from numpy.core.umath_tests,['unlabeled']
16976,open,PierreAndreNoel,"Setting a `MaskedArray` `a` as non-writable (i.e., `a.setflags(write=False)`) does not prevent writing to `a.mask`.

I believe this to be a bug. If you disagree, then this becomes a feature request for a way to set a `MaskedArray`'s mask as non-writable. My current workaround (i.e., `a._mask.setflags(write=False)`) requires access to the private member `_mask`.

### Reproducing code example:

```python
import pytest
import numpy.ma as ma

def test__masked_array__cant_write_after_setflags_write_false():
    a = ma.MaskedArray([0, 1, 2], mask=[True, True, False], hard_mask=True)
    a.setflags(write=False)
    with pytest.raises(ValueError):
        a[0] = 666
    with pytest.raises(ValueError):
        a.data[0] = 666
    with pytest.raises(ValueError):
        a.mask[0] = False  # THIS SHOULD RAISE AN ERROR BUT DOESN'T

def test__masked_array__cant_write_after_setflags_write_false__workaround():
    a = ma.MaskedArray([0, 1, 2], mask=[True, True, False], hard_mask=True)
    a.setflags(write=False)
    a._mask.setflags(write=False)  # WORKAROUND
    with pytest.raises(ValueError):
        a[0] = 666
    with pytest.raises(ValueError):
        a.data[0] = 666
    with pytest.raises(ValueError):
        a.mask[0] = False
```

The first test above fails, the second (with workaround) passes.

### Numpy/Python version information:

```
1.19.0 3.8.5 (default, Jul 22 2020, 12:39:25) 
[GCC 8.3.0]
```
",2020-07-30 14:37:23,,Bug: mask of non-writable MaskedArray are writeable,"['00 - Bug', 'component: numpy.ma']"
16975,open,senorita-py,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
print([np.int32(4)] == np.int32(4))
# [ True]
```
<!-- Remove these sections for a feature request -->

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2020-07-30 08:20:24,,Should NumPy scalars be array-scalars (e.g. convert lists to arrays in operators),['15 - Discussion']
16972,open,pulkin,"- I have several 1D arrays of varying but comparable lengths to be merged (`vstack`) into a contiguous 2D array.
- I merge them into a masked array where padding entries are masked out.
- I simply run `np.unique(return_inverse=True)` on the masked array.
- The output is two arrays: a masked `key` array with unique entries which optionally includes a single masked padding entry `-` and a plain `inverse` array with the size corresponding to the input.

I would expect the other way around: `key` array to be a plain 1D array while `inverse` to be masked. There are two separate issues here:

- `len(key)` should represent the number of unique entries. Right now it does not: the masking element (`999999` in the example below) may be present or may be not, depending on whether the mask is empty or not. This makes masking pretty much useless for `np.unique`: if I pass a masked array I clearly want to avoid masked entries in the `key` entries. I could equally just do `np.unique(masked_array.data)` otherwise.
- Given `np.unique` is a transparent operation (i.e. I can run it on both arrays and masked arrays) I would expect transparent output. Without knowing anything what `unique` does and what is it for, `inverse` should definitely be a masked array because it has its elements corresponding one-to-one to the input.

As a result of this inconsistency I have to (a) check whether anything has been masked at all (b) conditionally pick out the padding entry from `key` output (c) apply a mask to `inverse` output. Something like the following.

```python
def masked_unique(a):
    a = np.ma.masked_array(data=a.data, mask=a.mask, fill_value=a.data.max() + 1)
    key, inverse = np.unique(a, return_inverse=True)
    if np.any(a.mask):
        key = key[:len(key) - 1]
    return np.array(key), np.ma.masked_array(data=inverse, mask=a.mask)
```

Strictly speaking, I could equally run `np.unique` on raw `a.data` in the above example to fix this. I pretty much do all the job by myself.

<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
>>> import numpy as np
>>> a = np.ma.masked_array([1, 2, 3, 4], [0, 1, 0, 0])
>>> np.unique(a, return_inverse=True)
(masked_array(data=[1, 3, 4, --],
             mask=[False, False, False,  True],
       fill_value=999999), array([0, 3, 1, 2]))
```

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```
1.18.4 3.8.3 (default, May 29 2020, 00:00:00) 
[GCC 10.1.1 20200507 (Red Hat 10.1.1-1)]
```
",2020-07-29 14:27:29,,numpy.unique on masked arrays,['component: numpy.ma']
16967,open,tBuLi,"<!-- Please describe the issue in detail here, and fill in the fields below -->

I think it would be great to add [dual number](https://en.wikipedia.org/wiki/Dual_number) support to numpy. Dual numbers are numbers a + b ε where ε^2 = 0. These numbers have great utility in automatic differentiation, since from a Taylor series it directly follows that

f(x+ε) = f(x) + εf'(x)

Therefore one only has to look at the dual component of f(x + ε) to obtain the first derivative. Although there are already some libraries for this in python, it would obviously be much faster if this was in numpy itself, and it would also greatly increase the number of applications. The amount of new code would probably be similar or smaller to that of adding complex numbers themselves.

For automatic derivation of complex functions the coefficients of the dual number should be complex. And higher order derivatives can be evaluated using nested dual numbers, though that is probably less important for most users so if that is to exotic for a new dtype then that is not essential.

### Example code:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

Derivative of f(x) = x^2:
```python
>>> import numpy as np
>>> x = np.array([np.dual(3,1), np.dual(5,1)])
>>> x**2
np.array([np.dual(9,6), np.dual(25,10)])
```
Derivative of f(z) = z^2, z complex:
```python
>>> y = np.dual(3 + 4j, 1)
>>> y**2
np.dual(-7 + 24j, 6 + 8j)
```
Second derivative of f(x) = x^2 using nested dual (might be to exotic for a dtype?):
```python
>>> x = np.dual(np.dual(3, 1), np.dual(1, 0))
>>> x**2
np.dual(np.dual(9, 6), np.dual(6, 2))
```
I'm curious if there is interest in adding this new feature, or if it was decided in the past not to do this, why that is? I've searched but couldn't find any history on dual numbers in numpy.",2020-07-28 12:42:13,,Feature request: dual numbers as dtype,['15 - Discussion']
16964,open,Feelx234,"Hello,

I have been using the np.intersect1d function a lot recently and after looking at the implementation I think there are some optimizations possible. These optimizations are mostly effective when the arrays to be intersected differ in size.
Three main things I would like to change/contribute:

1. Using np.searchsorted instead of the current implementation when arrays are unequally sized
2. We can use min/max cutoff's to speedup computation
3. I would like to add an *assume_sorted keyword* argument which makes the whole algorithm run much faster when arrays are already sorted (which is the case if you chain multiple calls to np.intersect1d)
4. We can use early exit's when one of the arrays is zero, which should further increase speed in some rare cases while not cost anything otherwise

See below for test results.

### Current version of the code

Implementation of return_indices is still lacking!


```python
import numpy as np
def intersect_search(ar1, ar2, return_indices=False):
    """""" intersect the sorted arrays ar1 and ar2 using searchsorted
        the original numpy implementation concats the arrays and then sorts them
        this version is much faster if the size of the arrays is unequal
    """"""
    
    # find smaller array
    if len(ar1) < len(ar2):
        smaller_arr = ar1
        longer_arr = ar2
    else:
        longer_arr = ar1
        smaller_arr = ar2


    # make sure we are never find past the end of longer_arr
    i=len(smaller_arr)
    while smaller_arr[i-1]>longer_arr[-1]:
        i-=1

    smaller_arr=smaller_arr[:i]
    tmp1 = np.searchsorted(longer_arr, smaller_arr, 'left')
    mask = smaller_arr==longer_arr[tmp1]
    if return_indices:
        return smaller_arr[mask], mask 
    else:
        return smaller_arr[mask]



    
def dedup(ar, return_indices=False):
    """"""removes consecutive duplicate elements
       stolen from np.unique
    """"""
    mask = np.empty(ar.shape, dtype=np.bool_)
    mask[:1] = True
    mask[1:] = ar[1:] != ar[:-1]
    
    if return_indices:
        return (ar, mask)
    else:
        return ar
    
def get_bounded(ar1, ar2, return_indices = False, assume_sorted=False):
    """""" returns max/min projected ar1 and ar2
    max/min optimization:
    idea: range [123456789]
          ar1 = [123 5 7  ]
          ar2 = [   456 89]
    We can already discard large portions based on min/max values:
        ar1' =[    5 7  ]
        ar2' =[   456   ]
    
    For sorted the cost is O(log(len(ar1))) + O(log(len(ar2)))
    While it is O(len(ar1))+O(len(ar2)) otherwise
    
    """"""
    if assume_sorted:
        if ar1[0] < ar2[0]:
            lower1 = np.searchsorted(ar1, ar2[0])
            lower2 = 0
        else:
            lower2 = np.searchsorted(ar2, ar1[0])
            lower1 = 0


        if ar1[-1] < ar2[-1]:
            upper1 = np.searchsorted(ar1, ar2[-1], side='right')
            upper2 = len(ar2)

        else:
            upper2 = np.searchsorted(ar2, ar1[-1], side='right')
            upper1 = len(ar1)
        ar1 = ar1[lower1: upper1]
        ar2 = ar2[lower2: upper2]
        if return_indices:
            return ar1, ar2, slice(lower1, upper1),  slice(lower2, upper2)
        else:
            return ar1, ar2

    else:
        lowest1, largest1 = np.min(ar1), np.max(ar1)
        lowest2, largest2 = np.min(ar1), np.max(ar1)
        mask1 = np.logical_and(ar1 >= lowest2, ar1 <= largest2)
        mask2 = np.logical_and(ar2 >= lowest1, ar2 <= largest1)
        ar1 = ar1[mask1]
        ar2 = ar2[mask2]
        if return_indices:
            return ar1, ar2, mask1, mask2
        else:
            return ar1, ar2

def intersect1d(ar1, ar2, return_indices=False, assume_unique=False, assume_sorted=False, enable_min_max=False):
    if return_indices:
        return intersect1d_with_indices(ar1, ar2, assume_unique, assume_sorted)
    else:
        return intersect1d_no_indices(ar1, ar2, assume_unique, assume_sorted, enable_min_max)
    
def intersect1d_with_indices(ar1, ar2, assume_unique=False, assume_sorted=False):
    ar1 = np.asanyarray(ar1)
    ar2 = np.asanyarray(ar2)
    
    # early exit
    if len(ar1)==0 or len(ar2)==0:
        return (np.array([], dtype=ar1.dtype), # dtype correct?
                np.array([], dtype=np.uint32), # dtype correct?
                np.array([], dtype=np.uint32)) # dtype correct?
    
    # do min/max optimization
    ar1, ar2, m1_0, m2_0 = get_bounded(ar1, ar2, return_indices=True, assume_sorted=assume_sorted)
    
    # early exit, array sizes might have changed
    if len(ar1)==0 or len(ar2)==0:
        return (np.array([], dtype=ar1.dtype), # dtype correct?
                np.array([], dtype=np.uint32), # dtype correct?
                np.array([], dtype=np.uint32)) # dtype correct?
    m1_1 = None
    m2_1 = None
    if assume_unique:
        if not assume_sorted:
            ar1 = ar1.copy()
            ar1.sort()
            ar2 = ar2.copy()
            ar2.sort()
    else:
        if assume_sorted:
            # can omit sorting
            ar1, m1_1 = dedup(ar1)
            ar2, m2_1 = dedup(ar2)
        else:
            ar1, m1_1 = np.unique(ar1)
            ar2, m2_1 = np.unique(ar2)
    
    if max(len(ar1), len(ar2)) / min(len(ar1), len(ar2)) > 5.:
        # here we use the variant that is much better for larger array difference
        return intersect_search(ar1, ar2)
    else:
        # equivalent how it is currently implemented
        aux = np.concatenate((ar1, ar2))
        aux.sort() 

        mask = aux[1:] == aux[:-1]
        int1d = aux[:-1][mask]

        return int1d

def intersect1d_no_indices(ar1, ar2, assume_unique=False, assume_sorted=False, enable_min_max=False):
    
    
    ar1 = np.asanyarray(ar1)
    ar2 = np.asanyarray(ar2)
    
    # early exit
    if len(ar1)==0 or len(ar2)==0:
        return np.array([], dtype=ar1.dtype) # dtype correct?
    
    # do min/max optimization
    if enable_min_max:
        ar1, ar2 = get_bounded(ar1, ar2, assume_sorted=assume_sorted)
    
    # early exit, array sizes might have changed
    if len(ar1)==0 or len(ar2)==0:
        return np.array([], dtype=ar1.dtype) # dtype correct?
    
    if assume_unique:
        if not assume_sorted:
            ar1 = ar1.copy()
            ar1.sort()
            ar2 = ar2.copy()
            ar2.sort()
    else:
        if assume_sorted:
            # can omit sorting
            ar1 = dedup(ar1)
            ar2 = dedup(ar2)
        else:
            ar1 = np.unique(ar1)
            ar2 = np.unique(ar2)
    
    if max(len(ar1), len(ar2)) / min(len(ar1), len(ar2)) > 5.:
        # here we use the variant that is much better for larger array difference
        return intersect_search(ar1, ar2)
    else:
        # equivalent how it is currently implemented
        aux = np.concatenate((ar1, ar2))
        aux.sort() 

        mask = aux[1:] == aux[:-1]
        int1d = aux[:-1][mask]

        return int1d
```
### Test results
I ran a few tests on different kinds of data. The first array is of size 10^5 filled with random elements drawn from 0..10^6. The second array is of size 10^(5+i), i indicated in the x axis. The labels correspond to assume_unique, assume_sorted and use_min_max correspondingly. 



![sorted unique arrays](https://user-images.githubusercontent.com/52208598/88659240-0775e600-d0d5-11ea-8d5c-9dd9c9c6f5ca.png)

![sorted unique arrays](https://user-images.githubusercontent.com/52208598/88660172-861f5300-d0d6-11ea-91f1-302ac56d874b.png)

![shuffled unique arrays](https://user-images.githubusercontent.com/52208598/88659801-ef529680-d0d5-11ea-862a-900c59eb6cf5.png)


I think the images show clearly, that there is quite some speedup to be gained 3x-5x for unsorted unique data and orders of magnitude for sorted unique data. When using min/max optimization, there is additional speedup for already sorted data. While it is slower when already sorted cannot be assumed.",2020-07-28 11:28:34,,Optimizing np.intersect1d,"['01 - Enhancement', 'component: numpy.lib']"
16945,open,cjblocker,"This is essentially the same problem reported in #8597, but a part that I feel was not fully addressed when it was recently closed by a doc improvement. Essentially, `linspace`, `geomspace`, `arange`, `r_`, etc. will never infer a dtype with less precision than `float64` or `complex128`. It is unclear whether this is intended behavior.

The docs for `linspace`, `arange`, `logspace`, `geomspace` all have something along the lines of ""If dtype is not given, the data type is inferred from [other arguments]."" It clarifies that ints will be promoted to floats, but not that float32 will be upgraded to float64s, while float128 will be left alone (similarly with complex64/128/256).

```python
import numpy as np

print([ # same for geomspace, logspace
    np.linspace(np.float16(0.1), np.float16(0.5), 3).dtype,
    np.linspace(np.float32(0.1), np.float32(0.5), 3).dtype,
    np.linspace(np.float64(0.1), np.float64(0.5), 3).dtype,
    np.linspace(np.float128(0.1), np.float128(0.5), 3).dtype
    ])

print([
    np.arange(np.float16(0.1), np.float16(0.5), np.float16(0.1)).dtype,
    np.arange(np.float32(0.1), np.float32(0.5), np.float32(0.1)).dtype,
    np.arange(np.float64(0.1), np.float64(0.5), np.float64(0.1)).dtype,
    np.arange(np.float128(0.1), np.float128(0.5), np.float128(0.1)).dtype
    ])

print([
    np.r_[np.float16(0.1):np.float16(0.5):np.float16(0.1)].dtype,
    np.r_[np.float32(0.1):np.float32(0.5):np.float32(0.1)].dtype,
    np.r_[np.float64(0.1):np.float64(0.5):np.float64(0.1)].dtype,
    np.r_[np.float128(0.1):np.float128(0.5):np.float128(0.1)].dtype
    ])

print([
    np.mgrid[np.float16(0.1):np.float16(0.5):np.float16(0.1)].dtype,
    np.mgrid[np.float32(0.1):np.float32(0.5):np.float32(0.1)].dtype,
    np.mgrid[np.float64(0.1):np.float64(0.5):np.float64(0.1)].dtype,
    np.mgrid[np.float128(0.1):np.float128(0.5):np.float128(0.1)].dtype
    ])

print([ # note comma
    np.mgrid[np.float16(0.1):np.float16(0.5):np.float16(0.1),].dtype,
    np.mgrid[np.float32(0.1):np.float32(0.5):np.float32(0.1),].dtype,
    np.mgrid[np.float64(0.1):np.float64(0.5):np.float64(0.1),].dtype,
    np.mgrid[np.float128(0.1):np.float128(0.5):np.float128(0.1),].dtype
    ])
```
Output:
```python
# input   float16      float32           float64          float128
[dtype('float64'), dtype('float64'), dtype('float64'), dtype('float128')] # linspace, logspace, geomspace
[dtype('float64'), dtype('float64'), dtype('float64'), dtype('float128')] # arange
[dtype('float64'), dtype('float64'), dtype('float64'), dtype('float128')] # r_
[dtype('float64'), dtype('float64'), dtype('float64'), dtype('float128')] # mgrid
[dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64')]  # mgrid w/ comma
```

The `mgrid` w/ comma only runs with the recent bug fix (#16815), in which I thought all floats in these similar methods were cast to float64, so it is inconsistent with the rest.  @eric-wieser mentioned making `mgrid` infer lower precision types could be follow up work, which prompted this issue. I'm happy to fix that, but I'm not sure what the correct output should be. 

Personally I think the code for all these functions should be updated to match what the docs imply unless there is a reason the output needs the precision of at least a float64. Other option would be to just fix `mgrid` w/ comma to match the rest and maybe further clarify the docs.

### Numpy/Python version information:

1.20.0.dev0+f457a1a 3.7.7 (default, Mar 10 2020, 15:43:33) 
[Clang 11.0.0 (clang-1100.0.33.17)]

",2020-07-26 00:56:24,,"Should lower precision dtypes be inferred in linspace, etc ?","['00 - Bug', '04 - Documentation']"
16933,open,eric-wieser,"This is a feature suggestion based on #16248.

At the moment, `searchsorted(a, v, side='left' / 'right')` (same as `np.digitize(v, a, right=True / False)`, note swap of right and left) has semantics


side | returned index `i` satisfies
-- | --
left | `a[i-1] < v <= a[i]`
right | `a[i-1] <= v < a[i]`

We could augment this to:

`is_left[i - 1]` | `is_left[i]` | returned index i satisfies
-- | -- | --
True | True | `a[i-1] < v <= a[i]`
False | False | `a[i-1] <= v < a[i]`
True | False | `a[i-1] < v < a[i]`
False | True | `a[i-1] <= v <= a[i]`

This would then allow the behavior of ""all intervals half-open except the last"" to be specified as `is_left = [False, False, False, ... True]`, which means this subsumes the use case in #16248.

---

This unfortunately would lead to a slightly awkward API, with either:
* Slow arrays of strings, `searchsorted(a, v, side=['left', 'left', 'right'])`
* Two mutually-exclusive kwargs, `searchsorted(a, v, is_left=True)` and `searchsorted(a, v, side='left')`",2020-07-23 10:44:02,,Allow left/right to be specified elementwise in searchsorted,['01 - Enhancement']
16923,open,mganahl,"For int16 arrays, intersect1d is faster for `return_indices=True` than for `return_indices=False`
by quite a margin (6x on my computer). Is this behavior preferred? I would expect the opposite.


### Reproducing code example:

```python
import numpy as np
import timeit
D=20000
a1 = np.random.randint(0,10,D, dtype=np.int16)
a2 = np.random.randint(0,10,D, dtype=np.int16)
print(timeit.timeit(lambda: np.intersect1d(a1, a2, return_indices=False), number=100))
print(timeit.timeit(lambda: np.intersect1d(a1, a2, return_indices=True), number=100))
```
",2020-07-21 18:52:54,,np.intersect1d is several times faster for return_indices=True,['component: numpy.lib']
16921,open,seberg,"Unlike `longdouble` itself, which uses a custom converter, the complex longdouble `setitem` does not do such things, which means it uses only double precision for the conversion.",2020-07-21 13:13:35,,BUG: String to complex longdouble conversions are  not full precision,"['00 - Bug', 'component: numpy._core']"
16903,open,feature-engineer,"<!-- Please describe the issue in detail here, and fill in the fields below -->

bincount would only work for real weights, and not complex ones.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
np.bincount(np.arange(3), weights=1j * np.arange(3))
```

<!-- Remove these sections for a feature request -->

### Error message:

Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""<__array_function__ internals>"", line 6, in bincount
TypeError: Cannot cast array data from dtype('complex128') to dtype('float64') according to rule 'safe'

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information: 

1.18.4 3.7.7

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2020-07-19 13:58:09,,bincount fails for complex weights,"['01 - Enhancement', 'component: numpy._core']"
16881,open,david-zwicker,"I was using `arr.flat` to iterate over an array of unknown dimension. When the array only consisted of a single element, the code did not update the array, which I found surprising.

### Reproducing code example:

The code in question was something like this

```python
import numpy as np
a = np.array([[1., 2.], [3., 4.]])
for i in range(len(a)):
    for j in range(a[0].size):
        a[i].flat[j] = 0
a
# >>> array([[0., 0.],
#            [0., 0.]])
```
which correctly returns an array of all zeros in this case where `a.ndim == 2`.

However, if the input array only has a single dimension, the code produces unexpected results:
```python
import numpy as np
a = np.array([1., 2.])
for i in range(len(a)):
    for j in range(a[0].size):
        a[i].flat[j] = 0
a
# >>> array([1., 2.])
```
The array `a` is unchanged!

The underlying problem seems to be that the following code
```python 
a = np.array([1.])
a[0].flat[0] = 0
a
# >>> array([1.])
```
does not update the array `a`.

This happens with numpy version `1.19.0` on python `3.7.8 (default, Jun 29 2020, 13:05:45) [Clang 11.0.3 (clang-1103.0.32.62)]`.


",2020-07-16 10:03:23,,`flatiter` does not update underlying numpy array,['00 - Bug']
16876,open,seberg,"Consider the NumPy float32:
```
f32 = np.float32(2)
type(f32.astype(object))  # gives `float`
```
which is fair enough in a sense.  Our scalars are weird...

This also makes some stranger things work though:
```
class myf:
   def __radd__(self, other):
       return f""called backward with {type(other)}""

f32 + myf()
# ""called backward with <class 'float'>""
```
This works by first dispatching to the ufunc machinery, which converts both to arrays, which then casts `f32` to object and then calls the full operator (both ways in principle).

Now, that would enter a recursive call of course if it would not convert to the `float32` to a python float, so that:
```
myf() + np.float128()
```
segfaults due to infinite recursion I guess.  If you swap the order, things are more OK, although it probably crashes just as much if `np.float128` was instead a subclass of float128...",2020-07-15 16:02:25,,Object casts convert to python type (which makes some operations work magically),['unlabeled']
16844,open,han-kwang,"The default of `np.meshgrid` is `indexing='xy'`. This is not intuitive - intuitive would be: `indexing='ij'`. I have been bitten by this default behavior many times since I started using numpy in 2014 and as recently as a month ago. In the last couple of weeks, I've already answered two Stackoverflow questions that related to this [1](https://stackoverflow.com/a/62863692) and [2](https://stackoverflow.com/a/62492533).

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->
```python
import numpy as np
print(np.meshgrid(np.arange(2), np.arange(3), np.arange(4))[0].shape)
```
Generates output
```python
(3, 2, 4)
```
Most people would expect `(2, 3, 4)` unless they read the documentation very carefully. The documentation of the `indexing` parameter:

> indexing{‘xy’, ‘ij’}, optional -- Cartesian (‘xy’, default) or matrix (‘ij’) indexing of output. See Notes for more details.

is not helpful in my opinion. To this day, I don't really get what it the options have to do with ""cartesian"" or ""matrix""; all I can remember is: ""`'ij'` is the one I need."". I understand that the default needs to stay there for compatibility and that the arbitrary ""swap the order of the first two axes"" is likely inherited from how Matlab does it. But at least, the documentation could be more clear, for example:

> indexing{‘xy’, ‘ij’}, optional -- Cartesian (‘xy’, default) or matrix (‘ij’) indexing of output. **With 'xy' indexing, the order of the first two axes will be swapped.** See Notes for more details.

It could even be considered to provide an alternative function that behaves like `np.meshgrid(indexing='ij')`. I know that `np.mgrid` sort of does, but its syntax is rather esoteric. 

### Numpy/Python version information:
1.18.1 3.7.6 (default, Jan  8 2020, 19:59:22) 
[GCC 7.3.0]

",2020-07-13 14:13:45,,DOC: warn about the indexing='xy' default in np.meshgrid,"['04 - Documentation', 'component: numpy.lib']"
16801,open,mrityagi,"**NumPy Documentation needs an Explanations section so that we can explain some of the annoying Queries or Doubts which NumPy developers/contributors/users face in day to day life .**

I surely believe we all must be having some of such Doubts/Queries albeit on any topic  which we wished should be explained in the Official NumPy Documentation .

*We are taking  some steps to revamp the Documentation to make it more community ( Users , contributors , Developers) friendly . To achieve this , we have created this issue so as to collect such queries from the community and work on providing Explanations for the same in Official NumPy Documentation .*

**Do Comment any such Queries , Questions , Doubts in this issue .**",2020-07-11 06:58:41,,Tracking issue: which topics/questions require Explanations in NumPy Documentation?,"['04 - Documentation', 'Tracking / planning']"
16755,open,mdehoon,"When subclassing numpy.ndarray, numpy.outer returns a plain array instead of an instance of the subclass.

### Reproducing code example:

Based on the example in the section *Slightly more realistic example - attribute added to existing array* in the numpy documentation:

```python
import numpy as np

class RealisticInfoArray(np.ndarray):

    def __new__(cls, input_array, info=None):
        # Input array is an already formed ndarray instance
        # We first cast to be our class type
        obj = np.asarray(input_array).view(cls)
        # add the new attribute to the created instance
        obj.info = info
        # Finally, we must return the newly created object:
        return obj

    def __array_finalize__(self, obj):
        # see InfoArray.__array_finalize__ for comments
        if obj is None: return
        self.info = getattr(obj, 'info', None)

arr = np.arange(6)
arr.shape=(3,2)
obj = RealisticInfoArray(arr, info='information')
```

Now `obj` is an instance of `RealisticInfoArray`:
```
>>> obj
RealisticInfoArray([[0, 1],
                    [2, 3],
                    [4, 5]])
```

The dot product returns a `RealisticInfoArray` instance:
```
>>> np.dot(obj,obj.T)
RealisticInfoArray([[ 1,  3,  5],
                    [ 3, 13, 23],
                    [ 5, 23, 41]])
```
The inner product also returns a `RealisticInfoArray` instance:
```
>>> np.inner(obj, obj)
RealisticInfoArray([[ 1,  3,  5],
                    [ 3, 13, 23],
                    [ 5, 23, 41]])
```
Applying a ufunc returns a `RealisticInfoArray` instance:
```
>>> np.sin(obj)
RealisticInfoArray([[ 0.        ,  0.84147098],
                    [ 0.90929743,  0.14112001],
                    [-0.7568025 , -0.95892427]])
```

But the outer product returns a plain array:
```
>>> np.outer(obj, obj)
array([[ 0,  0,  0,  0,  0,  0],
       [ 0,  1,  2,  3,  4,  5],
       [ 0,  2,  4,  6,  8, 10],
       [ 0,  3,  6,  9, 12, 15],
       [ 0,  4,  8, 12, 16, 20],
       [ 0,  5, 10, 15, 20, 25]])
>>> type(_)
<class 'numpy.ndarray'>
```
I believe that this should return a `RealisticInfoArray` object instead.

### Numpy/Python version information:

```
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.19.0 3.6.2 (v3.6.2:5fd33b5926, Jul 16 2017, 20:11:06) 
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]
```

",2020-07-05 12:19:13,,numpy.outer and subclassing,['unlabeled']
16751,open,rossbar,"Many of the functions for creating masked arrays in `np.ma` have incorrect types associated with their return values, e.g.

```python
In [1]: np.ma.zeros?
...
Returns
-------
out : ndarray
    Array of zeros with the given shape, dtype, and order.
...
```
The above should be : `out : np.ma.MaskedArray` or similar.

For many of the array creation functions (`arange`, `zeros`, `ones`), the docstring is ported over from the corresponding `np` function via the `_convert2ma` class. One potential solution worth investigating might be modifying the `_convert2ma` class to automatically check/replace these return types.

### Error message:

N/A - incorrect docs

### Numpy/Python version information:

1.20.0.dev0+4771e3f 3.8.3 (default, May 17 2020, 18:15:42) 
[GCC 10.1.0]",2020-07-04 18:55:32,,DOC: Wrong return type for array creation functions in np.ma docstrings,"['04 - Documentation', 'component: numpy.ma', 'sprintable']"
16741,open,denis-bz,"<!-- Please describe the issue in detail here, and fill in the fields below -->
Numpy and scipy appear to use different LAPACK drivers for eigvalsh on macos. 
WIBNI: wouldn't it would be nice if they were the same
or if that's not easy, document the difference.

### Reproducing code example:
in a [gist](https://gist.github.com/denis-bz/6a9d7379c8edf965b0a997c2ec2471e1). Basically

    A = sparse.random 10000, 10000, density=.001 ).A  # toarray, dense
    time np.linalg.eigvalsh( A )
    time scipy.linalg.eigvalsh( A, driver=driver )

-> log

    np.linalg.eigvalsh: 201 sec
    scipy.linalg.eigvalsh: 83 sec  driver ev
    scipy.linalg.eigvalsh: 84 sec  driver evd

On another matrix from SuiteSparse, scipy was also > twice as fast.

Maybe this is only on macos ?
(Gnutime: 381% of 4 cores, so it's not that.)

numpy 1.19.0  scipy 1.5.0  python 3.7.6  plaform Darwin-14.5.0-x86_64-i386-64bit
",2020-07-03 10:43:31,, Do numpy and scipy use different LAPACK drivers for eigvalsh  on macos ?,['component: numpy.linalg']
16726,open,lhfriedman,"When numpy arrays are pickled and then unpickled, flag values don't appear to be preserved. This is a real problem for
multiprocessing which is how I encountered this error. I can hack around this, but the expected behavior for
 `pickle.loads(pickle.dumps(any_object))` should be as near a duplicate as possible. I guess I am saying that  `pickle.loads(pickle.dumps(any_object))` should act like like `numpy.asarray` and not not `numpy.copy`.

### Reproducing code example:

```python
import numpy as np
import pickle

a_0 = np.array([1,2,3])
a_0.setflags(write=False)
print('\na_0')
print(a_0)
print(a_0.flags)
a_0_pkl = pickle.dumps(a_0)
print('\na_0_pkl')
print(a_0_pkl)
a_0_unpkl = pickle.loads(a_0_pkl)
print('\na_0_unpkl')
print(a_0_unpkl)
print(a_0_unpkl.flags)
```

### Output

```python
a_0
[1 2 3]
  C_CONTIGUOUS : True
  F_CONTIGUOUS : True
  OWNDATA : True
  WRITEABLE : False
  ALIGNED : True
  WRITEBACKIFCOPY : False
  UPDATEIFCOPY : False


a_0_pkl
b'\x80\x03cnumpy.core.multiarray\n_reconstruct\nq\x00cnumpy\nndarray\nq\x01K\x00\x85q\x02C\x01bq\x03\x87q\x04Rq\x05(K\x01K\x03\x85q\x06cnumpy\ndtype\nq\x07X\x02\x00\x00\x00i4q\x08K\x00K\x01\x87q\tRq\n(K\x03X\x01\x00\x00\x00<q\x0bNNNJ\xff\xff\xff\xffJ\xff\xff\xff\xffK\x00tq\x0cb\x89C\x0c\x01\x00\x00\x00\x02\x00\x00\x00\x03\x00\x00\x00q\rtq\x0eb.'

a_0_unpkl
[1 2 3]
  C_CONTIGUOUS : True
  F_CONTIGUOUS : True
  OWNDATA : True
  WRITEABLE : True
  ALIGNED : True
  WRITEBACKIFCOPY : False
  UPDATEIFCOPY : False
```


### Numpy/Python version information:

1.18.1 3.7.7 (default, May  6 2020, 11:45:54) [MSC v.1916 64 bit (AMD64)]

",2020-07-01 21:54:10,,pickling numpy arrays does not preserve flags,['unlabeled']
16676,open,zhihaoy,"<!-- Please describe the issue in detail here, and fill in the fields below -->

Many NumPy APIs currently have return type `Any`.  `view`, `item`, `finfo`... But almost always, their type arguments are literals (`'d'`, `np.float64`, for example).  We should implement basic dependent types with Python 3.8 literal types in the typing module.

See also: https://www.python.org/dev/peps/pep-0586/",2020-06-24 07:19:17,,TYP: Literal types support in ``numpy.typing``,['Static typing']
16665,open,asmeurer,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

Empty arrays repr like `array([], shape=(0, 0), dtype=float64)`, but this is not actually valid because `shape` cannot be passed to the array constructor:

```python
>>> from numpy import *
>>> empty((0, 0))
array([], shape=(0, 0), dtype=float64)
>>> array([], shape=(0, 0), dtype=float64)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: 'shape' is an invalid keyword argument for array()
```

It would be useful if either `array([], shape=(0, 0), dtype=float64)` worked or empty arrays printed to another string that actually did work, like `empty(shape=(0, 0), dtype=float64)`. Aside from copy-pastability being useful, it's very confusing that it shows the `shape` keyword like this when it doesn't actually exist. 

<!-- Remove these sections for a feature request -->

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```
>>> np.__version__
'1.18.1'
```
",2020-06-22 21:27:04,,Empty arrays should repr valid code to recreate them,['01 - Enhancement']
16638,open,wgaylord,"When adding values that are not of a numpy type (For example uint8) to an ndarray there is no warning due to overflow given when an overflow occurs. I can reproduce this always.  Sometimes it does not throw overflow warning even when both types are numpy types but I can reproduce it consistantly.

This also occurs with subtraction,division and multiplication. 


### Reproducing code example:

```python
import numpy as np
array = np.ndarray(2,dtype=np.uint8) #can be any size array
array[0] = 255 #Max size for the array
print(""Before overflow""+str(array))
array[0] += 1 #Overflows to 0 but no error
print(""After overflow""+str(array))
array[0] = 255 #Max size for the array
print(""Before overflow""+str(array))
array[0] = array[0] + 1 #Overflows to 0 but no error
print(""After overflow""+str(array))
```

### Numpy/Python version information:

1.18.5 3.8.1 (tags/v3.8.1:1b293b6, Dec 18 2019, 22:39:24) [MSC v.1916 32 bit (Intel)]

",2020-06-19 16:39:01,,Overflow in ndarray only caught if both sides of operation are a numpy type.,['04 - Documentation']
16631,open,WarrenWeckesser,"The function`numpy.testing.assert_array_compare` has no docstring, and does not appear in the online documentation.  My guess is that the authors intended the function to be private.  If that is the case, the function should be renamed to `_assert_array_compare`, or removed from the `numpy.testing` namespace.  Given the lack of any documentation, we could probably do that without a deprecation.

If, on the other hand, `assert_array_compare` is intended to be a public function, then it needs a docstring.",2020-06-18 07:36:23,,Remove `assert_array_compare` from the public API?,"['component: numpy.lib', '03 - Maintenance', '30 - API']"
16624,open,seberg,"Since more such changes will inadvertently be added, there are a few changes that will happen with my array-coercion branch at this time, and just as much with other things later.  These are things that we should review before 1.20, and decide on the exact course of action.

1. `np.array([np.float64(np.inf)], dtype=np.int64)` as opposed to `np.array(np.float64(np.inf), dtype=np.int64)` used to use `float(val)` to assign the item. Which meant that bad float values (including out of bounds) would lead to errors.  The new code will use normal casting logic in all cases, which uses C-casting and typically ends up at the minimum integer. *The ideal solution will probably be to add warnings/errors also for casting in general, although we still need to decide how to do that best.*

2. The truthiness (and casting) of strings to bools is badly defined right now, see also gh-9875. The array coercion changes will have aligned some of these, but not necessarily for the better.  We should ensure that the new behaviour is not worse than the old, and generally push forward with fixing the situation. (The question is how slow we have to take it?)

3. The dtype discovery (mainly with respect to string length) is now improved.  This means that object arrays being coerced are always inspected correctly.  At the same time, the string length is now consistent, but always the normal casting version for numpy scalars (e.g. a float64).  This only affects numpy scalars and only array coercion.  *There is probably no need to do anything here, the behaviour was never consistent and the new behaviour errs on the safe side and is generally better*.

4. Not an issue as such (low priority):  The coercion cache is also used in some cases where the array is not coerced immediately.  This wastes some memory, and the code could be improved to allow doing no caching.

5. We should possibly anticipate (my preferred solution is an error), dtypes which are not conserved during array object creation.  These should only be dtypes with subarrays (and no structure). That should be pretty much impossible, but... See also gh-15471.

6. Consider simply making all DType classes HeapTypes. This would also allow to give them a repr, which may be copy-pastable in the future (if we stick with the square bracket notation). since it allows a period in the repr to make it: `np.dtype[np.float64]`.

7. Consider implementing `dtype.type` to look up the class attribute `DType.type` instead.  Right now these must always match, but we must retain the C-side `dtype->scalar_type` slot for backward compatibility.

8.  The Maping `pytype -> DType` is currently strong. This is an issue in theory for support of dynamically created `pytype <-> DType` pairs.  The solution to this is to a have weak mapping but set a `pytype.__associated_numpy_dtype__` slot when registering, which should be unproblematic for HeapTypes.  For static types everything has to be immortal, but we can allow that path for NumPy itself.  Anyone wanting to add a DType for another pytype (*with* automatic mapping/dtype discovery), has to make sure that NumPy can fill that attribute on the `pytype` (which is possible also for static types of course).


EDIT: The more important issues have been cleared out. The boolean one is still there, but will only affect numpy strings, and I think that is fine. (Of course we still need to clean up strings, but its so strange...)",2020-06-16 22:25:39,,TRACKING: Review and possibly address changes regard new dtypes,"['00 - Bug', '01 - Enhancement', '15 - Discussion', 'component: numpy._core', 'component: numpy.dtype', 'Tracking / planning']"
16585,open,jerabaul29,"- a matrix cannot be squeezed
- there is often no obvious way to detect that something is a matrix, because we live with an interpreter
- squeeze does not work as the user would naturally expect on matrices

This creates arguably subtle bugs: see for example: 

https://stackoverflow.com/questions/62342225/how-to-squeeze-when-numpy-squeeze-does-not-seem-to-squeeze?

So my question is: ""would it be reasonable to create a warning in the kind: 'warning, calling squeeze on a matrix type with a singleton dimension, this may provide unexpected results, see [possibly some doc link]'""?",2020-06-12 13:14:55,,provide a warning when trying to squeeze a matrix?,"['15 - Discussion', 'component: numpy.matrixlib']"
16575,open,ashaffer,"nanquantile seems to be significantly slower than regular quantile, in certain cases. They seem to be identical in the case where you're processing a one dimensional vector, and also identical if you're processing a two dimensional vector as a whole. But if you apply it along one axis of a two dimensional vector, the performance is terrible. I guess it's possible that there's some intrinsic reason for this, but it's hard to imagine what that could be.

### Reproducing code example:

```python
import numpy as np

a = np.random.uniform(size=(27, 100))
%timeit np.quantile(a, 0.8, axis=0)
%timeit np.nanquantile(a, 0.8, axis=0)
```

For me the output of this is:

```
147 µs ± 11.4 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
10.3 ms ± 953 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

<!-- Remove these sections for a feature request -->

### Numpy/Python version information:

```
1.18.5 3.6.4 (default, Mar  9 2018, 23:15:12) 
[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)]
```
",2020-06-11 06:04:14,,nanquantile is hundreds of times slower than quantile for certain cases,['component: numpy.lib']
16573,open,ysBach,"Hi! 
When I combine images, there are some cases when ""lower median"" has been used.
This is similar to ``np.median(array_3d, axis=0)``, but when there are even number of pixels along the axis, it chooses the lower of the two middle values, not the mean of them. 

I am about to implement lower median to my personal codes as a separate ``lmedian`` function or add ``choose={'mean', 'lower', 'upper'}`` option to median. But just to ask: Is it possible this feature can be implemented to numpy at least in the future?

If it goes against any philosophy of numpy, I am sorry and will close the issue. ",2020-06-11 04:24:50,,Feature Request: Median without average,"['01 - Enhancement', '15 - Discussion', 'component: numpy.lib']"
16569,open,rossbar,"The `tobytes` method of ndarray accepts `'K'` for the order keyword argument, but appears not to keep the order as expected:

### Reproducing code example:

```python
>>> a = np.array([[1, 2], [3, 4]], dtype=np.uint8, order='F')
>>> a.ravel('K')   # Keep array order
array([1, 3, 2, 4])
>>> a.tobytes('K')  # Expect consistency with ravel
b'\x01\x02\x03\x04'
```
See https://github.com/numpy/numpy/pull/16291#discussion_r426861913 and #16328 for related discussion. 

 - [ ] Fix bug (see #16328)
 - [ ] Update `tobytes()` docstring to add `K` option to `order=` (see: https://github.com/numpy/numpy/blob/68ebc2bad1ad6a99e9939eff205a1a6d936cf4fd/numpy/core/_add_newdocs.py#L3933-L3950)

### Numpy/Python version information:

1.20.0.dev0+1d799fd 3.8.3 (default, May 17 2020, 18:15:42) 
[GCC 10.1.0]

",2020-06-10 23:04:12,,BUG: `order='K'` behavior for `tobytes`,"['00 - Bug', 'component: numpy._core']"
16537,open,jakirkham,"When using out-of-band pickling with protocol 5 on nested NumPy object arrays, sometimes not all possible buffers that can be extracted for out-of-band transmission are extracted. This can be relevant when users are working with ragged arrays (via NumPy object arrays).

### Reproducing code example:

Here's one example that I ran into recently:

```python
#!/usr/bin/env python


import pickle
import numpy as np


a = np.empty((2,), dtype=object)
a[0] = np.array([np.arange(3)], dtype=object)
a[1] = np.arange(4, 6)

b = []
d = pickle.dumps(a, protocol=5, buffer_callback=b.append)

print(len(b))
```

Edit: A simpler example is included below:

```python
#!/usr/bin/env python


import pickle
import numpy as np


a = np.array([np.arange(3)], dtype=object)

b = []
d = pickle.dumps(a, protocol=5, buffer_callback=b.append)

print(len(b))
```

<!-- Remove these sections for a feature request -->

### Error message:

The first script above prints `1` instead of `2` as one might expect. The second script prints `0` instead of `1`.

### Numpy/Python version information:

```
$ python -c 'import sys, numpy; print(numpy.__version__, sys.version)'
1.18.5 3.8.3 | packaged by conda-forge | (default, Jun  1 2020, 17:21:09) 
[Clang 9.0.1 ]
```",2020-06-09 05:32:54,,Out-of-band pickling misses some buffers with nested object arrays,"['01 - Enhancement', '23 - Wish List']"
16499,open,lkjell,"<!-- Please describe the issue in detail here, and fill in the fields below -->
When a constant pad is performed the constant value is cast to the dtype of the input array. This is problematic for dtype int and constant value is float.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np

x = np.arange(10)
x = np.pad(x, 1, ""constant"", constant_values=3.3)
print(x)

x = np.arange(10)
x = np.pad(x, 1, ""constant"", constant_values=np.nan)
print(x)
```

<!-- Remove these sections for a feature request -->

### Error message:
The expected final array should have the constant value not converted to int. 

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
import sys, numpy; print(numpy.__version__, sys.version)

1.18.1 3.8.3 (default, May 17 2020, 18:15:42) 
[GCC 10.1.0]",2020-06-04 11:06:06,,BUG: numpy.pad pad constant is cast to input array dtype ,"['01 - Enhancement', '15 - Discussion', 'component: numpy.lib']"
16477,open,jakevdp,"Demonstration of the issue:
```python
>>> import numpy as np
>>> print(np.__version__)
1.18.4

>>> np.diagflat(1.0, k=2)
array([[0., 0., 1.],
       [0., 0., 0.],
       [0., 0., 0.]])

>>> np.diagflat(np.float64(1.0), k=2)
0.0
```
The issue comes from the function's use of `__array_wrap__`: https://github.com/numpy/numpy/blob/b7c27bd2a3817f59c84b004b87bba5db57d9a9b0/numpy/lib/twodim_base.py#L326-L329

It is not defined for python scalars, but for numpy scalars converts array output to a single scalar value:
```python
>>> np.float64(1.0).__array_wrap__(np.ones((2, 2)))
1.0
```
I'm not certain what the correct fix would be here, but it seems potentially confusing.",2020-06-01 23:14:32,,diagflat() has inconsistent behavior for scalar inputs,"['00 - Bug', 'component: numpy._core']"
16475,open,eric-wieser,"`select` is documented specifically to give the first condition priority if multiple are true.
`piecewise` does not document what happens, but does the opposite:
```python
>>> import numpy as np
>>> x = np.arange(5)

>>> np.piecewise(x, [x > 1, x > 3], [1, 3])
array([0, 0, 1, 1, 3])

>>> np.select([x > 1, x > 3], [1, 3])
array([0, 0, 1, 1, 1])
```

We should either:

* Document the fact that piecewise does things in reverse order
* Swap the order of piecewise, and document the new order
* Emit a deprecationwarning and eventually a `ValueError` if multiple conditions are ever true, so that the point becomes moot.

",2020-06-01 22:27:01,,np.piecewise has the opposite way of resolving ties to np.select,['component: numpy.lib']
16469,open,fostiropoulos,"It is much easier to write and more intuitive since it is also followed by other popular frameworks such as tensorflow. 

### Reproducing code example:

```python
import numpy as np
np.concat([np.zeros(10),np.ones(10)],axis=-1)
```

",2020-06-01 17:53:42,,[Feature Request] Add alias of np.concatenate as np.concat ,"['01 - Enhancement', 'component: numpy._core', '62 - Python API']"
16459,open,r-devulap,"Would it be useful to the community if the ufunc documentation listed the SIMD information for each ufunc (a simple list of architecture and the SIMD variant comes to mind). If this is something of interest, I can help populate a table that can be integrated to with the docs. 
",2020-06-01 00:33:21,,DOC: SIMD information in ufunc documentation,"['15 - Discussion', '04 - Documentation', 'component: SIMD']"
16453,open,eric-wieser,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

```python
>>> import numpy as np
>>> big_int = 2**53 + 1
>>> big_int64 = np.int64(big_int)
>>> big_int / (big_int * 1000)
0.001
>>> big_int64 / (big_int64 * 1000)
0.0009999999999999998
```

The cause here is that numpy computes `float(x) / float(y)`, but python does something cleverer.

I don't know if we want to fix this, but if we don't we certainly need to document it.
",2020-05-31 10:10:39,,Division of `int` has more precision than division of `np.int64`,"['00 - Bug', '15 - Discussion', '04 - Documentation']"
16429,open,sidhant007,"Currently in Python3 when `savetxt` is called on bytes array, it writes b-prefixed bytes to the file object. This leaks out python's internal representation of bytes (i.e b-prefixed) to a generic data exchange format (Ex. CSV) and the result is not round tripping, i.e I am unable to recover the bytes array (or its equivalent string representation in arbitrary encoding) using `loadtxt`

Similar issue has been observed in Pandas as seen [here](https://github.com/pandas-dev/pandas/issues/9712). 

### Reproducing code example:

```python
import numpy as np
import sys
x = np.array([b'A', b'A'], dtype='S10')
np.savetxt(sys.stdout, [x, x], fmt='%s')
```
Output in Python3:
```
b'A' b'A'
b'A' b'A'
```

Whereas output in Python 2.7:
```
A A
A A
```

### Numpy/Python version information:

1.18.1 3.7.6 (default, Jan  8 2020, 13:42:34)
[Clang 4.0.1 (tags/RELEASE_401/final)]",2020-05-29 16:14:15,,savetxt writes bytes with the b-prefixed notation in Python3,"['01 - Enhancement', 'component: numpy.lib']"
16425,open,bashtage,"I recently build with clang-cl and it found two issues that look like they may be not 100% right:

I don't know anything about the first. It looks a bit worrying to me since it seems to be referring to INTP size. But maybe it just needs an explicit cast.

The second is in random distributions. It probably needs an explicit cast since double cannot come close to representing the exact value on 64-bit platforms. 

```
numpy\core\src\multiarray\ctors.c(3153,47): warning: implicit conversion from 'long long' to 'double' changes value from 9223372036854775807 to 9223372036854775808 [-Wimplicit-int-float-conversion]
    if (!(NPY_MIN_INTP <= ivalue && ivalue <= NPY_MAX_INTP)) {
                                           ~~ ^~~~~~~~~~~~
numpy\core\include\numpy/npy_common.h(279,30): note: expanded from macro 'NPY_MAX_INTP'
        #define NPY_MAX_INTP NPY_MAX_LONGLONG
                             ^~~~~~~~~~~~~~~~
numpy\core\include\numpy/npy_common.h(619,28): note: expanded from macro 'NPY_MAX_LONGLONG'
#  define NPY_MAX_LONGLONG NPY_MAX_INT64
                           ^~~~~~~~~~~~~
numpy\core\include\numpy/npy_common.h(431,23): note: expanded from macro 'NPY_MAX_INT64'
#define NPY_MAX_INT64 NPY_LONGLONG_SUFFIX(9223372036854775807)
                      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
numpy\core\include\numpy/npy_common.h(326,39): note: expanded from macro 'NPY_LONGLONG_SUFFIX'
#    define NPY_LONGLONG_SUFFIX(x)   (x##i64)
                                      ^~~~~~
<scratch space>(96,1): note: expanded from here
9223372036854775807i64
^~~~~~~~~~~~~~~~~~~~~~
1 warning generated.
```

```
numpy\random\src\distributions\distributions.c(974,13): warning: implicit conversion from 'long long' to 'double' changes value from 9223372036854775807 to 9223372036854775808 [-Wimplicit-int-float-conversion]
    if (X > RAND_INT_MAX || X < 1.0) {
          ~ ^~~~~~~~~~~~
numpy\core\include\numpy/random/distributions.h(24,22): note: expanded from macro 'RAND_INT_MAX'
#define RAND_INT_MAX INT64_MAX
                     ^~~~~~~~~
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\stdint.h(55,26): note: expanded from macro 'INT64_MAX'
#define INT64_MAX        9223372036854775807i64
                         ^~~~~~~~~~~~~~~~~~~~~~
1 warning generated.
```

",2020-05-29 10:41:15,,Clang produces warnings about casts of large integers to doubles changing values,['unlabeled']
16391,open,weiji14,"<!-- Please describe the issue in detail here, and fill in the fields below -->

Not sure if this is expected behaviour, but converting a timedelta64 Not a Time (NaT) object to an uint64 type doesn't give NaN. Instead, it produces a large number.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np

timearray = np.array([np.timedelta64('NaT'), np.timedelta64(45969543659411542)])
# array([            'NaT', 45969543659411542], dtype=timedelta64)
```

Current:
```python
intarray = timearray.astype(dtype=np.uint64)
# array([9223372036854775808,   45969543659411542], dtype=uint64)
```

Expected:
```python
intarray = timearray.astype(dtype=np.uint64)
# array([           nan,   45969543659411542], dtype=uint64)
```

<!-- Remove these sections for a feature request -->

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

1.18.2 3.8.2 | packaged by conda-forge | (default, Apr 24 2020, 08:20:52) 
[GCC 7.3.0]",2020-05-26 22:02:38,,Numpy timedelta64 NaT not converted to uint64 NaN,"['01 - Enhancement', 'component: numpy._core', 'component: numpy.dtype', 'component: numpy.datetime64']"
16380,open,pv,"The `np.polynomial.Polynomial` does not seem to handle array-valued polynomials, although this is advertised at some places in its documentation.

E.g. `help(np.polynomial.Polynomial.fit)` [says](https://numpy.org/devdocs/reference/generated/numpy.polynomial.polynomial.Polynomial.fit.html#numpy.polynomial.polynomial.Polynomial.fit):

""yarray_like, shape (M,) or (M, K): y-coordinates of the sample points. Several data sets of sample points sharing the same x-coordinates can be fitted at once by passing in a 2D-array that contains one dataset per column.""

However, it doesn't seem to work. Maybe this is a left-over ""intended feature"" that didn't make it in the final implementation, or the documentation is just wrong. Even if so, this feature would be useful to have.

### Reproducing code example:
```python
import numpy as np
np.polynomial.Polynomial.fit([0, 1, 2], np.array([[1, 2, 3], [4, 5, 6]]).T, deg=2)
```

### Error message:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib64/python3.8/site-packages/numpy/polynomial/_polybase.py"", line 898, in fit
    return cls(coef, domain=domain, window=window)
  File ""/usr/lib64/python3.8/site-packages/numpy/polynomial/_polybase.py"", line 266, in __init__
    [coef] = pu.as_series([coef], trim=False)
  File ""/usr/lib64/python3.8/site-packages/numpy/polynomial/polyutils.py"", line 182, in as_series
    raise ValueError(""Coefficient array is not 1-d"")
ValueError: Coefficient array is not 1-d
```

### Numpy/Python version information:
1.18.4 3.8.2 (default, Feb 28 2020, 00:00:00)
",2020-05-26 12:55:07,,Array-valued np.polynomial.Polynomial doesn't work,['component: numpy.polynomial']
16377,open,ivirshup,"Using the newly added (#16128) `np.array_equal` keyword argument `equal_nan` fails when the values of the passed array aren't accepted by `np.isnan`. This occurs for values like strings.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
np.array_equal(
    np.array(list(""abc"")), np.array(list(""abc"")), equal_nan=True
)
```


### Error message:

```pytb
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<__array_function__ internals>"", line 6, in array_equal
  File ""/Users/isaac/github/numpy/numpy/core/numeric.py"", line 2381, in array_equal
    a1nan, a2nan = isnan(a1), isnan(a2)
TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
```

### Numpy/Python version information:

This occurs on both master and `1.19.0rc1`

```
1.20.0.dev0+3f11db4 3.7.7 (default, Apr 25 2020, 15:09:12) 
[Clang 11.0.3 (clang-1103.0.32.59)]

1.19.0rc1 3.7.7 (default, May  6 2020, 04:59:01) 
[Clang 4.0.1 (tags/RELEASE_401/final)]
```
",2020-05-26 05:36:48,,"array_equal(a, b, equal_nan=True) throws errors for array with non-numeric values","['component: numpy._core', '62 - Python API']"
16366,open,person142,"The docstring of `np.maximum_sctype` says

> Return the scalar type of highest precision of the same kind as the input.

and says that the argument should be a

> dtype or dtype specifier

Usually this works as expected:

```python
>>> np.maximum_sctype('S8')
<class 'numpy.bytes_'>
```

But if you pass it a random string it returns that string:

```python
>>> np.maximum_sctype('foo')
'foo'
```

whereas I would have expected it to raise an exception.

### Reproducing code example:
```python
>>> import numpy as np
>>> np.maximum_sctype('foo')
'foo'
>>> np.maximum_sctype(1)
1
```

### Error message:
None, but I would have expected the above examples to raise an exception.

### Numpy/Python version information:
```
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.18.2 3.7.6 (default, Dec 30 2019, 19:38:36)
[Clang 10.0.0 (clang-1000.11.45.5)]
```

",2020-05-24 21:23:02,,`np.maximum_sctype` propagates arbitrary strings,"['00 - Bug', 'component: numpy._core']"
16360,open,mattip,"In the exported fields of `numpy.dtype`, the `__init__.pxd` file currently has
```
ctypedef class numpy.dtype [object PyArray_Descr, check_size ignore]:
        # Use PyDataType_* macros when possible, however there are no macros
        # for accessing some of the fields, so some are defined.
        cdef PyTypeObject* typeobj
        cdef char kind
        cdef char type
        ...
```

`cdef char type` is misleading: the python `dtype.type` is actually the scalar type object that in the `PyArray_Descr` struct is called `typeobj`, and the `PyArray_Descr->type` is called `dtype.char` from python:

python | C
-------|------
type | typeobj
char | type

Now that gh-16266 added tests for `__init__.pxd`, we should add a test and fix this.",2020-05-23 19:56:11,,__init__.pxd mixes up python dtype.type and c-level PyArray_Descr->type,"['00 - Bug', 'component: numpy.dtype']"
16357,open,pv,"If a callback function of the form `f(..., *args)` with a variable-argument part in the signature is given to f2py as a callback with extra arguments, it gets called with wrong arguments.

The cause appears to be that f2py does (primitive) inspection of the function arguments in order to determine the right size for the preallocated argument list, and this apparently fails to deal with `*args` correctly.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```sh
#!/bin/sh
set -e -x -o pipefail
mkdir tmp
pushd tmp

cat <<EOF > foo.pyf
python module foo__user__routines 
    interface foo_user_interface
       subroutine callback(n)
         integer intent(in) :: n
       end subroutine callback
    end interface
end python module foo__user__routines

python module foo
    interface
       subroutine foo(callback)
         use foo__user__routines
         external callback
       end subroutine foo
    end interface
end python module foo
EOF

cat <<EOF > foo.f90
subroutine foo(callback)
  external callback
  call callback(42)
end subroutine foo
EOF

cat <<EOF > run.py
def good(x, arg):
    assert x == 42, (x, arg)
    assert arg == 123, (x, arg)

def bad(x, *arg):
    assert x == 42, (x, arg)
    assert arg == (123,), (x, arg)

import foo

foo.foo(good, callback_extra_args=(123,))
foo.foo(bad, callback_extra_args=(123,))
EOF

python3 -mnumpy.f2py -c -m foo foo.pyf foo.f90
python3 run.py || { set +x; echo ""FAIL""; exit 1; }
set +x; echo ""OK""
```

### Error message:
```
capi_return is NULL
Call-back cb_callback_in_foo__user__routines failed.
Traceback (most recent call last):
  File ""run.py"", line 12, in <module>
    foo.foo(bad, callback_extra_args=(123,))
  File ""run.py"", line 6, in bad
    assert x == 42, (x, arg)
AssertionError: (123, ())
```
Observe that the calls to `good` and `bad` is the same, but the latter is missing the first argument altogether.

### Numpy/Python version information:
1.18.4 3.8.2 (default, Feb 28 2020, 00:00:00)",2020-05-23 14:18:09,,ENH: Callback with arbitrary number arguments and keywords,"['01 - Enhancement', 'component: numpy.f2py']"
16352,open,jbrockmendel,"```
import numpy as np
from pandas._libs.tslibs.conversion import ensure_datetime64ns
arr = np.array([""2367-12-31 12:00:00""], dtype=""datetime64[h]"")

>>> arr.astype(""datetime64[ns]"")
array(['1783-06-11T12:25:26.290448384'], dtype='datetime64[ns]')

>>> ensure_datetime64ns(arr)
[...]
pandas._libs.tslibs.np_datetime.OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 2367-12-31 12:00:00
```

Is there a compelling reason numpy doesnt do this?",2020-05-22 21:35:15,,ENH: overflow-safe astype for datetime64/timedelta64 unit conversion,"['00 - Bug', 'component: numpy._core', 'component: numpy.dtype', 'component: numpy.datetime64']"
16330,open,rossbar,"The description of `__array__` in the [special methods and attributes](https://numpy.org/devdocs/reference/arrays.classes.html#special-attributes-and-methods) documentation contains a blurb about how `__array__` behaves in a special case re: the `out=` kwarg of a ufunc. However, a general description of `__array__` is not provided. 

The documentation here could be improved by providing a more general description of the `__array__` special method along the lines of the high-level descriptions given for all of the other special methods in the linked page. 

See discussions in #16130 and #13458 for additional background.",2020-05-21 18:37:23,,DOC: Provide general description of `__array__` in special methods docs,"['04 - Documentation', 'component: documentation']"
16319,open,anirudh2290,"From the related PR: #4246

> When it receives an out argument, np.take converts the out array to

> the dtype of the array being taken from. This results in wrong TypeErrors
> being raised when out has a larger dtype than the array (and not being
> raised when it has a smaller dtype, even though they should):

> >> a = np.arange(3, dtype=np.int16) + 128
> >> b = np.empty((3,), dtype=np.int8)
> >> a.take([0, 1, 2], out=b) # wrong, should raise an error
array([-128, -127, -126], dtype=int8)
> >> b = np.empty((3,), dtype=np.int32)
> >> a.take([0, 1, 2], out=b) # perfectly OK, but raises an error
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: Cannot cast array data from dtype('int32') to dtype('int16') according to the rule 'safe'
This PR makes an explicit casting check, then either sets NPY_ARRAY_FORCECAST
or explicitly raises the error.

The tests added check the logic for conversion between all pairs of ints and
floats only.


Please see the related PR for the existing work on the issue.",2020-05-20 20:54:38,,BUG: `take` casting logic with an `out=` argument,"['00 - Bug', 'component: numpy._core']"
16307,open,SrivastavaAnubhav,"<!-- Please describe the issue in detail here, and fill in the fields below -->
The docs for numpy.choose say that 
```np.choose(a,c) == np.array([c[a[I]][I] for I in ndi.ndindex(a.shape)])```
The following example shows that this is not the case. 

### Reproducing code example:
```
>>> A
array([[0, 1],
       [2, 3]])
>>> B
array([[4, 5],
       [6, 7]])
>>> C = np.array([A,B])
>>> C
array([[[0, 1],
        [2, 3]],

       [[4, 5],
        [6, 7]]])
>>> ind = np.array([0,1])
>>> np.choose(ind, C)
array([[0, 5],
       [2, 7]])
>>> np.array([C[ind[I]][I] for I in np.ndindex(ind.shape)])
array([[0, 1],
       [6, 7]])
```

I expected that `np.choose` would choose the rows of the output matrix, but I am new to numpy and might be misinterpreting the docs. Either way, the behavior of the two are **not** equivalent as the docs claim.
",2020-05-20 03:00:59,,numpy.choose broken for multidimensional arrays,"['04 - Documentation', '57 - Close?']"
16299,open,eric-wieser,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

```python
>>> from numpy.core.overrides import verify_matching_signatures
>>> def a(*, foo=1): pass
>>> def b(*, bar=2): pass
>>> verify_matching_signatures(a, b)
# does not fail
```

A correct implementation could look like:
```python
def verify_matching_signatures(implementation, dispatcher):
    import inspect
    implementation_sig = inspect.signature(implementation)
    dispatcher_sig = inspect.signature(dispatcher)

    # replace defaults with None and remove annotations
    implementation_with_none_sig = implementation_sig.replace(
        parameters=[
            p.replace(
                annotation=inspect.Parameter.empty,
                default=inspect.Parameter.empty if p.default is inspect.Parameter.empty else None
            )
            for p in implementation_sig.parameters.values()
        ],
        return_annotation=inspect.Signature.empty
    )

    if dispatcher_sig != implementation_with_none_sig:
        raise RuntimeError(
            'dispatcher for {} has the wrong function signature:\n'
            '  expected: {}\n'
            '  got:      {}'
            .format(
                implementation, implementation_with_none_sig, dispatcher_sig
            )
        )
```
which gives:
```
>>> verify_matching_signatures(a, b)
RuntimeError: dispatcher for <function a at 0x0000020080E14B80> has the wrong function signature:
  expected: (*, foo=None)
  got:      (*, bar=2)
```
Unfortunately, this implementation seems to slow down the import time by 5%.
",2020-05-19 09:43:01,,BUG: numpy.core.overrides.verify_matching_signatures ignores keyword-only arguments,['00 - Bug']
16281,open,kieranricardo,"<!-- Please describe the issue in detail here, and fill in the fields below -->

When specifying an output array with the `out` argument numpy `cumsum` creates an intermediate array to hold the result and then copies this data over to the `out` array.  This causes a spike in peak memory that could be avoided by directly storing and calculating the result with in `out` array.

### Reproducing code example:

```python
import tracemalloc
import numpy as np

data = np.arange(1000000)
out = np.zeros(len(data))

tracemalloc.start()
np.cumsum(data, out=out)
print(tracemalloc.get_traced_memory()) # output: (1606, 8_001_777)
tracemalloc.stop()
```

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

1.18.1 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 13:42:17) 
[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]
",2020-05-18 02:02:20,,ufunc.accumulate could avoid a full intermediate result when `out` requires casting,"['01 - Enhancement', 'component: numpy.ufunc']"
16264,open,anuppari,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:
```python
import numpy as np
class Example(np.ndarray):
    pass

a = np.array([np.array(i).view(Example) for i in range(3)], subok=True)
print(a)
print(type(a))
b = np.stack((np.array(i).view(Example) for i in range(3)))
print(b)
print(type(b))
```

These produce arrays of type `np.ndarray` rather than `Example`. It would be nice if there's a way to concatenate subclasses of `ndarray` and transparently maintain the class of the elements.

Functions like `numpy.stack`, `numpy.hstack`, `numpy.vstack`, `numpy.concatenate` could include a `subok` option to enable this. Unless there's already a way to do this?
",2020-05-16 19:44:16,,Add subok option to array concatenation functions,['33 - Question']
16195,open,jbrockmendel,"timedelta64 mod and divmod do not support numeric other.  floordiv behaves as expected:

```
td = pd.Timedelta(3456789)
td64 = td.to_timedelta64()

>>> divmod(td64, 2)
numpy.core._exceptions.UFuncTypeError: ufunc 'divmod' cannot use operands with types dtype('<m8[ns]') and dtype('int64')

>>> td64 % 2
numpy.core._exceptions.UFuncTypeError: ufunc 'remainder' cannot use operands with types dtype('<m8[ns]') and dtype('int64')

>>> td64 // 2
numpy.timedelta64(1728394,'ns')

>>> divmod(td, 2)
(Timedelta('0 days 00:00:00.001728394'), Timedelta('0 days 00:00:00.000000001'))
```

In pandas we implement mod and divmod in terms of floordiv (https://github.com/pandas-dev/pandas/blob/master/pandas/_libs/tslibs/timedeltas.pyx#L1440).  I'd be happy to port this (and the accompanying tests) upstream if numpy is interested.",2020-05-08 23:20:33,,ENH: timedelta64 mod and floordiv,"['00 - Bug', 'component: numpy.ufunc', 'component: numpy.datetime64']"
16179,open,anntzer,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

For
```python
import numpy as np
np.zeros(23).reshape(-1, 2, 3)
```
one gets
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-4-ff504f05abb6> in <module>
----> 1 np.zeros(23).reshape(-1, 2, 3)

ValueError: cannot reshape array of size 23 into shape (2,3)
```
but the reshape is not to `(2, 3)`; rather it is to `(-1, 2, 3)`.

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.18.4 3.8.2 (default, Apr  8 2020, 14:31:25) 
[GCC 9.3.0]
",2020-05-07 15:36:52,,slightly confusing error message for when calling reshape() with an invalid shape that includes -1,"['00 - Bug', 'component: numpy._core']"
16159,open,davrot,"<!-- Please describe the issue in detail here, and fill in the fields below -->
In [3]: np.arange(-3, 0, 0.5, dtype=int)                                       
Out[3]: array([-3, -2, -1,  0,  1,  2])

Well, to see a ""1"" and a ""2"" was a bit unexpected for us since both numbers are a bit bigger than 0. 

Normally, this is the result without dtype=int:
```
In [2]: np.arange(-3, 0, 0.5)                                                  
Out[2]: array([-3. , -2.5, -2. , -1.5, -1. , -0.5])
and we should get this with dtype=int:
In [4]: np.arange(-3, 0, 0.5).astype(int)                                      
Out[4]: array([-3, -2, -2, -1, -1,  0])
```
The numpy manual states: 
dtype : dtype
The type of the output array. If dtype is not given, infer the data type from the other input arguments.

Thus it should only effect the output array, right? 

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
print(np.arange(-3, 0, 0.5))
print(np.arange(-3, 0, 0.5, dtype=int))
print(np.arange(-3, 0, 0.5).astype(int))
```

<!-- Remove these sections for a feature request -->

### Error message:
No error message...

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:
We tested it under numpy '1.18.4' (pure Python 3.7.6) as well as '1.18.1' (Anaconda 3.7 with the latest update applied). Same result.

1.18.4 3.7.6 (default, Feb 28 2020, 15:25:38) 
[Clang 11.0.0 (https://github.com/llvm/llvm-project.git eefbff0082c5228e01611f7

1.18.1 3.7.4 (default, Aug 13 2019, 20:35:49) 
[GCC 7.3.0]

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2020-05-05 14:10:20,,Unexpected output from arange with dtype=int,"['00 - Bug', 'component: numpy._core', 'triaged']"
16157,open,lerela,"Using Numpy on iOS (for instance using Kivy-iOS) is impossible because of `linalg/umath_linalg.c.src` and `linalg/lapack_litemodule.c`. These files reference the methods `ccopy`, `dcopy`, `scopy`, `zcopy` and `xerbla` which are properly linked, most likely against Apple's Accelerate framework, but forbidden in the App Store (ITMS-90338: Non-public API usage). I'm not sure Numpy really is at fault here but renaming those symbols to the [public ones from Accelerate](https://developer.apple.com/documentation/accelerate/1513152-cblas_ccopy?language=objc) fixes the issue. 

I'm not creating a pull request here because the patch is against v1.16, not portable and meant for Kivy-iOS but it demonstrates a quick and dirty way to fix this issue should you run into it: https://github.com/lerela/numpy/pull/1

However there might be a cleaner, upstreamable solution.

[This Kivy-iOS issue](https://github.com/kivy/kivy-ios/issues/409) provides more context.",2020-05-05 08:02:23,,BUG: NumPy references reserved symbols on iOS,"['component: numpy.linalg', '32 - Installation']"
16152,open,nschloe,"MWE:
```python
import numpy as np

roots = np.array([1, 2, 3])
print(roots.dtype)
print(np.poly(roots).dtype)
```
```
int64
float64
```",2020-05-04 15:21:43,,numpy.poly doesn't preserve dtype,['component: numpy.lib']
16138,open,gdonval,"<!-- Please describe the issue in detail here, and fill in the fields below -->

Thanks to #2951, Numpy's scalar types are correctly determined by `isinstance()` using `Integral`/`Rational` and other types from `numbers`. Therefore something like this **works**:

```python
import numpy as np
from numbers import Integral
assert isinstance(np.int32(0), Integral)
foo = np.array([1, 2, 3])
assert isinstance(foo[1], Integral)
bar = np.array(1)
assert isinstance(bar[()], Integral)
```

However, many parts of CPython (all sequence types at least) and Numba are treating zero-dimensional arrays as numbers, yet those are not registered as numbers. 

### Reproducing code example:

That second assert **does NOT work**:

```python
import numpy as np
from numbers import Integral
fuzz = (1, 2, 3)
bar = np.array(1)
assert fuzz[bar] == 2  # works!
assert isinstance(bar, Integral)  # does NOT work!
```

While I understand why this is the case, it is inconsistent and while it might have to be ""solved"" in CPython/Numba and all the projects considering 0D arrays as numbers,  I want to have your opinion first. 

The current situation makes the implementation of sequence-like types much harder than it should by constantly having to special-case or workaround the problem in dreadful ways (e.g. apply a getitem on another sequence and check the returned type). 

### Error message:

```python
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-38-f69f6ad5cf58> in <module>
      4 bar = np.array(1)
      5 assert fuzz[bar] == 2
----> 6 assert isinstance(bar, Integral)

AssertionError: 
```

### Numpy/Python version information:

```
1.18.1 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:03:20) 
[GCC 7.3.0]
```",2020-05-02 14:06:07,,Register zero-dimensional arrays as Intergral/Rational/Real/Complex depending on dtype,['component: numpy._core']
16124,open,allefeld,"Coming to Python/NumPy from Matlab, I'm missing something like Matlab's three-argument colon-operator:

```
>> 0:0.1:1
ans =
  Columns 1 through 3
                         0                       0.1                       0.2
  Columns 4 through 6
                       0.3                       0.4                       0.5
  Columns 7 through 9
                       0.6                       0.7                       0.8
  Columns 10 through 11
                       0.9                         1
```

In Matlab, I used this operator routinely. It is especially useful when creating a range of values for plotting a function, e.g. `x = -1:0.1:1; plot(x, x.^2)`.

NumPy's alternatives for this are `linspace` and `arange`. However, the former makes it hard to hit specific values (in the plotting example, I have to figure out that there are 21 values), and the latter excludes the stop value. I get that `arange` follows the behavior of `range`, but for the mentioned application which has nothing to do with indexing it is just impractical.

I also understand that a naive implementation of the colon operator runs into floating point problems. However, in many years of using Matlab I never ran into a situation where it didn't do the expected thing. I now managed to find an archived [blog post](http://web.archive.org/web/20120213165003/http://www.mathworks.com/support/solutions/en/data/1-4FLI96/index.html?) that explains part of their implementation, with an attached [complete implementation](http://web.archive.org/web/20120213165003/http://www.mathworks.com/support/solutions/attachment.html?resid=1-4FLIOY&solution=1-4FLI96). I tested it, and as far as I can see it does exactly what the colon operator does.

Would you be open to including an implementation of this algorithm in NumPy? It could be called `colon` to make it easy for people switching, or `irange` where the `i` stands for ""inclusive"". For example with a signature `irange(start, stop, step=1)`.",2020-04-30 22:14:43,,ENH: equivalent to MATLAB's colon operator,['01 - Enhancement']
16059,open,rossbar,"It is possible to modify the attributes of the user classes (e.g. `np.polynomial.Polynomial`, `np.polynomial.Chebyshev`, etc.) in the `polynomial` package dynamically, resulting in all manner of unexpected behavior. The example below provides a short demonstration:

### Reproducing code example:

```python
>>> p = np.polynomial.Polynomial([1, 2, 3], window=[-1, 1], domain=[-1, 1])
>>> p
Polynomial([1., 2., 3.], domain=[-1.,  1.], window=[-1.,  1.])
>>> p.window = [-1, 1] # Supposed to be an array, so breaks repr
>>> p
Polynomial([1., 2., 3.], domain=[-1.,  1.], window=)
>>> p1 = np.polynomial.Polynomial([1, 2, 3])
>>> p2 = np.polynomial.Polynomial([2, 3, 4])
>>> p1.coef = ""foo"" # A more dramatic example: this is allowed, but shouldn't be
>>> p1 + p2
TypeError: unsupported operand type(s) for +: 'Polynomial' and 'Polynomial'
```

This is a small sampling of the potential issues, the root cause of which is the fact that the main attributes that define the polynomial (`coef`, `window`, and `domain`) are all mutable. When a new polynomail instance (of any kind) is created, there are validation checks in the constructor to ensure that the inputs conform to some constraints, but since these attributes are mutable, it is possible to modify polynomial instances after they are created and completely break how they operate.

It seems like the user classes were designed such that these main attributes of a polynomial are supposed to be read-only (or at least must always abide by certain conditions), but this is not enforced. 

### Numpy/Python version information:

Python version: 3.8.2
NumPy version: 1.19.0.dev0+bcb036a

",2020-04-23 23:33:09,,BUG: Mutability of user classes in polynomial package causes unexpected behavior,"['00 - Bug', 'component: numpy.polynomial']"
16052,open,Gattocrucco,"<!-- Please describe the issue in detail here, and fill in the fields below -->

```python
import numpy as np
x = object()
np.array([np.array(1)]) # array([1])
np.array(np.array(x))   # array(<object object at 0x11c4415d0>, dtype=object)
np.array([x])           # array([<object object at 0x11c4415d0>], dtype=object)
np.array([np.array(x)]) # array([array(<object object at 0x11c4415d0>, dtype=object)], dtype=object)
```

The last result is inconsistent. It should be `array([<object object at 0x11c4415d0>], dtype=object)`.

Related to [https://github.com/HIPS/autograd/issues/552]().

<!-- Remove these sections for a feature request -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```
1.18.2 3.8.2 (v3.8.2:7b3ab5921f, Feb 24 2020, 17:52:18) 
[Clang 6.0 (clang-600.0.57)]
```
",2020-04-23 14:29:33,,Different behaviour in np.array between object/scalar 0d arrays in iterable,"['00 - Bug', 'component: numpy._core', 'component: numpy.dtype']"
16004,open,eric-wieser,"Capturing some discussion from #15981

> I could also imagine `np.sort(c_arr, key=(c_arr.real, c_arr.imag))` as sugar for `np.take_along_axis(c_arr, np.lexsort((c_arr.real, c_arr.imag), axis), axis)`

_Originally posted by @shoyer in https://github.com/numpy/numpy/issues/15981#issuecomment-614110087_

> Or maybe `np.sortby(c_arr, key=(c_arr.real, c_arr.imag))`

_Originally posted by @shoyer in https://github.com/numpy/numpy/issues/15981#issuecomment-614110485_

> Which also would presumably bring about the existence of `np.max_by`, `np.min_by`, `np.search_sorted_by`, etc.

_Originally posted by @eric-wieser in https://github.com/numpy/numpy/issues/15981#issuecomment-614112068_

---

This will obvious need some discussion over design before anyone attempts an implementation",2020-04-17 08:47:50,,Add support for a key argument to sort / min / max,['01 - Enhancement']
15981,open,shoyer,"Python (version 3, at least) raises an error, but NumPy uses lexicographic order (see https://github.com/numpy/numpy/issues/11505 for discussion).

We should clean this up to match Python. This behavior is fine for sorting, but I don't think it makes sense for comparisons between numbers. I think most comparisons between complex numbers are probably latent bugs (as was the case where I discovered this).

My suggestion would be to start by issuing a DeprecationWarning, and then maybe make it a true error at some point in the distance future.

### Reproducing code example:

```python
>>> x = 1 + 1j
>>> y = 2 + 1j
>>> x < y
TypeError: '<' not supported between instances of 'complex' and 'complex'
>>> np.array(x) < np.array(y)
True
```

<!-- Remove these sections for a feature request -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.18.2 3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
",2020-04-15 00:54:27,,NumPy should warn (eventually raise an error?) on comparisons between complex numbers,"['01 - Enhancement', 'component: numpy._core', '07 - Deprecation', 'component: numpy.ufunc']"
15978,open,cosama,"If masked arrays are compared (through the `>`, `>=`, `<` and `<=` operators to constants, arrays or other masked arrays they access the masked out elements for comparison. In some extreme cases this fails, often causing a warning.

### Reproducing code example:

This example

```python
import numpy as np
d = np.ma.array([1, None], mask=[False, True])
d > 0
```
fails with a `TypeError: '>' not supported between instances of 'NoneType' and 'int'`.

This example

```python
import numpy as np
d = np.ma.array([1, np.nan], mask=[False, True])
d > 0
```

throws a warning `__main__:1: RuntimeWarning: invalid value encountered in greater`.

I think masked out elements should be set to `False` by default and not compared, so that operations such as:

```
d[d > 0] = 0
```

have the expected behavior. However that might cause issues in other cases such as:

```
np.all(d > 0)
```

This issue has been brought up before here: https://github.com/numpy/numpy/issues/4959, but the initial issue there seems more about using default functions such as `np.log` with masked array insted of the `np.ma` relative.

Even adding a `fill_value` seems not to fix this. At least in that case I would assume that for elements that are masked out the comparison happens with respect to the fill value.

```python
import numpy as np
d = np.ma.array([1, None], mask=[False, True], fill_value=-1)
d > 0
```

creates the same error as mentioned above.

### Numpy/Python version information:

* Numpy: '1.18.2'
* System: '3.7.6 (default, Jan 30 2020, 09:44:41) [GCC 9.2.1 20190827 (Red Hat 9.2.1-1)]'
",2020-04-14 18:12:15,,Masked fields should not be used in comparison,['component: numpy.ma']
15965,open,jeras,"Repeat only accepts int or int array for repeats, np.uint is not accepted. But the function still checks if repeats are negative. I am working with nonegative values this becomes an unnecessary conversion step.

### Reproducing code example:

```python
import numpy as np
x = np.array([1,2,3,4])
r = np.array([1,0,0,2], dtype=np.uint)
x.repeat(r)
```

### Error message:

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-2-9ac14301bd8a> in <module>
      2 x = np.array([1,2,3,4])
      3 r = np.array([1,0,0,2], dtype=np.uint)
----> 4 x.repeat(r)

TypeError: Cannot cast array data from dtype('uint64') to dtype('int64') according to the rule 'safe'
```

### Numpy/Python version information:

```
1.17.4 3.8.2 (default, Mar 13 2020, 10:14:16) 
[GCC 9.3.0]
```",2020-04-13 08:54:27,,np.repeat not accepting np.uint for repeats,['unlabeled']
15959,open,mruberry,"For example:

```
np.isclose(complex(float('inf'), 0), complex(float('inf'), 0))
: False

np.isclose(complex(1, float('inf')), complex(1, float('nan')), equal_nan=True)
: True
```

In the first case it seems like the same number should be close to itself, and in the latter case I think the numbers should not be close since NaN is not close to anything. 

Python's cmath.isclose behavior appears to demand that complex numbers containing -inf or inf must be identical to be close. It does not support equal_nan so complex numbers containing NaN are never close to other complex numbers. If equal_nan is True I would expect NaNs to act like infinities and require the numbers be identical to be ""close.""  ",2020-04-12 14:50:50,,isclose behavior on non-finite complex values seems incorrect,"['00 - Bug', 'component: numpy._core']"
15957,open,pv,"There's a race condition in Numpy distutils when using the Python distutils parallelization (e.g. `python setup.py build -j 3`). It can make Fortran compilation to use wrong `extra_f*_compiler_flags`.

The race is here: (numpy/distutils/commands/build_ext.py:build_ext.build_extension)
https://github.com/numpy/numpy/blob/b489287af94b32d6a6d550c6e05f1cf92adf3248/numpy/distutils/command/build_ext.py#L356

The `fcompiler` comes from `self._f77/f90_compiler` where `self` is the global `build_ext` object. So the compiler is a global object. Python distutils launches several calls to `build_extension` in parallel

https://github.com/python/cpython/blob/ee249d798ba08f065efbf4f450880a446c6ca49d/Lib/distutils/command/build_ext.py#L464

so they race each other here, and so an extension build can end up with flags from wrong extension.

### Reproducing code example:

<details>

```sh
#!/bin/bash
set -e

mkdir -p tmp

# Fortran file that requires -DMACRO flag given
cat <<EOF > tmp/foo0.f90
#ifndef MACRO
#error fail
#endif
EOF

# Fortran file that requires -DMACRO flag NOT given
cat <<EOF > tmp/foo1.f90
#ifdef MACRO
#error fail
#endif
EOF

# Add filler to increase compile time, to better trigger races
for k in `seq 1 100`; do
    cat <<EOF >> tmp/foo0.f90
subroutine foo_$k()
end subroutine foo_$k
EOF
    cat <<EOF >> tmp/foo1.f90
subroutine foo_$k()
end subroutine foo_$k
EOF
done

# 10 .pyf wrappers with no routines
for j in `seq 1 10`; do
    cat <<EOF > tmp/foo$j.pyf
python module foo$j
  interface
  end interface
end python module
EOF
done

# Simple setup.py
cat <<EOF > tmp/setup.py
import setuptools
import time
from numpy.distutils.core import setup, Extension

extensions = [
    Extension(name=""foo{}"".format(j),
              sources=[""foo{}.pyf"".format(j), ""foo{}.f90"".format(j % 2)],
              extra_f90_compile_args=[""-cpp"", ""-DMACRO=''""] if j % 2 == 0 else [""-cpp""])
    for j in range(1, 11)
]

setup(ext_modules=extensions)
EOF

# Check build status
echo ""Serial build:""
rm -rf tmp/build
(cd tmp && python setup.py build) > tmp/build1.log 2>&1 || { cat tmp/build1.log; echo ""FAIL""; exit 1; }
echo ""OK""

echo ""Parallel build:""
rm -rf tmp/build
(cd tmp && python setup.py build -j 10) > tmp/build2.log 2>&1 || { cat tmp/build2.log; echo ""FAIL""; exit 1; }
echo ""OK""

```

</details>

### Numpy/Python version information:

1.18.2 3.8.2 (default, Feb 28 2020, 00:00:00)",2020-04-12 12:59:14,,Race condition in Numpy distutils w/ Python distutils `-j`,"['00 - Bug', 'component: numpy.distutils']"
15934,open,ragonneau,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

The [first example of the official documentation](https://numpy.org/devdocs/f2py/python-usage.html#call-back-arguments) does not work on Windows for any version of Python 64 bits with MinGW-W64 (``bin`` of MinGW-W64 added to ``PATH``, compiler precised by ``--compiler=mingw32``). Thank you!!

<!-- Remove these sections for a feature request -->

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->
```
running build
running config_cc
unifing config_cc, config, build_clib, build_ext, build commands --compiler options
running config_fc
unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options
running build_src
build_src
building extension ""callback"" sources
f2py options: []
f2py:> C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\callbackmodule.c
creating C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7
Reading fortran codes...
Reading file 'callback.f' (format:fix,strict)
Post-processing...
Block: callback
Block: foo
Block: fun
appenddecl: ""intent"" not implemented.
Post-processing (stage 2)...
Building modules...
Constructing call-back function ""cb_fun_in_foo__user__routines""
def fun(i): return r
Building module ""callback""...
Constructing wrapper function ""foo""...
r = foo(fun,[fun_extra_args])
Wrote C/API module ""callback"" to file ""C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\callbackmodule.c""
adding 'C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.c' to sources.
adding 'C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7' to include_dirs.
copying C:\Python37\lib\site-packages\numpy\f2py\src\fortranobject.c -> C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7
copying C:\Python37\lib\site-packages\numpy\f2py\src\fortranobject.h -> C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7
build_src: building npy-pkg config files
running build_ext
Cannot build msvcr library: ""vcruntime140d.dll"" not found
customize Mingw32CCompiler
customize Mingw32CCompiler using build_ext
get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'
customize GnuFCompiler
Could not locate executable g77
Could not locate executable f77
customize IntelVisualFCompiler
Could not locate executable ifort
Could not locate executable ifl
customize AbsoftFCompiler
Could not locate executable f90
customize CompaqVisualFCompiler
Could not locate executable DF
customize IntelItaniumVisualFCompiler
Could not locate executable efl
customize Gnu95FCompiler
Found executable C:\MinGW\bin\gfortran.exe
customize Gnu95FCompiler
customize Gnu95FCompiler using build_ext
building 'callback' extension
compiling C sources
C compiler: gcc -g -DDEBUG -DMS_WIN64 -O0 -Wall -Wstrict-prototypes
creating C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users
creating C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago
creating C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata
creating C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local
creating C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp
creating C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b
creating C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7
compile options: '-DNPY_MINGW_USE_CUSTOM_MSVCR -D__MSVCRT_VERSION__=0x1900 -IC:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7 -IC:\Python37\lib\site-packages\numpy\core\include -IC:\Python37\include -IC:\Python37\include -c'
gcc -g -DDEBUG -DMS_WIN64 -O0 -Wall -Wstrict-prototypes -DNPY_MINGW_USE_CUSTOM_MSVCR -D__MSVCRT_VERSION__=0x1900 -IC:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7 -IC:\Python37\lib\site-packages\numpy\core\include -IC:\Python37\include -IC:\Python37\include -c C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.c -o C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.o
gcc -g -DDEBUG -DMS_WIN64 -O0 -Wall -Wstrict-prototypes -DNPY_MINGW_USE_CUSTOM_MSVCR -D__MSVCRT_VERSION__=0x1900 -IC:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7 -IC:\Python37\lib\site-packages\numpy\core\include -IC:\Python37\include -IC:\Python37\include -c C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\callbackmodule.c -o C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\callbackmodule.o
In file included from C:\Python37\include/Python.h:87:0,
from C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.h:7,
from C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.c:2:
C:\Python37\include/pytime.h:123:59: warning: 'struct timeval' declared inside parameter list will not be visible outside ofIn file  thiins defincluded from C:\Python37\include/Python.h:87:0,
ition or declaration
PyAP
from C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\callbackmodule.c:14:
C:\Python37\include/pytime.h:123:59: warning: 'struct timI_FUNC(int) _PyTime_FromTimeval(_PyTime_t *tp, struct timeval *tv);
eval' declared inside parameter list will not be visi
^~~~~~~
ble outside of this definition or declaration
PyAPI_FUNC(int) _PyTime_FromTimeval(_PyTCime_t *tp, struct timeval *tv);
:\Python37\include/pyti me.h:130:12: warning: '
struct timeval' declared inside parameter list will not be visible outsi     d
e of
this definition or declaration
struct time
^~~~~~~
val *tv,
C:\Python37\include/pytime.h:130:12: warning: 'struct timeval' declared inside parameter list will not be visible outsid   is
definition or declaration
struct timeval *tv,
^
~~~~~~
^C:\Python37\include/pytime.h:135:12: warni~~~~~~
ng: 'struct timevC:\Python37\include/pytime.h:135:12: warning: 'struct timeval' dal' eclared inside parameter list will not be visible outside of this definition or declaration
struct timedeclaredval *tv,
inside parameter list will not be visible outside of this definition or declaration
struct timeval *tv,
^~~~~~~
^~~~~~~
In file included from C:\Python37\lib\site-packages\numpy\core\include/numpy/ndarraytypes.h:1832:0,
from C:\PythonIn fil37\lib\sitee included from C:\Python37\-packages\numpy\clore\include/numpy/ndarrayobject.h:12,
from C:\ib\siPython37\lib\site-packages\numpy\core\includte-packages\numpy\core\include/numpy/arrayobe/numpy/ndarrject.h:4,
aytypes.h from C:\Users\trago\AppData\Local\Tem:1832:0,
p\tmpyv  from C:\Python37\lib\site-packages\numpy\core\include/numpy/ndarrayobject.h:12,
from C:\Python37\lib\site-packages\numpy\core\include/lj2t2b\srcnumpy/arra.wiyobject.h:4,
from C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.h:13,
from C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.c:2:
C:\Python37\lib\site-packages\numpy\core\includn-amd64-3.7\fortranobject.h:13,
from C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\callbackmodule.c:15:
C:\Python37\lib\site-packages\numpy\core\include/numpy/npy_1_7_deprecated_api.h:14:9: note: #pragma message: C:\Python37\lib\site-packages\numpy\core\incle/ude/numpy/npy_1_7_deprecated_api.h(14) : Warning Msg: Using deprecated NumPy API, disable it with #define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION
#pragma message(_WARN___LOC__""Using deprecated NumPy API, disable it with "" \
^~~~~~~
numpy/npy_1_7_deprecated_api.h:14:9: note: #pragma message: C:\Python37\lib\site-packages\numpy\core\include/numpy/npy_1_7_deprecated_api.h(14) : Warning Msg: Using deprecated NumPy API, disable it with #define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION
#pragma message(_WARN___LOC__""Using deprecated NumPy API, disable it with "" \
^~~~~~~
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.c: In function 'format_def':
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.c:133:32: warning: unknown conversion type character 'l' in format [-Wformat=]
n = PyOS_snprintf(p, size, ""array(%"" NPY_INTP_FMT, def.dims.d[0]);
^~~~~~~~~
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.c:133:32: warning: too many arguments for format [-Wformat-extra-args]
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.c:141:36: warning: unknown conversion type character 'l' in format [-Wformat=]
n = PyOS_snprintf(p, size, "",%"" NPY_INTP_FMT, def.dims.d[i]);
^~~~
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.c:141:36: warning: too many arguments for format [-Wformat-extra-args]
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.c: In function 'fortran_doc':
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.c:255:60: warning: unknown conversion type character 'z' in format [-Wformat=]
fprintf(stderr, ""fortranobject.c: fortran_doc: len(p)=%zd>%zd=size:""
^
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.c:255:64: warning: unknown conversion type character 'z' in format [-Wformat=]
fprintf(stderr, ""fortranobject.c: fortran_doc: len(p)=%zd>%zd=size:""
^
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.c:255:21: warning: too many arguments for format [-Wformat-extra-args]
fprintf(stderr, ""fortranobject.c: fortran_doc: len(p)=%zd>%zd=size:""
^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.c: In function 'array_from_pyobj':
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.c:707:43: warning: unknown conversion type character 'l' in format [-Wformat=]
sprintf(mess+strlen(mess),""%"" NPY_INTP_FMT "","",dims[i]);
^~~
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.c:707:43: warning: too many arguments for format [-Wformat-extra-args]
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.c:755:25: warning: unknown conversion type character 'l' in format [-Wformat=]
"" -- expected at least elsize=%d but got %"" NPY_INTP_FMT,
^
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.c:755:25: warning: too many arguments for format [-Wformat-extra-args]
"" -- expected at least elsize=%d but got %"" NPY_INTP_FMT,
^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.c:797:25: warning: unknown conversion type character 'l' in format [-Wformat=]
"" -- expected elsize=%d but got %"" NPY_INTP_FMT,
^
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.c:797:25: warning: too many arguments for format [-Wformat-extra-args]
"" -- expected elsize=%d but got %"" NPY_INTP_FMT,
^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compiling Fortran sources
Fortran f77 compiler: C:\MinGW\bin\gfortran.exe -Wall -g -ffixed-form -fno-second-underscore -O3 -funroll-loops
Fortran f90 compiler: C:\MinGW\bin\gfortran.exe -Wall -g -fno-second-underscore -O3 -funroll-loops
Fortran fix compiler: C:\MinGW\bin\gfortran.exe -Wall -g -ffixed-form -fno-second-underscore -Wall -g -fno-second-underscore -O3 -funroll-loops
compile options: '-IC:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\src.win-amd64-3.7 -IC:\Python37\lib\site-packages\numpy\core\include -IC:\Python37\include -IC:\Python37\include -c'
gfortran.exe:f77: callback.f
C:\MinGW\bin\gfortran.exe -Wall -g -Wall -g -shared C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\callbackmodule.o C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.o C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\callback.o -Lc:\mingw\lib\gcc\mingw32\6.3.0 -LC:\Python37\libs -LC:\Python37\PCbuild\amd64 -lpython37 -lgfortran -o .\callback.cp37-win_amd64.pyd
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\callbackmodule.o: In function `import_array':
C:/Python37/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1466: undefined reference to `_imp__PyImport_ImportModule'
C:/Python37/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1472: undefined reference to `_imp__PyObject_GetAttrString'
C:/Python37/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1475: undefined reference to `_imp__PyExc_AttributeError'
C:/Python37/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1475: undefined reference to `_imp__PyErr_SetString'
C:/Python37/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1480: undefined reference to `_imp__PyCapsule_Type'
C:/Python37/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1481: undefined reference to `_imp__PyExc_RuntimeError'
C:/Python37/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1481: undefined reference to `_imp__PyErr_SetString'
C:/Python37/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1485: undefined reference to `_imp__PyCapsule_GetPointer'
C:/Python37/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1496: undefined reference to `_imp__PyExc_RuntimeError'
C:/Python37/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1496: undefined reference to `_imp__PyErr_SetString'
C:/Python37/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1502: undefined reference to `_imp__PyExc_RuntimeError'
C:/Python37/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1502: undefined reference to `_imp__PyErr_Format'
C:/Python37/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1508: undefined reference to `_imp__PyExc_RuntimeError'
C:/Python37/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1508: undefined reference to `_imp__PyErr_Format'
C:/Python37/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1520: undefined reference to `_imp__PyExc_RuntimeError'
C:/Python37/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1520: undefined reference to `_imp__PyErr_Format'
C:/Python37/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1531: undefined reference to `_imp__PyExc_RuntimeError'
C:/Python37/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1531: undefined reference to `_imp__PyErr_Format'
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\callbackmodule.o: In function `double_from_pyobj':
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:109: undefined reference to `_imp__PyFloat_Type'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:109: undefined reference to `_imp__PyFloat_Type'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:109: undefined reference to `_imp__PyType_IsSubtype'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:117: undefined reference to `_imp__PyNumber_Float'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:127: undefined reference to `_imp__PyComplex_Type'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:127: undefined reference to `_imp__PyComplex_Type'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:127: undefined reference to `_imp__PyType_IsSubtype'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:128: undefined reference to `_imp__PyObject_GetAttrString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:131: undefined reference to `_imp__PySequence_Check'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:132: undefined reference to `_imp__PySequence_GetItem'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:134: undefined reference to `_imp__PyErr_Clear'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:139: undefined reference to `_imp__PyErr_Occurred'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:141: undefined reference to `_imp__PyErr_SetString'
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\callbackmodule.o: In function `create_cb_arglist':
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:153: undefined reference to `_imp__PyFunction_Type'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:159: undefined reference to `_imp__PyObject_HasAttrString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:160: undefined reference to `_imp__PyObject_GetAttrString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:162: undefined reference to `_imp__PyObject_HasAttrString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:163: undefined reference to `_imp__PyObject_GetAttrString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:164: undefined reference to `_imp__PyObject_HasAttrString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:165: undefined reference to `_imp__PyObject_GetAttrString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:171: undefined reference to `_imp__PyTuple_Size'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:178: undefined reference to `_imp__PyTuple_Size'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:185: undefined reference to `_imp__PyTuple_Size'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:199: undefined reference to `_imp__PyObject_HasAttrString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:200: undefined reference to `_imp__PyObject_GetAttrString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:200: undefined reference to `_imp__PyObject_HasAttrString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:205: undefined reference to `_imp__PyObject_GetAttrString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:210: undefined reference to `_imp__PyLong_AsLong'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:216: undefined reference to `_imp__PyObject_HasAttrString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:217: undefined reference to `_imp__PyObject_GetAttrString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:222: undefined reference to `_imp__PyTuple_Size'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:227: undefined reference to `_imp__PyTuple_Size'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:239: undefined reference to `_imp__PyTuple_New'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:241: undefined reference to `_imp___Py_NoneStruct'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:241: undefined reference to `_imp___Py_NoneStruct'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:242: undefined reference to `_imp___Py_NoneStruct'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:246: undefined reference to `_imp__PyTuple_GetItem'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:254: undefined reference to `_imp__PyErr_Occurred'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:255: undefined reference to `_imp__PyErr_SetString'
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\callbackmodule.o: In function `cb_fun_in_foo__user__routines':
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:299: undefined reference to `_imp__PyObject_GetAttrString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:302: undefined reference to `_imp__PyErr_SetString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:313: undefined reference to `_imp__PyObject_GetAttrString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:315: undefined reference to `_imp__PySequence_Tuple'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:317: undefined reference to `_imp__PyErr_SetString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:321: undefined reference to `_imp__PyErr_Clear'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:322: undefined reference to `_imp__Py_BuildValue'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:326: undefined reference to `_imp__PyErr_SetString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:339: undefined reference to `_imp__PyLong_FromLong'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:339: undefined reference to `_imp__PyTuple_SetItem'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:356: undefined reference to `_imp__PyObject_CallObject'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:366: undefined reference to `_imp___Py_NoneStruct'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:368: undefined reference to `_imp__Py_BuildValue'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:371: undefined reference to `_imp__Py_BuildValue'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:373: undefined reference to `_imp__PyTuple_Size'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:377: undefined reference to `_imp__PyTuple_GetItem'
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\callbackmodule.o: In function `f2py_rout_callback_foo':     C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:423: undefined reference to `_imp___Py_NoneStruct'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:435: undefined reference to `_imp__PyTuple_Type'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:435: undefined reference to `_imp__PyArg_ParseTupleAndKeywords'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:465: undefined reference to `_imp__PyErr_Occurred'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:475: undefined reference to `_imp__Py_BuildValue'
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\callbackmodule.o: In function `PyInit_callback':
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:552: undefined reference to `_imp__PyModule_Create2'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:556: undefined reference to `_imp__PyType_Type'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:557: undefined reference to `_imp__PyErr_Print'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:557: undefined reference to `_imp__PyExc_ImportError'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:557: undefined reference to `_imp__PyErr_SetString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:558: undefined reference to `_imp__PyErr_Occurred'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:559: undefined reference to `_imp__PyExc_ImportError'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:559: undefined reference to `_imp__PyErr_SetString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:560: undefined reference to `_imp__PyModule_GetDict'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:561: undefined reference to `_imp__PyBytes_FromString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:562: undefined reference to `_imp__PyDict_SetItemString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:565: undefined reference to `_imp__PyUnicode_FromString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:572: undefined reference to `_imp__PyDict_SetItemString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:574: undefined reference to `_imp__PyErr_NewException'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:579: undefined reference to `_imp__PyDict_SetItemString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/callbackmodule.c:583: undefined reference to `_imp__PyDict_SetItemString'
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.o: In function `F2PyDict_SetItemString':      C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:24: undefined reference to `_imp__PyErr_Occurred'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:25: undefined reference to `_imp__PyErr_Print'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:26: undefined reference to `_imp__PyErr_Clear'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:30: undefined reference to `_imp__PyDict_SetItemString'
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.o: In function `PyFortranObject_New':
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:45: undefined reference to `_imp___PyObject_New'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:49: undefined reference to `_imp__PyDict_New'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:67: undefined reference to `_imp__PyDict_SetItemString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:85: undefined reference to `_imp__PyDict_SetItemString'
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.o: In function `PyFortranObject_NewAsAttr':   C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:98: undefined reference to `_imp___PyObject_New'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:100: undefined reference to `_imp__PyDict_New'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:101: undefined reference to `_imp__PyObject_Free'
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.o: In function `fortran_dealloc':
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:114: undefined reference to `_imp__PyObject_Free'
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.o: In function `format_def':
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:133: undefined reference to `_imp__PyOS_snprintf'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:141: undefined reference to `_imp__PyOS_snprintf'
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.o: In function `fortran_doc':
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:178: undefined reference to `_imp__PyMem_Malloc'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:180: undefined reference to `_imp__PyErr_NoMemory'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:194: undefined reference to `_imp__PyOS_snprintf'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:204: undefined reference to `_imp__PyOS_snprintf'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:246: undefined reference to `_imp__PyUnicode_FromStringAndSize'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:251: undefined reference to `_imp__PyMem_Free'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:258: undefined reference to `_imp__PyMem_Free'
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.o: In function `fortran_getattr':
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:275: undefined reference to `_imp__PyDict_GetItemString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:301: undefined reference to `_imp___Py_NoneStruct'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:301: undefined reference to `_imp___Py_NoneStruct'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:301: undefined reference to `_imp___Py_NoneStruct'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:310: undefined reference to `_imp__PyUnicode_FromString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:313: undefined reference to `_imp__PyUnicode_Concat'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:323: undefined reference to `_imp__PyDict_SetItemString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:329: undefined reference to `_imp__PyDict_SetItemString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:336: undefined reference to `_imp__PyUnicode_FromString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:337: undefined reference to `_imp__PyObject_GenericGetAttr'
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.o: In function `fortran_setattr':
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:353: undefined reference to `_imp__PyExc_AttributeError'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:353: undefined reference to `_imp__PyErr_SetString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:360: undefined reference to `_imp___Py_NoneStruct'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:395: undefined reference to `_imp__PyDict_New'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:400: undefined reference to `_imp__PyDict_DelItemString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:402: undefined reference to `_imp__PyExc_AttributeError'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:402: undefined reference to `_imp__PyErr_SetString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:406: undefined reference to `_imp__PyDict_SetItemString'
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.o: In function `fortran_call':
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:417: undefined reference to `_imp__PyExc_RuntimeError'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:417: undefined reference to `_imp__PyErr_Format'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:427: undefined reference to `_imp__PyExc_TypeError'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:427: undefined reference to `_imp__PyErr_Format'
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.o: In function `fortran_repr':
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:435: undefined reference to `_imp__PyObject_GetAttrString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:436: undefined reference to `_imp__PyErr_Clear'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:439: undefined reference to `_imp__PyUnicode_FromFormat'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:442: undefined reference to `_imp__PyUnicode_FromString'
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.o: In function `array_from_pyobj':
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:698: undefined reference to `_imp___Py_NoneStruct'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:699: undefined reference to `_imp___Py_NoneStruct'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:709: undefined reference to `_imp__PyExc_ValueError'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:709: undefined reference to `_imp__PyErr_SetString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:736: undefined reference to `_imp__PyType_IsSubtype'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:759: undefined reference to `_imp__PyExc_ValueError'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:759: undefined reference to `_imp__PyErr_SetString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:806: undefined reference to `_imp__PyExc_ValueError'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:806: undefined reference to `_imp__PyErr_SetString'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:842: undefined reference to `_imp__PyExc_TypeError'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:842: undefined reference to `_imp__PyErr_SetString'
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.o: In function `check_and_fix_dimensions':    C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:906: undefined reference to `_imp__PyExc_ValueError'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:906: undefined reference to `_imp__PyErr_Format'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:920: undefined reference to `_imp__PyExc_ValueError'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:920: undefined reference to `_imp__PyErr_Format'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:934: undefined reference to `_imp__PyExc_ValueError'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:934: undefined reference to `_imp__PyErr_Format'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:949: undefined reference to `_imp__PyExc_ValueError'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:949: undefined reference to `_imp__PyErr_Format'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:960: undefined reference to `_imp__PyExc_ValueError'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:960: undefined reference to `_imp__PyErr_Format'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:975: undefined reference to `_imp__PyExc_ValueError'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:975: undefined reference to `_imp__PyErr_Format'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:988: undefined reference to `_imp__PyExc_ValueError'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:988: undefined reference to `_imp__PyErr_Format'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:1029: undefined reference to `_imp__PyExc_ValueError'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:1029: undefined reference to `_imp__PyErr_SetString'
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.o: In function `F2PyCapsule_FromVoidPtr':     C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:1060: undefined reference to `_imp__PyCapsule_New'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:1062: undefined reference to `_imp__PyErr_Clear'
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.o: In function `F2PyCapsule_AsVoidPtr':
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:1070: undefined reference to `_imp__PyCapsule_GetPointer'
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:1072: undefined reference to `_imp__PyErr_Clear'
C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.o: In function `F2PyCapsule_Check':
C:/Users/trago/AppData/Local/Temp/tmpyvlj2t2b/src.win-amd64-3.7/fortranobject.c:1080: undefined reference to `_imp__PyCapsule_Type'
collect2.exe: error: ld returned 1 exit status
error: Command ""C:\MinGW\bin\gfortran.exe -Wall -g -Wall -g -shared C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\callbackmodule.o C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\users\trago\appdata\local\temp\tmpyvlj2t2b\src.win-amd64-3.7\fortranobject.o C:\Users\trago\AppData\Local\Temp\tmpyvlj2t2b\Release\callback.o -Lc:\mingw\lib\gcc\mingw32\6.3.0 -LC:\Python37\libs -LC:\Python37\PCbuild\amd64 -lpython37 -lgfortran -o .\callback.cp37-win_amd64.pyd"" failed with exit status 1```
### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.18.2 3.7.7 (tags/v3.7.7:d7c567b08f, Mar 10 2020, 10:41:24) [MSC v.1900 64 bit (AMD64)]

",2020-04-08 07:45:30,,F2PY fails to wrap subroutine with call-back function on Windows for Python 64 bits with MinGW-W64,['component: numpy.f2py']
15922,open,elmanm,"To be able to vectorise random sampling with/without replacement is would be useful to be able to set the `replace` flag in `random.choice` to a tuple matching the shape of `size`.  i.e. 
```
# THIS IS NOT A WORKING CODE
from numpy import np

x = [i for i in range(100)]
k = np.random.poission(0.5, 10)
samples = np.random.choice(x, size=(10, 10), replace=(False, True), p=k)
```
This would allow for a much neater way to produce a 2D array where each row is a sample from the same population taken without replacement.
",2020-04-07 09:32:00,,Feature request: Set replace flag with tuple for nd array,"['01 - Enhancement', '15 - Discussion', 'component: numpy.random', '62 - Python API']"
15911,open,charris,"Possible candidate for GSOC. The testing will be long running, so should be separate and run occasionally. Organization open to discussion.
<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
<< your code here >>
```

<!-- Remove these sections for a feature request -->

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2020-04-04 23:29:47,,Validate the statistics of the random distributions,"['17 - Task', '05 - Testing', 'component: numpy.random']"
15897,open,mlondschien,"It appears that `np.nan` gets converted to the string `""nan""` when initializing an array.

### Reproducing code example:

```python
import numpy as np
np.array([""some string"", np.nan])
```
prints
```
array(['some string', 'nan'], dtype='<U11')
```

Similar behavior can be observed for other generators, such as `hstack`, `concat`, `repeat` and `tile`.

### Expected behavior

I would expect that the above returns
```
array(['some string', nan], dtype='object')
```
with the second element of the array being the object `np.nan`.

### Numpy/Python version information:

1.18.1 3.8.2

",2020-04-02 12:24:42,,"DEP: Disallow `np.promote_types(string, other) -> string`","['15 - Discussion', '07 - Deprecation', 'component: numpy.dtype']"
15896,open,hameerabbasi,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

```python
>>> import numpy as np
>>> f = 5.1 * 10**73
>>> round(f, -73)
5e+73
>>> round(np.float64(f), -73)
5.0000000000000004e+73
```

### Numpy/Python version information:

```python
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.18.1 3.8.2 | packaged by conda-forge | (default, Mar 23 2020, 17:55:48)
[Clang 9.0.1 ]
```

",2020-04-02 09:17:32,,`round` produces a slightly incorrect result for large inputs,['00 - Bug']
15863,open,erfannariman,"I used this functionality quite often and I think would fit in `numpy`. It's to shift values over an invalid value to a certain direction over certain axis.

**Note**: this code and functionality was provided by Divakar on [SO](https://stackoverflow.com/a/44559180/9081267)

### Reproducing code example:

```python
a = np.array([[1, 0, 2, 0],
              [3, 0, 4, 0],
              [5, 0, 6, 0],
              [6, 7, 0, 8]])

def justify(a, invalid_val=0, axis=1, side='left'):    
    """"""
    Justifies a 2D array

    Parameters
    ----------
    A : ndarray
        Input array to be justified
    axis : int
        Axis along which justification is to be made
    side : str
        Direction of justification. It could be 'left', 'right', 'up', 'down'
        It should be 'left' or 'right' for axis=1 and 'up' or 'down' for axis=0.

    """"""

    if invalid_val is np.nan:
        mask = ~np.isnan(a)
    else:
        mask = a!=invalid_val
    justified_mask = np.sort(mask,axis=axis)
    if (side=='up') | (side=='left'):
        justified_mask = np.flip(justified_mask,axis=axis)
    out = np.full(a.shape, invalid_val) 
    if axis==1:
        out[justified_mask] = a[mask]
    else:
        out.T[justified_mask.T] = a.T[mask.T]
    return out
```
**Output**
```
In [473]: a # input array
Out[473]: 
array([[1, 0, 2, 0],
       [3, 0, 4, 0],
       [5, 0, 6, 0],
       [6, 7, 0, 8]])

In [474]: justify(a, axis=0, side='up')
Out[474]: 
array([[1, 7, 2, 8],
       [3, 0, 4, 0],
       [5, 0, 6, 0],
       [6, 0, 0, 0]])

In [475]: justify(a, axis=0, side='down')
Out[475]: 
array([[1, 0, 0, 0],
       [3, 0, 2, 0],
       [5, 0, 4, 0],
       [6, 7, 6, 8]])

In [476]: justify(a, axis=1, side='left')
Out[476]: 
array([[1, 2, 0, 0],
       [3, 4, 0, 0],
       [5, 6, 0, 0],
       [6, 7, 8, 0]])

In [477]: justify(a, axis=1, side='right')
Out[477]: 
array([[0, 0, 1, 2],
       [0, 0, 3, 4],
       [0, 0, 5, 6],
       [0, 6, 7, 8]])
```

",2020-03-29 14:33:28,,ENH: Add justify functionality,"['01 - Enhancement', 'component: numpy.ma']"
15859,open,adeak,"I could find at least five places where single-character type codes appear, and some of these don't correspond to the same total collection of types. In other words some types are missing in some places, and I can't tell how much of these differences is on purpose.

  1. Where I originally started from: type coercion tables from `from numpy.testing import print_coercion_tables`
  2. dtypes reference https://docs.scipy.org/doc/numpy/reference/arrays.dtypes.html
  3. scalar types reference https://docs.scipy.org/doc/numpy/reference/arrays.scalars.html#built-in-scalar-types
  4. `np.typename` https://docs.scipy.org/doc/numpy/reference/generated/numpy.typename.html
  5. `np.typecodes` (a dict)

I'm aware that 1. is not documentation, and not exactly public. I know from https://github.com/numpy/numpy/issues/7869#issuecomment-234872466 that 2. and 3. on my list don't have to agree, though I don't understand this in detail. 4. seems documented and references 5., so I guess those are public (although I only bumped into `np.typename` by accident).

The dtype codes referenced by each of the above (as of 1.17.4 on linux), respectively:
  1. `set('? b h i l q p B H I L Q P e f d g F D G S U V O M m'.split())` (copied from the output of the import)
  2. `set('Hd')` (examples for ""one-character strings""), `set('?bBiufcmMOSaUV')` (comprehensive-looking ""array-protocol type strings"") 
  3. `set('?bhilqpBHILQPefdgFDGO')` along with the flexible types `'S#'`, `'U#'`, `'V#'`
  4. `set(np.typename.__globals__['_namefromtype'])`, same as `set('S1 ? b B h H i I l L q Q f d g F D G S U V O'.split())`
  5. `np.typecodes['All']` i.e. `'?bhilqpBHILQPefdgFDGSUVOMm'`

Observations:
  * Among these 1. and 5. are the most comprehensive and they contain the same 26 codes. 
  * Set 3. is missing `{'m', 'M'}`, i.e. `np.typecodes['Datetime']`
  * Set 4. is further missing `{'P', 'e', 'p'}`: I believe these are intp/uintp and half-precision. It also contains `S1` but ""character"" is not really a separate type, so it's OK for this to be missing from the other codes. I understand if intp/uintp don't have an item in `np.typename`, because these might be platform-specific and they are (always?) aliases to other concrete integer types.
  * Set 2. is altogether missing `'DlIPQeqhpFLGg'` and unlike the others it contains `{'u', 'c', 'a'}`. From https://github.com/numpy/numpy/issues/7869 I suspect this is just a different set of characters as the others, so this is probably just an apples-and-oranges mistake on my part.

I know I'm confused about 2., so that's probably not an issue (although reading the docs the distinction between the two kinds of type codes is unclear to me). However, I'd expect `np.typename` to correspond to `np.typecodes`, and as such include at least `'m'` and `'M'`, and maybe `'e'`. And I'd expect [the array scalar docs](https://docs.scipy.org/doc/numpy/reference/arrays.scalars.html#built-in-scalar-types) ([devdocs version](https://numpy.org/devdocs/reference/arrays.scalars.html#built-in-scalar-types)) to contain the datetimes too.",2020-03-28 16:08:58,,DOC: Missing single-character type codes,"['04 - Documentation', 'sprintable']"
15856,open,jaewooklee93,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np

A = np.array([[1, 1], [0, 0]], np.bool)
B = np.array([[1, 0], [1, 0]], np.bool)

print(A, '\n')
print(B, '\n')

print(A + B, '\n') # np.logical_or
print(A * B, '\n') # np.logical_and
print(A ^ B, '\n') # np.logical_xor
print(A - B, '\n') # ???
```

<!-- Remove these sections for a feature request -->

### Error message:

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-24-023301cbb346> in <module>
     10 print(A * B, '\n') # np.logical_and
     11 print(A ^ B, '\n') # np.logical_xor
---> 12 print(A - B, '\n') # ???

TypeError: numpy boolean subtract, the `-` operator, is deprecated, use the bitwise_xor, the `^` operator, or the logical_xor function instead.

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:
1.15.4 3.6.10 |Anaconda, Inc.| (default, Jan  7 2020, 21:14:29) 
[GCC 7.3.0]
<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

## Feature request

It's pretty suprising fact that `numpy` doesn't have any boolean subtracting functionality. Error message says it is deprecated, though, I couldn't find any reason or reference for decision about such deprecation.

I think it's quite natural to have `np.logical_subtract` as follows, since equivalent set subtraction 
$A - B = A \cap B^c$ 
operation is used widespread.

```Python
True - True == False
True - False == True
False - True == False # Sadly, False - True == -1 == True in python
False - False == False

def logical_subtract(A, B):
    return A.astype(np.int) - B.astype(np.int) == 1

print(logical_subtract(A, B))
```",2020-03-28 12:26:51,,numpy has no boolean subtract functionality,"['15 - Discussion', 'component: numpy._core']"
15849,open,cedric-coussinet,"In the function **numpy.core.records.fromarrays**, the implicit conversion of floating values to unsigned integers fails when the last value in the column is nan.

### Reproducing code example:

```python
import numpy as np

x = np.array([['a', np.nan], ['b', 1.0]], dtype=object)
arr = np.core.records.fromarrays(x.transpose(), formats='<U2, uint32', names='col1 ,col2')

# No Error: 
# rec.array([('a', 0), ('b', 1)],  dtype=[('col1', '<U2'), ('col2', '<u4')])

x = np.array([['a', 1.0], ['b', np.nan]], dtype=object) 
arr = np.core.records.fromarrays(x.transpose(), formats='<U2, uint32', names='col1, col2')
```
### Error message:

AppData\Local\Continuum\anaconda3\lib\site-packages\numpy\core\records.py"", line 635, in fromarrays
    _array[_names[i]] = arrayList[i]

ValueError: cannot convert float NaN to integer

### Numpy/Python version information:

1.16.5 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]",2020-03-27 18:17:54,,Conversion of nan in numpy.core.records.fromarrays,"['00 - Bug', 'component: numpy._core']"
15846,open,pdebuyl,"There are plenty of `+SKIP` doctests directives in the documentation. Ideally, we should not need them, see  #14970 for background information.

Procedure: edit the rst files to make the tests more robust or correct. Then, run

```
python tools/refguide_check.py --rst
```

to check the result.

This issue should be considered after #15848
",2020-03-27 11:07:15,,DOC: remove SKIP in the rst doctests,"['05 - Testing', '04 - Documentation']"
15845,open,pdebuyl,"The facilities for doctests from the standard library lack support for scientific computing usecases.

The @astropy project developed a plugin for pytest that helps with floating point comparison, the optional use of remote data, and enables rules for skipping some tests.

See:
- The plugin https://github.com/astropy/pytest-doctestplus
- The discussion in NumPy's DOC issues: https://github.com/numpy/numpy/pull/15703#discussion_r388093856 https://github.com/numpy/numpy/pull/15720 and https://github.com/numpy/numpy/issues/14970#issuecomment-604583509

The proposal is to adapt or replace `tools/refguide_check.py` for the doctests.",2020-03-27 11:06:09,,DOC: migrate to pytest and astropy/pytest-doctestplus,"['05 - Testing', '04 - Documentation']"
15814,open,eric-wieser,"Now that C99 is our minimum requirement, we ought to be able to use standard boolean types for our internal API functions.

What we need to be wary of is that `sizeof(bool)` is not always 1, so we need to be careful of using it in

* public API function signatures containing `npy_bool`, which would be an ABI change
* public API function signatures containing `npy_bool*`, which would be an API change
* public structs, which would be an ABI change
* contexts where we're accessing array data, where `npy_bool` actually represents the underlying representation of `np.bool_`

",2020-03-23 17:00:02,,Investigate replacing `npy_bool` with `bool`,['15 - Discussion']
15807,open,WarrenWeckesser,"When the array of coefficients is a masked array, the polynomial classes such as `np.polynomial.Polynomial` and `np.polynomial.Hermite` silently discard the mask.

For example,
```
In [2]: np.__version__
Out[2]: '1.19.0.dev0+5b126f8'
```
`c` is a masked array, with `c[1]` masked:
```
In [3]: c = np.ma.masked_array([1.0, -1.0, 3.0], mask=[0, 1, 0])

In [4]: c
Out[4]: 
masked_array(data=[1.0, --, 3.0],
             mask=[False,  True, False],
       fill_value=1e+20)
```
Use `c` as the coefficients of a `Polynomial`.   The mask is ignored, resulting in the value -1 appearing in the coefficients:
```
In [5]: p = np.polynomial.Polynomial(c)

In [6]: p
Out[6]: Polynomial([ 1., -1.,  3.], domain=[-1,  1], window=[-1,  1])
```
",2020-03-23 00:53:16,,BUG: Polynomial classes in `np.polynomial` silently discard the mask of masked arrays.,"['00 - Bug', 'component: numpy.ma', 'component: numpy.polynomial']"
15798,open,stochashtic,"Discovered this edge case today when optimising a simulation to use `searchsorted+insert` rather than `replace/append+sort/argsort`. The situation arises when you are trying to insert multiple values into a sorted array that would be collectively inserted at the same location. Take the case of the end of the array. `searchsorted` will return N for all the indices. This is ambiguous when passed to 'insert' as the order matters. In the case of my Python/Numpy implementation, if the values to be inserted are ordered, all is fine; however, if not sorted, then an unsorted array results.

```
>>> import numpy as np
>>>
>>> a = np.array([0,1,2,3,4,5])
>>> insert_values = np.array([6,7])
>>> insert_indices = np.searchsorted(a, insert_values)
>>> insert_indices
array([6, 6])
>>> np.insert(a, insert_indices, insert_values)
array([0, 1, 2, 3, 4, 5, 6, 7])
>>>
>>> a = np.array([0,1,2,3,4,5])
>>> insert_values = np.array([7,6])
>>> insert_indices = np.searchsorted(a, insert_values)
>>> insert_indices
array([6, 6])
>>> np.insert(a, insert_indices, insert_values)
array([0, 1, 2, 3, 4, 5, 7, 6])
```

At minimum I would recommend a warning in the documentation. The immediate solution is to sort the `insert_values` in such a situation.

I would sheepishly propose an explicit general solution immune to the implementation of `insert` is desirable. This scenario wouldn't be uncommon and the main motivation of using `searchsorted` + `insert` is to maintain a sorted array when adding general values. Checking and looping over the individual values to handle it is not scalable/general/pretty.

### Reproducing code example:
```python
import numpy as np

# input array is ordered                                                                                                                                                                              
a = np.array([0,1,2,3,4,5])
insert_values = np.array([6,7])
insert_indices = np.searchsorted(a, insert_values)
print(np.insert(a, insert_indices, insert_values))

# input array is not ordered                                                                                                                                                                          
a = np.array([0,1,2,3,4,5])
insert_values = np.array([7,6])
insert_indices = np.searchsorted(a, insert_values)
print(np.insert(a, insert_indices, insert_values))
```

### Numpy/Python version information:
1.17.2 3.7.4 (default, Aug 13 2019, 20:35:49) 
[GCC 7.3.0]",2020-03-22 13:03:31,,numpy.searchsorted + numpy.insert edge case will result in unsorted list,['04 - Documentation']
15792,open,Irio,"Prior of writing it, it should probably be defined the scope of this documentation page. In my understanding, based on the definition in [NEP 44](https://numpy.org/neps/nep-0044-restructuring-numpy-docs.html), that would be a Reference Guide.

I see two possible ways of putting it:

1. A short reference of commands and similarities/differences, similar to [NumPy for Matlab users](https://numpy.org/devdocs/user/numpy-for-matlab-users.html).
2. A longer reference that not only cites similarities/differences, but helps the user to be productive while following NumPy standards.

The befenits I see in the first option: it would be much shorter, thus possibly quicker to write and publish; also, with the right presentation, it should be easy to read, allowing users to quickly start using NumPy to accomplish their goals.

In favor of the second option, we have the assumption that a longer introduction to NumPy, SciPy, and Python would give them a more productive start. For instance, R users usually have [R Markdown](https://rmarkdown.rstudio.com/) right on their side, while NumPy users may decide to choose [Jupyter](https://jupyter.org/); dataframes are part of R, while NumPy users could do same things in pure NumPy or use [Pandas](https://pandas.pydata.org/) on top of it. With this in mind, the second option would contain an introduction to the SciPy ecossystem rather than be limited to NumPy.

Some ideas of topics:

* Philosophical differences between the two environments
* Data types
    * vectors, lists, and dataframes vs NumPy and Pandas equivalents
    * how to get the shape of an object
* Reading documentation of functions
    * `?`, `??`, and `help`
    * official websites
* Reproducible research
    * R Markdown vs Jupyter
* Distributions (RStudio vs Anaconda perhaps?)
* Indexing
    * subsetting
    * views
    * copy
* Element-wise operations
    * tell how NumPy will give them that, but pure Python might cause weird behavior
* Applying functions on elements/dimensions/array
* Descriptive statistics
* Other statistical functions/packages
* Reading/writing data to disk
* Plotting
* tydiverse vs SciPy
* Debugging/IDEs
* Interfacing between the two
* Deployment

Depending on the scope to be defined, some or few may be taken away from this main reference page. In that case, they might generate tutorials that would be linked for furter reading.",2020-03-21 15:44:03,,Add documentation page on migrating from R,"['04 - Documentation', '57 - Close?']"
15779,open,rossbar,"`poly1d` from the old-style polynomial API returns objects of different types from binary operations between `poly1d` objects and arrays depending on the ordering of the operations:

```python
>>> p = np.poly1d([1, 2, 3])
>>> a = np.array([1, 2, 3])
>>> p + a
poly1d([2, 4, 6])
>>> a + p
array([2, 4, 6])
```

Note that this issue only pertains to the old `poly1d` class. The recommended polynomial API from the `np.polynomial` package does not have this problem.

A fix for this was attempted in #15756, but subsequent discussion in that thread made clear that it was an inadequate solution as it introduced silent, backwards-incompatible behavior changes.

### Numpy/Python version information:

1.19.0.dev0+965b41d 3.8.2 (default, Feb 26 2020, 22:21:03) 
[GCC 9.2.1 20200130]",2020-03-19 04:04:42,,poly1d binary operations return inconsistent type with array input,['component: numpy.lib']
15770,open,ggmartins,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import pandas as pd
import numpy as np
df_log = pd.DataFrame(columns=['ts', 'event'])
ts_start = 1582582300
ts_end = ts_start + 30
df_log['ts']=pd.Series(np.arange(ts_start, ts_end))
df_log.loc[df_log['ts']==1582582300 + 3, 'event']=""Entered room""
df_log.loc[df_log['ts']==1582582300 + 12, 'event']=""Door opened""
df_log.loc[df_log['ts']==1582582300 + 18, 'event']=""Door closed""
df_log.loc[df_log['ts']==1582582300 + 25, 'event']=""Left room""
df_log['ts']=pd.to_datetime(df_log[""ts""], unit='s', utc=True).dt.tz_convert('US/Central')
for i in range(len(df_log.dtypes)):
    dtype = df_log.dtypes[i]
    print(""testing:"", dtype)
    if np.issubdtype(dtype, np.datetime64):
         print(""is subtype"")
```

<!-- Remove these sections for a feature request -->

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->
```
testing: datetime64[ns, US/Central]
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-2-225a4521e525> in <module>
     13     dtype = df_log.dtypes[i]
     14     print(""testing:"", dtype)
---> 15     if np.issubdtype(dtype, np.datetime64):
     16          print(""is subtype"")

/usr/local/lib/python3.7/site-packages/numpy/core/numerictypes.py in issubdtype(arg1, arg2)
    391     """"""
    392     if not issubclass_(arg1, generic):
--> 393         arg1 = dtype(arg1).type
    394     if not issubclass_(arg2, generic):
    395         arg2_orig = arg2

TypeError: data type not understood
```

### Numpy/Python version information:
```
print(numpy.__version__, sys.version)
1.18.1 3.7.6 (default, Dec 30 2019, 19:38:26) 
[Clang 11.0.0 (clang-1100.0.33.16)]
```
<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2020-03-17 19:09:20,,“TypeError: data type not understood” comparing dtype np.datetime64 with issubdtype,"['component: numpy.datetime64', '33 - Question']"
15767,open,alonme,"When creating a `numpy.array` of a class that inherits from both str and Enum the dtype is `dtype='<UX`
where X is the length of the string. the data saved is X chars from the string representation of the enum.

I would expect the minimal value to be object, so the Enum is preserved.
Problem is of course solved when passing `object` as dtype

### Reproducing code example:

```python
>>> import numpy as np
>>> from enum import Enum
>>> class StrEnum(str, Enum):
...    A = 'a'
...    B = 'bb'
...
>>> np.array([StrEnum.A])
array(['S'], dtype='<U1')
>>> np.array([StrEnum.B])
array(['St'], dtype='<U2')
```

### Numpy/Python version information:
1.18.0 3.7.5 (default, Nov  1 2019, 02:16:23)
[Clang 11.0.0 (clang-1100.0.33.8)]

",2020-03-17 16:58:46,,Numpy.array - wrong dtype for 'string enum',"['00 - Bug', 'component: numpy.dtype']"
15763,open,mattip,"EDIT: TL;DR: the initial problem description below results from np.float128 being a double-double and the manylinux2014 image using glibc2.17, which blacklists some glibc trig functions triggering use of our replacements which are 80-bit double specific

Original issue report:
<details>

On some systems, `LDBL_EPSILON`, which is defined as a number that fits `(1.0L + LDBL_EPSILON) != 1.0L && (1.0L + LDBL_EPSILON/2) == 1.0L`, is 0. This causes problems in [complex math routines](https://github.com/numpy/numpy/blob/v1.18.1/numpy/core/src/npymath/npy_math_complex.c.src#L60), especially when calculating the [recipricol](https://github.com/numpy/numpy/blob/v1.18.1/numpy/core/src/npymath/npy_math_complex.c.src#L1408) as `const npy_longdouble RECIP_EPSILON = 1.0L / LDBL_EPSILON`;

I discovered this when running `-mfull` tests on ppc64le. I got an overflow warning when calling `np.arcsinh`, and debugging led to the conclusion that `LDBL_EPSILON` is `0`.

One possible mitigation is to add `RECIP_EPSILON` to the pre-calculated values on the line above it, but that presumes that the internal representation of longdouble, which [wikipedia says](https://en.wikipedia.org/wiki/Long_double) may be problematic. On the other hand, all the values in these calculations seem to presume a X86 80-bit long double.

Thoughts? ",2020-03-16 15:38:03,,"BUG: ppc64le uses double-double for np.float128, routines need adjustment","['00 - Bug', 'component: numpy._core']"
15754,open,isuruf,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

Building numpy on ppc64le using gcc 7.3.0

### Error message:


```
  numpy/core/src/npymath/npy_math_complex.c.src: In function '_do_hard_workl':

  numpy/core/src/npymath/npy_math_complex.c.src:1057:5: warning: floating constant truncated to zero [-Woverflow]

       const npy_longdouble FOUR_SQRT_MIN = 7.3344154702193886625e-2466l;

       ^~~~~

  numpy/core/src/npymath/npy_math_complex.c.src: In function '_clog_for_large_valuesl':

  numpy/core/src/npymath/npy_math_complex.c.src:1198:5: warning: floating constant exceeds range of 'long double' [-Woverflow]

       const npy_longdouble QUARTER_SQRT_MAX = 2.7268703390485398235e+2465l;

       ^~~~~

  numpy/core/src/npymath/npy_math_complex.c.src:1199:5: warning: floating constant truncated to zero [-Woverflow]

       const npy_longdouble SQRT_MIN = 1.8336038675548471656e-2466l;

       ^~~~~

  numpy/core/src/npymath/npy_math_complex.c.src: In function '_sum_squaresl':

  numpy/core/src/npymath/npy_math_complex.c.src:1496:1: warning: floating constant truncated to zero [-Woverflow]

   const npy_longdouble SQRT_MIN = 1.8336038675548471656e-2466l;

   ^~~~~
```

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

NumPy 1.18.1, Python 3.6.10",2020-03-14 17:17:25,,Underflow in npy_math_complex.c.src on ppc64le,"['00 - Bug', 'component: numpy._core']"
15742,open,eric-wieser,"Currently, our implementation of the matmul ufunc is intelligent, and is able to pass appropriate transpose flags to BLAS to handle transposed contiguous arrays.

For `A`, `B`, and `C` as contiguous 2D arrays, the inner loop is intelligent enough to map `np.matmul(B.T, A.T, out=C.T)` to `np.matmul(A, B, out=C)`:

https://github.com/numpy/numpy/blob/59a97520cf0aa68b92a775d809e1cb67d886b50c/numpy/core/src/umath/matmul.c.src#L476-L491

However when the `out` argument is omitted, the ufunc machinery pre-allocates `out` with ""C"" memory ordering, which is not the ""F"" ordering that `C.T` has. Ideally, we'd be able to allocate our array such that we can make `o_c_blasable` or `o_f_blasable` true as necessary.

As part of @seberg's ufunc work, it would be great if ufuncs could be involved in the output allocation machinery.",2020-03-11 15:05:35,,"Allow `matmul(B.T, A.T).T` to be optimized to `np.matmul(A, B)`","['01 - Enhancement', 'component: numpy.ufunc']"
15741,open,Ablinne,"Note: This may just be a documentation issue, if the observed behaviour is indeed the intended outcome. In that case, adding a note explaining this and the workaround (see below) to the documentation might be sufficient.

The function `lib.recfunctions.append_field` does not apply the `fill_value` argument to new fields where the data is shorter than the `base` input array. Instead, the default value for masked arrays is used, even when `usemask=False` is specified.

```
import numpy as np
import numpy.lib.recfunctions as nprf
inarr = np.array([(1,2),(3,4)], dtype=[('A', 'u1'), ('B', 'f8')])
fv = 0
outarr = nprf.append_fields(inarr, names=['C'], data=[[]], dtypes=[np.uint8], fill_value=fv, usemask=False)
assert outarr['C'][0] == fv
```




### Error message:

```
AssertionError                            Traceback (most recent call last)
<ipython-input-145-a276cc41a09c> in <module>
      4 fv = 0
      5 outarr = nprf.append_fields(inarr, names=['C'], data=[[]], dtypes=[np.uint8], fill_value=fv, usemask=False)
----> 6 assert outarr['C'][0] == fv
```

### Numpy/Python version information:

```
1.18.1 3.8.2 | packaged by conda-forge | (default, Feb 28 2020, 17:15:22) 
[GCC 7.3.0]
```

### Side note

The reason for the behaviour is that while `lib.recfunctions.append_field` passes the `fill_value` on to `merge_arrays`, the latter function is applied to `base` and `data` [separately](https://github.com/numpy/numpy/blob/2e9169601aff252a661b845399ec61c3e575407f/numpy/lib/recfunctions.py#L718). This means that `merge_arrays` can not know how many records `base` has and can not fill the values up to the required length. A better solution could be to call `merge_arrays` only once, passing both `base` and `data`.

An alternative snippet which produces the desired result, which can be used as a workaround:
```
import numpy as np
import numpy.lib.recfunctions as nprf
inarr = np.array([(1,2),(3,4)], dtype=[('A', 'u1'), ('B', 'f8')])
fv = 0
outarr = nprf.merge_arrays([inarr, np.array([], dtype=[('C', 'u1')])], flatten=True, fill_value=fv)
assert outarr['C'][0] == fv
```",2020-03-11 13:49:44,,inconsistent use of `fill_value` in `lib.recfunctions.append_field`,"['00 - Bug', 'component: numpy.lib']"
15737,open,asmeurer,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
>>> a = np.ones(1)
>>> a.dot(a).dot(a)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'numpy.float64' object has no attribute 'dot'
>>> np.dot(np.dot(a, a), a)
array([1.])
```

SymPy's lambdify uses `array.dot` to code generate matrix multiplication for `sympy.MatrixSymbol`, which leads to this error if one of the intermediate matrices has shape 1x1. We are going to change it to use `np.dot` so that it works, but I think that scalar types should have the `dot` method so that they can work in array contexts, especially since `dot` can return a scalar. 

See https://github.com/sympy/sympy/issues/18824

<!-- Remove these sections for a feature request -->

### Error message:

```
AttributeError: 'numpy.float64' object has no attribute 'dot'
```

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

```
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.16.5 3.7.3 | packaged by conda-forge | (default, Mar 27 2019, 15:43:19)
[Clang 4.0.1 (tags/RELEASE_401/final)]
```",2020-03-10 22:42:36,,Scalars do not have a dot method,"['00 - Bug', 'Priority: low']"
15726,open,anoldmaninthesea,"The documentation related to numpy.fromfunction states:

> numpy.fromfunction(function, shape, **kwargs)
> Construct an array by executing a function over each coordinate.
> The resulting array therefore has a value fn(x, y, z) at coordinate (x, y, z).

However, when I run the following:
f=lambda m,n: (m,n)
np.fromfunction(f,(6,6),dtype=int)

I don't obtain an array from a list of tuples, but instead this:

> (array([[0, 0, 0, 0, 0, 0],
>         [1, 1, 1, 1, 1, 1],
>         [2, 2, 2, 2, 2, 2],
>         [3, 3, 3, 3, 3, 3],
>         [4, 4, 4, 4, 4, 4],
>         [5, 5, 5, 5, 5, 5]]), array([[0, 1, 2, 3, 4, 5],
>         [0, 1, 2, 3, 4, 5],
>         [0, 1, 2, 3, 4, 5],
>         [0, 1, 2, 3, 4, 5],
>         [0, 1, 2, 3, 4, 5],
>         [0, 1, 2, 3, 4, 5]]))
",2020-03-08 22:45:17,,Documentation for numpy.fromfunction induces an erroneous interpretation!,"['04 - Documentation', 'component: numpy._core']"
15713,open,mganahl,"Numpy's `unique` function is about 10 times slower when acting on arrays with `ndim=2` as
compared to `ndim=1` arrays. This is even true if the 2d array is a trivial expansion of an original 1d array (see below).
### Reproducing code example:

```python
import numpy as np
import time
arr = np.random.randint(0,20,1000)
mat = np.expand_dims(arr,1)
t1 = time.time()
for _ in range(100):
    u = np.unique(arr)
print((time.time() - t1)/100)
t1 = time.time()
for _ in range(100):
    u2 = np.unique(mat, axis=0)
print((time.time() - t1)/100) #this is ~10 times slower
assert np.all(np.squeeze(u2)==u)
```

### Numpy/Python version information:

1.18.1

",2020-03-05 18:36:26,,"np.unique(arr, axis) slow for axis=0","['component: numpy.lib', '33 - Question']"
15712,open,raphaelquast,"<!-- Please describe the issue in detail here, and fill in the fields below -->

There seems to be a problem when converting pandas Timestamps at 'ns' resolution into numpy `datetime64[ns]` arrays... 

The following lines show that the **nanosecond values are lost in the conversion**!
I'm not sure if the problem is with pandas or numpy but it results in some annoying problems like
the fact that you can not search for a converted value in case the nanoseconds are !=0

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
import pandas as pd

x = pd.Timestamp('2017-01-01 09:58:50.624025600', unit='ns')
y = np.array(x, dtype=np.dtype('datetime64[ns]'))
y2 = np.array(x.value, dtype=np.dtype('datetime64[ns]'))

print(x) # the original pandas Timestamp
>>> '2017-01-01 09:58:50.624025600'
print(y)
>>> '2017-01-01T09:58:50.624025000'
print(y2)
>>> '2017-01-01T09:58:50.624025600'

print(y - y2):
>>> 'numpy.timedelta64(-600,'ns')'
```

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.18.1 3.6.7 (default, Dec  6 2019, 07:03:06) [MSC v.1900 64 bit (AMD64)]
",2020-03-05 17:05:17,,conversion of pandas Timestamps to numpy datetime64[ns] looses nanoseconds,['component: numpy.datetime64']
15692,open,flothesof,"I've been going through the NumPy: the absolute basics for beginners tutorial and would like to submit some comments. I'm more of an advanced user but I was curious to see what was in it.
I find it very nice, but I have a few comments and points I think are worthy of discussion. I've listed them by section below:

- section What is an array?
   - problematic (is this np.rank which is deprecated or np.linalg.rank or...) use of the term `rank` --> shouldn't this be ndims instead? 
- More information about arrays
   - the printout of the 2D array says ""Your array has 2 axes. The first axis has a length of 2 and the second axis has a length of 3."" Wouldn't it be helpful for beginners to learn that there is a convention about the printing of the first axis as vertical and the second as horizontal? (it's implicit in the text but may be helpful to some)
- section Adding, removing, and sorting elements: 
   - I think the part about removing elements from an array is too obscure for beginners because it's given without an example and just says ""In order to remove elements from an array, it’s simple to use indexing to select the elements that you want to keep.""
   - overall this section is about ""sorting and concatenating"": it would make sense to rename it like that (and I would suggest to get rid of the parts about removing elements)
- section How to create an array from existing data
   - there is no .view() example (I think it would make sense to have an example to illustrate) although there is a slicing example that returns a view on an array that is then modified. In a sense this makes me wonder if .view() needs to be mentioned at all, since it will often be used implicitly only (would reduce the number of things a beginner has to deal with) (of course the view concept needs to stay!)

If anyone finds these suggestions make sense, I would be happy to reformulate these comments in the form of a PR.

Regards,
Florian

",2020-03-03 19:17:05,,Suggestions regarding the absolute basics for beginners tutorial,"['01 - Enhancement', '04 - Documentation', 'component: documentation']"
15690,open,eric-wieser,"Reproduction:

```python
>>> def test(n):
...     inds = np.zeros((n, 5), dtype=np.intp)
...     return np.ravel_multi_index(inds, (1,)*n)
>>> test(2)
array([0, 0, 0, 0, 0], dtype=int64)
>>> test(1)
array([0, 0, 0, 0, 0], dtype=int64)
>>> test(0)
ValueError: At least one iterator operand must be non-NULL
```

The expected result is `array([0, 0, 0, 0, 0], dtype=int64)` for all cases.

Relates to gh-13934.",2020-03-03 15:18:05,,BUG: ravel_multi_index does not work on 0D arrays,['00 - Bug']
15681,open,myakhlakov,"https://github.com/numpy/numpy/blob/d9b1e32cb8ef90d6b4a47853241db2a28146a57d/numpy/lib/index_tricks.py#L883-L949

Hello there! 
Is it ok that `diag_indices` function returns two links on one index array?

For example, I'm trying to use this function to acces not only to the main diagonal but also to the diagonal with the given offset (like using `offset` parameter in `np.diagonal`). To implement that I do something like this:
```python
X = np.array(...) # some 2D array
n, m = X.shape # without loss of generality assume that n <= m
xi, yi = np.diag_indices(n)
yi += offset
xi, yi = xi[np.logical_and(yi < m,yi >= 0)], yi[np.logical_and(yi < m, yi >= 0)]

print(xi, yi) # print to check if values are different (they aren't)
print(xi is yi) # print to check out if instances are different (they aren't)
```

To fix this issue I suggest to do the following:

```python
...
return list(arange(n) for i in range(ndim))
```",2020-03-02 08:15:03,,diag_indices function issue?,"['01 - Enhancement', 'component: numpy.lib']"
15674,open,hippo91,"This issue concerns the documentation of `numpy`, that's why i'm not using the standard issue template.
When adding functions that are aliases of another ones (as `abs` which is an alias of `absolute`) in 
`ufunc.rst` the link toward the specific documentation is broken.
According to @rossbar : ""The failure to generate autolinks to `numpy` aliases is a more general problem that affects more than just the `ufunc` docs (for example: `np.row_stack`, which is an alias of `np.vstack` has the same issue, though the documentation is generated by a different mechanism).""
I'v tried to fix this, but the way the documentation is built is still obscure to me.",2020-03-01 10:06:29,,Alias-documenting problem,['04 - Documentation']
15641,open,dbankmann,"It is not clear what should be expected behavior of `matmul` function, when just one argument has more than 2 dimensions. 

I had assumed, that in the following example `a@e` would work as a batch matrix vector multiplication, as if `e` was of shape `(200,40,1)`.
### Reproducing code example:
```python
import numpy as np
a = np.random.random((200,50,40))
b = np.random.random((200,40,50))
c = np.random.random((200,50))
d = np.random.random((50,40))
e = np.random.random((200,40))
(a @ b).shape
(a @ c).shape
(a @ d).shape
(a @ e).shape
```


### Error message:

```
Traceback (most recent call last):
  File ""test.py"", line 8, in <module>
    (a @ c).shape
ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 200 is different from 40)
    (a @ d).shape
ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 50 is different from 40)
    (a @ e).shape
ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 200 is different from 40)


```

### Numpy/Python version information:
1.18.1 3.8.1 (default, Jan 22 2020, 06:38:00) 
[GCC 9.2.0]
",2020-02-25 09:02:42,,"matmul ambigious behavior if only one argument is n-dimensional, n>2","['04 - Documentation', 'component: numpy.ufunc']"
15638,open,cjw85,"When constructing compound types giving a tuple of tuples, instead of a list of tuples as is common in docs, leads to unexpected results.

### Reproducing code example:

```
import numpy as np
print(np.__version__)
print()

dts = [
    [('int',int), ('float',float)],
    (('int',int), ('float',float))
]

for dt in dts:
    print(dt)
    print(np.dtype(dt))
    print(np.full(5, 10, dtype=dt))
    print()
```

On my system this is producing:

```
1.18.1 3.6.5 (default, Jun 17 2018, 12:13:06)
[GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.2)]

[('int', <class 'int'>), ('float', <class 'float'>)]
[('int', '<i8'), ('float', '<f8')]
[(10, 10.) (10, 10.) (10, 10.) (10, 10.) (10, 10.)]

(('int', <class 'int'>), ('float', <class 'float'>))
int64
[10 10 10 10 10]```

",2020-02-24 21:40:56,,Unexpected compound datatype construction,"['00 - Bug', 'component: numpy.dtype']"
15635,open,athibaul,"Hello,

My team had quite a headache today when finding out that the inconsistent behavior of our code was due to an unsafe cast in np.full_like. The problematic code can be summarized as:

```python
import numpy as np
t0 = 20.5
#...
# Doesn't this look like it would behave like a ufunc ?
temperature = lambda x: np.full_like(x, t0)
#...
print(temperature([0.1, 0.7, 2.3])) # This gives floats as expected
print(temperature(0)) # This gives an int, thus losing precision
```
Of course, this behavior is consistent with the documentation, and simply adding `dtype=float` fixed our problem. However I would have not expected to be able to _downcast implicitly_.

Is there a good reason for this behavior? Shouldn't there be at least some kind of warning ?",2020-02-24 18:18:40,,Unsafe cast in np.full_like,"['01 - Enhancement', '15 - Discussion', 'component: numpy.lib', 'component: numpy.dtype', '62 - Python API']"
15630,open,rgommers,"This came up in https://github.com/pytorch/pytorch/issues/33568. This seems odd:
```
>>> np.clip([3 + 4.j], -1, 2)                                                                                       
array([2.+0.j])
>>> np.clip([3 + 4.j], -1+1.j, 2+12.j)  # imaginary component goes up                                                                   
array([2.+12.j])
>>> np.clip([1 + 4.j], -1+1.j, 2+12.j)  # imaginary component doesn't go up                                                                             
array([1.+4.j])
```

The only test for complex input is that it doesn't segfault. 

Reasonable behavior could be one of:
- Clip real and imaginary parts separately
- Clip absolute value, not changing the phase
There may be other options. As long as it's documented, I'm not sure it matters much which choice is made.

I don't think this is a very important issue, but it would be nice to at least have the desired behavior documented here.",2020-02-23 02:48:39,,np.clip with complex input is untested and has odd behavior,"['00 - Bug', 'component: numpy._core']"
15625,open,joaoe,"### Reproducing code example:

I was trying to calculate a mean from an array. Then I decided to do some filtering, which changes the input array/list to an iterator.
```
np.mean([0,1,2]) # OK
np.mean(value for value in [0,1,2]if value > 0) # CRASH
```

because internally `mean` calls `array(sequence)`. And this happens:
```
>>> np.array(iter([1,2]))
array(<list_iterator object at 0x000001CFC0F96608>, dtype=object)

>>> np.ndarray(iter([1,2]))  # for reference
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: expected sequence object with len >= 0 or a single integer
```
It seems very weird for generators and iterators to just be placed inside an array, and counter-intuitive compared to many other stdlib functions, like all(), any(), sum(), iter(), list(), etc...

Suggestion: traverse iterators and generators when creating `ndarray` and `array` and make the array with its elements.
There might be someone that wants to preserve the old behavior, so one could write `array([iterator])`

### Numpy/Python version information:
```
1.18.1 3.7.5 (tags/v3.7.5:5c02a39a0b, Oct 15 2019, 00:11:34) [MSC v.1916 64 bit (AMD64)]
```
",2020-02-21 14:08:43,,Construct ndarray/array from iterator/generator,['15 - Discussion']
15605,open,mattip,"We have a start for testing input, output accuracy for ufuncs in the `numpy/core/tests/test_umath_accuracy.py` tests (thanks @r-devulap!). It is patterned loosely  after the CPython [cmath tests](https://github.com/python/cpython/blob/v3.8.1/Lib/test/test_cmath.py), which use a [data file of values](https://github.com/python/cpython/blob/v3.8.1/Lib/test/cmath_testcases.txt). The test is a good start, here are a few ways it could be improved:
- the data files (for instance [here](https://github.com/numpy/numpy/blob/master/numpy/core/tests/data/umath-validation-set-sin) are binary only. Maybe the float value could be in a comment at the end of the line?
- in the CPython cmath tests, every test has a name. The test runs to completion and the failing names are printed
- the test should run as a pytest.mark.parametrize test for each ufunc tested
- currently the tests are AVX only since various glibc implementations of math functions vary. Those variations should either be documented or worked around
- more functions should be tested, and more dtypes as well. Various other tests should be consilidated into this one: like the ones in [test_half]()
- the [README about the data files](https://github.com/numpy/numpy/blob/master/numpy/core/tests/data/umath-validation-set-README) should more clearly document how to generate correct values.
- ufuncs should have a way to report the loop chosen so we don't have to guess which SIMD option is active",2020-02-19 08:00:33,,TST: improve the ufunc accuracy tests in test_umath_accuracy,"['01 - Enhancement', '05 - Testing']"
15601,open,Wrzlprmft,"In the following code, I calculate the correlation coefficients of an array with missing data in three ways:

* Using NumPy’s masked arrays and `ma.corrcoef`.
* Using the `corr` method of Pandas dataframes.
* Going through all pairs and applying `np.corrcoef` to manually masked arrays.

The results of the last two methods agree which suggests that they are correct. The result using masked arrays does not match, suggesting there is something wrong with it. Moreover, the computation of `ma.corrcoef` takes excessively long for larger arrays.

``` Python 3
import numpy as np
from pandas import DataFrame

# Three implementations of the correlation coefficient for missing data

cc_masked = lambda data: np.ma.corrcoef(np.ma.masked_invalid(data))

cc_pandas = lambda data: DataFrame(data).T.corr().values

def cc_manual(data):
	n = len(data)
	correlation_matrix = np.empty((n,n))
	for i in range(n):
		for j in range(n):
			mask = np.isfinite(data[i]) & np.isfinite(data[j])
			corr = np.corrcoef( data[i][mask], data[j][mask] )
			correlation_matrix[i,j] = corr[0,1]
	return correlation_matrix

# Generate test data
n = 42
m = 137
q = 0.07

R = np.random.uniform(-1,1,(n,n))
C = R @ R.T
data = np.random.multivariate_normal(np.zeros(n),C,m).T
nan_mask = np.random.choice( [True,False], size=data.shape, p=[q,1-q] )
data[nan_mask] = np.nan

# Compare correlation coefficients
np.testing.assert_almost_equal( cc_manual(data), cc_pandas(data) )
np.testing.assert_almost_equal( cc_manual(data), cc_masked(data) )
```

### Version information:
NumPy: 1.18.1
Python: 3.7.5 (default, Nov 20 2019, 09:21:52) [GCC 9.2.1 20191008]",2020-02-18 19:38:20,,BUG: ma.corrcoef produces wrong results and takes very long.,"['00 - Bug', 'component: numpy.ma', 'sprintable']"
15587,open,Wrzlprmft,"Consider the following examples:
``` Python 3
import numpy as np

x = np.random.random((5,5)).astype(object)
for function in [np.average,np.corrcoef,np.cov]:
	try:
		function(x)
	except AttributeError as message:
		print(message)
```

This raises error messages along the lines of:

	'float' object has no attribute 'shape'

The tracebacks don’t give much more information for an inexperienced end user:

``` Python
Traceback (most recent call last):
  File ""object_array.py"", line 10, in <module>
    function(x)
  File ""<__array_function__ internals>"", line 6, in cov
  File ""/home/wrzlprmft/.local/lib/python3.7/site-packages/numpy/lib/function_base.py"", line 2431, in cov
    avg, w_sum = average(X, axis=1, weights=w, returned=True)
  File ""<__array_function__ internals>"", line 6, in average
  File ""/home/wrzlprmft/.local/lib/python3.7/site-packages/numpy/lib/function_base.py"", line 428, in average
    if scl.shape != avg.shape:
AttributeError: 'float' object has no attribute 'shape'
```

This raises two points:

* I know that you probably do not want to use object arrays for such purposes, but shouldn’t they work anyway? For example, `np.mean` works with them.
* If object arrays will not be supported, can the error message be more informative?

#### Version information:
NumPy: 1.18.1
Python: 3.7.5
",2020-02-17 15:08:16,,Some functions raise an unhelpful error when applied to object array,['unlabeled']
15581,open,anerokhi,Random distribution functions like `numpy.random.binomial` allocate a new out array if the input parameters are arrays. This may lead to high memory consumption. Please add the `out` parameter to such functions.,2020-02-17 02:16:17,,ENH: Add `out` parameter to all Generator distributions,"['01 - Enhancement', 'component: numpy.random']"
15571,open,allefeld,"If I develop a module that uses random numbers from `numpy.random`, and follow the recommendation to use `numpy.random.default_rng()` to create a generator, e.g. in the module's initialization code, then every time the module is freshly imported I get a new sequence of random numbers.

Now it is often useful, for example for running exactly the same simulation several times, to initialize the generator to a specific state, and this creation function provides that possibility. I can do that for testing purposes during development, but in the final version I would of course use the form in which ""fresh entropy"" is used.

Now, if someone else were to use my module, and they want exactly the same behavior on multiple runs, e.g. for testing their code which builds on mine, they would not have the possibility to enforce that for my module short of editing my code. I think it would be better to offer such a user an API, such that in normal use a ""fresh entropy"" generator is created, but it is also possible to set a specific one. I believe that is what ""passing around a generator"" refers to in [NEP 19](https://numpy.org/neps/nep-0019-rng-policy.html).

Finally my question: Is there a recommended way to provide such an API? One possibility would have to have a consistently named module-global variable (e.g. `__rng__`), which could be read to use the same generator elsewhere, or set to force the module using another one.

I understand the concerns that lead to avoid having a ""default global instance"". But the drawback is what I described above: for a program consisting of different pieces all using `numpy.random`, but from different authors, it is not easily possible to enforce a specific RNG generator seed.",2020-02-15 06:35:01,,DOC: Recommended best practices for passing around a random bit generator,"['04 - Documentation', 'component: numpy.random']"
15569,open,amueller,"This is a follow-up on a discussion at matplotlib here:
https://github.com/matplotlib/matplotlib/issues/16403

I think 'auto' would be a better default than 10 for 'bins' for visualization purposes. The matplotlib team is hesitant to have a default value different from the numpy one. So I wanted to ask what you think about a change of default here in numpy.
",2020-02-14 20:03:31,,Feature request: change hist bins default to 'auto',"['15 - Discussion', '54 - Needs decision', '30 - API', '62 - Python API']"
15567,open,tachukao,"I've been trying to figure out how `np.dot` works for multi-dimensional arrays (dimension > 2). I couldn't quite locate where this is specified in the source code. I assume this is done in C with multiple calls to cblas `gemm`. Any pointers will be most appreciated!



",2020-02-14 10:17:20,,"DOC: how to find your way in the code, for instance the dot function","['04 - Documentation', 'component: numpy._core', 'sprintable']"
15531,open,seberg,"The ordering of DTypes is ill defined, due to the fact that equality is much strong than established by casting (forcing the same byte order):
```
In [1]: f8b = np.dtype("">f8"")  
In [2]: f8l = np.dtype(""<f8"")

In [3]: f8l <= f8b
Out[3]: True

In [4]: f8l <= f8b
Out[4]: True

In [5]: f8l == f8b
Out[5]: False
```
which is not transitive. I am not sure this is a huge issue, but maybe there is just no point in defining `<` and `>` based on casting for dtypes, since `can_cast` is reasonable and it is not a common operation.
The meaning of `==` should be set in stone, just due to the amount of code that will be subtly wrong if it does not enforce native byte order anymore.",2020-02-06 17:00:48,,BUG: Ordering of dtypes is ill defined,"['00 - Bug', '15 - Discussion', 'component: numpy.dtype']"
15530,open,vandenheuvel,"I think it would be practical to allow a generalized usage of the multivariate_* functions.

Currently, it is possible to use the `np.random.normal` function with multiple parameters at once:
```python
means, stds = np.array([0, 1e2]), np.array([1e-1, 1e-3])
assert(stds.shape[0] == means.shape[0])
x = np.random.normal(means, stds, size=(5, means.shape[0]))
print(x, x.shape)
```
giving for example
```
[[-4.10286750e-02  1.00000498e+02]
 [ 1.06635675e-01  1.00000462e+02]
 [ 1.03887646e-01  1.00001344e+02]
 [ 4.45630787e-02  9.99997015e+01]
 [ 1.22726902e-01  1.00001742e+02]] (5, 2)
```

I think this could be useful:
```python
dim = 2
multivariate_means = np.tile(means, (dim, 1))
multivariate_stds = np.tile(np.eye(dim), (dim, 1, 1))
assert(multivariate_means.shape[0] == multivariate_stds.shape[0])
x = np.random.multivariate_normal(means, stds, size=(5, means.shape[0], dim))
print(x, x.shape)
```
giving for example an array of shape ```(5, 2, 3)```.

Summarized: the input arguments of the multivariate_* could perhaps allow inputs to be of one dimension higher.",2020-02-06 10:45:12,,Feature request: generalizing the random.multivariate_* functions,"['01 - Enhancement', 'component: numpy.random']"
15517,open,francois-durand,"The functions isnan, isposinf and isneginf do not accept a fraction as an argument. Note that, however, math.nan accepts a fraction (but the math module has the drawback of not having the equivalent of isposinf and isneginf).

### Reproducing code example:

```python
import numpy as np
from fractions import Fraction
test = np.isnan(Fraction(1, 2))
```

### Error message:

```
TypeError                                 Traceback (most recent call last)
<ipython-input-1-bead9135402d> in <module>
      1 import numpy as np
      2 from fractions import Fraction
----> 3 np.isnan(Fraction(1, 2))

TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
```

### Numpy/Python version information:

1.16.4 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]
",2020-02-05 08:39:35,,"isnan, isposinf and isneginf do not accept fractions.Fraction","['component: numpy.ufunc', '33 - Question']"
15499,open,dsaxton,"```python
import sys
import numpy as np

arr = np.array([1, 2, 3, 4, 5], dtype=object)
arr[::2] = np.nan

print(arr)
# [nan 2 nan 4 nan]

bins = np.array([1, 3, 5])

# Inserts into same position (incorrect)
print(bins.searchsorted(arr))                                                                                                        
# array([0, 1, 0, 1, 0])

# Now inserts into different positions (correct)
print(bins.searchsorted(arr.astype(float)))
# array([3, 1, 3, 2, 3])

np.__version__
# '1.18.1'

sys.version
# '3.7.4 (default, Aug 13 2019, 15:17:50) \n[Clang 4.0.1 (tags/RELEASE_401/final)]'
```

In the example here `searchsorted` assigns both 2 and 4 from `arr` to the same index within `bins`, but 2 should go to 1 and 4 to 2. The output is correct when we cast `bins` to float instead of object. Also, for whatever reason I can only reproduce this issue when the `NaN` values are evenly spaced.",2020-02-03 18:43:06,,BUG: searchsorted with object arrays containing nan,"['00 - Bug', 'component: numpy._core']"
15495,open,VukW,"Sorry, if it is duplicate, but I didn't succeed to formulate search query properly.

When inserting data out of array boundaries, sometimes numpy don't  throw an error

### Reproducing code example:

```python
import numpy as np
a =  np.array([1,2,3])
a[10:12] = [10, 10]   # raises a ValueError
a[10:11] = [10]   # does nothing, no exception, just silently passes
```

I expect both lines behave consistently, f.ex. throw the same ValueError exception

### Numpy/Python version information:
numpy 1.18.1
Python 3.6.4 (v3.6.4:d48ecebad5, Dec 18 2017, 21:07:28) 
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]

",2020-02-03 12:43:35,,Inconsistent behavior if fill data outside of array boundaries,"['04 - Documentation', '33 - Question']"
15488,open,dmbelov,"<!-- Please describe the issue in detail here, and fill in the fields below -->
We used to the following property: if `x` is a dtype then `numpy.dtype(x.descr)` is the same dtype.  This property is broken in NumPy 1.8 when one specifies metadata to dtype.  Is it possible to at least fix that `descr` returned by dtype can **always** be evaluated back to a dtype?

Related issues:  https://github.com/h5py/h5py/issues/1307

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np

# 1. metadata is lost for simple dtype
s_dt = np.dtype('S8', metadata={'msg': 'Hello'})
print(s_dt.descr)
# prints [('', '|S8')]

# 2. metadata is returned for structured dtype, but it cannot be evaluated into dtype
struct_dtype = np.dtype([('a', s_dt)])
print(struct_dtype.descr)
# prints [('a', ('|S8', {'msg': 'Hello'}))]

# next line raises a ValueError exception
# ValueError: invalid shape in fixed-type tuple.
np.dtype(struct_dtype.descr)
```
### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.18.1 3.8.1 | packaged by conda-forge | (default, Jan 29 2020, 14:55:04) 
[GCC 7.3.0]
",2020-02-01 23:58:45,,Inconsistent behavior of dtype.descr with metadata,"['15 - Discussion', 'component: numpy.dtype']"
15479,open,shoyer,"# Reproducing code example:

Here's my dummy class that implement `__array_ufunc__` for ufuncs and arithmetic:
```python
import numpy as np

class Wrapper(np.lib.mixins.NDArrayOperatorsMixin):
  def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):
    return 'yes'
```

It works fine on its own, and scalar arithmetic works fine when wrapped in a dtype=object numpy.array. But calling ufuncs on the object array raises a strange TypeError:
```python
print(Wrapper() + 1)  # 'yes'
print(np.sqrt(Wrapper()))  # 'yes'
print(np.array(Wrapper()) + 1)  # 'yes'
print(np.sqrt(np.array(Wrapper())))  # errors
```

<!-- Remove these sections for a feature request -->

### Error message:

```
AttributeError                            Traceback (most recent call last)
AttributeError: 'Wrapper' object has no attribute 'sqrt'

The above exception was the direct cause of the following exception:

TypeError                                 Traceback (most recent call last)
<ipython-input-59-ace91090b167> in <module>()
      8 print(np.sqrt(Wrapper()))  # 'yes'
      9 print(np.array(Wrapper()) + 1)  # 'yes'
---> 10 print(np.sqrt(np.array(Wrapper())))  # errors

TypeError: loop of ufunc does not support argument 0 of type Wrapper which has no callable sqrt method
```

I'm not sure what exactly is going on, but it seems that we don't call actual ufuncs on the elements of `dtype=object` arrays, and instead only look for methods.

### Numpy/Python version information:

1.17.5 3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
",2020-01-30 23:52:43,,ufuncs on object arrays should call __array_ufunc__ if it exists on the wrapped objects,"['01 - Enhancement', '15 - Discussion', 'component: numpy.ufunc']"
15477,open,eric-wieser,"```python
>>> s = np.str_(""\0\0"")
>>> list(s)
['\0', '\0']
>>> len(s)
2
>>> s
''  # wat
```
```python
>>> b = np.bytes_(2)
>>> list(b)
[0, 0]
>>> len(b)
2
>>> b
b''  # wat
```
Caused by

https://github.com/numpy/numpy/blob/f15026ce782f3b8c291cde5363b86f4f1bc88ad8/numpy/core/src/multiarray/scalartypes.c.src#L389-L425

It looks to me like we should just remove our overloads of these functions, they don't make any sense.",2020-01-30 13:34:14,,BUG: np.str_ and np.bytes_ object have a confusing __repr__ and __str__,"['00 - Bug', 'component: numpy.dtype']"
15475,open,lagru,"I'd like to suggest a new `mode` (maybe named `""ignore""`) which treats `multi_index` as index offsets rather than direct positional indices meaning: out-of-bounds indices would be calculated as normal indices, neither wrapped nor clipped and returned without raising an exception.

Over at https://github.com/scikit-image/scikit-image/pull/4209#discussion_r356135738 we would have liked to use [np.ravel_multi_index](https://numpy.org/devdocs/reference/generated/numpy.ravel_multi_index.html) for a use case which is not (yet) supported. The new mode would have allowed us to adjust the `multi_index` corresponding to a structuring element by its center index and simply pass the result into `ravel_multi_index` to compute the offsets in question.

I am willing to give this a try myself but first wanted to check in with you. I haven't looked to deeply at the code dealing with this yet but my current understanding is that it may be enough to simply add a case that ignores `j < 0 || j >= m` to this [switch statement](https://github.com/numpy/numpy/blob/0555d967147f3604beafc53a0300f44c9135d3af/numpy/core/src/multiarray/compiled_base.c#L966).


",2020-01-30 09:09:27,,"ENH: Suggest ""ignore"" mode for ravel_multi_index","['01 - Enhancement', 'component: numpy._core']"
15473,open,eric-wieser,"I've seen a bunch of uses of `np.int8(myarr)` in the wild, but that suggests a pattern that just doesn't transfer well:
```python
>>> a = np.array(32, dtype=np.int8)
>>> a1 = np.array([32], dtype=np.int8)
>>> a3 = np.array([48, 49, 50], dtype=np.int8)

# performs a cast, like `.astype`
>>> np.int32(a), np.int32(a1), np.int32(a3)
(32, array([48, 49, 50]))
>>> np.complex_(a), np.complex_(a1), np.complex_(a3)
((32+0j), array([32.+0.j]), array([48.+0.j, 49.+0.j, 50.+0.j]))
>>> np.object_(a), np.object_(a1), np.object_(a3)
(32, array([32], dtype=object), array([48, 49, 50], dtype=object))

# ok but inconsistent
>>> np.str_(a), np.str_(a1), np.str_(a3)
('32', '[32]', '[48 49 50]')

# wat[1]
>>> np.float_(a), np.float_(a1), np.float_(a3)
(32.0, 32.0, array([48., 49., 50.]))

# wat[:]
>>> np.bytes_(a), np.bytes_(a1), np.bytes_(a3)
(b'', b' ', b'012')  # empty string is `bytes(32).rstrip('\0')`

# fails
>>> np.timedelta64(a), np.timedelta64(a1), np.timedelta64(a3)
ValueError: Could not convert object to NumPy timedelta
>>> np.datetime64(a), np.datetime64(a1), np.datetime64(a3)
ValueError: Could not convert object to NumPy datetime
```

Perhaps we should deprecate and suggest users use `np.array(..., dtype=...)` for these cases instead.",2020-01-30 00:33:22,,Scalar constructors behave inconsistently on arrays,"['00 - Bug', 'component: numpy._core', 'component: numpy.dtype']"
15470,open,proste,"Creation of `np.recarray` via `np.rec.fromarrays(...)` fails if provided `arrayList` are empty lists and `dtype` is non-trivial (containing sub-arrays).

In this particular case the function call has enough information to create empty array equivalent to output of e.g., `np.empty(shape=0, dtype=dtype).view(np.recarray)`.

### Reproducing code example:

```python
import numpy as np

np.rec.fromarrays([[], []], dtype=[('foo', np.int32, 2), ('bar', np.int32)])
```

### Error message:
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-2-e2e53c33181b> in <module>
----> 1 np.rec.fromarrays([[], []], dtype=[('foo', np.int32, 2), ('bar', np.int32)])

<environment_dirname>/lib/python3.8/site-packages/numpy/core/records.py in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder)
    636         testshape = obj.shape[:obj.ndim - nn]
    637         if testshape != shape:
--> 638             raise ValueError(""array-shape mismatch in array %d"" % k)
    639 
    640     _array = recarray(shape, descr)

ValueError: array-shape mismatch in array 1
```

### Numpy/Python version information:
```
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.18.1 3.8.1 (default, Jan  8 2020, 23:09:20) 
[GCC 9.2.0]
```
",2020-01-29 15:04:59,,np.rec.fromarrays(...) may fail if resulting array is going to be empty,"['00 - Bug', 'component: numpy._core']"
15453,open,mdehoon,"I am looking for an easy way to access array elements by (string) key (as well as by index).
Suppose I have an array like this:
```
x = array([[0, 4, 9, 1],
           [1, 3, 9, 1],
           [3, 5, 6, 2],
           [6, 2, 7, 5]])
```
I am looking for way to specify a set of keys (for example `('A', 'C', 'G', 'T')`) that can be used as an alias for an index. So `x['A', 'C']`, `x[0,'C']`, `x['A', 1]`, and `x[0,1]` all return the value 4; `x['G', :]` is the same as `x[2, :]`, and so on. I know that this can be achieved by subclassing a numpy array and overriding `__getitem__` and `__setitem__`, but subclassing gets complicated very quickly.

I understand that a pandas `DataFrame` has this capability, but as far as I understand it does not subclass from a numpy array, and therefore (unlike numpy arrays) does not provide the buffer protocol in C code.
",2020-01-28 12:06:31,,Accessing array elements by key,['33 - Question']
15450,open,eric-wieser,"This function:

1. Calls `PyObject_GetBuffer`
2. Extracts a data pointer
3. Calls `PyBuffer_Release`
4. Returns the pointer to the caller

`PyBuffer_Release` calls `PyBufferProcs.bf_releasebuffer(PyObject *exporter, Py_buffer *view)`, which according to the docs may _""free all memory associated with `view`""_, and gives no requirement that `view` remains alive as long as `exporter`.

So in principle a type that allocates a buffer for itself on the fly in `bf_getbuffer` and deletes it when the last uses releases it will cause a use-after-free in numpy.

I don't know if any implementers of the buffer protocol actually do this, but my reading of it is that they are permitted to.",2020-01-27 20:11:51,,BUG: PyArray_BufferConverter is unsafe,"['00 - Bug', 'Project']"
15442,open,tantrev,"Data for the tests below may be found by downloading [test.txt](https://github.com/numpy/numpy/files/4115466/test.txt).  The fourth test below should return True, but returns False.

### Reproducing code example:
```
import numpy as np

inputFile = ""test.txt"" #test file - has 99 rows, 1025 columns

#First test reading the whole file
a1 = np.fromfile(inputFile,dtype=('uint8',1025),sep="""")
b1 = np.fromfile(inputFile,dtype=('uint8',1025),sep="""")
print(""First test:"",(a1.ravel()==b1.ravel()).all())

#Second test reading the whole file with explicit count and offset parameters
a2 = np.fromfile(inputFile,dtype=('uint8',1025),sep="""",count=99,offset=0)
b2 = np.fromfile(inputFile,dtype=('uint8',1025),sep="""",count=99,offset=0)
print(""Second test:"",(a2.ravel()==b2.ravel()).all())

#Third test - sanity check:make sure a1=a2 and b1=b2
print(""Third test - part a):"",(a1.ravel()==a2.ravel()).all())
print(""Third test - part b):"",(b1.ravel()==b2.ravel()).all())

#Fourth test with differing count parameters that should otherwise be equivalent
a4 = np.fromfile(inputFile,dtype=('uint8',1025),sep="""",count=99,offset=0)
b4 = np.fromfile(inputFile,dtype=('uint8',1025),sep="""",count=100,offset=0)
print(""Fourth test:"",(a4.ravel()==b4.ravel()).all())

```
### Error message:

The output from the code above:
```
First test: True
Second test: True
Third test - part a): True
Third test - part b): True
Fourth test: False
```
### Numpy/Python version information:
1.17.2 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]
",2020-01-27 03:21:25,,DEP: fromfile returns shorter array if `count` argument exceeds file content,"['component: numpy._core', '07 - Deprecation', '62 - Python API']"
15438,open,Antetokounpo,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
a = np.array([11, 20, 32])
np.around(a, decimals=-1, out=a)
```

<!-- Remove these sections for a feature request -->

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->
```
Traceback (most recent call last):
  File ""/home/a/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py"", line 61, in _wrapfunc
    return bound(*args, **kwds)
TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test2.py"", line 5, in <module>
    np.around(a, decimals=-1, out=a)
  File ""<__array_function__ internals>"", line 6, in around
  File ""/home/a/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py"", line 3224, in around
    return _wrapfunc(a, 'round', decimals=decimals, out=out)
  File ""/home/a/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py"", line 70, in _wrapfunc
    return _wrapit(obj, method, *args, **kwds)
  File ""/home/a/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py"", line 47, in _wrapit
    result = getattr(asarray(obj), method)(*args, **kwds)
TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''
```
### Numpy/Python version information:
1.18.1 3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2020-01-26 17:21:08,,TypeError when using np.around() on an integer array with in-place option set,"['01 - Enhancement', 'component: numpy._core', '62 - Python API']"
15380,open,vandenheuvel,"Is there a way the split function could be generalized? I think it would be useful to split matrices into a grid of submatrices, for example.

I'm currently trying to implement such a function in python using numpy, and I can't seem to find an idiomatic way to do it. Could someone point me to some pointers on how to handle different axes sizes? Has someone tried to generalize strides? If someone could link some resources, which I'm having a hard time finding, that would be appreciated!",2020-01-21 22:02:41,,Feature request: generalizing split,"['01 - Enhancement', 'component: numpy.lib']"
15357,open,jrathman,"import numpy
print(numpy.sin(True))  #gives **incorrect** answer 0.8314
print(numpy.sin(+True))  #give correct answer 0.8414709848078965... = sin(1)

### Numpy/Python version information:
1.16.5 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]",2020-01-20 20:05:58,,numpy.sin(True) gives result different from numpy.sin(1),"['component: numpy.ufunc', 'component: numpy.dtype', '33 - Question']"
15332,open,languitar,"For the special case shown below, `histogram_bin_edges` with strategy `doane` hangs forever and wastes a single CPU and all system memory.

### Reproducing code example:

Attention, will start to eat up all system memory!

```python
import numpy as np
np.histogram_bin_edges([0.0, 0.0, 1.0], ""doane"", range=(0.0, 1056964608.0))
```

The error will not appear if only a single `0.0` is passed in the array.

Same thing happens with:
```python
import numpy as np
np.histogram_bin_edges([0.0, 1.0, -1.0], ""doane"", range=(-1.0, 4294967296.0))
```

### Error message:

No error message, code just hangs and wastes all memory.

### Numpy/Python version information:

```
1.18.1 3.8.1 (default, Jan  8 2020, 23:09:20) 
[GCC 9.2.0]
```

the same thing happens with:
```
1.17.4 3.7.4 (default, Nov 21 2019, 11:30:33) 
[GCC 9.2.0]
```

(Issue found in a unit test using hypothesis)",2020-01-15 11:39:29,,Memory leak and hang in histogram_bin_edges with strategy doane,"['15 - Discussion', 'component: numpy.lib']"
15331,open,mdickinson,"The current `numpy.spacing` documentation seems inaccurate. [At the top](https://github.com/numpy/numpy/blob/b7c27bd2a3817f59c84b004b87bba5db57d9a9b0/numpy/core/code_generators/ufunc_docstrings.py#L3424), it says:

> Return the distance between x and the nearest adjacent number.

But this isn't right for powers of 2: for example, if `x = 1.0`, the nearest representable float to `x` is `1.0 - 2**-53`, so the distance would be `2.**-53`. But in this case, `spacing` gives `2.0**-52`.

It's also not immediately clear from the description what the expected sign of the result is. From experimentation, it looks as though `np.spacing(-x)` is `-np.spacing(x)`, except in the case of zeros, where the result is the same for both negative and positive zeros.
",2020-01-15 09:49:51,,numpy.spacing documentation inaccuracies,['04 - Documentation']
15320,open,Kai-Striega,"<!-- Please describe the issue in detail here, and fill in the fields below -->
The test TestClip.test_NaT_propagation xfails due to errors in the test, rather than the test conditions not being met. This is similar to #15319, where the parameter `arr` is expected then `a` is used by the test. However there are several more reasons for the test to fail prematurely:

1. Passing `nat` to `np.clip` is deprecated and raises a DepcrecationWarning
2. Cannot cast ufunc 'clip' input 1 from dtype('float64') to dtype('<m8') with casting rule 'same_kind'

I believe that these were changed previously and missed due to the test being expected to fail.

### Reproducing code example:
```
$ python runtests.py -v -t numpy/core/tests/test_numeric.py::TestClip::test_NaT_propagation -- --runxfail
Building, see build.log...
Build OK
NumPy version 1.19.0.dev0+b757fb3
NumPy relaxed strides checking option: True
============================= test session starts ==============================
platform linux -- Python 3.7.5, pytest-5.3.2, py-1.8.1, pluggy-0.13.1
rootdir: /mnt/c/Users/Kai/Develop/numpy-git, inifile: pytest.ini
plugins: forked-1.1.3, xdist-1.31.0
collected 1 item

numpy/core/tests/test_numeric.py F                                       [100%]

=================================== FAILURES ===================================
_______________ TestClip.test_NaT_propagation[arr0-amin0-amax0] ________________

self = <numpy.core.tests.test_numeric.TestClip object at 0x7f9b0cfde840>
arr = array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=timedelta64)
amin = numpy.timedelta64('NaT')
amax = array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)

    @pytest.mark.xfail(reason=""propagation doesn't match spec"")
    @pytest.mark.parametrize(""arr, amin, amax"", [
        (np.array([1] * 10, dtype='m8'),
         np.timedelta64('NaT'),
         np.zeros(10, dtype=np.int32)),
    ])
    def test_NaT_propagation(self, arr, amin, amax):
        # NOTE: the expected function spec doesn't
        # propagate NaT, but clip() now does
>       expected = np.minimum(np.maximum(a, amin), amax)
E       NameError: name 'a' is not defined

amax       = array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)
amin       = numpy.timedelta64('NaT')
arr        = array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=timedelta64)
self       = <numpy.core.tests.test_numeric.TestClip object at 0x7f9b0cfde840>

numpy/core/tests/test_numeric.py:2018: NameError
============================== 1 failed in 0.82s ===============================
```

### Numpy/Python version information:

```python
>>> import sys, numpy
>>> numpy.__version__, sys.version
(1.19.0.dev0+b757fb3, 3.7.5 (default, Nov  7 2019, 10:50:52) [GCC 8.3.0])
```
",2020-01-12 07:55:21,,TST: TestClip.test_NaT_propagation xfails unexpectedly,"['05 - Testing', 'component: numpy.datetime64']"
15318,open,ZisIsNotZis,"## Introduction

`numpy` is an important tool in scientific computation (e.g. optimization), and visualization is an important subject in this field. Sometimes, after done some numerical optimization, people would want to visualize the result of how things get optimized. In order to do so, a common way is to draw some raw video (by manipulating array) and play that later using tools such as `ffmpeg`.

With advanced indexing, point drawing can be extremely simple, but line/box drawing is getting inconvenient. What people usually have is indexes of start/end point. Since they can't put the indexes in slice, neither can they unite the tiny slices to a big slice, they have to explicitly draw using (nested) for loop + tiny-slice assigning. This is not taking advantage of `numpy`'s vectorization, and is slow due to huge python overhead.

## Definition

One way to solve this is allowing advanced slicing (i.e. slice array using array). A straightforward example would be

```python
import numpy as np
a = np.arange(8)
a[[0,1,3]:[1,3,6]]  # ""should"" be [[0], [1,2], [3,4,5]]
```
The problem is, when applied on RHS, advanced slicing have to generate a ""physical"" array, which might make the result non-rectangular and unsupported by `numpy`.

On the contrary, indexing on LHS is fully ""imaginary"". Therefore, advanced slicing becomes possible:

```python
import numpy as np
a = np.arange(8)
a[[0,1,3]:[1,3,6]] = [0,1,2]
a # should be [0, 1, 1, 2, 2, 2, 6, 7]  
```
Since this makes `ndarray` much more powerful, I would strongly request such an enhancement. There are two set of detailed rules on my mind:

### Possible Rule 1
* On RHS: advanced slicing is disallowed.
* On LHS: advanced slicing broadcasts with regular advanced indexing. All axes sliced such way will be invisible (in terms of broadcasting) to RHS, and all values/operations from RHS will be broadcasted to all elements in these invisible axes.

### Possible Rule 2
* Generally: advanced slicing broadcasts with advanced index.
* On RHS: advanced slicing should produce a valid array, otherwise an exception is raised.
* On LHS: advanced slicing might produce array with invalid axes (i.e. axes that have different shapes inside). Those axes will have a shape of `?`, which is only broadcastable against shape `1`.

## Example

Draw 10 random rectangles on a 256x256 black area with random grayscale
```python
import numpy as np
a = np.zeros((256, 256))
xBeg,xEnd = np.random.randint(0, 256, (2,10)).sort(0)
yBeg,yEnd = np.random.randint(0, 256, (2,10)).sort(0)
colors = np.random.rand(10)
if POSSIBLE_RULE == 1:
    a[xBeg:xEnd,yBeg:yEnd] = colors
else:
    # `a[xBeg:xEnd,yBeg:yEnd]` will have shape `10,?,?,3`
    a[xBeg:xEnd,yBeg:yEnd] = colors[:,None,None]
```

Given info of 10 rectangles in 1000 frames, and color of each rectangle, draw them into a raw video of 256x256
```python
import numpy as np
def draw_advanced_slice(xBeg, xEnd, yBeg, yEnd, colors):
    assert xBeg.shape == xEnd.shape == yBeg.shape == yEnd.shape == (1000,10)
    assert colors.shape == (10,3)
    a = np.memmap('video.raw', mode='w+', shape=(1000,256,256,3))
    if POSSIBLE_RULE == 1:
        a[np.arange(1000)[:,None], xBeg:xEnd, yBeg:yEnd] = colors
    else:
        # LHS will have shape `1000,10,?,?,3`
        a[np.arange(1000)[:,None], xBeg:xEnd, yBeg:yEnd] = colors[:,None,None]
```

Compared to the current implementation
```python
import numpy as np
def draw_without_advanced_slice(xBeg, xEnd, yBeg, yEnd, colors):
    assert xBeg.shape == xEnd.shape == yBeg.shape == yEnd.shape == (1000,10)
    assert colors.shape == (10,3)
    a = np.memmap('video.raw', mode='w+', shape=(1000,256,256,3))
    for a, xBeg, xEnd, yBeg, yEnd in zip(a, xBeg, xEnd, yBeg, yEnd):
        for xBeg, xEnd, yBeg, yEnd, colors in zip(xBeg, xEnd, yBeg, yEnd, colors):
            a[xBeg:xEnd, yBeg:yEnd] = colors
```

Things other than assigning. This might be more tricky because it cannot be translated to `a[xs:ys] = a[xs:ys] * b`
```python
import numpy as np
a = np.arange(8)
if POSSIBLE_RULE == 1:
    a[[0,1,3]:[1,3,6]] *= [0,1,2]
else:
    # LHS will have shape `3,?`
    a[[0,1,3]:[1,3,6]] *= np.array([0,1,2])[:,None]
a # should be [0, 1, 2, 6, 8, 10, 6, 7]  
```

## Note

Another way to solve non-rectangular broadcast assigning problem is using a masked-array. But generating such a mask is inefficient, and sometimes non-trivial. It might take huge memory (and long time) to generate the mask in the first place",2020-01-12 02:39:45,,Feature request: advanced slicing (on left hand side),"['23 - Wish List', 'component: numpy._core']"
15317,open,eric-wieser,"Many of the C functions in numpy have code that is something like:
```C
PyObject *foo(PyObject *obj) {
    if (PyUnicode_Check(obj)) {
        /* accept unicode input */
        PyObject *obj_bytes = PyUnicode_AsASCIIString(obj);
        if (obj_bytes == NULL) {
             return NULL;
        }
        PyObject *ret = foo(obj_bytes);
        Py_DECREF(obj_bytes);
    }
    
    char *str = NULL;
    Py_ssize_t length = 0;
    if (PyBytes_AsStringAndSize(obj, &str, &length) < 0) {
        return NULL;
    }

    // work with bytes
}
```
This construction means they raise `UnicodeEncodeError` for some illegal inputs, which is often less useful than the message would normally be

It would be better to work with the unicode objects directly, by copying the approach taken in the C code changes in #15261, notably [these lines](https://github.com/numpy/numpy/commit/e83bd4658de5d7ffb6a460f68d2638c1b0a35068#diff-475dc2a28ab28625b0f02d7f006683abL1411-R1419) and [these lines](https://github.com/numpy/numpy/commit/e83bd4658de5d7ffb6a460f68d2638c1b0a35068#diff-475dc2a28ab28625b0f02d7f006683abL1497-R1502).

---

Spawned from discussion in https://github.com/numpy/numpy/pull/15261#issuecomment-573367589",2020-01-12 00:57:06,,Clean up internal callers of PyUnicode_AsASCIIString,"['03 - Maintenance', 'sprintable - C']"
15292,open,jthielen,"In the [Writing custom array containers guide](https://docs.scipy.org/doc/numpy/user/basics.dispatch.html), the signature for `__array__` is given as

```python
def __array__(self):
```

which does not include [the optional `dtype` argument](https://docs.scipy.org/doc/numpy/reference/arrays.classes.html#numpy.class.__array__). If a container class implementer follows the signature in the guide (like I did in https://github.com/hgrecco/pint/pull/953), it breaks functionality with libraries ([such as matplotlib](https://github.com/hgrecco/pint/issues/974#issuecomment-571847475)) that rely on being able to use the `dtype` argument.

Would it be reasonable to update the custom containers guide to include this optional argument, with an added example block to illustrate its use? If so, I can submit a PR.

xref https://github.com/hgrecco/pint/issues/974, https://github.com/hgrecco/pint/pull/975",2020-01-08 18:29:59,,DOC: dtype optional argument to __array__ not included in custom container guide,"['04 - Documentation', 'sprintable']"
15219,open,admercs,"`nanmean` and `nanvar` raise `RuntimeWarning` exceptions that are not called by their non-nan counterpart functions.

### Reproducing code example:

```python
import numpy as np

test = np.zeros((2, 3, 4))
test[:,:,1:2] = np.nan

np.mean(test, axis=0)    # OK
np.nanmean(test, axis=0) # RuntimeWarning: Mean of empty slice

np.var(test, axis=0)     # OK
np.nanvar(test, axis=0)  # RuntimeWarning: Degrees of freedom <= 0 for slice.
```

### Error message:

`RuntimeWarning: Mean of empty slice`
`RuntimeWarning: Degrees of freedom <= 0 for slice.`

### Numpy/Python version information:

`1.17.2 3.7.4 (default, Aug 13 2019, 15:17:50)
[Clang 4.0.1 (tags/RELEASE_401/final)]`
",2020-01-01 20:16:50,,nanmean and nanvar raise odd RuntimeWarning,"['15 - Discussion', '62 - Python API']"
15200,open,jthielen,"<!-- Please describe the issue in detail here, and fill in the fields below -->
When using a [custom array container](https://docs.scipy.org/doc/numpy-1.17.0/user/basics.dispatch.html) that implements `__array_ufunc__` and is meant to be able to wrap MaskedArrays, non-commutativity occurs in binary operations with MaskedArrays. Despite what the below comment mentions:

https://github.com/numpy/numpy/blob/v1.17.3/numpy/ma/core.py#L3960-L3972

it does not [properly defer](https://numpy.org/neps/nep-0013-ufunc-overrides.html#type-casting-hierarchy), as seen in the example below. I am unfortunately not well-acquainted with the MaskedArray internals, so I don't know what would be a good way forward for a fix.

xref https://github.com/hgrecco/pint/issues/633

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
import numpy.lib.mixins


class WrappedArray(numpy.lib.mixins.NDArrayOperatorsMixin):
    __array_priority__ = 20

    def __init__(self, array, **attrs):
        self._array = array
        self.attrs = attrs

    def __repr__(self):
        return f""{self.__class__.__name__}(\n{self._array}\n{self.attrs}\n)""

    def __array__(self):
        return np.asarray(self._array)

    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):
        if method == '__call__':
            inputs = [arg._array if isinstance(arg, self.__class__) else arg
                      for arg in inputs]
            return self.__class__(ufunc(*inputs, **kwargs), **self.attrs)
        else:
            return NotImplemented


# Show basic wrap
w = WrappedArray(np.arange(3), test=1)
print(w)
print()

# Wrapping MaskedArrays works fine
wm = WrappedArray(np.ma.masked_array([1, 3, 5], mask=[False, True, False]),
                  test=2)
print(wm)
print()

# Operations with ndarrays work fine
a = np.array([2, 0, 1])
print(a * w)
print()

# Operations with masked arrays are not commutative
m = np.ma.masked_array([2, 0, 1], mask=[False, True, False])

# Good
print(w * m)
print()

# Bad
print(m * w)
```

Output:

```
WrappedArray(
[0 1 2]
{'test': 1}
)

WrappedArray(
[1 -- 5]
{'test': 2}
)

WrappedArray(
[0 0 2]
{'test': 1}
)

WrappedArray(
[0 -- 2]
{'test': 1}
)

[0 -- 2]
```

<!-- Remove these sections for a feature request -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

1.17.3 3.6.7 | packaged by conda-forge | (default, Nov  6 2019, 16:19:42) 
[GCC 7.3.0]
",2019-12-30 04:17:30,,BUG: MaskedArray does not seem to respect ufunc dispatch hierarchy,['component: numpy.ma']
15140,open,mhvk,"I wouldn't quite dare to call this a bug, but it is a small inconsistency that perhaps leads to some refactoring that makes everything better. In the process of automatic wrapping of Standards of Astronomy functions (see #15139), some functions were wrapped that are meant to just give access to some standard numbers/positions/matrices. They are wrapped with signatures like `->(3,3)` and they work, but only if one gives an explicit output. If nothing is given, the iterator raises `ValueError: At least one iterator operand must be non-NULL`. 

This of course is not illogical, but in principle, what should happen here is quite obvious, so perhaps it can nevertheless be remedied...",2019-12-20 19:57:02,,Gufuncs with zero inputs(!) only work with explicit output,['component: numpy.ufunc']
15128,open,regstrtn,"Currently np.argmax, np.argmin, np.max and np.min return the maximum and minimum values respectively. 
`numpy.argmax(a, axis=None, out=None)`
Often times we need to get the topKmax or topKmin values, mostly in the case of machine learning workloads where you need top K classes for a given input. 
Requesting support for these scenarios in numpy. This scenario could be supported by np.max (and other functions), or there could be a separate family of methods np.maxK, minK etc? ",2019-12-19 06:12:42,,Feature request: Allow np.argmax to output top K maximum values,"['01 - Enhancement', '62 - Python API']"
15123,open,jakirkham,It would be really useful when operating with other array libraries to support `__array_function__` on `numpy.random` functions (like `shuffle`) so as to dispatch to other implementations for those arrays.,2019-12-17 19:13:16,,Adding `__array_function__` support to `random` operations (like `shuffle`),"['component: numpy.random', 'component: __array_function__']"
15103,open,zhcui,"Hi all,

Currently numpy.tensordot does not support an `out` argument that allows to modify a passed-in array (e.g. in np.dot there is a possible argument `out`). Is it possible to support this in future?

Thank you very much.

Best,
Zhihao",2019-12-13 01:21:21,,Will tensordot support an 'out' argument?,"['01 - Enhancement', 'component: numpy._core']"
15008,open,eric-wieser,"```
>>> np.interp(""2"", [1, 2], [10, 20])
20.0
```

This really ought to be a `TypeError`...",2019-11-29 09:56:28,,BUG: np.interp casts strings to floats,"['00 - Bug', 'component: numpy._core']"
15002,open,AntSimi,"When i apply mask generated by ma.zeros on the zeros matrix, i have an behaviour which i don't understand.

### Reproducing code example:

```python
import numpy as np
a = np.ma.zeros(5)
# add a dimensions
print(a[~a.mask])
# Seem work correctly
print(a[a.mask])
# But when we check dimensions??:
print(a[a.mask].shape)
```

### Result
```python
print(a[~a.mask])
[[0. 0. 0. 0. 0.]]
print(a[a.mask])
[]
print(a[a.mask].shape)
(0, 5)
```

### Numpy/Python version information:
1.16.5 3.7.4 (default, Aug 13 2019, 20:35:49) 
[GCC 7.3.0]


",2019-11-28 14:12:10,,Masked array dimensions added when apply mask value?,"['00 - Bug', '01 - Enhancement', 'component: numpy.ma']"
14999,open,eric-wieser,"Something we could perhaps integrate into our release script

Town crier solved the problem of PRs needing to move their release notes each time they missed a release.

Unfortunately, this doesn't solve the problem of PRs needing to change the version numbers in `..versionadded`, `..versionchanged`, etc docstrings.

In order to prevent stale PRs, we could consider building a release script that finds a `NEXT_RELEASE_VERSION` placeholder in these places, and replaces it with the release number.

We might also want to extend this to the deprecation comments in the C code, which are of the form `/* NumPy 1.18.0 2019-11-28 */`",2019-11-28 00:21:18,,Auto-update ..versionadded etc as part of the release,"['01 - Enhancement', '14 - Release']"
14997,open,chaburkland,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Brief
Numpy is lacking an optimization in the `in1d` function (which is used by `isin`). If numpy detects that either array has an `object` dtype, or that they meet a specific size relationship (namely, `len(ar2) < 10 * len(ar1) ** 0.145`), it will proceed to evaluate at a O(n*m) speed, where n = len(arr1) and m = len(arr2).

This is significant, since `object` dtypes are an extremely common use case, and O(n*m) scales very poorly. If the user is unfortunate enough to have large data sets that meet either of these two requirements, their performance will be very poor. The existing source code is only optimized for dealing with non-object arrays that don't meet a specific size relationship. This is a problem.

### Reproducing code example:

The following example is a bit contrived, but it demonstrates how significant the performance loss can be if the user is unfortunate to have an array with an `object` dtype.

```python
import numpy as np
from time import time

arr1 = np.arange(1000000, dtype=object)

# This will take around 10/15 seconds.
start = time()
arr2 = np.array([-1]*1000 + [0], dtype=object)  # Increasing to 10000 slows to ~120 seconds
result = np.isin(arr1, arr2)
assert result[0]  # 0 is in both sets.
print('Without hashtable speed: ', time() - start)


# This will take no longer than ~0.1 seconds, regardless of how large arr2 is 
start = time()
arr3 = np.array([n for n in set(arr2)], dtype=object)
result = np.isin(arr1, arr3)
assert result[0]  # 0 is in both sets
print('With hashtable speed: ', time() - start)
```

### Suggestions

As hinted at in my code, some form of hashtable seems like the best approach to take advantage of constant lookup time. This will reduce the time complexity from O(n*m) to O(n + m), as now the only overhead needed will be to do a single pass through `ar2` to build a hashtable.",2019-11-27 23:35:14,,isin() and in1d() perform slowly on object arrays,"['01 - Enhancement', '04 - Documentation', 'component: numpy.lib']"
14992,open,dstorer,"Using np.matmul to multiply a numpy array with its transpose produces an error when the array is masked and it is not a square array. (When the array is either masked and square, or unmasked and any shape it works, it only doesn't work when the array is both masked and not square). I am working with numpy version 1.17.3, and python version 3.7.5.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
x = np.tile([1,2,3,4],(3,1))
mask = np.tile([0,1,0,0],(3,1))
mx = np.ma.masked_array(x, mask=mask)
np.matmul(my.T,my)
```

### Error message:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/lustre/aoc/projects/hera/dstorer/env/envs/hera/lib/python3.7/site-packages/numpy/ma/core.py"", line 3040, in __array_wrap__
    m = reduce(mask_or, [getmaskarray(arg) for arg in input_args])
  File ""/lustre/aoc/projects/hera/dstorer/env/envs/hera/lib/python3.7/site-packages/numpy/ma/core.py"", line 1753, in mask_or
    return make_mask(umath.logical_or(m1, m2), copy=copy, shrink=shrink)
ValueError: operands could not be broadcast together with shapes (4,3) (3,4)
```





",2019-11-27 20:01:27,,matmul produces error for masked arrays that aren't square,"['00 - Bug', 'component: numpy.ma']"
14952,open,brodderickrodriguez,"<!-- Please describe the issue in detail here, and fill in the fields below -->

Per [documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.chararray.astype.html#numpy.chararray.astype), a char array should be able to be casted to a float ndarray as long as the array entires are reals. Docs say this may cause a `ComplexWarning`, however, I'm seeing a `ValueError`.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.17.2 3.7.4 (v3.7.4:e09359112e, Jul  8 2019, 14:54:52)
[Clang 6.0 (clang-600.0.57)]
>>> a = numpy.chararray((3,3))
>>> a[:] = '1'
>>> a
chararray([[b'1', b'1', b'1'],
           [b'1', b'1', b'1'],
           [b'1', b'1', b'1']], dtype='|S1')
>>> a.astype(numpy.float)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/core/defchararray.py"", line 1989, in __array_finalize__
    raise ValueError(""Can only create a chararray from string data."")
ValueError: Can only create a chararray from string data.
>>> a.real.astype(numpy.float)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/core/defchararray.py"", line 1989, in __array_finalize__
    raise ValueError(""Can only create a chararray from string data."")
ValueError: Can only create a chararray from string data.
>>>
```

",2019-11-21 14:10:46,,DEP: Deprecate chararray in more than just documentation,['07 - Deprecation']
14941,open,mbakker7,"The `interp` function can not handle `datetime64` dtypes (docs are not explicit about this; they state that `x` and `xp` should be values). Error message is that array cannot be cast `to dtype('float64') according to the rule 'safe'`. But conversion seems easy.

### Reproducing code example:

```
import numpy as np
d1 = np.datetime64('2019-01-01')
d2 = np.datetime64('2019-01-03')
xp = np.array([d1, d2])
yp = np.array([1, 3])
x = np.datetime64('2019-01-02')
np.interp(x, xp, yp)  # produces error
```
while the following works fine
```
np.interp(x.astype('float64'), xp.astype('float64'), yp)
```
So why is that not done in the `interp` function?",2019-11-19 20:37:21,,"interp can not handle datetime64, while fix seems easy","['01 - Enhancement', 'component: numpy.lib', 'component: numpy.datetime64']"
14925,open,seberg,"The `dot` function (unlike ufuncs, and the ufunc based `matmul`, `@`) does not check for floating point warning/error flags. It would probably better if it would, although it is likely not high priority. The example being:

```
np.dot([1,1], [-np.inf, np.inf])
```
Which should be setting the CPU invalid value produced flag on the NaN result, but it is not caught by NumPy.

(phased out from gh-14788)",2019-11-18 01:59:11,,BUG: Dot product does not check for floating point warning flags,"['00 - Bug', 'component: numpy._core']"
14909,open,robparrishqc,"<!-- Please describe the issue in detail here, and fill in the fields below -->

`numpy` (superb library - no slight intended here!) does not seem to support Kahan or butterfly summation in `cumsum`, which is critically important for accuracy in large arrays of numbers of similar magnitude. 

### Reproducing code example:

`import numpy as np; print(np.cumsum(np.ones((2**28,), dtype=np.float32))[-1] - 2**28)`

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
print(np.cumsum(np.ones((2**28,), dtype=np.float32))[-1] - 2**28)
# Should be zero
```

Note that `sum` obviously uses butterfly summation: 

```python
print(np.sum(np.ones((2**28,), dtype=np.float32)) - 2**28)
# Is zero on any machine I can find
```
",2019-11-15 00:07:53,,ENH: Stable summation method for cumsum (add.reduce),"['01 - Enhancement', '15 - Discussion', 'component: numpy.ufunc']"
14891,open,jakirkham,"With the various different array libraries in place, it's gotten a bit tricky to write general code that works for all of them. Previously we might checking if some array is an instance of a NumPy `ndarray`, but that doesn't generalize. One solution people have come up with are ""duck"" checks where various attributes like `shape`, `dtype`, etc. are checked. This can work or can run into issues depending on whether these checks are representative of what the user is looking for.

In particular we have some arrays that are concrete (like NumPy, CuPy, Sparse, etc.). These arrays own memory and can easily manipulate values they contain or perform IO. Other arrays are abstract (like Dask, Xarray, etc.). They indirectly hold data through other objects and are not always able to manipulate their contents easily. Serialization for these arrays is an orchestration of serialization of their contents or perhaps their state depending.

It's useful to understand when one has a concrete array vs. an abstract array. However we lack a mechanism to do that today. Am raising this issue so that we can discuss how we might solve this.

cc @mrocklin @pentschev @rgommers @shoyer",2019-11-12 17:33:02,,Checking array types (concrete vs. abstract),['15 - Discussion']
14884,open,shoyer,"This came up on Twitter: https://twitter.com/jeremyphoward/status/1193610182489542657

There's a warning in the docs, but nobody reads the docs!

I can't think of any use cases for the current behavior (treating a set like a scalar), but I guess there's a consistency argument for keeping it?",2019-11-12 03:31:32,,Deprecate passing a set into np.isin as the second argument?,['component: numpy.lib']
14837,open,eric-wieser,"As discussed in #5972, `np.full` broadcasts its scalar when assigning. This isn't documented.",2019-11-06 14:23:52,,DOC: np.full does not mention it braodcasts its argument,"['15 - Discussion', '05 - Testing']"
14828,open,gregreen,"When attempting to add an `ndarray` of type `int64` to an `ndarray` of type `uint64`, `numpy` throws a `TypeError`:

    TypeError: Cannot cast ufunc add output from dtype('float64') to dtype('uint64') with casting rule 'same_kind'

The error message incorrectly identifies the type of the first array as `float64`, when it is actually `int64`.

### Reproducing code example:

```python
import numpy as np
x = np.arange(5, dtype='u8')
y = np.arange(5, dtype='i8')
x += y
```

### Error message:

    TypeError: Cannot cast ufunc add output from dtype('float64') to dtype('uint64') with casting rule 'same_kind'

### Numpy/Python version information:

Tested with Python 3:

    1.16.4 3.6.8 (default, Oct  7 2019, 12:59:55) 
    [GCC 8.3.0]

Also with Python 2:

    ('1.15.1', '2.7.15+ (default, Oct  7 2019, 17:39:04) \n[GCC 7.4.0]')",2019-11-04 17:16:41,,Improving the error message when adding i8 array to u8 array,"['01 - Enhancement', '04 - Documentation', 'component: numpy.ufunc']"
14816,open,sursu,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:
This works well:
```python
m = np.random.normal(size=3)
x = np.random.normal(loc=m, scale=1)
```
This does not:
```python
P = [[1,0.5],[0.5,1]]
M = np.random.multivariate_normal(mean=[0,0], cov=P, size=3)
X = np.random.multivariate_normal(mean=M, cov=P)
```
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-157-c99a31aba9db> in <module>
      1 M = np.random.multivariate_normal(mean=[0,0], cov=P, size=3)
----> 2 X = np.random.multivariate_normal(mean=M, cov=P)

mtrand.pyx in numpy.random.mtrand.RandomState.multivariate_normal()

ValueError: mean must be 1 dimensional
```

### Numpy/Python version information:
1.17.2 / 3.7.4 (default, Aug 13 2019, 20:35:49) 
[GCC 7.3.0]",2019-10-31 18:06:07,,ENH: random.multivariate_normal should broadcast input,"['01 - Enhancement', 'component: numpy.random']"
14804,open,shrikedehyperion,"I am using numpy to calulate the number of each element in an uint8 dtype ndarray, but I met some strange issue,like this:
    
    a = np.random.randint(0, 256, (10980, 10980)).astype(np.uint8)
    s_values, s_idx, s_counts = np.unique(a, return_inverse=True, return_counts=True)
    print(lenth(s_values)  # 256
    print(s_counts)
    
    [470996 472336 472608 469978 471579 470698 470149 471167 470199 471153
    471447 471972 470542 471369 470223 472189 470976 470971 471516 472120
    470907 470808 470535 471825 470549 470794 471589 471267 470354 471701
    472085 471199 469698 471675 470855 471282 470304 470156 471347 469881
    470653 470901 470820 470253 470350 471043 470839 470262 470185 471895
    470491 470297 471471 472009 471284 470593 470489 470297 470743 470461
    472091 470997 472254 470388 470334 471444 470368 470054 468965 471678
    470659 471519 470368 471808 470282 470356 471117 470904 470146 471554
    470983 470734 471132 472531 471347 471159 471958 470035 470082 470979
    471059 471713 472054 472314 470982 470142 471811 469707 471153 470774
    470882 469754 470347 471465 471326 470490 470157 470703 470851 471749
    470820 471016 472073 471125 470411 471177 470608 472016 470410 470624
    470940 471711 471198 471620 470899 470480 471047 471037 470763 469869
    471405 471485 470928 470446 470314 469986 471456 470344 469462 471189
    471236 470927 470971 470620 471029 470045 470194 471149 472302 470903
    470800 471068 471584 469641 471862 471931 471446 471432 469624 471306
    470597 470624 471715 470632 470675 469995 472048 472247 470595 470474
    470176 470209 469369 470637 471426 471391 470602 471379 469996 471050
    470192 470801 470168 470905 471115 471436 471910 471125 469920 470043
    470541 470743 471300 471162 471920 472646 471269 471604 469770 470841
    470523 471890 470018 470805 470178 471287 470340 470491 470361 470354
    470911 469871 470247 471402 470242 470931 471327 471024 472331 470700
    471708 470661 470969 471026 471450 471053 470415 470623 470546 470612
    470266 470994 471355 470044 470713 471846 471249 471964 470706 469506
    470391 471127 471511 472138 471170 471721 471438 471965 471573 471211
    471939 470819 469529 470699 470280 471779]

When the array is not masked, the lenth of s_value and S-counts is correct, and we can see that the number of element 0 is 470996.but if i masked 0 element, things got changed.
    
    # mask 0 value
    y = np.ma.masked_equal(a, 0)

    s_values2, s_idx2, s_counts2 = np.unique(y, return_inverse=True, return_counts=True)

    print(len(s_values2))  # 471534

Now normally s_values2 should only have 256 values, but in my case, it shows 471534 values. If np.ma.masked_equal works right, this processure should only convert element 0 into a value that is not within the range of (0,256), so it will be recongized as a masked value, but the lenth of the s_value should still be 256, that is from 1 to 255 plus a masked value which is 0, but things aren't going as I expected. I don't know why this is happening.
    
Note, this issue only happens when the dtype is uint8, if the dtype is float64, no matter we masked this array or not, this won't happen, also, if the array is not that big, like a 1000*1000 uin8 ndarray, it will also works right.",2019-10-30 10:00:09,,Using numpy.unique with a masked array get some strange issue,"['00 - Bug', 'component: numpy.ma']"
14801,open,keewis,"The changes to `maybe_get_attr` in #14745 mean that `PyArray_LookupSpecial` and `PyArray_LookupSpecial_OnInstance` may raise errors other than `AttributeError`.

In order keep the previous behaviour for every call other than the fix for #14735, all other calls have been changed to explicitly clear errors if these calls returned `NULL`.

This issue is used to track this and to make searching for these calls easier (search for `gh-14801`).",2019-10-29 22:04:47,,propagate errors where appropriate from PyArray_LookupSpecial,['unlabeled']
14788,open,DWesl,"<!-- Please describe the issue in detail here, and fill in the fields below -->
The dot product of 0 and np.inf depends on the array shapes.  Using `@` rather than the `dot()` methods produces consistent results.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
>>> import numpy as np
>>> print(np.zeros((1,), float).dot(np.full((1, 2), np.inf, float)))
[0. 0.]
>>> print(np.zeros((2, 1), float).dot(np.full((1, 2), np.inf, float)))
[[nan nan]
 [nan nan]]
>>> print(np.zeros((1,), float) @ (np.full((1, 2), np.inf, float)))
[nan nan]
>>> print(np.zeros((2, 1), float) @ (np.full((1, 2), np.inf, float)))
[[nan nan]
 [nan nan]]
```

<!-- Remove these sections for a feature request -->

### Error message:
Using `@` produces `RuntimeWarning: invalid value encountered in matmul`, but the `dot()` method produces no warnings.

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:
Numpy 1.16.5 on python 2.7.16, 3.6.9, 3.5.7, 3.7.4, and 3.8.0b4
GCC/GFortran with openblas 0.3.7, reference lapack.
<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
[GCC 7.4.0] 1.16.5
3.6.9 (default, Jul 21 2019, 14:33:59)
",2019-10-26 13:44:55,,"np.dot(0, inf) result changes based on shape",['00 - Bug']
14778,open,mattip,"Gathered from gh-14604, gh-14517 and the discussions.

- [x]  ""To summarize I need to draw random ints of a given C type from continually changing ranges, either one-by-one or small batch-by-small batch.""
- [ ] Someone asked how to use random in a ufunc.
- [ ] ""An ideal API would allow projects like https://github.com/deepmind/torch-randomkit/tree/master/randomkit or numba to consume the code in NumPy without vendoring it.""

- [ ] ""There are c++ applications which use boost::random, would be nice to be able to swap it for numpy.random.""

- [ ] ""Using the existing distributions from Cython was a requested feature and an explicit goal, yes. There are users waiting for this."" (mattip: but isn't this supported via Generator?)

- [x] ""Numba would definitely appreciate C functions to access the random distribution implementations, and 
- [ ] have a side-project (numba-scipy) that is making the Cython wrapped functions in SciPy visible to Numba"" (mattip: I think we do this after gh-14608 from _generator.so via these cdef extern declarations, but that requires the H file.

- [x] Someone who wants to write a new BitGenerator, i.e., the [numpy/bitgenerator repo](https://github.com/numpy/bitgenerator) without needing to have the numpy code as a git submodule

Some of these are already be handled by gh-14604, but I am putting them here for completeness

Edit: turned into a checklist",2019-10-25 03:50:39,,API: provide examples of use of numpy random API via user stories,"['component: numpy.random', '30 - API']"
14768,open,aarchiba,"<!-- Please describe the issue in detail here, and fill in the fields below -->
If I use `np,broadcast_arrays` to convert a collection of arrays to the same shape, and if one of them is a broadcast_array object, then *all* of them get flagged with the warn_on_write attribute even if some are completely untouched by the operation. This is particularly unpleasant because there is no apparent way to remove this flag from an array without making a copy; `np.require(a, requirements=['C','W'])` does nothing to remove the flag because the array already meets those criteria. 

### Reproducing code example:
```python
In [6]: a, b = np.broadcast_arrays(np.arange(2), 0)                                                                    

In [7]: a.flags                                                                                                        
Out[7]: 
  C_CONTIGUOUS : True
  F_CONTIGUOUS : True
  OWNDATA : False
  WRITEABLE : True  (with WARN_ON_WRITE=True)
  ALIGNED : True
  WRITEBACKIFCOPY : False
  UPDATEIFCOPY : False

In [8]: a.shape                                                                                                        
Out[8]: (2,)

In [10]: np.require(a, requirements=['C', 'W']).flags                                                                  
/home/archibald/.virtualenvs/astropy/lib/python3.7/site-packages/numpy/core/_asarray.py:321: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.
  if not arr.flags[prop]:
Out[10]: 
  C_CONTIGUOUS : True
  F_CONTIGUOUS : True
  OWNDATA : False
  WRITEABLE : True  (with WARN_ON_WRITE=True)
  ALIGNED : True
  WRITEBACKIFCOPY : False
  UPDATEIFCOPY : False
```

### Numpy/Python version information:

1.17.1 3.7.3 (default, Apr  3 2019, 05:39:12) 
[GCC 8.3.0]
",2019-10-23 23:09:17,,np.broadcast_arrays flags both arrays as warn_on_write even if one needed no broadcasting ,['component: numpy.lib']
14753,open,danielhrisca,"<!-- Please describe the issue in detail here, and fill in the fields below -->
Some binary file formats use half precision complex numbers (float16 for the real and imaginary parts)

Since numpy already supports half precision floats wouldn't it be possible to support the complex counterpart?

Size | Float | Complex
--|--|--
2| f2| ???
4| f4 |c8
8| f8 |c16





",2019-10-22 11:17:12,,ENH: half precision complex,"['01 - Enhancement', '23 - Wish List', 'component: numpy.dtype']"
14728,open,SorteKanin,"<!-- Please describe the issue in detail here, and fill in the fields below -->

Currently, in order to sort an array in reverse order you have to use some form of [strange hack](https://stackoverflow.com/questions/26984414/efficiently-sorting-a-numpy-array-in-descending-order), which may or may not be guaranteed to work in the future.

I suggest adding a reverse parameter to np.sort, similar to the native Python sort() function. This would make the hacks unnecessary and improve readability of code, by making the sorting order explicit.

This feature should not result in any performance problems - after all, the sorting function simply have to reverse the boolean result when comparing values.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np

a = np.arange(10)
np.random.shuffle(a)
a.sort(reverse = True)
print(a) # should print ""[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]""
```",2019-10-16 19:54:02,,ENH: Reverse parameter in np.sort(),"['01 - Enhancement', 'component: numpy._core', 'triaged']"
14716,open,rogerlew,"<!-- Please describe the issue in detail here, and fill in the fields below -->
`numpy.quantile` does not ignore masked values with masked_arrays. Is this a bug or a feature?

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
>>> import numpy as np
>>> x = np.ma.masked_array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], mask=[0, 0, 0, 0, 0, 1, 1, 1, 1, 1])
>>> np.quantile(x, [0.1, 0.75, 0.9])
array([0.9 , 6.75, 8.1 ])
>>> np.quantile(x[~x.mask], [0.1, 0.5, 0.75, 0.9])
array([0.4, 2. , 3. , 3.6])   # <- this is what I'm expecting
```

<!-- Remove these sections for a feature request -->

### Error message:

N/A

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.17.1 3.7.4 (default, Jul  9 2019, 18:13:23) 
[Clang 10.0.1 (clang-1001.0.46.4)]

Thank you!
Roger
",2019-10-15 20:49:49,,np.quantile does not ignore masked values with masked_arrays,['component: numpy.ma']
14624,open,zbruick,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

The following code sample passes, but is expected to fail:

```python
from numpy.testing import assert_array_almost_equal
assert_array_almost_equal(np.array([10, 20]), np.ma.array([10, np.nan], mask=[False, True]), 2)
```

### Numpy/Python version information:
python 3.7.3
numpy 1.17.2

",2019-10-01 20:45:56,,BUG: assert_array_almost_equal fails to compare array with masked array,"['15 - Discussion', '05 - Testing', 'component: numpy.ma']"
14599,open,batterseapower,"This should return an array of two NaNs, but actually returns a single NaN:

```python
np.nanquantile([], [0, 0.5])
```

`np.quantile` throws an error if given an empty input array (as expected) so is not affected by this issue.

### Numpy/Python version information:

```
1.17.0 3.6.1 (v3.6.1:69c0db5, Mar 21 2017, 18:41:36) [MSC v.1900 64 bit (AMD64)]
```

",2019-09-26 05:11:08,,nanquantile does not return array of correct length if input is empty,"['00 - Bug', 'component: numpy.lib']"
14574,open,2sn,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
np.exp(8j * np.arctan(np.array(1, dtype=np.float16))).imag                                                              
np.exp(8j * np.arctan(np.array(1, dtype=np.float32))).imag                                                              
np.exp(8j * np.arctan(np.array(1, dtype=np.float64))).imag                                                              
np.exp(8j * np.arctan(np.array(1, dtype=np.float128))).imag                                                            
```

<!-- Remove these sections for a feature request -->

### Output:

```python
-0.0019353059714989746
1.7484556000744883e-07
-2.4492935982947064e-16
 1.0033115225336664047e-19
```
**The last one seems inappropriately inexact**, more what you would expect from a `float80` rather than a `float128`.  (Yes, the `exp` then operates on `comples256` but all variants I tried get the same accuracy.)  Is the quad precision internally just using the Intel built-in 80 bit floats rather than real 128 bit arithmetics?  If so, why waste the space and not provide a `float80` data type?  Alignment issues would not be a lot worse than for float16, which is provided.  

Maybe, since I cannot find a reference to it at [https://docs.scipy.org/doc/numpy/user/basics.types.html](url) it is only experimental at this time?

It would be good if the documentation could elaborate on precision of `float128`/`complex256`

I think libraries should be available to do 128 bit precision arithmetics where not supported by hardware, to have most transparent and as expected behaviour for users, even if slow.

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

numpy: 1.17.2
python: 3.7.4
",2019-09-22 08:33:13,,Options for implementing a quadruple-precision dtype,['unlabeled']
14556,open,lesshaste,"<!-- Please describe the issue in detail here, and fill in the fields below -->


Multiplying two matrices is much slower when the matrices have dtype int64 than when they have dtype float64. This is even true if you include the time to first convert the matrices to floats and then back again.

### Reproducing code example:


<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
a = np.arange(1000 * 1000, dtype=np.int64).reshape((1000,1000))
b = a.copy()
%timeit a @ b
%timeit (a.astype(np.float64) @ b.astype(np.float64)).astype(np.int64)
afloat = a.astype(np.float64) 
bfloat = b.astype(np.float64)
%timeit afloat @ bfloat
```
The results on my PC are:

%timeit (a.astype(np.float64) @ b.astype(np.float64)).astype(np.int64)
37 ms ± 948 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

%timeit a @ b
2.44 s ± 49.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

afloat = a.astype(np.float64) 
bfloat = b.astype(np.float64)
%timeit afloat @ bfloat
21.6 ms ± 521 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

<!-- Remove these sections for a feature request -->



### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

1.17.2 3.5.2 (default, Jul 10 2019, 11:58:48) 
[GCC 5.4.0 20160609]",2019-09-20 16:50:13,,Matrix multiplication much slower for ints than floats,"['01 - Enhancement', '57 - Close?', 'component: numpy.ufunc']"
14479,open,prhbrt,"[**Colab**](https://colab.research.google.com/drive/1RS95WhNQdD83MFzySYGuMPqLwxrfSkc8)

I think a general use case of numpy.ndarrays is loading data from several files into one array, these could be pickles, images, or anything that can easily be loaded as a `numpy.ndarray`. In this case all these files would have the same `shape` and `dtype`, and would have normal strides. Is there an elegant way to load such files, avoiding duplicate code and memory overhead?

E.g. files created like this:

    import numpy
    from skimage.io import imsave, imread
    from glob import glob

    sz = (128, 128, 1)

    for i, color in enumerate([[0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 0, 1], [1, 1, 0]]):
      nice_image = numpy.clip(
          (0.95 + 0.1 * numpy.random.randn(*sz)) * [[color]],
          0, 1)
      imsave(
        f'nice-image-{i}.png',
        (255 * nice_image).astype(numpy.uint8)
      )

One way to open the files would be like this, but it would need twice the memory that is actually needed to first store the list and then create the array.

    images = numpy.concatenate([
      imread(filename)[numpy.newaxis] for filename in filenames
    ], axis=0)

Alternatively:

    first_image = imread(filenames[0])

    images = numpy.zeros((len(filenames), ) + im0.shape, dtype=im0.dtype)
    images[0] = first_image

    for image, filename in zip(images[1:], filenames[1:]):
      image[...] = imread(filename)

But this has two places where the data is loaded, so duplicate code, which makes it non-transparent and easily introduces bugs.

Would it make sense for the numpy api to have a `fromiter_nd` for example:

    images = numpy.fromiter_nd((
        imread(filename)[numpy.newaxis]
        for filename in filenames
    ), axis=0, length=len(filenames))

Existing functions fail in the following manor:

 * `numpy.fromiter` Assumes numbers and creates something 1D,
 * `numpy.stack`, `numpy.concatenate`, do not inspect the first item and use probably known generator length information to preallocate memory, and hence need twice the memory at peak.



",2019-09-11 10:15:52,,Generate `numpy.ndarray` from iterable with automatic memory pre-allocation,['33 - Question']
14478,open,eric-wieser,"Extracted from #14014.

Given a 2d array like:
```python
array([[list(['0.1', '0.2', '0.3'])],
       [list(['0.3', '0.4', '0.5'])],
       [list(['0.5', '0.6', '0.7'])]], dtype=object)
```

it would be nice to have some way to convert the array intothe 3d
```python
array([[['0.1', '0.2', '0.3'])],
       [['0.3', '0.4', '0.5']],
       [['0.5', '0.6', '0.7']]], dtype=object)
```

Presumably, if we built this it would be natural to provide a function to go in the other direction.

Filing this primarily so that I can close the original issue.",2019-09-11 08:47:56,,Provide functions to convert the last axis of object arrays to and from lists,['01 - Enhancement']
14462,open,fionaRust,"When passing an array with data type `float32` into `np.ma.average` there is a difference in the data type of the returned result depending on whether the input array is masked or not. In the case where the input data is a `MaskedArray`, the data type of the result is `float64`. If the input data in not a `MaskedArray`, the data type of the result is `float32`. 

This seems to relate the the information in the docstrings for `np.ma.average` (https://docs.scipy.org/doc/numpy-1.17.0/reference/generated/numpy.ma.average.html) and `np.average` (https://docs.scipy.org/doc/numpy-1.17.0/reference/generated/numpy.average.html), which describe different behaviour relating to the data type of the output array.

Is there any scope to unify the behaviour of these two functions? It would be helpful if the data type of the output arrays were consistent.

The same behaviour also exists for `np.var` and `np.ma.var`

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np

data = np.ones((3,3,3), dtype=np.float32)
mean_unmasked = np.ma.average(data, axis=0)
print(mean_unmasked.dtype) # Prints float32

data_masked = np.ma.MaskedArray(
    np.ones((3,3,3), dtype=np.float32), 
    np.ones((3,3,3), dtype=bool))
masked_average = np.ma.average(data_masked, axis=0)
print(masked_average.dtype) # Prints float64
```

### Numpy/Python version information:
numpy versions tested: 1.15.4 and 1.17.0
",2019-09-09 12:55:24,,Return data types of np.average and np.ma.average behave differently,"['00 - Bug', 'component: numpy.ma']"
16549,open,sobolevn,"Hi! Thanks for this awesome project!

I am [TypedDjango](https://github.com/TypedDjango) team member, we maintain types for, well, django. And we do pretty much the same job.

For example, we also test our types the similar way as you do in [`tests/`](https://github.com/numpy/numpy-stubs/tree/master/tests). We even created a tool called [`pytest-mypy-plugins`](https://github.com/typeddjango/pytest-mypy-plugins) ([announcing post](https://sobolevn.me/2019/08/testing-mypy-types)) to help us with this task. Maybe it will be also helpful to you as well. 

That's how the simplest test looks like:

```yaml
- case: compose_two_wrong_functions
  main: |
    from returns.functions import compose

    def first(num: int) -> float:
        return float(num)

    def second(num: float) -> str:
        return str(num)

    reveal_type(compose(first, second)(1))  # N: builtins.str*
```

In case you like - I can send a PR with the refactored tests. Or we can collaborate on this together. 

Ask any questions you have! 

P.S. This project was listed in [awesome-python-stubs](https://github.com/typeddjango/awesome-python-stubs) list.",2019-09-09 10:03:52,,Consider using pytest-mypy-plugins,"['01 - Enhancement', 'Static typing']"
14412,open,rth,"With `ndarray.astype(..., casting=""unsafe"")` float arrays with NaN/inf will be converted to `np.iinfo(dtype).min/max`,
```py
>>> np.array([1, np.nan], dtype=np.float32).astype(np.int32)
array([          1, -2147483648], dtype=int32)
>>> np.array([1, np.inf], dtype=np.float32).astype(np.int32)
array([          1, -2147483648], dtype=int32)
```
(there are also some inconsistencies there cf https://github.com/numpy/numpy/issues/6109). This is very bad in practical applications as output data will be wrong by orders of magnitude. Other [`casting` options](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.astype.html) simply disallow casting float to int.

At the same time, casting from `dtype=np.object` works as expected,
```py
>>> np.array([1, np.inf], dtype=np.object).astype(np.int32)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
OverflowError: cannot convert float infinity to integer
```

It could be useful to have some additional `casting` option allowing to convert int to float, but would error on NaN or inf. This could be done manually,
```py
if array.dtype.kind == 'f' and np.dtype(dtype).kind == 'i':
   # check for overflows and NaN
   _assert_all_finite(array)
result = array.astype(dtype)
```
but having it in numpy would be useful.

I also wonder if there is really a case when not raising on such conversions is meaningful (even with `casting='unsafe'`). For instance pandas does this conversions as expected (using numpy dtypes),
```py
>>> pd.Series([1, np.nan], dtype=np.float64).astype(np.int)
[...]
ValueError: Cannot convert non-finite values (NA or inf) to integer
```

### Numpy/Python version information:

```
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.16.4 3.7.3 (default, Mar 27 2019, 22:11:17) [GCC 7.3.0]
```
",2019-09-03 06:38:37,,"Raise errors on float->int conversions with NaN, inf ",['unlabeled']
14402,open,ewmoore,"It would be really nice if `np.fill_diagonal` could fill other diagonals besides the main diagonal.  This would match the `offset` argument of `np.diagonal.`

Shouldn't require much code to allow this.",2019-08-30 22:07:10,,Allow np.fill_diagonal to fill sub or super diagonals too,"['01 - Enhancement', 'component: numpy.lib']"
14397,open,tacaswell,"Also reported upstream at https://bugs.python.org/issue37980

With numpy 1.17.1 and python37 the following does not warn 

```python
sorted([1, 2], reverse=np.bool_(True))
```

but with py38

```
In [2]: sorted([1, 2], reverse=np.bool_(True))                                                                                                                  
<ipython-input-2-6726f33270df>:1: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index
  sorted([1, 2], reverse=np.bool_(True))
Out[2]: [2, 1]
```

This bisects to https://github.com/python/cpython/pull/11952 which use `__index__` in more places which is tripping the deprecation in a surprising place.

```python
sorted([1, 2], reverse=bool(np.bool_(True)))
```

Works in all cases.",2019-08-29 16:47:54,,regression in py38 with bool-as-index deprecation warning,['unlabeled']
14396,open,allComputableThings,"np.asarray believes sets are scalar:

```
>>> np.asarray(1).shape
()   # scalar
>>> np.asarray({1,2,3}).shape
()  # scalar
```

np.isscalar does not:
```
>>> np.isscalar(1)
True
>>> np.isscalar({1,2,3})
False
```

I'd argues that asarray should always returns an scalar if a scalar is passed, and a non-scalar if a non-scalar is passed. (to be consistent with its treatment when a plain number is passed in).

Related to #8538 ? ",2019-08-29 14:35:09,,asarray array disagrees with isscalar about what is a scalar (at least for sets),"['00 - Bug', 'component: numpy._core', '50 - Duplicate']"
14383,open,asosnovsky,"The idea is to use python's 3.6 new annotation syntax to define new dtypes.  (see example below)

```python
@np.dtype
class Point:
    x: np.float32
    y: np.float32

np.array( [2, 2], dtype=Point )
```

I implemented one potential way to do this in this [gist](https://gist.github.com/asosnovsky/8eb38ec46c1281bdc55f279800a66cc9), I think that some further discussion should be had prior to implementing this, but I believe it could make defining new structured dtypes more concise.",2019-08-27 21:23:02,,Using annotations to declare dtypes,['component: numpy.dtype']
14371,open,negedng,"<!-- Please describe the issue in detail here, and fill in the fields below -->
Similar to amin's new initial and where parameters, argmin should have options to mask too.
Thanks!

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
def argmin(a, initial=None, where=None):
    if where is None:
        return np.argmin(a)
    if initial is None:
        raise(""To use a where mask one has to specify 'initial'"")
    return np.argmin([a[i] if where[i] else initial for i in range(len(a))])
```",2019-08-27 03:48:21,,ENH: Adding where for argmin,"['23 - Wish List', 'component: numpy._core']"
14367,open,clbarnes,"There are ~3 entry points for people trying to do polynomial fitting. `numpy.polyfit` is referred to by practically all external documentation, but is not recommended as it is less numerically stable than the alternatives. `numpy.polynomial.polynomial.Polynomial`'s class method `fit` is the preferred entry point; it uses a more numerically stable method and provides additional API convenience. `numpy.polynomial.polynomial.polyfit` uses the same method as `Polynomial`.

`numpy.polyfit` is the oldest, mathematically worst, and least recommended option. However, it still has capabilities which the others do not: specifically, the ability to return the covariance matrix (`cov=False`) argument, and an equivalent which works on masked arrays (`np.ma.extras.polyfit`).

https://github.com/numpy/numpy/pull/13601 more thoroughly documents the fact that `numpy.polyfit` is not recommended, but if it's to be deprecated-by-docs, we should at least make it possible to replicate all of its functionality using the recommended API. 

### To do

- [ ] masked array implementation
- [ ] get covariance matrix from `polyfit`/`fit`

Issue requested by @mhvk https://github.com/numpy/numpy/pull/13601#issuecomment-494544751",2019-08-26 20:55:11,,Round out Polynomial.fit method,"['01 - Enhancement', 'component: numpy.polynomial']"
14332,open,petarmhg,"Using the einsum `optimal` flag over the `greedy` flag seems to actually slow down einsum, at least in this case:
![optimal_einsum](https://user-images.githubusercontent.com/16225848/63525194-45cd5300-c4cb-11e9-87ac-6ab7cbfbbdd4.png)

![greedy_einsum](https://user-images.githubusercontent.com/16225848/63525263-62698b00-c4cb-11e9-9378-1391c102ba92.png)

An ~8x slowdown seems quite problematic. The slowdown in the `optimal` einsum seems to come from the third contraction, but I am unsure why exactly it is so costly.

Imposing a memory cap results in an einsum run faster than both `greedy` and `optimal`:
![memory_optimal_einsum](https://user-images.githubusercontent.com/16225848/63525466-c0966e00-c4cb-11e9-9867-07a22911fdb7.png)

I would have chalked up the difference to einsum not being able to fit intermediary arrays in memory, but running the same code on more computers with more memory yields similar results (i.e. `(""optimal"", 10**7)` is faster than `greedy` is faster than `optimal`).

Also, the largest intermediate array for both the `greedy` and `optimal` algorithms are the same size, and so I can't see why memory would be the cause of the slowdown.

Relevant code in text:
```python
import numpy as np

x = np.random.randn(100, 300, 75, 10)
w = np.random.randn(100, 300, 3)
At = np.random.randn(8, 10)
G = np.random.randn(10, 3)
Bt = np.random.randn(10, 10)

np.einsum(""mn,nr,fcr,nh,octh->oftm"", At, G, w, Bt, x, optimize=""optimal"")
np.einsum(""mn,nr,fcr,nh,octh->oftm"", At, G, w, Bt, x, optimize=""greedy"")
np.einsum(""mn,nr,fcr,nh,octh->oftm"", At, G, w, Bt, x, optimize=(""optimal"", 10**7))
```

### Numpy/Python version information:

1.17.0 3.7.3 (default, Mar 27 2019, 16:54:48) 
[Clang 4.0.1 (tags/RELEASE_401/final)]",2019-08-22 19:41:00,,Einsum `optimal` substantially slower than `greedy`,['component: numpy.einsum']
14331,open,hameerabbasi,"(Original issue copied from dask/dask#4632)

Would it be possible to try `__array_interface__[""typestr""]` when calling `np.dtype(obj)`?

```python
>>> import numpy as np
>>> from xnd import array
>>> x = array([1, 2, 3])
>>> dt = np.dtype(x)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: data type not understood
>>> x.__array_interface__['typestr']
'=q'
```

From the original issue:

> Would it be possible to try `x.__array_interface__[""typestr""]` in situations like the following?
> 
> I would prefer `xnd.array.dtype` to return `ndt()`, so it needs to be translated for NumPy:
> 
> ```
> >>> from xnd import array
> >>> import dask.array as da
> >>> 
> >>> x = array([1,2,3])
> >>> x.dtype
> ndt(""int64"")
> >>> x.__array_interface__[""typestr""]
> '=q'
> >>> 
> >>> d = da.from_array(x, chunks=(5,), asarray=False)
> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""/home/stefan/dbg/lib/python3.8/site-packages/dask/array/core.py"", line 2096, in from_array
>     chunks = normalize_chunks(chunks, x.shape, dtype=x.dtype)
>   File ""/home/stefan/dbg/lib/python3.8/site-packages/dask/array/core.py"", line 1846, in normalize_chunks
>     dtype = np.dtype(dtype)
> TypeError: data type not understood
> ```",2019-08-22 18:12:19,,"Try __array_interface__[""typestr""] when calling np.dtype(obj)",['15 - Discussion']
14317,open,fj128,"If we use an ndarray with a non-default dtype, take an empty slice and attempt to add an empty range to it:

### Reproducing code example:

```python
import numpy as np
v = np.zeros(5, dtype=np.int32)
v[0 : 0] += range(0)
```

### Error message:

    Exception has occurred: UFuncTypeError
    Cannot cast ufunc 'add' output from dtype('float64') to dtype('int32') with casting rule 'same_kind'

Non-empty slice and range works obviously, assignment instead of += works, using default dtype works, np.arange works.

I checked performance and np.arange seems to be about a hundred times faster for any reasonably sized arrays (up to and including 10 million elements where `range` version becomes unbearably slow) anyways, so everyone should use that. It's still faster even for very small arrays.

But the error is pretty confusing, especially since as you can imagine zero-length slices arise in pretty confusing corner cases to begin with.

Also, it would be nice to have numpy.xarange or something, or mention that even arange is way faster than `range` in the documentation.

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.17.0 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)]
",2019-08-21 15:41:51,,"Adding an empty range() to an empty slice throws ""Cannot cast ufunc 'add' output ...""","['01 - Enhancement', '23 - Wish List', '15 - Discussion']"
14294,open,endolith,"[`@lru_cache`](https://docs.python.org/3/library/functools.html#functools.lru_cache) doesn't work with numpy arrays because they're not hashable, but often a slow function takes a numpy array as input and outputs something without mutating the input array, and it would be nice if those functions could be memoized.  

I'm hacking around this by caching based on input.tobytes() and input.shape, but it would be nice if there were an official way built into numpy.  I know there are third-party solutions, too, such as:

* https://gist.github.com/dpo/1222577
* http://numpy-discussion.10968.n7.nabble.com/memoization-with-ndarray-arguments-tp20264p20266.html

### Numpy/Python version information:

1.16.2 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]

",2019-08-18 14:50:35,,ENH: Decorator to cache functions that take numpy arrays,"['01 - Enhancement', '23 - Wish List']"
14277,open,Sohl-Dickstein,"If the argument `shift` is an array (or similar datatype), but `axis` is None or a scalar, then the behavior of np.roll is undefined by the docs. In practice, np.roll silently does unintuitive things rather than throwing an error. For instance, running
``` python
x = np.array([[1,2],[3,4]])
print('x\n', x)
y = np.roll( x, [-1,1] )
print('y\n', y)
y_expected = np.roll( x, [-1,1], (0,1) )
print('y_expected\n', y_expected)
z = np.roll( x, [1,2,-3] )
print('z\n', z)
```
produces output
> x
>  [[1 2]
>  [3 4]]
> y
>  [[1 2]
>  [3 4]]
> y_expected
>  [[4 3]
>  [2 1]]
> z
>  [[1 2]
>  [3 4]]
Here, `z` is identical to `y` is identical to `x`. However, a naive expectation would be that `y` would produce the same output as `y_expected`, and that `z` would generate an error.

The numpy documentation states that if `shift` is a tuple, then `axis` must be a tuple of the same length. I believe the easiest fix would be to raise an error if `shift` and `axis` have different lengths. ",2019-08-15 03:15:41,,np.roll produces unexpected behavior when shift is a tuple but axis is not,"['00 - Bug', 'component: numpy.lib']"
14254,open,raamana,"This is a feature request related to encoding missing values in user data. So far we have used `np.NaN` to indicate when a values is missing. However, it doesn't really capture a lot of info around these missing values. Info that is crucial in advanced biostats research projects such as type of missingness (random or not), reasons why they are missing, and different codes for different combinations for these factors. Hence, I am wondering if there is a possibility to avoid the poor man's mask of NaNs with a richer representation via additional special values in numpy, in addition to `Inf` and `NaN`.

While masked arrays do help to *process* numpy arrays with missing data, they do not help *represent* them in their full complexity.

Some possibilities include: `np.MaR` (missing at random), `np.MaCR`, `np.MNaR` etc which can take encoding value, such as 9999, -1, 888 etc depending on what the user/domain deems appropriate for their application. They can all be internally reduced to `np.NaN` for arithmetic purposes, when in undefined situations.

Thanks for considering this and sharing your thoughts.

",2019-08-11 16:05:58,,ENH: Builtin code for missing values,['component: numpy.ma']
14243,open,jklymak,"Looking at extending the support for `datetime64` in matplotlib, in particular to allow plotting times that are outside years 0001-9999.  Lots of science applications have millennial time scales, so being able to have support for a large time range is desirable, and is a definite strength of `datetime64`.

However, our tick formatters use `datetime.strftime`, and allow all the formatting strings for those (i.e. %Y, %y, %x, %g etc).  However, we can't convert `datetime64` to `datetime` for years outside 0001-9999 so we can't use strftime (directly). 

Is there any way to get what we want?  We can do a song and dance to force a `datetime64` to be between 0001-9999 by adding or subtracting multiples of 400 years, and then reformatting just the year part of the string, but it would be nice if there were something more robust.  Not sure if `datetime_as_string` could be enhanced to take a `fmt=str` optional kwarg?",2019-08-09 20:09:48,,datetime64: strftime ,"['01 - Enhancement', 'component: numpy.datetime64']"
14239,open,Zac-HD,"We found this one with Hypothesis, HypothesisWorks/hypothesis#1963, and I added a workaround at the PyCon sprints but only now got around to opening an upstream issue.

Fields with the empty string as their name are re-named `f{index}`; it would be a sufficient fix to detect if there is another field with that as an explicit name and choose something else.  The idea would be that the `field ... occurs more than once` would *only* occur if a name was actually specified more than once by the user.

Defaulting to `f{index_of_other_use}` would be confusing, and does not cover multiple or multi-step collisions.  Any other deterministic name might have to consider number-of-fields options; imagine for example the rule ""increment the field number"" on names `"""", ""f0"", ""f1"", ...`.

I would therefore also be happy with an improved error message, that `The field at index _x_ is named """", which defaults to ""f_x_"", but this name is already in use at index _y_.`.  

### Reproducing code example:

```python
import numpy as np

# raises `ValueError: field 'f0' occurs more than once`
np.dtype([("""", int), (""f0"", int)])

# Forecloses the easy ""swap the names"" solution
np.dtype([("""", int), (""f0"", int), (""f1"", int)])
```

### Numpy/Python version information:

Numpy 1.14.5 and 1.17.0; Python 3.7 and earlier.",2019-08-09 07:08:40,,"ValueError for `np.dtype([("""", int), (""f0"", int)])`",['unlabeled']
14225,open,pts314,"I'm processing a number of csvs in a single script, and sometimes csvs will be empty, which causes a StopIteration exception. This is not serious, but I think it would be helpful if one or more of the following happened:
1) This simply be documented: people should know that this happens so they can wrap the section in a try catch block
2) The exception should be caught and re-passed with a more intuitive name, something like InvalidFile
3) A new flag should be added/old flag expanded to optionally silence the exception and return an empty array

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->


```python
import numpy as np
import os
inFile = ""/tmp/example.csv""
os.system(f""touch {inFile}"")
my_data = np.genfromtxt(inFile, delimiter=',', skip_header=1, invalid_raise=false)
```
### Error message:

<!-- Full error message, if any (starting from line Traceback: ...) -->
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/#####/lib/python3.6/site-packages/numpy/lib/npyio.py"", line 1780, in genfromtxt
    next(fhd)
StopIteration

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.17.0 3.6.3 (default, Apr 24 2018, 18:36:28)
[GCC 4.7.2]

",2019-08-07 20:31:43,,numpy.genfromtxt raises StopIteration error on empty file,['unlabeled']
14208,open,talkaminker,"<!-- Please describe the issue in detail here, and fill in the fields below -->
elementwise == on numpy array with objects returns wrong answer if array has a ""\x00"" in it

### Reproducing code example:
```python
import numpy as np
a = np.array([""\x00""], dtype=object)
a==""\x00""
>>> array([False])
a = np.array([""\x01""], dtype=object)
a==""\x01""
>>> array([ True])
```

Notes: wrong answer is given even if using bytes (`b""\x00""` instead of strings)
`dtype=object` is important (otherwise the answer is correct)

### Numpy/Python version information:
1.15.4 3.7.1 (default, Oct 28 2018, 08:39:03) [MSC v.1912 64 bit (AMD64)]
Tested aslo on python 2.7.18 on linux on numpy 1.16.4 got the same bad results
",2019-08-06 11:13:25,,"wrong answer in elementwise == with ""\x00"" string",['00 - Bug']
14168,open,rschiewer,"When using a nditer with any kind of index flag like 'c_index', 'f_index' or 'multi_index', the index will always be zero if the 'buffered' flag is used as well. If this is intended behavior, it does not seem to be documented though.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
a = np.arange(4*4*3).reshape(4,4,3)
it = np.nditer(a, flags=['multi_index', 'buffered'])
while not it.finished:
    print(it.multi_index)
    it.iternext()
```

### Numpy/Python version information:

**numpy version:** 1.17.0
**python version:** 3.7.4 (default, Jul  9 2019, 18:13:23) [Clang 10.0.1 (clang-1001.0.46.4)]

",2019-07-31 12:11:43,,nditer usage with index and buffered flags,"['00 - Bug', 'component: numpy._core']"
14119,open,aheays,"Creating an empty array of object dtype is twenty times slower for a recarray than a normal array of the same size.  This is a limiting operation in my particular use case and I hope might be correctable.

### Reproducing code example:

```python
import numpy as np
import time
timer = time.time()
np.empty([10000000],object)
print('Time elapsed large array:   ',format(time.time()-timer,'12.6f'))
timer = time.time()
np.empty([10000000],[('x',object)])
print('Time elapsed large recarray:',format(time.time()-timer,'12.6f'))
```
### Numpy/Python version information:

1.16.2 3.7.3 (default, Apr  3 2019, 05:39:12) 
[GCC 8.3.0]
",2019-07-25 17:17:59,,Slow creation of empty recarray with object dtype,"['01 - Enhancement', 'Priority: low']"
14104,open,petrovlesha,"I am trying to convert integer columns from structured array to unstructured. But it fails at `_view_is_safe` func, which just check `hasobject` property that is not changed. It worked on 1.13.3 but broken after update to 1.16.0.

### Reproducing code example:

```python
import numpy as np
array = np.ones((100,), dtype=[('user', np.object), ('item', np.float), ('value', np.float)])
array[['item','value']].view((np.float, 2))
```
or
```python
from np.lib.recfunctions import structured_to_unstructured
structured_to_unstructured(array[['item','value']])

```


### Error message:

```
  File ""/home/petrovalex/.virtualenvs/np/local/lib/python2.7/site-packages/numpy/core/_internal.py"", line 494, in _view_is_safe
    raise TypeError(""Cannot change data-type for object array."")
TypeError: Cannot change data-type for object array.
```




### Numpy/Python version information:
`('1.16.4', '2.7.3 (default, Oct 26 2016, 21:01:49) \n[GCC 4.6.3]')`
<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2019-07-24 15:11:40,,Broken dtype hasobject check,"['00 - Bug', '15 - Discussion', '06 - Regression', 'component: numpy.dtype']"
14103,open,charris,"See comment at #14091 .

<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
<< your code here >>
```

<!-- Remove these sections for a feature request -->

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2019-07-24 14:51:55,,assert_array_almost_equal_nulp does not deal with NaN.,['unlabeled']
14090,open,david-cortes,"Since there is no absolute minimum for timestamp, if I want to make a comparison that would always return more/less, my first thought is to use infinite, which works for pretty much every other dtype (and makes for clear loop procedure initialization):
```python
import numpy as np
np.datetime64(""2019-01-01"") > (-np.inf)
```
```
TypeError                                 Traceback (most recent call last)
<ipython-input-6-df4bfdeeac74> in <module>
----> 1 np.datetime64(""2019-01-01"") > (-np.inf)

TypeError: invalid type promotion
```

-----

Contrast with R:
```r
as.Date(""2019-01-01"") > (-Inf)
```
```
TRUE
```
```r
as.POSIXct(""2019-01-01"") > (-Inf)
```
```
TRUE
```",2019-07-23 20:03:10,,Cannot compare timestamp against infinite,"['15 - Discussion', 'component: numpy.datetime64']"
14066,open,david-cortes,"**Steps to reproduce:**
* Create a `datetime64` object.
* Convert said object to its numeric representation.
* Try to convert it back to `datetime64`.

**Expected behavior:** should convert it back.

**Actual behavior:** throws error.

```python
import numpy as np
np.datetime64(""2019-01-01"").astype(""datetime64[s]"").astype(int)
>>> 1546300800
np.datetime64(1546300800, ""s"")
>>> numpy.datetime64('2019-01-01T00:00:00')
np.datetime64(np.datetime64(""2019-01-01"").astype(""datetime64[s]"").astype(int), ""s"")
```
```
ValueError: Could not convert object to NumPy datetime
```",2019-07-20 14:48:13,,Cannot convert timestamp to number back and forth,"['component: numpy.dtype', 'component: numpy.datetime64']"
14020,open,mrocklin,"I would like for there to be an easy way to convert a `numpy.ndarray -> numpy.ndarray` Python function into a generalized universal function (gufunc).

### Motivation

In many cases we have a Python function that operates on a multi-dimensional array and produces another multi-dimensional array.  As an example, consider the Scikit-Image function, [`skimage.feature.canny`](https://scikit-image.org/docs/dev/auto_examples/edges/plot_canny.html), which consumes and produces a 2d NumPy array.

```python
raw_image = np.random.random((1000, 1000))
processed_image = skimage.feature.canny(raw_image)
```

If I have a stack of these images I would like for the same function to broadcast across extra dimensions.


```python
raw_stack = np.random.random((10, 1000, 1000))
processed_stack = skimage.feature.canny(raw_stack)  # this doesn't work today
```

This logic could be placed inside each function, but it would be nice to implement it it once and then decorate functions in the future

```python
@numpy.guvectorize(signature=""(n, m) -> (n, m)"")  # or something like this
def canny(...):
    ...
```

### GUFuncs

The behavior that I want there is exactly the behavior of GUFuncs, but currently they're hard to construct, except with CPython or with Numba.

In many cases a pure Python decorator that just used for loops would be welcome here.  The performance hit of using Python here is relatively low because the function that we're looping over is fairly costly (often 100s of milliseconds).  This is the sort of decorator that a project like Scikit-Image (cc @jni and @stefanv ) or ITK (cc @thewtex) would probably be happy to use.

From a Dask perspective this would be great because we can handle gufuncs nicely, rechunking user-provided Dask arrays so that all core dimensions are single-chunked and then automatically and lazily applied across broadcast dimensions.

This conversation came out of the SciPy conference, where @seberg and @mattip discussed it.",2019-07-15 22:26:26,,Turn a Python function into a GUFunc,"['01 - Enhancement', 'Project']"
14018,open,2sn,"Trying to add a non-scalar field to a record array fails; the error messages seem to indicate to me that something internal goes wrong by double-adding extra dimensions.

### Reproducing code example:

```python
import numpy as np
from numpy.lib import recfunctions as rfn

x = np.recarray(12, dtype = np.dtype([(""A"", "">3f8"")]))
B = np.ndarray((12,), dtype='>2f8')
rfn.rec_append_fields(x,'B',B,'>2f8')
```

### Error message:

the first `rfn` line gives

```python
ValueError                                Traceback (most recent call last)
<ipython-input-60-9757c3508aea> in <module>
----> 1 rfn.rec_append_fields(x,'B',B,'>2f8')

~/Python/lib/python3.7/site-packages/numpy/lib/recfunctions.py in rec_append_fields(base, names, data, dtypes)
    785     """"""
    786     return append_fields(base, names, data=data, dtypes=dtypes,
--> 787                          asrecarray=True, usemask=False)
    788 
    789 

~/Python/lib/python3.7/site-packages/numpy/lib/recfunctions.py in append_fields(base, names, data, dtypes, fill_value, usemask, asrecarray)
    729                 raise ValueError(msg)
    730         data = [np.array(a, copy=False, subok=True, dtype=d).view([(n, d)])
--> 731                 for (a, n, d) in zip(data, names, dtypes)]
    732     #
    733     base = merge_arrays(base, usemask=usemask, fill_value=fill_value)

~/Python/lib/python3.7/site-packages/numpy/lib/recfunctions.py in <listcomp>(.0)
    729                 raise ValueError(msg)
    730         data = [np.array(a, copy=False, subok=True, dtype=d).view([(n, d)])
--> 731                 for (a, n, d) in zip(data, names, dtypes)]
    732     #
    733     base = merge_arrays(base, usemask=usemask, fill_value=fill_value)

ValueError: could not broadcast input array from shape (12,2) into shape (12,2,2)
```

This is the bug.

### Alternative attempts:

```python
import numpy as np
from numpy.lib import recfunctions as rfn

x = np.recarray(12, dtype = np.dtype([(""A"", "">3f8"")]))
B = np.ndarray((12,), dtype='>2f8')
rfn.rec_append_fields(x,'B',B,'>f8')
```
gives
```python
ValueError                                Traceback (most recent call last)
<ipython-input-61-a921ddea4a34> in <module>
----> 1 rfn.rec_append_fields(x,'B',B,'>f8')

~/Python/lib/python3.7/site-packages/numpy/lib/recfunctions.py in rec_append_fields(base, names, data, dtypes)
    785     """"""
    786     return append_fields(base, names, data=data, dtypes=dtypes,
--> 787                          asrecarray=True, usemask=False)
    788 
    789 

~/Python/lib/python3.7/site-packages/numpy/lib/recfunctions.py in append_fields(base, names, data, dtypes, fill_value, usemask, asrecarray)
    742         dtype=get_fieldspec(base.dtype) + get_fieldspec(data.dtype))
    743     output = recursive_fill_fields(base, output)
--> 744     output = recursive_fill_fields(data, output)
    745     #
    746     return _fix_output(output, usemask=usemask, asrecarray=asrecarray)

~/Python/lib/python3.7/site-packages/numpy/lib/recfunctions.py in recursive_fill_fields(input, output)
     74             recursive_fill_fields(current, output[field])
     75         else:
---> 76             output[field][:len(current)] = current
     77     return output
     78 

~/Python/lib/python3.7/site-packages/numpy/ma/core.py in __setitem__(self, indx, value)
   3327         elif not self._hardmask:
   3328             # Set the data, then the mask
-> 3329             _data[indx] = dval
   3330             _mask[indx] = mval
   3331         elif hasattr(indx, 'dtype') and (indx.dtype == MaskType):

ValueError: could not broadcast input array from shape (12,2) into shape (12)
```
as should be expected.

### Numpy/Python version information:

1.16.4 3.7.4 (default, Jul  9 2019, 22:41:57) 
[GCC 9.1.1 20190503 (Red Hat 9.1.1-1)]
Python 3.7.4 (default, Jul  9 2019, 22:41:57) 
Python 7.6.1 -- An enhanced Interactive Python. Type '?' for help.


<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2019-07-15 18:33:35,,recarray adding multi-dimensional fields,['00 - Bug']
14012,open,KreativeKrise,"My application uses the python package gensim, which requires numpy. I want to run some unit tests, but always get `RuntimeError: implement_array_function method already has a docstring`. It was tested on Windows 10 and Docker with python 3.7.4. The problem seems to exist since numpy 1.16.0. If I use numpy 1.15.4 it works without problems.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import unittest
import numpy as np


class StartUpTest(unittest.TestCase):

    def setUp(self):
        return

    def testDefaultEndpoint(self):
        True
```

<!-- Remove these sections for a feature request -->

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

```
Traceback (most recent call last):
  File ""C:\Users\kreativekrise\AppData\Local\Programs\Python\Python37-32\lib\unittest\loader.py"", line 436, in _find_test_path
    module = self._get_module_from_name(name)
  File ""C:\Users\kreativekrise\AppData\Local\Programs\Python\Python37-32\lib\unittest\loader.py"", line 377, in _get_module_from_name
    __import__(name)
  File ""D:\work\projects\myapp\test\startup_test.py"", line 2, in <module>
    import numpy as np
  File ""C:\Users\kreativekrise\AppData\Local\Programs\Python\Python37-32\lib\site-packages\numpy\__init__.py"", line 142, in <module>
    from . import core
  File ""C:\Users\kreativekrise\AppData\Local\Programs\Python\Python37-32\lib\site-packages\numpy\core\__init__.py"", line 40, in <module>
    from . import multiarray
  File ""C:\Users\kreativekrise\AppData\Local\Programs\Python\Python37-32\lib\site-packages\numpy\core\multiarray.py"", line 12, in <module>
    from . import overrides
  File ""C:\Users\kreativekrise\AppData\Local\Programs\Python\Python37-32\lib\site-packages\numpy\core\overrides.py"", line 46, in <module>
    """""")
RuntimeError: implement_array_function method already has a docstring
```

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

1.16.4 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)]",2019-07-15 12:35:06,,Unittest: RuntimeError: implement_array_function method already has a docstring,"['00 - Bug', 'component: numpy._core']"
13965,open,ritzvik,"<!-- Please describe the issue in detail here, and fill in the fields below -->
Indices raising and lowering are a very crucial part of differential geometry and particularly important in relativistic physics.

See https://en.wikipedia.org/wiki/Raising_and_lowering_indices

Example : 
  - A 4D tensor can have index configuration `ulll`, and can be changed to `lulu` with the help of a metric tensor, here `u` denotes contravariant index and `l` denotes covariant index.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
T = np.zeros(shape=(4,4,4,4), dtype=float)
# substitute some values in T
M = np.zeros(shape=(4,4), dtype=float)
# substitute some values in M(Metric Tensor)
T_ = np.change_config(T, metric=M, old='ulll', new='lulu')
print(T_)
# Example snippet
```",2019-07-11 07:42:36,,Feature Request: Support indices raising and lowering in N-dimensional numpy arrays,['01 - Enhancement']
13918,open,degasus,"Numpy gladly ships an interface file for SWIG, however, by default it only generates wrappers for 32 bit integers as the dimension type, which overflows after 2**31 elements:
https://github.com/numpy/numpy/blob/master/tools/swig/numpy.i#L3138

Even worse, the documentation suggests to use the 32 bit addressing: https://github.com/numpy/numpy/blob/master/tools/swig/README#L49

In my opinion, size_t or npy_intp should be the default DIM_TYPE instead of those fixed 32 bit integers. But I see that we still need to generate those 32 bit wrappers for a transition period.",2019-07-05 07:15:39,,SWIG: numpy.i uses by default the DIM_TYPE int32,['component: swig']
13898,open,mattip,"xref gh-13739

A negative shift is undefined in C. Python raises a ValueError, numpy returns the undefined values. We should at least warn, and probably raise an Error.

```
>>> a = np.arange(4)
>>> 10 << -1
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ValueError: negative shift count
>>> a << -1
array([-9223372036854775808,                    0, -9223372036854775808,
                          0])
```",2019-07-03 01:33:03,,BUG: shift with a negative number should fail,"['00 - Bug', 'component: numpy.ufunc']"
13878,open,jakobjakobson13,"In the loadtxt routine you have to explicitly specify all the columns you want to read in as soon as you want to discard one column. However, I normally use the first row and first column for labels. So my data looks similar to the following example:
```python
>>> import io
>>> import numpy as np
>>> a = "" \tsample1\tsample2\n280K\t0\t1\n300K\t2\t3""
>>> print(a)
 	sample1	sample2
280K	0	1
300K	2	3
```

To import the data to numy the only way that worked for me was the following command:
```python
np.loadtxt(io.StringIO(a), skiprows=1, usecols=(1, 2))
```

Unfortunately this becomes very unhandy for longer tables. So could you provide a solution similar to `skiprows` or an inverted selection of columns in a way like `usecols=(1:)` or something that way?

Regards",2019-07-01 09:26:09,,ENH: Allow loadtxt to ignore a single column,"['01 - Enhancement', 'component: numpy.lib']"
13872,open,rgommers,"I think this is a bug, but not 100% sure. The `ndarray.__array_function__` implementation seems special, it's not recognized when applying `array_function_dispatch` to a function outside of NumPy (which NEP 18 suggests is possible).

To try, I add the following lines to PyTorch at the end of `torch.__init__.py`:
```
def _sum_dispatcher(input, dtype=None):
    return (input, dtype)

_sum = sum
@_np.core.overrides.array_function_dispatch(_sum_dispatcher)
def sum(input, dtype=None):
    return _sum(input)  # don't worry about the missing `dtype` here, that's a torch issue
```

Then, I run the following (on 1.16.4 with the envvar enabled; need to rebuild to try master - EDIT: same for current master):
```
import numpy as np
import torch
import sparse
import dask.array

t = torch.Tensor([1, 2])
x = t.numpy()
s = sparse.as_coo(x)
d = dask.array.from_array(x)

print(""Sum of tensor t: "", torch.sum(t))
print(""Sum of dask array d: "", torch.sum(d))
# Okay, let's add a compute()
print(""Sum of dask array d (evaluated): "", torch.sum(d).compute())
print(""Sum of sparse array s: "", torch.sum(s))
print(""Sum of ndarray x: "", torch.sum(x))
```

This gives:
```
Sum of tensor t:  tensor(3.)
Sum of dask array d:  dask.array<sum-aggregate, shape=(), dtype=float32, chunksize=()>
Sum of dask array d (evaluated):  3.0
Sum of sparse array s:  3.0
Traceback (most recent call last):
  File ""try_torch_array_function.py"", line 18, in <module>
    print(""Sum of ndarray x: "", torch.sum(x))
  File ""/Users/rgommers/anaconda3/envs/pytorch/lib/python3.7/site-packages/numpy/core/overrides.py"", line 165, in public_api
    implementation, public_api, relevant_args, args, kwargs)
  File ""/Users/rgommers/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/__init__.py"", line 326, in sum
    return _sum(input)
TypeError: sum(): argument 'input' (position 1) must be Tensor, not numpy.ndarray
```

So it works fine with Dask and pydata/sparse, but fails with NumPy - the traceback indicates that the dispatch to `numpy.sum` is not happening at all. Not expected I think?",2019-06-30 03:36:21,,use __array_function__ on functions outside numpy,"['00 - Bug', 'component: __array_function__']"
13846,open,ZisIsNotZis,"I know this is not a big problem. Just mentioning here.

`sort` of a `memmap` (which uses different memory and is not a mmap anymore) is of type `memmap`.

I ""kind of"" remember one time that I got a readonly `memmap` with some non-inplace operaton, but this might not be true since I can't remember it clearly.

### Reproducing code example:

```python
import numpy as np
a = np.memmap('xxx.B', mode='r')
b = np.sort(a)
np.shares_memory(a, b), type(b), b.flags.writeable
```

### Output:

```
(False, numpy.memmap, True)
```

### Numpy/Python version information:

1.16.4 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) 
[GCC 7.3.0]",2019-06-27 05:14:02,,`np.sort` of `memmap` is still `memmap`,['unlabeled']
13836,open,ZisIsNotZis,"### Reproducing code example:

```python
a = np.ones(2**27, 'f')
%timeit np.log(a)
%timeit np.log2(a)
%timeit np.log10(a)

a = np.ones(2**27, 'd')
%timeit np.log(a)
%timeit np.log2(a)
%timeit np.log10(a)
```

### Output:

```
74.5 ms ± 68.5 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
484 ms ± 4.51 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
92.3 ms ± 37.6 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

181 ms ± 28.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
693 ms ± 499 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
186 ms ± 3.7 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

### Numpy/Python version information:

1.16.4 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) 
[GCC 7.3.0]

### Comparison

With my straight forward implementation of `log2`

```c
#include <math.h>
void f(int len, float* x, float* out){
	for(int i=0; i<len; i++)
		out[i] = log2f(x[i]);
}
void d(int len, double* x, double* out){
	for(int i=0; i<len; i++)
		out[i] = log2(x[i]);
}
```

```python
from ctypes import CDLL, c_uint64
log2f = CDLL('tmp/log2.so').f
log2  = CDLL('tmp/log2.so').d

a   = np.ones(2**27, 'f')
out = np.empty(2**27, 'f')
%timeit log2f(len(a), c_uint64(a.ctypes.data), c_uint64(out.ctypes.data))

a   = np.ones(2**27, 'd')
out = np.empty(2**27, 'd')
%timeit log2(len(a), c_uint64(a.ctypes.data), c_uint64(out.ctypes.data))
```

The result shows

```
239 ms ± 142 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)

421 ms ± 676 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

So I feel something must be wrong inside `np.log2` that make it slower than a naive single-threaded implementation? Theoretically it should not be slower than `np.log10` anyway
",2019-06-26 02:56:17,,`log2` way slower than `log` or `log10`,"['01 - Enhancement', 'component: numpy.ufunc']"
13835,open,eric-wieser,"As touched upon in in #13797 

`matrix` was a collection of behaviors, some good and some bad:
* `.I` and `.H` attributes. These were handy.
* `*` means `matmul` - not inherently bad, but perhaps surprising, and certainly less necessary in the presence of `@`. I think there might still be some value in allowing ""multiply (`*`)"" on matrix objects to mean what `@` does on array objects - it allows them to be dropped into some equations in place of scalars
* Everything is always exactly 2d. This was the cause of almost all the pain within numpy when handling subclasses - it broke liskov substitution

I think there might be a good use case for a matrix with some subset of the following rules:
* Construction fails if the input array is not `ndim >= 2`
* Helpers for (stacks of) row and column vectors:
  * `matrix.rowvec(rows) == matrix(rows[..., None, :])`
  * `matrix.colvec(cols) == matrix(cols[..., :, None])`
* Simpler indexing rules:
  * Shape of the returned array always matches `ndarray`
  * `m[..., i, j]`, where `i` and `j` index and preserve the last two dimensions, returns a `matrix` object. Any type of indexing that discards the last two dimensions downcasts to `array`
* `.T` means `linalg.transpose(self)`, `.H` means `.T.conj()`
* `.I`  either means `linalg.inverse(self)` or perhaps an object with `__mul__ = linalg.solve`

Such as class would not be compatible with `np.matrix`, but it might be a good candidate for a new `numpy/matrix` package.",2019-06-26 02:34:48,,A replacement for `np.matrix`,"['01 - Enhancement', 'component: numpy.linalg']"
13831,open,jakirkham,"Opening this issue after some discussion with @shoyer, @pentschev, and @mrocklin in issue ( https://github.com/dask/dask/issues/4883 ). AIUI [this was discussed in NEP 22]( http://www.numpy.org/neps/nep-0022-ndarray-duck-typing-overview.html#principle-3-focus-on-protocols ) (so I'm mainly parroting other people's ideas here to renew discussion and correct my own misunderstanding ;).

It would be useful for various downstream array libraries to have a function to ensure we have some duck array (like `ndarray`). This would be somewhat similar to `np.asanyarray`, but without the requirement of subclassing. It would allow libraries to return [their own (duck) array type]( https://github.com/dask/dask/issues/4883#issuecomment-501470895 ). If no suitable conversion was supported by the object, we could fallback to handle `ndarray` subclasses, `ndarray`s, and coercion of other things (nested lists) to `ndarray`s.

cc @njsmith (who coauthored NEP 22)",2019-06-25 11:58:11,,Supporting duck array coercion,"['01 - Enhancement', 'component: numpy._core']"
13826,open,goerz,"The `numpy.vectorize` documentation states ""The docstring is taken from the input function to `vectorize` unless it is specified"". However, `vectorize` is implemented in numpy as a class (with a `__call__` method to make the result function-like), and sets `__doc__` as an *instance* attribute. The same probably applies to other `ufunc`-based features in numpy. Unfortunately, none of Python's documentation systems have support for instance docstrings, or at least not `pydoc` (the built-in `help` or `?`/`??` in IPython) and Sphinx. As a results, `help` shows the *class* docstring, which is the documentation of `numpy.vectorize`, not the *instance* docstring of the vectorized function. See http://numpy-discussion.10968.n7.nabble.com/numpy-vectorize-docstrings-not-shown-by-help-command-tt38551.html and https://stackoverflow.com/questions/29577849/docstring-mismatch-using-numpys-vectorize.
Thus, while numpy's documentation is correct to the letter (the `__doc__` attribute is preserved), relying on non-existent support for instance docstrings makes the feature rather pointless.

I do think that `pydoc`/Sphinx *should* have support for instance docstrings (although I wasn't able to find any open issue indicating that such a feature is currently planned). However, even if this support were to be added, numpy's implementation of `vectorize` and other `ufunc`-based features is still questionable: Sphinx does categorization when generating the documentation, see https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html, distinguishing between classes, functions, exceptions, data, etc. The problem with `numpy.vectorize` is that it moves the decorated function from the ""function"" category to the ""data"" category, which (depending on the details of how Sphinx is used) likely causes the vectorize-decorated functions to not be included in the documentation at all.

Thus, I would strongly recommend that numpy should not use classes as function decorators: all this breakage of the documentation systems can be avoided by implementing the decorator as a function with the appropriate closures instead.",2019-06-24 17:52:56,,vectorize does not play well with pydoc and Sphinx,['unlabeled']
13810,open,jakobjakobson13,"### Reproducing code example:
Given that you have data in the following file `test_data.csv`:
```csv
0,1   0,2
0,3   0,4
```
`np.loadtxt` can't load the data directly as it does not support a comma as a decimal separator 
```python3
import numpy as np
np.loadtxt(""test_data.csv"")
```
### Error message:
```bash
ValueError: could not convert string to float: b'0,1'
```

### Background for this feature request
Many countries use a comma as a decimal separator (see [https://en.wikipedia.org/wiki/Decimal_separator](https://en.wikipedia.org/wiki/Decimal_separator)). So if a user from that country has a computer with local language setting, LibreOffice and other software (that respects the local language settings) will save/export numerical data with a comma instead of a point as a digit separator.

To make live easier for them an option to switch between a dot and a comma for the decimal separator would be great. Thanks
",2019-06-20 16:28:02,,ENH: Support comma decimal separator in loadtxt,"['23 - Wish List', 'component: numpy.lib', 'defunct — difficulty: Intermediate']"
13809,open,jabl,"Hello,

this is a heads-up based on a quick perusal of the numpy LAPACK calling code.

The R developers recent discovered fishy behavior in some functionality depending on LAPACK. It turned out to be due to them not passing the string length argument to LAPACK procedures expecting `CHARACTER(len=1)` arguments. Based on a quick look the numpy LAPACK interface commits the same sin.  For more information see

https://gcc.gnu.org/bugzilla/show_bug.cgi?id=90329

and for CBLAS and LAPACKE (which it seems numpy is not using, but seems to have become the catch-all issue on github): 

https://github.com/Reference-LAPACK/lapack/issues/339",2019-06-20 16:03:18,,C Fortran ABI issues with character arguments,"['00 - Bug', 'component: numpy.linalg', 'component: numpy._core', 'component: numpy.f2py']"
13797,open,sclel016,"Hi,

For people doing signal processing, a concise way to express the Hermitian Transpose would lead to more readable code. Currently, the syntax for a Hermitian transpose of an array is

`A.conj().T`

The syntax used in the matrix class `.H` should be ported to numpy.array. A similar comment could be made for the matrix inverse and `.I`.

ref: [StackExchange](https://stackoverflow.com/questions/26932461/conjugate-transpose-operator-h-in-numpy)
",2019-06-17 14:24:00,,Hermitian Transpose Syntax,"['23 - Wish List', 'component: numpy._core', 'Project']"
13779,open,nedclimaterisk,"It would be really nice if numpy's datetime64's had convenience properties for `year`, `month`, `day` (of month), `hour`, `minute`, `second`, each of which would return an integer value. `dayofweek` and `dayofyear` etc would also be nice.

Pandas' DatetimeIndex does this, and it's really useful, but it's not great to have to convert data just to use that.


",2019-06-14 02:54:03,,"ENH: year, month, day convenience properties on datetime64","['01 - Enhancement', '23 - Wish List', 'component: numpy.datetime64']"
13768,open,clbarnes,"A number of libraries try to replicate numpy's handling of array slices to one extent or another, but it's long and fidgety (and not quick) when you take into account slices, integers, ellipses, new axes, and tuples thereof. I would love to see a function like this:

```python
from typing import Union, Tuple, TypeVar

SliceLike = Union[ellipsis, slice, int, None]
SliceArgs = TypeVar(""SliceArgs"", SliceLike, Tuple[SliceLike])

def rationalise_indices(
    getitem_arg: SliceArgs, array_shape: Tuple[int]
) -> Tuple[Tuple[slice], List[int]]:
    # insert magic here
    return slices, axes_to_squeeze
```

This function would:

- Realise implied ellipses
- Replace `None`s in `slice` objects with `0`, `array_shape[axis]` and `1` respectively
- Replace negative integers, including those in `slice` objects, with their positive equivalent
- Ensure there is only one `Ellipsis`, and replace it with the correct number of non-`None` `slice` objects

There might be better output types than a tuple of slices, as that doesn't capture new axes and you need another step to figure out if there are any 0-length dimensions.

I have an implementation of something like this but it's ugly, doesn't handle new axes or negative strides, and almost certainly is not correct in other ways: https://gist.github.com/clbarnes/b6a3d15b20bdff1f11a2043138a564f9",2019-06-12 21:06:43,,"ENH, API: provide a canonical implementation of array getitem argument handling as an API","['01 - Enhancement', '23 - Wish List', '30 - API']"
13765,open,degrootc,"When attempting to create a view of a indexed structured array, in order to create a a standard ndarray, the view method does not appropriately resize the output array.  It appears to instead preserve the size of the full structured array.

I have two examples in the code:
1) Uses the prescribed methodology from https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.view.html
2) Uses column stack to combine each column slice

As you can see, example #1 is the incorrect size and has garbage from memory in the middle column.  Example #2 works as expected.

### Reproducing code example:
```python
import numpy as np
test_arr = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12)],
                    dtype=[('a', np.int8), ('b', np.int8), ('c', np.int8)])

temp_arr = (test_arr[['a', 'c']]
            .copy()
            .view(np.int8)
            .reshape(test_arr.shape + (-1,)))

print('using view:\n', temp_arr)

temp_arr = None
for col_name in ['a', 'c']:
    temp_field_arr = test_arr[col_name]

    if temp_arr is None:
        temp_arr = temp_field_arr.copy()
    else:
        temp_arr = np.column_stack((temp_arr,
                                    temp_field_arr.copy()))

print('using column_stack:\n', temp_arr)
```
### Numpy/Python version information:
python 3.7.3
numpy 1.16.4

import sys, numpy; print(numpy.__version__, sys.version)
1.16.4 3.7.3 (default, Mar 27 2019, 22:11:17)
[GCC 7.3.0]
",2019-06-12 17:56:47,,view does not work on indexed structured arrays,"['00 - Bug', '50 - Duplicate']"
13738,open,david-cortes,"**Steps to reproduce:**
* Create a two-dimensional numpy array.
* Convert the array to Fortran order (ordered by columns vs. the default order by rows).
* Subset the array with a boolean mask.

**Expected behavior:** should output the subsetted array with the same order as the original input array.

**Actual behavior:** outputs a C array (ordered by rows).

Example:
```python
import numpy as np
a = np.asfortranarray(np.c_[np.arange(3), np.arange(3)])
print(a.flags['F_CONTIGUOUS'])
subset = np.array([True, True, False])
a_subset = a[subset]
print(a_subset.flags['F_CONTIGUOUS'])
```",2019-06-08 19:25:25,,Indexing a Fortran-order array produces a C-order array,['unlabeled']
13735,open,rgov,"I have noticed projects using Numpy often make unnecessary copies, leading to greatly inflated memory usage when dealing with large arrays. It would be useful to have a profiling mode to detect when an unnecessary copy has been made.

Here is an algorithm that might work:

1. When an array is copied, through `ndarray.copy()` or otherwise, it keeps a weak reference to the parent from which it was copied. The parent also keeps a list of weak references to its children.

2. When an array is released, a hash will be computed across its properties and contents and stored into its extant children. If its parent still exists, it will compare itself with the parent directly; if not, it will use the stored hash. If it has not diverged from the parent, a warning is emitted.

4. If an array does diverge from its ancestor, but the underlying storage did not change, it should warn that a view could have been used instead. (Also for slices?)

5. Patterns like `np.array([arr])` might be similarly recommended to `np.expand_dims(arr, axis=0)` or other approaches that do not require copying of the underlying storage.

6. Array lifetime events like the time of creation and time of deletion should be kept in a cache. This would be useful for diagnostics, and for returning a prioritized list of unnecessary copies (weighted by total number of bytes copied over the lifetime of the program).

7. Potentially it could recommend in-place operations like `a += 1` instead of `a = a + 1`. Something like: this was involved in an operation that resulted in a new array of the same shape, and then was never accessed again. But it should like there could be false positives here.

Any thoughts? I've tried wrapping the `ndarray` class to add some of this, but I'd like to know what other approaches could be taken.",2019-06-07 17:24:43,,Instrumentation for unnecessary copies and other optimizations,['unlabeled']
13733,open,dcolascione,"In [45]: np.can_cast(np.int64, np.float64, casting=""safe"")
Out[45]: True

Is that right? Not every int64 can be represented a a float64.",2019-06-07 16:05:46,,Should can_cast report true in safe mode for long->float?,['unlabeled']
13732,open,aldanor,"Here's the actual use case: Intel has recently released a ""Strict CNR mode"" for MKL (in 2019.3 release), see: https://software.intel.com/en-us/mkl-linux-developer-guide-reproducibility-conditions

Basically, for a small subset of BLAS functions, it guarantees strict bit-wise reproducibility regardless of the number of threads used etc:

> In strict CNR mode, Intel MKL provides bitwise reproducible results for a limited set of functions and code branches even when the number of threads changes. These routines and branches support strict CNR mode (64-bit libraries only):
> 
> * ?gemm, ?symm, ?hemm, ?trsm and their CBLAS equivalents (cblas_?gemm, cblas_?symm, cblas_?hemm, and cblas_?trsm).
> * Intel® Advanced Vector Extensions 2 (Intel® AVX2) or Intel® Advanced Vector Extensions 512 (Intel® AVX-512).

For whatever reason, `*gemv` functions are not listed, but `*gemm` are - so all your matrix-by-matrix dot products can now be numerically stable, but not matrix-vector...

Is there are way to force numpy to use e.g. dgemm/sgemm and not dgemv/sgemv, when multiplying (1 x n) by (n x m)? 

I've stumbled upon this piece of code which, IIUC, tries to be ""smart"" and treats single-row/single-column matrices as vectors when dispatching to blas routines, so you can't force it to use 'mm' versions of blas functions instead of 'mv' even if you want to:
https://github.com/numpy/numpy/blob/500035617f02202d616ad1cf047c111f2c2efcb4/numpy/core/src/common/cblasfuncs.c#L156-L178

",2019-06-07 11:56:33,,"NumPy tries to be too smart when dispatching to BLAS (gemm vs gemv), which prevents using MKL's strict CNR in full",['unlabeled']
13731,open,yashjakhotiya,"For an array that has same dimension lengths as other arrays except that it misses a last dimension, instead of throwing a ValueError, can we add an axis to it at the end with a Data Conversion Warning and proceed when using numpy.hstack?

This helps in the case of stacking a 1-D array to an existing matrix.  

### Reproducing code example:

```python
import numpy as np
matrix = np.array([[1, 4], [2, 5], [3, 6]])
one_d_array = np.array([7, 8, 9])
print(np.hstack((matrix, one_d_array)))
```

### Error message:
Traceback (most recent call last):
  File ""stack_vectors_and_1d_array.py"", line 4, in <module>
    print(np.hstack((matrix, one_d_array)))
  File ""/usr/local/python/python-3.7/std/lib64/python3.7/site-packages/numpy/core/shape_base.py"", line 288, in hstack
    return _nx.concatenate(arrs, 1)
ValueError: all the input arrays must have same number of dimensions

### Desired Output:
Data Conversion Warning : Reshaping array 'one_d_array' of shape (3,) to shape (3, 1)
array([[1, 4, 7],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2, 5, 8],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3, 6, 9]])

### Numpy/Python version information:
1.14.3 3.7.3 (default, Apr  9 2019, 18:48:27)
[GCC 7.3.1 20180303 (Red Hat 7.3.1-5)]

Similarly in np.vstack, for an array matching all dimension lengths except that it misses the first one, can we reshape it to (1, -1) with a Data Conversion Warning?
",2019-06-07 10:36:21,,Should numpy.hstack add an axis to an array missing the last one?,['unlabeled']
13719,open,Gattocrucco,"<!-- Please describe the issue in detail here, and fill in the fields below -->

If I pass an integer as `shape` parameter to `numpy.lib.format.open_memmap` I expect it to create a 1D array, and so it happens. But when I try to load back the file, it is corrupted.

I think that `open_memmap` shall either raise an error on shape not being a tuple of integers or sanitize it.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
from numpy.lib import format as nplf
a = nplf.open_memmap('test.npy', shape=10, dtype=float, mode='w+')
del a
b = np.load('test.npy') # Fails
```

<!-- Remove these sections for a feature request -->

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

```
Traceback (most recent call last):
  File ""script.py"", line 9, in <module>
    b = np.load('test.npy')
  File ""/Users/giacomo/Documents/Programmi/python3_virtualenv/lib/python3.6/site-packages/numpy/lib/npyio.py"", line 447, in load
    pickle_kwargs=pickle_kwargs)
  File ""/Users/giacomo/Documents/Programmi/python3_virtualenv/lib/python3.6/site-packages/numpy/lib/format.py"", line 686, in read_array
    shape, fortran_order, dtype = _read_array_header(fp, version)
  File ""/Users/giacomo/Documents/Programmi/python3_virtualenv/lib/python3.6/site-packages/numpy/lib/format.py"", line 562, in _read_array_header
    raise ValueError(msg.format(d['shape']))
ValueError: shape is not valid: 10
```

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

```
1.16.4 3.6.4 (v3.6.4:d48ecebad5, Dec 18 2017, 21:07:28) 
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]
```
",2019-06-04 17:41:12,,shape parameter in lib.format.open_memmap is allowed non-tuple but causes errors,['00 - Bug']
13718,open,seibert,"A Numba contributor has been working on adding support for `np.cross` to Numba (numba/numba#4128), and this raised the issue that `np.cross` has an unusual type signature.  It feels like two related, but distinct functions have been glued together:

1. A gufunc-like function that returns 3D vector cross products of 3D vector inputs, with the convenience feature to assume a zero z-component if the final dimension of (only) one of the input arrays has length 2.  The number of dimensions of the output is equal to the number of dimensions on the inputs.

2. A different gufunc-like function that returns the z component of the 3D vector cross product of 2D vector inputs.  This is only selected if both inputs have length two in their last dimension.  The number of dimensions on the output is *one less* than the number of dimensions of the inputs.

Aside from making the documentation of this function confusing (it took me several tries to understand these two modes, assuming I'm not still confused), it also makes it not possible to write a Numba type signature for this function because the number of returned dimensions depends on the exact length of the last dimension on each input.  (Numba considers ndim, layout, and dtype part of the array type for purposes of code generation, but not the actual contents of shape.)

One could argue this is a Numba problem rather than a NumPy problem, but we suspect that attempts to construct a typing hinting system (like https://github.com/numpy/numpy-stubs)  for NumPy functions that describes the relationship between input and output dimensions will also trip over `np.cross`.   Our workaround in Numba will be only support mode #1 above, and create a separate `numba.cross2d` function that does mode #2, and raise compilations to direct users to the right one.

For similar reasons, adding a `np.cross2d` to NumPy might be a useful way to evolve toward an easier to understand `np.cross` and also make type hinting possible, although backward compatibility would require supporting both modes in `np.cross` for some time.

It is also totally fair to mark this as WONTFIX because this ship has already sailed.  :)",2019-06-04 17:34:54,,Splitting np.cross into np.cross and np.cross2d?,"['01 - Enhancement', 'component: numpy._core']"
13717,open,mihaibujanca,"OS: Ubuntu 18.04
numpy version: 1.16.4 (also tried with 1.15.2). Also tried doing this in a new, clean venv.
Python: 2.7.15rc1

This seems to be related to #13484 

```
terminate called after throwing an instance of 'pybind11::error_already_set'
  what():  ImportError: 
Original error was: /home/mihai/.local/lib/python2.7/site-packages/numpy/core/_multiarray_umath.so: undefined symbol: PyExc_UserWarning


At:
  /home/mihai/.local/lib/python2.7/site-packages/numpy/core/__init__.py(74): <module>
  /home/mihai/.local/lib/python2.7/site-packages/numpy/__init__.py(142): <module>)
```
When using v1.15.2, I got the same error but in `multiarray.so`. I suppose it's the same problem, they might just get loaded in a different order for some reason.

I'm using pybind11 to call a python script from C++. This is wrapped into a single `segmenter.so` library. 

Now the strange part comes: I can run my program as a standalone application with `segmenter.so` linked without any issues.
However, if I wrap my application into a library that gets dynamically loaded at runtime, I end up with the above error. The library contains the same code, without the `main()`

So it goes something like:
`script.py` is called by `segmenter.so` which is linked to application => runs
`script.py` is called by `segmenter.so` linked to `application.so` and loaded at runtime => breaks

Safety checks:
- Numpy runs in the python console
- There are no conflicts with a different version. The path for the running application numpy matches the path for the one that breaks. `PYTHONPATH` and `PATH` are the same when running both.
- Other packages such as sys and opencv can be imported in both the library and the application


Any tips / ideas would be really useful. Otherwise I could try isolating it into an example but I have a feeling this will be hard to replicate.",2019-06-04 17:07:44,,DOC: dlopen(_multiarray_umath.so) requires dlopen(libpython),['04 - Documentation']
13712,open,nate-bryant,"<!-- Please describe the issue in detail here, and fill in the fields below -->
I'm running into an error using matmul with single dimension pandas data frames. The below code worked as expected in 1.15.4 but after I upgraded to 1.16.3 I get the below error message. matmul works if I convert the data frames to numpy arrays but could previously handle the data frames.
### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import pandas as pd
import numpy as np

d = {'col1': [0.0037448, -0.002455324, 0.003062666, 0.000768503, 0.002309015]}
df = pd.DataFrame(data=d)
dfT = df.T

np.matmul(dfT, df)
```

<!-- Remove these sections for a feature request -->

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

```
Traceback (most recent call last):
  File ""MatmulBugTest.py"", line 8, in <module>
    np.matmul(dfT, df)
  File ""C:\Users\bryant_n\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\generic.py"", line 1909, in __array_wrap__
    return self._constructor(result, **d).__finalize__(self)
  File ""C:\Users\bryant_n\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py"", line 424, in __init__
    copy=copy)
  File ""C:\Users\bryant_n\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py"", line 167, in init_ndarray
    return create_block_manager_from_blocks([values], [columns, index])
  File ""C:\Users\bryant_n\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\managers.py"", line 1660, in create_block_manager_from_blocks
    construction_error(tot_items, blocks[0].shape[1:], axes, e)
  File ""C:\Users\bryant_n\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\managers.py"", line 1691, in construction_error
    passed, implied))
ValueError: Shape of passed values is (1, 1), indices imply (1, 5)
```

### Numpy/Python version information:
1.16.3 3.7.0 (default, Jun 28 2018, 08:04:48) [MSC v.1912 64 bit (AMD64)]
<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2019-06-04 13:51:01,,np.matmul with Pandas data frames fails in pandas.NDFrame.__array_wrap__,"['00 - Bug', '57 - Close?', 'component: numpy.ufunc']"
13709,open,oldnaari,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
from numpy import ma
arr = ma.masked_all((3, 3, 2))
arr[0,0] = 1
out = ma.compress_nd(arr, axis=(0, 1))
```

This code sets **out** to the following value:
```
array([], shape=(0, 0, 2), dtype=float64)
```

The expected value is:
```
array([[1., 1.]])
```

### Numpy/Python version information:
```
1.16.2 3.6.7 (default, Oct 22 2018, 11:32:17) 
[GCC 8.2.0]
```

",2019-06-04 07:15:20,,ma.compress_nd behaves not as expected,"['00 - Bug', '04 - Documentation', 'component: numpy.ma']"
13701,open,anntzer,"It would be nice if a structured-stype constructor taking `**kwargs` was available, ie.
```
np.dtype(foo=int, bar=(float, 2))
```
being equivalent to
```
np.dtype([(""foo"", int), (""bar"", float, 2)])
```
the latter being quite a mouthful to type (I realize that this can be made into a one-line helper function too..).

It is true that further overloading the (already very overloaded) np.dtype constructor results in an ambiguity, because the following is currently legal:
```
In [1]: np.dtype(dtype=np.uint16)
Out[1]: dtype('uint16')
```
(even though the docs say that the first argument is named ""obj"" -- but it's actually named ""dtype""...)

However, I think it would make sense to make the first argument (when used in that semantic) positional only, so that (after some deprecation period) `np.dtype(dtype=np.uint16)` means `np.dtype([(""dtype"", np.uint16)])`.
Obviously, if there are no fields named ""dtype"", there is no ambiguity even as of now.

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.16.3 3.7.3 (default, Mar 27 2019, 22:11:17) 
[GCC 7.3.0]
",2019-06-03 07:34:55,,Nicer structured-dtype constructor,"['01 - Enhancement', 'component: numpy.dtype']"
13699,open,mdickinson,"From a [Stack Overflow question](https://stackoverflow.com/q/56368092/270986): `round` operations on `float16` can easily (and surprisingly) return infinities due to intermediate overflow:

```python
>>> import numpy as np
>>> np.round(np.float16(2.0), 5)
/opt/local/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/core/fromnumeric.py:56: RuntimeWarning: overflow encountered in multiply
  return getattr(obj, method)(*args, **kwds)
inf
```
The expected result in this case was `2.0` (as an `np.float16`).

The exact same problem exists for `float32` and `float64`, of course; it's just that users are less likely to run into it: thanks to the larger dynamic range of those types, you'd have to be right at the extremes of the range to see this.

```python
>>> np.round(np.float64(1e304), 5)
inf
```

But for `float16`, the dynamic range is so small that you're effectively _always_ at the extremes of the range.

I don't know if there's anything that can reasonably done here to fix this without significantly impacting performance, but it seemed worth reporting. I imagine an element-by-element careful round would be significantly slower than the current scale-rint-unscale implementation. But maybe it's possible to change the implementation to use `float64` for intermediate calculations when the target type is `float16` or `float32`?

(Versions: NumPy 1.16.3, Python 3.7.3)

",2019-06-02 10:01:27,,Surprising overflows in np.round of float16.,"['00 - Bug', 'component: numpy._core']"
13694,open,mattip,"In #13664 @luzpaz used codespell to find typos. We could maybe do this by adding this to a travis CI run. I wonder how many false positives it would generate. While we are at it, we could check new code similarily for flake8 violations.

`mapfile -t files < <(git diff --name-only $TRAVIS_COMMIT_RANGE); codespell -d -q 3 -S ""$CODESPELL_SKIPS"" -I ./whitelist.txt  ""${files[@]}""`",2019-06-01 19:44:02,,"CI: use codespell, flake8 to check code before merging?","['01 - Enhancement', 'component: CI']"
13683,open,anntzer,"One would expect that `np.full(shape, fill, dtype)` works as `t = np.empty(shape, dtype); t[:] = fill; return t` but...

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

the following works
```python
In [3]: t = np.empty(10, [(""a"", int), (""b"", float)])                                                                

In [4]: t[:] = (0, np.nan)                                                                                          

In [5]: t                                                                                                           
Out[5]: 
numpy.ndarray(
    [(0, nan), (0, nan), (0, nan), (0, nan), (0, nan), (0, nan), (0, nan),
     (0, nan), (0, nan), (0, nan)], dtype=[('a', '<i8'), ('b', '<f8')])
```
but the following doesn't
```
In [1]: np.full(10, (0, np.nan), [(""a"", int), (""b"", float)])                                                        
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1-b4402a777ba5> in <module>
----> 1 np.full(10, (0, np.nan), [(""a"", int), (""b"", float)])

~/.local/lib/python3.7/site-packages/numpy/core/numeric.py in full(shape, fill_value, dtype, order)
    334         dtype = array(fill_value).dtype
    335     a = empty(shape, dtype, order)
--> 336     multiarray.copyto(a, fill_value, casting='unsafe')
    337     return a
    338 

ValueError: could not broadcast input array from shape (2) into shape (10)
```

As a side effect, even if it was decided that the above was not supported, the following certainly gives a wrong result:
```
In [9]: np.full(2, (0, np.nan), [(""a"", int), (""b"", float)])                                                         
Out[9]: 
numpy.ndarray(
    [(                   0,  0.), (-9223372036854775808, nan)],
    dtype=[('a', '<i8'), ('b', '<f8')]
)
```

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```
1.16.3 3.7.3 (default, Mar 27 2019, 22:11:17) 
[GCC 7.3.0]
```

",2019-05-31 13:43:43,,"np.full(..., dtype=structured-dtype) does not work as expected","['00 - Bug', 'component: numpy._core', 'component: numpy.dtype']"
13654,open,seberg,"Followup to gh-13590. `strides` and `shape`/`PyArray_DIMS`, can be NULL, in which case we sometimes call `memcopy` with a `NULL` argument and copying 0 bytes. This is undefined and should be avoided.

```
git grep -E ""memcpy\((PyArray_(STRIDES|DIMS)|([^-]*->)?(dim|stri))""
```
may find some such issues,
```
memcpy\(.*?, sizeof(npt_intp).*?\)
```
should find more (although I think it requires multi-line search).

It may be good to implement a `npy_memcpy` or similar helper function.",2019-05-28 17:15:37,,MAINT: Fix undefined behaviour issues with memcpy,"['component: numpy._core', '03 - Maintenance']"
13628,open,mattip,"Split out of #13399 

```
class Struct(ctypes.Structure):
    _fields_ = [('a', ctypes.c_uint8), ('b', ctypes.c_uint16)]

data = (Struct * 4)()
np.array(data)  # ok
np.array(list(data))
# RuntimeWarning: A builtin ctypes object gave a PEP3118 format string that does not match its itemsize, so a best-guess will be made of the data type. Newer versions of python may behave correctly.
# ValueError: invalid literal for int() with base 10: b'\x00\x00\x00\x00'
```

We should either correctly parse an iterable of buffers or raise a better error.",2019-05-26 05:23:17,,BUG: converting list of ctypes.Structure to ndarray,"['00 - Bug', 'component: numpy.ctypes']"
13604,open,bbbbbbbbba,"<!-- Please describe the issue in detail here, and fill in the fields below -->
I am computing density histograms of a data set with some outliers that appear with small but non-negligible frequency (say around 10%). Including the outliers would skew the plot too much, so I exclude them using the `range` keyword argument. However, this causes `np.histogram` to ignore them completely, over-presenting the probability density everywhere else.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
print(np.histogram([1.5, 3.5, 5.5, 7.5, 9.5, 100], bins = 10, range = (0, 10), density = True))

# (array([0. , 0.2, 0. , 0.2, 0. , 0.2, 0. , 0.2, 0. , 0.2]), array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]))
```

Ideally, I want the density values to be 0.16666667 (1/6) instead of 0.2 (1/5).

### Current workaround:
Of course I can just choose not to use the `density = True` keyword argument, and manually divide by the number of samples and the bin width (easier with equal-width bins). However, this is cumbersome, especially when I am indirectly calling `np.histogram` through `matplotlib.pyplot.hist`. An option to directly compute the correct probability density would be welcome.

### Suggestion for implementation:
I think accounting for outliers when calculating the probability density might be the desired behavior in most cases where it matters. If this is the case, then maybe this behavior can be the default. In any case, it may be good to allow a ""third value"" (e.g. a specific string) for the `density` keyword argument to enable the non-default behavior.",2019-05-22 11:24:59,,ENH: An option on how density histograms handle outliers,"['00 - Bug', 'component: numpy.lib']"
13602,open,clbarnes,"See #7478 and #13601 
 
## History

(not necessarily in chronological order)

1. A certain JVM-based linear algebra package had a function, `polyfit`, for fitting polynomials which made some weird design choices, like returning the coefficients highest-degree first.
2. numpy, in an attempt to support fugitives from said environment, created the function `numpy.polyfit` which aped that design choice
3. numpy implemented `numpy.ma.polyfit` for masked arrays, using `numpy.polyfit`
4. In an attempt to fix the mistakes of history, numpy created the function `numpy.polynomial.polynomial.polyfit` with almost exactly the same signature, but with a more sensible coefficient ordering, and quietly preferred that people use that instead
5. People were confused by these two very similar functions (#7478); also the new function could not return a covariance matrix and it did not have a masked array counterpart
6. Powering on towards both API nirvana and worn-out keyboards, numpy introduced the `numpy.polynomial.polynomial.Polynomial` class, and documented in `numpy.polyfit` that that was the preferred way of fitting polynomials, although that also had no masked implementation and also did not return a covariance matrix

## The problem

Most external documentation/tutorials recommends using `numpy.polyfit`. The `numpy.polyfit` documentation recommends using `numpy.polynomial.polynomial.Polynomial`. People who come across `numpy.polynomial.polynomial.polyfit` get it confused with `numpy.polyfit`, with good reason. People who need masked arrays or covariance matrices are stuck with the oldest, least recommended API.

## To do

- For easily-confused functions, explicitly point out its partner's existence, the differences, and which is preferred (see #13601 )
- Point out that the `Polynomial` class is preferred in all cases
- Implement masked arrays and covariance matrix return from `Polynomial.fit`, so that it can be *used* in all cases.
- (liberally splash deprecation warnings around in future)",2019-05-21 20:55:40,,polyfit (poly1d) / polynomial.polyfit / Polynomial ambiguities,"['01 - Enhancement', '04 - Documentation', 'component: numpy.ma', 'component: numpy.polynomial']"
13579,open,topper-123,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

```python
>>> n = 1_000_000
>>> arr = np.array([1] * n + [2] * n + [3] * n, dtype='int8')
>>> %timeit arr.searchsorted(2)  # search for a python int
7.38 ms   # slow
>>> code = np.int16(1)  # numpy int, but different dtype
>>> %timeit arr.searchsorted(code)
4.04 ms  # slow
>>> code = np.int8(1)
>>> %timeit arr.searchsorted(code)  # same dtype
2.57 µs  # fast
```

The cause of the slowness is that searchsorted upcasts the input array ``arr`` and value ``v`` to the same dtype, if they have different dtypes, and this costs time. This is not always needed.

In Pandas we've worked around this by making special integer type checks in a custom version of searchsorted, but IMO everyone would benefit if this/something similar was put into numpy instead.

See https://github.com/pandas-dev/pandas/blob/master/pandas/core/algorithms.py#L1726 for the pandas version. The relevant PR is https://github.com/pandas-dev/pandas/pull/22034/. The pandas version of runs at 17.3 µs. This is slower than the optimal numpy version, probably because it's done in python + makes some checks that likely wouldn't be needed to do in in numpy.

BTW, I wouldn't be able to implement this in numpy, because I don't know C... 

### Numpy/Python version information:

1.15.4 3.6.7 |Anaconda, Inc.| (default, Oct 28 2018, 19:44:12) [MSC v.1915 64 bit (AMD64)]
",2019-05-17 06:29:38,,Speed problem for searchsorted when different integer dtypes,"['01 - Enhancement', 'component: numpy._core']"
13572,open,juliantaylor,"A common problem for parallel applications is to determine the amount number of parallel processing threads to start.
For numerical applications it is usually pretty easy, use as many as you have physical cores available (hyperthreading is usually quite useless for these applications).

Unfortunately there is no easy way to get this number as there are numerous platform specific issues to consider, most importantly the classical methods like reading /proc/cpuinfo or pythons `os.cpu_count` functions are useless in many modern containerized deployment environments as they report the number of cpu cores of the host but do not consider limitations of the container (via cgroups on linux) or cpu affinities.

For the latter python3 has a function, `len(os.sched_getaffinity(0))´ but for the former so far I know python has no solution yet.

This often leads to oversubscription of the available resources and causing unnecessary synchronization overheads due to too high thread counts, maybe wasted memory for unnecessary per thread memory buffers and causes difficulties for operators as your applications will constantly trigger cpu throttling alerts unless manually configured.

I think it would be nice if we add a function to numpy which can be used to get an accurate number for supported platforms and fall back to pythons method for unsupported ones.

On linux this involves reading the cgroup cpu quotas (https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt) and cpu affinities.
Unfortunately I do not know how to do it on other platforms though linux is probably the most major container platform after windows and for that we might find someone who can help us.

Of course this functionality really belongs into python, but even if added python is harder to update than numpy and releases slower which favors implementing it first in numpy and maybe later falling back to python functions instead of our own code.
It is also more difficult, best I could get from upstream last time I tried was the note to `sched_getaffinity` in the documentation of `os.cpu_count` though at the time the linux quota scheduler did not exist yet or was still mostly unavailable to major distributions and containerized environments where not as dominant as they are today.

Usable memory has similar issues and functions to determine it accurately could also be added.

Thoughts?
My major concern is that I could only do the implementation for linux but we support a lot more platforms and also that numpy itself does not actually do any parallel operations directly so it would be a function purely for consumption in libraries that are using numpy.",2019-05-15 20:49:30,,add function to retrieve amount of usable cpu/memory,['15 - Discussion']
13569,open,studywolf,"I forgot to put brackets around my vector for when creating a 2 element array, but instead of throwing an error, a scalar value was returned with the datatype of the second element. Seemed like it might be unexpected behaviour to be able to pass in a variable as the `dtype`. I didn't catch it when running the code because no error was thrown and the scalar output worked with all the subsequent math.

### Reproducing code example:

```python
import numpy as np

np.array(0, np.float64(0))
```

### Numpy/Python version information:

1.16.2 3.5.6 |Anaconda, Inc.| (default, Aug 26 2018, 21:41:56) 
[GCC 7.3.0]


",2019-05-15 15:30:04,,Unexpected behaviour creating numpy.array,"['07 - Deprecation', 'defunct — difficulty: Intermediate']"
13557,open,juliantaylor,"Our iterator avoids buffers whenever possible, this is the case as usually accessing data with strides is faster than copying to a buffer for a contigous loop. This is also the case for our vectorized basic math functions (+-*/).

Since we merged vectorized exp and log we have vectorized ufuncs that are heavily cpu bound instead of memory bound.
For these functions it can be very worthwhile to use buffered iterators as only then the vectorized code can be used.
Another advantage is that it we guarantee the same results regardless of strides of the input data. (A property that in current master might be lost but could also be restored without a buffered iterator by using the vector code on scalar data though that might be a bit slower)

```
import numpy as np
d = np.random.rand(1000, 50).astype(np.float32)
print(""2d strided -> buffered iterator"")
%timeit np.exp(d[::2])
d = np.random.rand(1000* 50).astype(np.float32)
print(""1d strided unbuffered iterator"")
%timeit np.exp(d[::2])


2d strided -> buffered iterator
10000 loops, best of 3: 77.3 µs per loop
1d strided unbuffered iterator
1000 loops, best of 3: 1.02 ms per loop
```

Btw vectorizing the functions for strided data adds complexity and would likely be slower than our buffer approach. I would not recommend pursuing it.",2019-05-14 18:09:45,,ENH: always use buffered iterator for vectorized math,"['01 - Enhancement', 'component: numpy.ufunc']"
13555,open,syrte,"### Reproducing code example:
```python
import numpy as np
np.interp(np.nan, [1], [2])
```
returns `2.0` while `nan` is expected.

It returns `nan` correctly when `len(x) > 1`
```
np.interp(np.nan, [1, 1], [2, 2])
```

My numpy version: 1.15.3

Related issue: https://github.com/numpy/numpy/issues/605

",2019-05-14 13:40:24,,"BUG: np.interp(nan, x, y) doesn't return nan when len(x)=1","['00 - Bug', '54 - Needs decision']"
13553,open,tloredo,"I'm trying to use f2py to generate a module from the following snippet of Fortran code, from the [Fortran Wiki](http://fortranwiki.org/fortran/show/integration), lightly edited (I took out the `associate` which caused other problems, and switched to `real*8`):
```
function trapz(x, y) result(r)
  real*8, dimension(:), intent(in)  :: x         ! Nodes
  real*8, dimension(size(x)), intent(in)  :: y   ! Function values
  real*8              :: r                       ! Integral of y(x)
  integer :: n

  ! Integrate using the trapezoidal rule
  n = size(x)
  r = sum((y(1+1:n-0) + y(1+0:n-1))*(x(1+1:n-0) - x(1+0:n-1)))/2
end function trapz
```
I have that in a file `_trapz.f90`. If I use f2py to generate the module directly:

```
python -m numpy.f2py -c _trapz.f90 -m _trapz
```
It appears to work fine; it generates the .so. But if I instead generate a .pyf:
```
python -m numpy.f2py _trapz.f90 -m _trapz -h _trapz.pyf
```
(which runs without warnings or errors), and then generate the module from it (without editing the .pyf):
```
python -m numpy.f2py -c _trapz.pyf _trapz.f90
```
then I get errors from the module build:

### Error message:

```
...
compiling Fortran sources
Fortran f77 compiler: /usr/local/bin/gfortran -Wall -g -ffixed-form -fno-second-underscore -m64 -fPIC -O3 -funroll-loops
Fortran f90 compiler: /usr/local/bin/gfortran -Wall -g -fno-second-underscore -m64 -fPIC -O3 -funroll-loops
Fortran fix compiler: /usr/local/bin/gfortran -Wall -g -ffixed-form -fno-second-underscore -Wall -g -fno-second-underscore -m64 -fPIC -O3 -funroll-loops
compile options: '-I/var/folders/9y/zmj12xx97jv4m19jg0ywpsg00000gn/T/tmplv7hp3b_/src.macosx-10.7-x86_64-3.6 -I/Users/loredo/anaconda/envs/py36/lib/python3.6/site-packages/numpy/core/include -I/Users/loredo/anaconda/envs/py36/include/python3.6m -c'
gfortran:f90: _trapz.f90
gfortran:f77: /var/folders/9y/zmj12xx97jv4m19jg0ywpsg00000gn/T/tmplv7hp3b_/src.macosx-10.7-x86_64-3.6/_trapz-f2pywrappers.f
/var/folders/9y/zmj12xx97jv4m19jg0ywpsg00000gn/T/tmplv7hp3b_/src.macosx-10.7-x86_64-3.6/_trapz-f2pywrappers.f:13:17:
           real*8, dimension(size(x)),intent(in),depend(x) :: y
                 1
Error: Invalid character in name at (1)
/var/folders/9y/zmj12xx97jv4m19jg0ywpsg00000gn/T/tmplv7hp3b_/src.macosx-10.7-x86_64-3.6/_trapz-f2pywrappers.f:17:22:

       trapzf2pywrap = trapz(x, y)
                      1
Error: Type mismatch in argument ‘y’ at (1); passed REAL(8) to REAL(4)
error: Command ""/usr/local/bin/gfortran -Wall -g -ffixed-form -fno-second-underscore -m64 -fPIC -O3 -funroll-loops -I/var/folders/9y/zmj12xx97jv4m19jg0ywpsg00000gn/T/tmplv7hp3b_/src.macosx-10.7-x86_64-3.6 -I/Users/loredo/anaconda/envs/py36/lib/python3.6/site-packages/numpy/core/include -I/Users/loredo/anaconda/envs/py36/include/python3.6m -c -c /var/folders/9y/zmj12xx97jv4m19jg0ywpsg00000gn/T/tmplv7hp3b_/src.macosx-10.7-x86_64-3.6/_trapz-f2pywrappers.f -o /var/folders/9y/zmj12xx97jv4m19jg0ywpsg00000gn/T/tmplv7hp3b_/var/folders/9y/zmj12xx97jv4m19jg0ywpsg00000gn/T/tmplv7hp3b_/src.macosx-10.7-x86_64-3.6/_trapz-f2pywrappers.o"" failed with exit status 1
```

I have no trouble building a module from another simple function this way. Am I overlooking a problem in the Fortran code? I don't see any invalid names (might this be treating free-form code as fixed-form?), and I don't see the origin of the type mismatch. And neither problem arose going straight to the module.



### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

macOS 10.13.6, gfortran-6.3.0 (latest from GCC for this OS)

>>> import sys, numpy; print(numpy.__version__, sys.version)
1.16.2 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46)
[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]

",2019-05-14 05:32:39,,f2py builds successfully direct from .f90 but fails if I generate .pyf first,['component: numpy.f2py']
13547,open,superbobry,"`PyArray_FromAny` does not necessarily return an array of the requested DType. Specifically, when called with a memoryview object, the DType is ignored (i.e. there is no validation nor casting).

### Reproducing code example:

The following snippet works because `__array__` allows for casting

```python
import numpy as np
>>> import numpy as np
>>> condition = np.array([])
>>> condition.dtype
dtype('float64')
>>> arr = np.array([])
>>> np.compress(condition, arr)
array([], dtype=float64)
```

However, if we wrap `condition` in a `memoryview` we get an error

```python
>>> np.compress(memoryview(condition), arr)
Traceback (most recent call last):
  File ""[...]/numpy/core/fromnumeric.py"", line 56, in _wrapfunc
    return getattr(obj, method)(*args, **kwds)
TypeError: Cannot cast array data from dtype('float64') to dtype('bool') according to the rule 'safe'
```
### Numpy/Python version information:

```
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.16.3 3.6.5 (default, Mar 31 2018, 05:34:57) 
[GCC 7.3.0]
```",2019-05-13 13:06:08,,PyArray_FromAny ignores newtype with memoryview,"['component: numpy._core', '07 - Deprecation', 'defunct — difficulty: Intermediate']"
13531,open,seberg,"We should probably not allow this:

```
np.add.reduce([1], initial=np.nan)
```

EDIT: Above now fails, but only because NaN is so terrible, we still allow other floats, like `3.5`",2019-05-11 20:03:59,,BUG: Use safe casting (or same kind casting) for initial kwarg,"['01 - Enhancement', '07 - Deprecation']"
13527,open,seberg,"Using the github api, i tis possible to look at all issues/PRs as well as move them to project boards (and edit the project board lists). So it may be nice to reproduce something like Pauli's website but put it on the project boards. @eric-wieser can you post Pauli's website?",2019-05-11 18:34:17,,TRIAGE: See if we can automate project boards using github API,"['17 - Task', '03 - Maintenance']"
13519,open,DarkestDuckster,"# Concrete example of a C extension class subtyping Numpy ndarray

While the [Python Docs](https://docs.python.org/3/extending/extending.html) show examples on how to implement a simple, example for writing a custom extension class and the [NumPy Dosc](https://docs.scipy.org/doc/numpy/user/c-info.beyond-basics.html#subtyping-the-ndarray-in-c) say that it is possible to do this, there is very little information on how to actually properly implement extension classes.

How should construction be handled? Are there any arbitrary rules on how to implement the classes? Everything I seem to do causes segfaults.

I have been looking for information on this subject for a while now, but the only thing that I find is that nobody is making ndarray extension classes in c. If I am a fool and simply don't know how to use google properly, please redirect me to where I can find the information I need.

If this request is not relevant, please close it.",2019-05-10 02:59:20,,DOC: No concrete example for C extension classes,['04 - Documentation']
13510,open,lepmik,"As far as I understand, there are currently two ways to close a memmap ""file""; `del fp` or `fp._mmap.close()`. However, the former only closes the file if `fp` is the only reference to the memmap and the latter crashes the python interpreter if there exists another reference to the memmap.

Are there any plans of implementing a close method i.e. `fp.close()`? It would be much appreciated.

",2019-05-08 19:38:07,,Method to explicitly close memmap,['unlabeled']
13478,open,tylerjereddy,"`test_sdot_bug_8577()` fails under Intel SDE emulation, as observed in #13466.

There's no evidence this is related to the Skylake issue in #13401. So, this issue is a bookmark to fix this test for emulation purposes in the future, and something for me to refer to when I blacklist the test under emulation.

Most likely, in the test the usage of `sys.executable` subprocess call needs adjutment to work with the process attachment machinery used by the emulator. I'd fix it myself, but there's already too much unplanned work associated with this & I want to focus in on the actual linalg + Skylake failure.",2019-05-05 21:12:14,,TST: test_sdot_bug_8577 fails under emulation,"['05 - Testing', 'component: numpy.linalg']"
13470,open,mpenkov,"When the numpy.fromfile and numpy.tofile receive file-like [gzip.GzipFile](https://docs.python.org/3.7/library/gzip.html#gzip.GzipFile) objects as input, they behave unexpectedly:

- tofile leaves the output mangled
- fromfile reads incorrect or incomplete data

### Reproducing code example:

```python
import argparse
import gzip
import io
import os
import os.path as P

import numpy as np

MAGIC_STRING = b'the quick brown fox jumps over the lazy dog'
MATRIX = np.matrix(list(range(10)), dtype=np.dtype('float32'))
TMP_FILE = 'reproduce.dat'


def read_matrix(fin):
    mat = np.fromfile(fin, dtype=np.dtype('float32'), count=MATRIX.size)
    print('expected: %r' % MATRIX)
    print('actual: %r' % mat)


def write_matrix(mat, fout):
    mat.tofile(fout)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--gz', action='store_true', help='enable gzip compression')
    parser.add_argument('--clean', action='store_true', help='delete temporary files')
    args = parser.parse_args()

    open_function = gzip.GzipFile if args.gz else io.open

    if args.clean and P.isfile(TMP_FILE):
        os.unlink(TMP_FILE)

    if not P.isfile(TMP_FILE):
        with open_function(TMP_FILE, 'wb') as fout:
            fout.write(MAGIC_STRING)
            write_matrix(MATRIX, fout)

    with open_function(TMP_FILE, 'rb') as fin:
        print(fin.read(len(MAGIC_STRING)))
        read_matrix(fin)


if __name__ == '__main__':
    main()
```

The above does the following:

- Write a byte string followed by a matrix (using tofile) to a temporary file
- Read back the contents of the temporary file

An example session:

```
$ python reproduce.py --clean  # working with ordinary io.open streams
b'the quick brown fox jumps over the lazy dog'
expected: matrix([[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]], dtype=float32)
actual: array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32)
$ python reproduce.py --clean --gz  # broken, tofile produces garbage
Traceback (most recent call last):
  File ""reproduce.py"", line 46, in <module>
    main()
  File ""reproduce.py"", line 41, in main
    print(fin.read(len(MAGIC_STRING)))
  File ""/usr/lib/python3.5/gzip.py"", line 274, in read
    return self._buffer.read(size)
  File ""/usr/lib/python3.5/_compression.py"", line 68, in readinto
    data = self.read(len(byte_view))
  File ""/usr/lib/python3.5/gzip.py"", line 469, in read
    uncompress = self._decompressor.decompress(buf, size)
zlib.error: Error -3 while decompressing data: invalid stored block lengths
$ python reproduce.py --clean  # work around the above problem
b'the quick brown fox jumps over the lazy dog'
expected: matrix([[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]], dtype=float32)
actual: array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32)
$ gzip reproduce.dat
$ mv reproduce.dat{.gz,}
$ python reproduce.py --gz  # illustrate broken fromfile
b'the quick brown fox jumps over the lazy dog'
expected: matrix([[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]], dtype=float32)
actual: array([-1.7907840e+08, -2.1929670e+05,  1.8607268e+11, -9.5393936e+23,
        1.8948086e-13,  7.8181330e-08,  6.4089775e-02,  1.7406825e-18,
       -5.9227965e+20,  1.1410567e-38], dtype=float32)
```
### Error message:

A failed tofile causes fromfile to fail with the following exception (see above):

```
Traceback (most recent call last):
  File ""reproduce.py"", line 46, in <module>
    main()
  File ""reproduce.py"", line 41, in main
    print(fin.read(len(MAGIC_STRING)))
  File ""/usr/lib/python3.5/gzip.py"", line 274, in read
    return self._buffer.read(size)
  File ""/usr/lib/python3.5/_compression.py"", line 68, in readinto
    data = self.read(len(byte_view))
  File ""/usr/lib/python3.5/gzip.py"", line 469, in read
    uncompress = self._decompressor.decompress(buf, size)
zlib.error: Error -3 while decompressing data: invalid stored block lengths
```

A failed fromfile causes unexpected values in the resulting matrix.

### Numpy/Python version information:

```python
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.16.2 3.5.2 (default, Sep 14 2017, 22:51:06) 
[GCC 5.4.0 20160609]
```

### See Also

https://github.com/RaRe-Technologies/gensim/issues/2473 for a real-world example of this bug.",2019-05-04 11:55:57,,fromfile/tofile not working as expected with gzip file as input,"['00 - Bug', '04 - Documentation']"
13442,open,mattip,In Pr #7593 there is a discussion of padding vs. using `fftshift`/`ifftshift`. It seems padidng is never the correct thing to do. We should document the correct techniques.,2019-04-30 21:31:01,,Better document fftshift and ifftshift in the context of padding,"['04 - Documentation', 'component: numpy.fft']"
13428,open,seberg,"```python
arr = np.array([""2019"", ""2018""], dtype=""datetime64[ms]"")
arr1 = arr + np.timedelta64(1, ""s"")

out = np.subtract(arr, arr1, dtype=np.dtype(""timedelta64[s]""))
print(""Out dtype is timedelta, but not [s]:"", out.dtype)  # This should probably be [s]

out = np.zeros(2, np.dtype(""timedelta64[s]""))
np.subtract(arr, arr1, dtype=np.dtype(""timedelta64[s]""), out=out)

# These are both fine:
print(""The value is correct, and (obviousl) the out dtype is also:"")
print(out.dtype, out)
```

For the first example, it would probably be better if the dtype request is honoured (although I have to check up on how the `dtype` argument is used generally).

EDIT: And I forgot to even check the out dtype in e.g. the NaT Resolvers, I am sure there are more of these issues, but they do seem low priority anyway, I doubt `dtype` is used, and at least in the NaT cases, there is basically no point in using them.",2019-04-29 21:27:52,,BUG: Timedelta/Datetime unit is ignored for ufunc `dtype` kwarg (as well as NaT Resolvers),"['00 - Bug', 'component: numpy.datetime64', 'Priority: low']"
13418,open,jiayi797,"# Code

```python
import numpy as np

a = np.array(['11','22']) 
print(a) # output ['11' '22']
a[0] = '121'
print(a) # output ['12' '22'] !!!!
```

when I want to reset a[0] as a longer string, it will cut off my new value. it cause a bug in my system and it takes my whole week. >_<

I rewrite it to :
```python
a = np.array(['11','22'],dtype = 'U10') 
print(a) # output ['11' '22']
a[0] = '121'
print(a) # output ['121' '22'] 
```

it seems work well. While I think it might cause confused again in future.  ",2019-04-28 12:43:47,,Warn about string unsafe casting (assigning long string into shorter),"['01 - Enhancement', '15 - Discussion', 'component: numpy.dtype']"
13408,open,pllim,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
c = np.ma.array([(1, 3, 5), (2, 4, 6)],
                mask=[( True, False, False),
                      (False, False, False)],
                dtype=[('a', '<i4'), ('b', '<i4'), ('c', '<i4')])
c.count()
```

Expected output: `5`

xref astropy/astropy#8643

<!-- Remove these sections for a feature request -->

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

```
TypeError                                 Traceback (most recent call last)
<ipython-input-30-bdab9583c490> in <module>
----> 1 c.count()

...\numpy\ma\core.py in count(self, axis, keepdims)
   4492             return 0
   4493
-> 4494         return (~m).sum(axis=axis, dtype=np.intp, **kwargs)
   4495
   4496     def ravel(self, order='C'):

TypeError: ufunc 'invert' not supported for the input types, and the inputs coul
d not be safely coerced to any supported types according to the casting rule ''s
afe''
```

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```
1.16.3 3.7.1 | packaged by conda-forge | (default, Nov 13 2018, 19:01:41) [MSC v.1900 64 bit (AMD64)]
```",2019-04-26 14:19:41,,BUG: Cannot count masked array with dtype,"['15 - Discussion', '54 - Needs decision']"
13375,open,eric-wieser,"Asking for methods that exist on builtin float types but not numpy types gives:
```python
>>> set(dir(float)) - set(dir(np.float32))
{'__getformat__',
 '__getnewargs__',
 '__setformat__',
 '__trunc__',
 'as_integer_ratio',
 'fromhex',
 'hex',
 'is_integer'}
```

Filtering these to the ones that are not CPython implementation details, we have:
* [ ] `__trunc__(self) -> int`, which will enable `math.trunc(numpy_float)`
* [x] `is_integer(self) -> bool` (done in #19803)
* [x] `as_integer_ratio(self) -> Tuple[int, int]` (done in #10741)
* [ ] `hex(self) -> str`
* [ ] `fromhex(cls: Type[FloatType], s: str) -> FloatType`

For all of these, the best approach is probably to port the CPython implementations, taking care to handle the extended precision of `long double`s.

Note that we get these methods for free on `np.float64` due to inheriting from `float`",2019-04-20 20:11:40,,Add methods from the builtin float types to the numpy floating point types,"['01 - Enhancement', 'Project']"
13373,open,eric-wieser,"```python
class Foo(np.ndarray):
    pass

f = np.array(1).view(Foo)
f.test = 2
f2 = pickle.loads(pickle.dumps(f))
print(f2.test)  # AttributeError
```
Root cause of #13338",2019-04-20 03:28:40,,Attributes of ndarray subclasses are not pickled,['00 - Bug']
13372,open,eric-wieser,"It would be great if we could use more precision here:
```python
>>> import numpy as np
>>> import time
>>> np.datetime64(""now"", ""ns""), np.datetime64(int(time.time()*1e9), ""ns"")
(numpy.datetime64('2019-04-20T03:03:01.000000000'),
 numpy.datetime64('2019-04-20T03:03:01.598897664'))
```
Using `_PyTime_GetSystemClock()` (`#include <py_time.h>`) instead of `time()` might be appropriate.",2019-04-20 03:04:48,,"np.datetime(""now"", ""ns"") rounds to the nearest second","['01 - Enhancement', 'component: numpy.datetime64']"
13357,open,mreineck,"When trying to FFT a real array forward and backwards, I get an error:
```

import numpy as np
a=np.random.rand(1)
np.fft.irfftn(np.fft.rfftn(a))

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-11-16174261ff2c> in <module>()
      1 import numpy as np
      2 a=np.random.rand(1)
----> 3 np.fft.irfftn(np.fft.rfftn(a))

/usr/lib/python3/dist-packages/numpy/fft/fftpack.py in irfftn(a, s, axes, norm)
   1230     for ii in range(len(axes)-1):
   1231         a = ifft(a, s[ii], axes[ii], norm)
-> 1232     a = irfft(a, s[-1], axes[-1], norm)
   1233     return a
   1234 

/usr/lib/python3/dist-packages/numpy/fft/fftpack.py in irfft(a, n, axis, norm)
    464     unitary = _unitary(norm)
    465     output = _raw_fft(a, n, axis, fftpack.rffti, fftpack.rfftb,
--> 466                       _real_fft_cache)
    467     return output * (1 / (sqrt(n) if unitary else n))
    468 

/usr/lib/python3/dist-packages/numpy/fft/fftpack.py in _raw_fft(a, n, axis, init_function, work_function, fft_cache)
     54     if n < 1:
     55         raise ValueError(""Invalid number of FFT data points (%d) specified.""
---> 56                          % n)
     57 
     58     # We have to ensure that only a single thread can access a wsave array

ValueError: Invalid number of FFT data points (0) specified.

```

If I'm passing the optional parameter `s=(1,)`, things work as expected.
My question is: why is the default dimension of the output array chosen in a way that it breaks for input length 1? Intuitively I would have expected a default output length of `2*input length - 1`, because this choice never loses information during the transform.

The docstring for `irfftn` is unfortunately also quite ambiguous as far as the output array size is concerned.",2019-04-17 13:24:49,,irfftn: default output array size is not helpful,"['01 - Enhancement', 'component: numpy.fft', '54 - Needs decision']"
13356,open,ghost,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->
```shell
(base) E:\backup\workspace\NonIdeal\Supertrapp\OriginalSRC\source>f2py -m test test.for

(base) E:\backup\workspace\NonIdeal\Supertrapp\OriginalSRC\source>call ""C:\Anaconda3\Scripts\\..\python.exe"" ""C:\Anaconda3\Scripts\\f2py.py"" -m test test.for
Reading fortran codes...
        Reading file 'test.for' (format:fix,strict)
Post-processing...
        Block: test
{}
In: :test:test.for:ETA0
vars2fortran: No typespec for argument ""TK"".
{'attrspec': ['dimension(*)']}
In: :test:test.for:ETA0
vars2fortran: No typespec for argument ""X"".
                        Block: ETA0
Post-processing (stage 2)...
Building modules...
        Building module ""test""...
                Creating wrapper for Fortran function ""ETA0""(""ETA0"")...
                Constructing wrapper function ""ETA0""...
getarrdims:warning: assumed shape array, using 0 instead of '*'
                  ETA0 = ETA0(TK,X)
                Creating wrapper for Fortran function ""ETA1""(""ETA1"")...
Traceback (most recent call last):
  File ""C:\Anaconda3\Scripts\\f2py.py"", line 28, in <module>
    main()
  File ""C:\Anaconda3\lib\site-packages\numpy\f2py\f2py2e.py"", line 650, in main
    run_main(sys.argv[1:])
  File ""C:\Anaconda3\lib\site-packages\numpy\f2py\f2py2e.py"", line 440, in run_main
    ret = buildmodules(postlist)
  File ""C:\Anaconda3\lib\site-packages\numpy\f2py\f2py2e.py"", line 384, in buildmodules
    dict_append(ret[mnames[i]], rules.buildmodule(modules[i], um))
  File ""C:\Anaconda3\lib\site-packages\numpy\f2py\rules.py"", line 1196, in buildmodule
    api, wrap = buildapi(nb)
  File ""C:\Anaconda3\lib\site-packages\numpy\f2py\rules.py"", line 1348, in buildapi
    rout, wrap = func2subr.assubr(rout)
  File ""C:\Anaconda3\lib\site-packages\numpy\f2py\func2subr.py"", line 278, in assubr
    fvar = rout['vars'][fname]
KeyError: 'ETA1'

(base) E:\backup\workspace\NonIdeal\Supertrapp\OriginalSRC\source>python
Python 3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy as np
>>> np.__version__
'1.13.3'
>>> exit()
```


`test.for`
```Fortran
      DOUBLE PRECISION FUNCTION ETA0(TK,X)
CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
C
C     PURPOSE -- THIS ROUTINE CALCULATES THE VISCOSITY OF A DILUTE
C                GAS MIXTURE OR PURE FLUID ASSUMING THAT THE COMPONENTS
C                OBEY A LENNARD-JONES FORCE LAW.
C
CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
      IMPLICIT DOUBLE PRECISION (A-H,O-Z)
      PARAMETER (MX = 20, MK = MX*(MX+1)/2, MX1 = MX + 1)
      COMMON /COMPR/  PC(MX),DC(MX),TC(MX),W(MX),CMW(MX),TB(MX),ZC(MX),
     1                CPC(MX,7), HRF(MX), SRF(MX),AKS(MX),OMK(MK),
     2                OML(MK), PRKIJ(MK),TTP(MX),DHF(MX),DVP(MX)
      COMMON /COMPI/ NC, ND(MX), NQ
      COMMON /LJ /  SIG(MX),EPS(MX),LJFLAG(MX)
      DIMENSION EOK(MX,MX), SIGMA(MX,MX), RM(MX,MX), H(MX,MX1), X(*)
      DATA DCON, VCON / 32.0300D0, 26.69167D0 /
C
      NC1 = NC + 1
      DO 020 I = 1, NC
      DO 020 J = 1, NC
      RM(I,J) = 2.0 * CMW(I) * CMW(J) / (CMW(I) + CMW(J))
      EOK(I,J) = SQRT(abs(EPS(I) * EPS(J)))
  020 SIGMA(I,J) = 0.5 * (SIG(I) + SIG(J))
C
  040 DO 080 I = 1, NC
      DO 080 J = 1, NC
      DIJ = 0.0
      IF (I.EQ.J) DIJ = 1.0
      SUML = 0.0
      DO 060 L = 1, NC
      TS = TK / EOK(I,L)
      TERM = SQRT(RM(I,L)*TK) / SIGMA(I,L)**2
      RHODIL = DCON * TERM / OMEGAS(1,1,TS)
      ETAIL  = VCON * TERM / OMEGAS(2,2,TS)
      DJL = 0.0
      IF (J.EQ.L) DJL = 1.0
      TERM = (RM(J,J)/(RHODIL*RM(L,L)))*(DIJ-DJL) + 0.5*(DIJ+DJL)/ETAIL
      SUML = SUML + X(L) * RM(I,L) * RM(I,L) * TERM
  060 CONTINUE
      H(I,J) = SUML / (RM(I,I) * RM(J,J))
  080 CONTINUE
      DO 100 I = 1, NC
  100 H(I,NC1) = 1.0
      DO 160 I = 1, NC
      I1 = I + 1
      DO 120 J = I1, NC1
  120 H(I,J) = H(I,J) / H(I,I)
      H(I,I) = 1.0
      DO 160 J = 1, NC
      IF (J.EQ.I) GO TO 160
      DO 140 K = I1, NC1
  140 H(J,K) = H(J,K) - H(J,I) * H(I,K)
      H(J,I) = 0.0
  160 CONTINUE
      ETA = 0.0
      DO 180 I = 1, NC
  180 ETA = ETA + X(I) * H(I,NC1)
      ETA0 = ETA
      RETURN
C
C                  THIS ENTRY CALCULATES THE VISCOSITY OF A
C                  PURE COMPONENT ""K"" AT TK
C
      ENTRY ETA1(TK,KK)
      TS = TK / EPS(KK)
      TERM = SQRT(CMW(KK)*TK) / SIG(KK)**2
      ETA1  = VCON * TERM / OMEGAS(2,2,TS)
      RETURN
      END
```


<!-- Remove these sections for a feature request -->

### Error message:

<!-- Full error message, if any (starting from line Traceback: ...) -->
```
Reading fortran codes...
        Reading file 'test.for' (format:fix,strict)
Post-processing...
        Block: test
{}
In: :test:test.for:ETA0
vars2fortran: No typespec for argument ""TK"".
{'attrspec': ['dimension(*)']}
In: :test:test.for:ETA0
vars2fortran: No typespec for argument ""X"".
                        Block: ETA0
Post-processing (stage 2)...
Building modules...
        Building module ""test""...
                Creating wrapper for Fortran function ""ETA0""(""ETA0"")...
                Constructing wrapper function ""ETA0""...
getarrdims:warning: assumed shape array, using 0 instead of '*'
                  ETA0 = ETA0(TK,X)
                Creating wrapper for Fortran function ""ETA1""(""ETA1"")...
Traceback (most recent call last):
  File ""C:\Anaconda3\Scripts\\f2py.py"", line 28, in <module>
    main()
  File ""C:\Anaconda3\lib\site-packages\numpy\f2py\f2py2e.py"", line 650, in main
    run_main(sys.argv[1:])
  File ""C:\Anaconda3\lib\site-packages\numpy\f2py\f2py2e.py"", line 440, in run_main
    ret = buildmodules(postlist)
  File ""C:\Anaconda3\lib\site-packages\numpy\f2py\f2py2e.py"", line 384, in buildmodules
    dict_append(ret[mnames[i]], rules.buildmodule(modules[i], um))
  File ""C:\Anaconda3\lib\site-packages\numpy\f2py\rules.py"", line 1196, in buildmodule
    api, wrap = buildapi(nb)
  File ""C:\Anaconda3\lib\site-packages\numpy\f2py\rules.py"", line 1348, in buildapi
    rout, wrap = func2subr.assubr(rout)
  File ""C:\Anaconda3\lib\site-packages\numpy\f2py\func2subr.py"", line 278, in assubr
    fvar = rout['vars'][fname]
KeyError: 'ETA1'

```
### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
`1.13.3 3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)]`
",2019-04-17 13:17:16,,f2py fail to generate *.pyf file for a F77 source code,['component: numpy.f2py']
13349,open,nrhinehart,"<!-- Please describe the issue in detail here, and fill in the fields below -->

I found a few basic input configurations to `np.arange` that yield (very) incorrect outputs. When the output dtype is integer, the results with a floating-point step are unexpected. I'm not familiar with its implementation, but I expected the output to appear roughly as if floating point numbers were generated internally, and then rounded to integers for output storage. The cases where all `0`s are generated are particularly concerning. The `stop` parameter is often wildly violated -- the max output can be much larger than the `stop` parameter, or can be positive when `(start, stop)` spans only negative numbers. Only the `dtype=float32` cases are expected, all other cases are not expected.

Although the documentation warns about unexpected output, I think these failure modes are too unexpected to happen silently.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
$ python
Python 3.5.2 (default, Nov 12 2018, 13:43:14) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy as np
>>> print(np.arange(start=0, stop=5, step=0.5, dtype=np.float32))
[0.  0.5 1.  1.5 2.  2.5 3.  3.5 4.  4.5]
>>> print(np.arange(start=0, stop=5, step=0.5, dtype=np.int32))
[0 0 0 0 0 0 0 0 0 0]
>>> print(np.arange(start=0, stop=5, step=0.5, dtype=np.int64))
[0 0 0 0 0 0 0 0 0 0]
>>> print(np.arange(start=-2, stop=2, step=0.5, dtype=np.float32))
[-2.  -1.5 -1.  -0.5  0.   0.5  1.   1.5]
>>> print(np.arange(start=-2, stop=2, step=0.5, dtype=np.int32))
[-2 -1  0  1  2  3  4  5]
>>> print(np.arange(start=-2, stop=2, step=0.5, dtype=np.int64))
[-2 -1  0  1  2  3  4  5]
>>> print(np.arange(start=-10, stop=10, step=0.5, dtype=np.float32))
[-10.   -9.5  -9.   -8.5  -8.   -7.5  -7.   -6.5  -6.   -5.5  -5.   -4.5
  -4.   -3.5  -3.   -2.5  -2.   -1.5  -1.   -0.5   0.    0.5   1.    1.5
   2.    2.5   3.    3.5   4.    4.5   5.    5.5   6.    6.5   7.    7.5
   8.    8.5   9.    9.5]
>>> print(np.arange(start=-10, stop=10, step=0.5, dtype=np.int32))
[-10  -9  -8  -7  -6  -5  -4  -3  -2  -1   0   1   2   3   4   5   6   7
   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25
  26  27  28  29]
>>> print(np.arange(start=-10, stop=10, step=0.5, dtype=np.int64))
[-10  -9  -8  -7  -6  -5  -4  -3  -2  -1   0   1   2   3   4   5   6   7
   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25
  26  27  28  29]
>>> print(np.arange(start=-5, stop=0, step=0.5, dtype=np.float32))
[-5.  -4.5 -4.  -3.5 -3.  -2.5 -2.  -1.5 -1.  -0.5]
>>> print(np.arange(start=-5, stop=0, step=0.5, dtype=np.int32))
[-5 -4 -3 -2 -1  0  1  2  3  4]
>>> print(np.arange(start=-5, stop=0, step=0.5, dtype=np.int64))
[-5 -4 -3 -2 -1  0  1  2  3  4]
>>> print(np.arange(start=-1, stop=0, step=0.1, dtype=np.float32))
[-1.         -0.9        -0.79999995 -0.6999999  -0.5999999  -0.49999988
 -0.39999986 -0.29999983 -0.19999981 -0.09999979]
>>> print(np.arange(start=-1, stop=0, step=0.1, dtype=np.int32))
[-1  0  1  2  3  4  5  6  7  8]
>>> print(np.arange(start=-1, stop=0, step=0.1, dtype=np.int64))
[-1  0  1  2  3  4  5  6  7  8]
>>> print(np.__version__)
1.14.5
```

<!-- Remove these sections for a feature request -->
<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

numpy version '1.14.5'

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2019-04-16 15:41:49,,np.arange outputs very unexpected with integer dtype and floating point step,['00 - Bug']
13338,open,lstolcman,"Hi. I have a big array (shapes 40k x 40k and bigger), and I want to use `memmap` and `multiprocessing` to fill it. I coded simple example to test it, and it does not work on windows. Expected result is to have values saved by each process (see screenshots).

Any suggestions?

Valid result (tested on virtualboxed xubuntu and MacOS):
![obraz](https://user-images.githubusercontent.com/4583553/56160653-06947000-5fc8-11e9-9732-8fdd7772506d.png)

Invalid result - array empty (Windows 7):
![obraz](https://user-images.githubusercontent.com/4583553/56160625-f2507300-5fc7-11e9-9259-d6e7741e05a6.png)


### Reproducing code example:

```python
import os
import numpy as np
import time
import multiprocessing as mp


def child(v, lock, m, p):
    np.random.seed(int.from_bytes(os.urandom(4), byteorder='little'))
    while True:
        vval = 0
        with lock:
            if v.value > 0:
                v.value -= 1
                vval = v.value
            else:
                break
        sl = np.random.rand()
        print(f'p{p} sleeping for {sl}')
        time.sleep(sl)
        m[0, vval] = p
        print(f'proc {p}: memmap[{vval}]={p} memmap={m}')



if __name__ == '__main__':
    memmap = np.memmap(
        'test1.memmap',
        dtype='int',
        mode='w+', shape=(1,10)
        )

    value = mp.Value('i', 10)
    lock = mp.Lock()
    procs = [mp.Process(
        target=child,
        args=(value, lock, memmap, i)
        ) for i in range(1,3)]
    for proc in procs:
        proc.start()
    for proc in procs:
        proc.join()
    
    print(f'after join, result:')
    print(memmap)

```

### Numpy/Python version information:
windows: `1.16.2 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]`
linux: `1.16.2 3.6.7 (default, Oct 22 2018, 11:32:17) [GCC 8.2.0]`
",2019-04-15 19:50:02,,np.memmap works differently on windows and linux/mac,['00 - Bug']
13319,open,apontzen,"<!-- Please describe the issue in detail here, and fill in the fields below -->
`numpy.fromfile` is drastically inefficient for small reads on python 3; orders of magnitude slower than the same call on python 2. I believe this is because of changes made in response to #4118, keeping things in sync despite the IO buffering in python 3.

Naively implementing a pure python version of `fromfile` reveals that better performance is available, even in python 3.x. Is it possible to improve the performance of numpy.fromfile to match such a reference implementation?

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
import contextlib
import time
import sys
import os

@contextlib.contextmanager
def timeme(label):
    start = time.time()
    yield
    end = time.time()
    diff = (end-start)*1000
    print(""%s: %.1f ms""%(label.rjust(45), diff))

def py_fromfile(f, dtype, num):
    buf = np.empty(num, dtype)
    f.readinto(buf)
    return buf

def make_test_file():
    test_data = np.random.uniform(size=10**6)
    with open('testdata', 'wb') as f:
        test_data.tofile(f)

def remove_test_file():
    os.remove(""testdata"")

def time_read(num_chunks, reading_method):
    py_version = sys.version_info
    reading_name = reading_method.__name__
    test_label = 'Read in %d chunk(s), %s, py%d.%d'%(num_chunks, reading_name,
                                                 py_version[0], py_version[1])
    make_test_file()

    with timeme(test_label):
        with open('testdata', 'rb') as f:
            for i in range(num_chunks):
                reading_method(f, np.float32, 10**6//num_chunks)

    remove_test_file()

time_read(1, np.fromfile)
time_read(10000, np.fromfile)
time_read(10000, py_fromfile)
```

Running with any recent version of numpy on python 2.7 and 3.7 respectively gives results along the following lines:

```
          Read in 1 chunk(s), fromfile, py3.7: 2.0 ms
      Read in 10000 chunk(s), fromfile, py3.7: 128.0 ms
   Read in 10000 chunk(s), py_fromfile, py3.7: 16.3 ms

          Read in 1 chunk(s), fromfile, py2.7: 2.0 ms
      Read in 10000 chunk(s), fromfile, py2.7: 6.6 ms
   Read in 10000 chunk(s), py_fromfile, py2.7: 20.6 ms
```

Thus, on python 2.7, `numpy.fromfile` is as efficient as it can be (far more efficient than the pure python implementation), but on python 3.7 (and other 3.x) `numpy.fromfile` drastically underperforms relative to the pure python implementation. This suggests a better implementation for `fromfile` may be possible.

### Numpy/Python version information:

This can be reproduced on all recent versions of numpy as far as I can tell. However, the stats given above are from the following two setups:

python 2:
```
1.16.2 2.7.16 |Anaconda, Inc.| (default, Mar 14 2019, 16:24:02) 
[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]
```

python 3:
```
1.16.2 3.7.2 (default, Dec 29 2018, 00:00:04) 
[Clang 4.0.1 (tags/RELEASE_401/final)]
```



",2019-04-13 09:54:55,,Performance of fromfile on Python 3,['unlabeled']
13317,open,dmbelov,"<!-- Please describe the issue in detail here, and fill in the fields below -->
There is a bug in function `add.at` that causes **core dump** on certain unaligned arrays.  Below is an example that shows the problem.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np

x_a = np.zeros(10, dtype='i8')
y_i = np.array(list(zip(np.arange(10), np.arange(10), np.ones(10, dtype='i1'))), dtype=[('a', 'i4'), ('b', 'i4'), ('c', 'i1')])
a_idx_i = np.arange(10, dtype='i8')

# the next line results in *core dump* with the following error msg:
# python: numpy/core/src/multiarray/lowlevel_strided_loops.c.src:823: _aligned_cast_int_to_long: Assertion `npy_is_aligned(src, __builtin_offsetof(struct {char c; npy_int v;}, v))' failed.
# Aborted
np.add.at(x_a, a_idx_i, y_i['b'])

```

Note, that the code above works without any errors if one changes  type `i4` to `i8` in the definition of `y_i` or removes the field `c` from the definition of `y_i`.

<!-- Remove these sections for a feature request -->

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->
```
python: numpy/core/src/multiarray/lowlevel_strided_loops.c.src:823: _aligned_cast_int_to_long: Assertion `npy_is_aligned(src, __builtin_offsetof(struct {char c; npy_int v;}, v))' failed.
Aborted
```

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:
<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
`('1.15.4', '2.7.15 |Anaconda custom (64-bit)| (default, Oct 10 2018, 21:32:13) \n[GCC 7.3.0]')
`
",2019-04-12 16:13:41,,Bug in function add.at (core dump),"['00 - Bug', 'component: numpy.ufunc']"
13307,open,tylerjereddy,"This came up in a recent community call, as a point of discussion on behalf of @zemanj. We didn't discuss it much in their absence, but I am placing the same summary of the idea here for potential interest/ assessment. Mostly for the low-level gurus, I suspect.

> The way NumPy implements broadcasting when executing operations such as `matrix += vector` on C-contiguous arrays apparently copies data to intermediate buffers more often than necessary before calling vectorized routines (e.g., `sse2_binary_add_FLOAT()`). See also the `gdb` log in [this comment](https://github.com/MDAnalysis/mdanalysis/issues/2210#issuecomment-480805965) and the `kcachegrind` visualization in [this comment](https://github.com/MDAnalysis/mdanalysis/issues/2210#issuecomment-480817042). There is [some interest](https://github.com/MDAnalysis/mdanalysis/pull/2211#discussion_r274025811) in shortcuts around `npyiter_copy_to_buffers()`",2019-04-11 19:24:23,,ENH: broadcasting perf on C-contiguous arrays,"['01 - Enhancement', '23 - Wish List', 'Project']"
13285,open,kavvkon,"<!-- Please describe the issue in detail here, and fill in the fields below -->
numpy minimum() does not work on Int64Index. 
It works with the built-in min() and pandas ufunc min() though.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
import pandas as pd
>>> x = pd.Int64Index([1,2,3])
>>> x.min()
1
>>> min(x)
1
```

<!-- Remove these sections for a feature request -->

### Error message:

<!-- Full error message, if any (starting from line Traceback: ...) -->

```python
>>> np.min(x)
 Traceback (most recent call last)
<ipython-input-6-0e92ebdb9ae5> in <module>
----> 1 np.min(x)

C:\PGM\KAVVKON\lib\site-packages\numpy\core\fromnumeric.py in amin(a, axis, out, keepdims, initial)
   2440     """"""
   2441     return _wrapreduction(a, np.minimum, 'min', axis, None, out, keepdims=keepdims,
-> 2442                           initial=initial)
   2443
   2444

C:\PGM\KAVVKON\lib\site-packages\numpy\core\fromnumeric.py in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)
     79                 return reduction(axis=axis, dtype=dtype, out=out, **passkwargs)
     80             else:
---> 81                 return reduction(axis=axis, out=out, **passkwargs)
     82
     83     return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

TypeError: min() got an unexpected keyword argument 'axis'

```

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```python
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.15.4 3.6.8 |Anaconda 4.4.0 (64-bit)| (default, Dec 30 2018, 18:50:55) [MSC v.1915 64 bit (AMD64)]
```",2019-04-08 17:34:40,,Cannot use np.min() on pd.Int64Index,['unlabeled']
13284,open,maxnoe,"<!-- Please describe the issue in detail here, and fill in the fields below -->

`np.log` is much slower when given an array of integers, compared to floats.
Casting the integer array to floats makes it a lot faster.


### Reproducing code example
```python:
In [2]: import numpy as np

In [3]: a = np.random.uniform(1, 10_000, 100_000)                                                                   

In [4]: b = a.astype('int64')                                                                                       

In [5]: %timeit np.log(a)                                                                                           
116 µs ± 377 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)

In [6]: %timeit np.log(b)                                                                                           
2.16 ms ± 18.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

In [7]: %timeit np.log(b.astype(float))                                                                             
178 µs ± 246 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)

In [8]: np.allclose(np.log(b), np.log(b.astype(float))) 
Out[8]: True

```
### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```
1.16.2 3.7.2 (default, Dec 29 2018, 06:19:36) 
[GCC 7.3.0]
```

",2019-04-08 16:39:09,,log much slower on integers than floats,['unlabeled']
13248,open,bv-research,"<!-- Please describe the issue in detail here, and fill in the fields below -->
Using PyInstaller with Python optimization flag [-OO](https://docs.python.org/3/using/cmdline.html#cmdoption-oo) (which additionally removes docstrings) crashes while importing numpy package.

While running generated executable file the next error shows up:
```bash
TypeError: add_docstring() argument 2 must be str, not None
```

The current implementation is:
https://github.com/numpy/numpy/blob/d89c4c7e7850578e5ee61e3e09abd86318906975/numpy/core/overrides.py#L160-L168
By default `docs_from_dispatcher` is `True`, due to the optimization flag the `dispatcher` will have no docstrings. `dispatcher.__doc__` will be `None`

https://github.com/numpy/numpy/blob/d89c4c7e7850578e5ee61e3e09abd86318906975/numpy/core/overrides.py#L143-L144

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
print(np.array([1,2,3]))
```

## Running PyInstaller
```bash
python -OO -m PyInstaller example.py
```
<!-- Remove these sections for a feature request -->

### Error message:
```bash
Traceback (most recent call last):
  File ""test.py"", line 1, in <module>
    import numpy as np
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""C:\Users\bv\AppData\Local\Programs\Python\Python37\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 627, in exec_module
  File ""site-packages\numpy\__init__.py"", line 142, in <module>
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""C:\Users\bv\AppData\Local\Programs\Python\Python37\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 627, in exec_module
  File ""site-packages\numpy\core\__init__.py"", line 40, in <module>
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""C:\Users\bv\AppData\Local\Programs\Python\Python37\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 627, in exec_module
  File ""site-packages\numpy\core\multiarray.py"", line 74, in <module>
  File ""site-packages\numpy\core\overrides.py"", line 186, in decorator
  File ""site-packages\numpy\core\overrides.py"", line 150, in decorator
TypeError: add_docstring() argument 2 must be str, not None
```
<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:
Numpy/Python version information:
1.16.2 3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 22:22:05) [MSC v.1916 64 bit (AMD64)]

### Other modules version information:
PyInstaller==3.4

### System information:
Windows 10.0.16299
<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2019-04-02 13:45:57,,"BUG: TypeError: add_docstring() argument 2 must be str, not None",['unlabeled']
13233,open,2sn,"<!-- Please describe the issue in detail here, and fill in the fields below -->

It would be great to have an ""axes"" keyword in `np.cross` for consistent API to `np.tensordot`.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
a = [1, 2, 3]
b = [4, 5, 6]
np.cross(a, b, axes = (-1, -1))
```
Currently, rather ugly `axisa`, `axisb`, `axisc`, or just `axis` have to be provided.  I assume it could be added to overwrite the existing keywords for backward compatibility.   I propose that the new `axes` keyword may be an iterable (or tuple) of length two or three, withe the third value defaulting to `axisc` or `-1`.

",2019-03-31 13:25:45,,enhancement: axes keyword for cross,"['01 - Enhancement', 'component: numpy._core']"
13232,open,Saluev,"<!-- Please describe the issue in detail here, and fill in the fields below -->
When I generate two complex matrices of particular size and try to multiply their real parts, numpy gets stuck.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
A = np.zeros((2048, 4096), dtype=np.complex128)
B = np.zeros((4096, 2048), dtype=np.complex128)
A.real @ B.real
# Expected to run in few seconds, but runs forever. ^C doesn't work.
```

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.16.2 3.6.1 (default, Apr  4 2017, 09:40:51) 
[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)]
",2019-03-31 12:51:53,,Can't multiply real parts of two complex matrices,['unlabeled']
13229,open,Saluev,"<!-- Please describe the issue in detail here, and fill in the fields below -->

I've been doing some benchmarks on complex matrix multiplication in NumPy and noticed that some quite simple optimizations haven't been implemented. Is there a design-related reason for that or I can try to contribute it?

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
m, n, k = (2048, 4096, 2048)
A = np.random.uniform(size=(m, n)) + 1j * np.random.uniform(size=(m, n))
B = np.random.uniform(size=(n, k)) + 1j * np.random.uniform(size=(n, k))

C = A @ B;
# CPU times: user 13.8 s, sys: 84.6 ms, total: 13.9 s
# Wall time: 7.45 s

C1 = A.real @ B.real
C2 = A.imag @ B.imag
C3 = (A.real + A.imag) @ (B.real + B.imag)
C = (C1 - C2) + 1j * (C3 - C1 - C2);
# CPU times: user 7.4 s, sys: 178 ms, total: 7.57 s
# Wall time: 3.37 s

# Check that relative error is fine
np.linalg.norm(C - A@B) / np.linalg.norm(A@B)
# 3.9738720532213243e-16
```

(Execution time is measured by Jupyter's `%%time` magic command.)

### Numpy/Python version information:

1.13.1 3.6.1 (default, Apr  4 2017, 09:40:51) 
[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)]


",2019-03-31 11:55:02,,Complex matmul is very slow (and here's a 2x speedup),"['component: numpy.linalg', 'component: numpy._core', '28 - Benchmark']"
13213,open,rnhmjoj,"### Reproducing code example:
```python
import numpy as np
import numpy.lib.recfunctions as rfn
x = np.array([(None)], dtype=[('a', object)])
rfn.rename_fields(x, {'a': 'A'})
```

### Error message:

```
Traceback (most recent call last):
  File ""python3-3.7.2-env/lib/python3.7/site-packages/numpy/lib/recfunctions.py"", line 668, in rename_fields
    return base.view(newdtype)
  File ""python3-3.7.2-env/lib/python3.7/site-packages/numpy/core/_internal.py"", line 494, in _view_is_safe
    raise TypeError(""Cannot change data-type for object array."")
TypeError: Cannot change data-type for object array.
```

### Numpy/Python version information:

Numpy: 1.16.1
Python: 3.7.2
",2019-03-29 10:01:22,,rfn.rename_fields fails if arrays contains objects,"['00 - Bug', 'component: numpy.lib']"
13201,open,Kai-Striega,"Currently `asarray` et al. ensure passed values are arrays with the given properties e.g. `ascontiguousarray` ensures an array with row-major (c like) storage, returning a copy of the array if it is not. A simple use case could be ensuring an array is column-major before being passed to a fortran function e.g. in [SciPy](https://github.com/scipy/scipy/blob/12fe2866a9d489d2e4eabd279d1abb930a0c96b6/scipy/linalg/_interpolative_backend.py#L231-L232):

```python
A = np.asfortranarray(A) # ensures `A` is a fortran array
idx, rnorms = _id.iddr_id(A, k) # fortran function, changes `A` inplace
```

If the array already satisfies the requirements a new copy is **not** made, as, in general, it is not required. This leads to bugs (see [gh-9793](https://github.com/scipy/scipy/issues/9793), [gh-9964](https://github.com/scipy/scipy/issues/9964)) when the underlying function changes the  array in place, which can be hard to debug, as the array is sometimes copied and sometimes not.

In these situations, forcing a copy would be useful. e.g. the above code may have the signature:

```python
A = np.asfortranarray(A, copy=True)
idx, rnorms = _id.iddr_id(A, k) 
```

Which would *always* return a copied array. Looking at the source code this should be relatively simple by exposing the `copy` argument used in the functions and updating the documentation.

If exposing `copy` seems like a good idea, I would be willing to implement/document it.",2019-03-28 06:22:27,,Expose `copy` argument in asanyarray et al.,['unlabeled']
13199,open,0x0L,"Hello,

It looks like numpy is allocating a whole new array to compute the second moment as seen in [_methods.py](https://github.com/numpy/numpy/blob/e6147b9bf580361f2f74ac72003f81e957587528/numpy/core/_methods.py#L117).

This could be easily improved by adding a low level function to compute the sum of squares (or any power for fast nth moment computation) along a dimension and expanding the square in `_var`.

It does not look too complicated but I'm really unsure on how to do the low level function (`loops.c.src` ?)

This should help with https://github.com/scikit-learn/scikit-learn/issues/5651
",2019-03-27 21:16:49,,"var, std memory consumption","['01 - Enhancement', 'component: numpy._core']"
13197,open,eric-wieser,"Creating an issue for this so we can track it

N days ought to be less than N weeks, but that falls apart when near the upper bound of the ranges:

```python
import numpy as np
d = 2**62
print(np.datetime64(d, 'W'))
# numpy.datetime64('-12626367463881308-09-18')
```
",2019-03-27 04:38:46,,"np.datetime64(big, 'W') overflows","['00 - Bug', 'component: numpy.datetime64']"
13190,open,Koushikphy,"I have a Fortran 90 code, with OpenMP implemented for some parts. Now to compile the Fortran code using f2py with OpenMP support I have to pass the f90 compiler flags. Now the code should compile with OpenMP support only if I provide the OpenMP relevant flag, unless it must be compiled as a normal serial code. This works as expected for gfortran but for ifort it doesn't. Lets explain this,

if I run, 
(gfortran serial mode)

    f2py -c adt.f90 -m adt_module 
    *Gives output (last few lines)*
    Fortran f77 compiler: /usr/bin/gfortran -Wall -g -ffixed-form -fno-second-underscore -fPIC -O3 -funroll-loops
    Fortran f90 compiler: /usr/bin/gfortran -Wall -g -fno-second-underscore -fPIC -O3 -funroll-loops
    Fortran fix compiler: /usr/bin/gfortran -Wall -g -ffixed-form -fno-second-underscore -Wall -g -fno-second-underscore -fPIC -O3 -funroll-loops


(gfortran parallel mode)

    f2py -c adt.f90 -m adt_module --f90flags='-fopenmp' -lgomp 
    *output*
    Fortran f77 compiler: /usr/bin/gfortran -Wall -g -ffixed-form -fno-second-underscore -fPIC -O3 -funroll-loops
    Fortran f90 compiler: /usr/bin/gfortran -fopenmp -fPIC -O3 -funroll-loops
    Fortran fix compiler: /usr/bin/gfortran -Wall -g -ffixed-form -fno-second-underscore -fopenmp -fPIC -O3 -funroll-loops


As you can clearly see there is no -fopenmp flag during the compilation in serial mode, as expected as I haven't pass the required flag

Now for ifort

(ifort parallel mode)

    f2py -c adt.f90 -m adt_module --fcompiler=intelem --f90flags='-qopenmp' -liomp5
    *Output*
    Fortran f77 compiler: /opt/intel/compilers_and_libraries_2017.4.196/linux/bin/intel64/ifort -FI -fPIC -fp-model strict -O1 -qopenmp 
    Fortran f90 compiler: /opt/intel/compilers_and_libraries_2017.4.196/linux/bin/intel64/ifort -FR -qopenmp -fPIC -fp-model strict -O1 -qopenmp 
    Fortran fix compiler: /opt/intel/compilers_and_libraries_2017.4.196/linux/bin/intel64/ifort -FI -qopenmp -fPIC -fp-model strict -O1 -qopenmp

(ifort serial mode)

    f2py -c adt.f90 -m adt_module --fcompiler=intelem
    *Output*
    Fortran f77 compiler: /opt/intel/compilers_and_libraries_2017.4.196/linux/bin/intel64/ifort -FI -fPIC -fp-model strict -O1 -qopenmp 
    Fortran f90 compiler: /opt/intel/compilers_and_libraries_2017.4.196/linux/bin/intel64/ifort -FR -fPIC -fp-model strict -O1 -qopenmp 
    Fortran fix compiler: /opt/intel/compilers_and_libraries_2017.4.196/linux/bin/intel64/ifort -FI -fPIC -fp-model strict -O1 -qopenmp

Now, here's the problem, see here compiling is done with -qopenmp flag for serial mode but I haven't passed it in command line.

Python/Numpy version:
import sys, numpy; print(numpy.__version__, sys.version)
('1.15.4', '2.7.5 (default, Oct 30 2018, 23:45:53) \n[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)]')
",2019-03-26 11:39:56,,f2py compile with the -qopenmp ifort flag even in the serial mode,['component: numpy.distutils']
13189,open,oceandatalab,"<!-- Please describe the issue in detail here, and fill in the fields below -->
It seems the behavior of either ``numpy.random.normal`` or ``numpy.abs`` is not consistent across platforms (at least on Linux and macOS). We use the output of ``numpy.abs`` as the ``scale`` parameter passed to ``numpy.random.normal``: a ValueError is raised when the array passed to ``numpy.abs`` contains a NaN value on some systems whereas the statement is executed without error on others.

Besides, when passing ``numpy.array([numpy.nan])`` (which is the output of ``numpy.abs(numpy.array([numpy.nan]))``) as the ``scale`` parameter, there is no error, so there might be a problem with the in-memory representation of the result returned by ``numpy.abs`` on some systems.

We tested our code on Linux and didn't check for NaNs at first: since we have been facing issues with wheels on multiple Python packages lately, our first move has been to reinstall NumPy from the source distribution with:
``` bash
pip uninstall -y numpy
pip install --nobinary :all: numpy
```
and with this the ValueError went away, so we thought it was just another issue caused by the libraries linked in the wheel.

But then we tested on macOS and this workaround didn't work. Reinstalling both Cython and NumPy with ``--no-binary :all:`` had no effect.

Sorry if this is a bit confuse, the example code and the outputs provided below should make things clearer.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->
```python
import numpy
numpy.abs(numpy.array([numpy.nan]))
numpy.random.normal(0.0, numpy.array([numpy.nan]), 1)
numpy.random.normal(0.0, numpy.abs(numpy.array([numpy.nan])), 1)
```

<!-- Remove these sections for a feature request -->

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->
```ipython
Traceback (most recent call last)
<ipython-input-5-26abddb224ea> in <module>
----> 1 numpy.random.normal(0, numpy.abs(numpy.array([numpy.nan])), 1)

mtrand.pyx in mtrand.RandomState.normal()

ValueError: scale < 0
```

On Linux with a NumPy package installed using the wheel:
```python
Python 3.7.2 (default, Jan 10 2019, 23:51:51)
[GCC 8.2.1 20181127] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy
>>> numpy.abs(numpy.array([numpy.nan]))
array([nan])
>>> numpy.random.normal(0.0, numpy.array([numpy.nan]), 1)
array([nan])
>>> numpy.random.normal(0.0, numpy.abs(numpy.array([numpy.nan])), 1)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""mtrand.pyx"", line 1658, in mtrand.RandomState.normal
ValueError: scale < 0
```

On Linux with a NumPy package reinstalled from source:
```python
Python 3.7.2 (default, Jan 10 2019, 23:51:51)
[GCC 8.2.1 20181127] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy
>>> numpy.abs(numpy.array([numpy.nan]))
array([nan])
>>> numpy.random.normal(0.0, numpy.array([numpy.nan]), 1)
array([nan])
>>> numpy.random.normal(0.0, numpy.abs(numpy.array([numpy.nan])), 1)
array([nan])
```

On macOS with a NumPy package reinstalled from source (the wheel version fails as well):
```ipython
Python 3.7.2 (default, Feb 12 2019, 08:15:36)
Type 'copyright', 'credits' or 'license' for more information
IPython 7.2.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import numpy

In [2]: numpy.abs(numpy.array([numpy.nan]))
Out[2]: array([nan])

In [3]: numpy.random.normal(0.0, numpy.array([numpy.nan]), 1)
Out[3]: array([nan])

In [4]: numpy.random.normal(0, numpy.abs(numpy.array([numpy.nan])), 1)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-3-26abddb224ea> in <module>
----> 1 numpy.random.normal(0, numpy.abs(numpy.array([numpy.nan])), 1)

mtrand.pyx in mtrand.RandomState.normal()

ValueError: scale < 0
```

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
On ArchLinux:
```python
import sys, numpy; print(numpy.__version__, sys.version)
1.16.2 3.7.2 (default, Jan 10 2019, 23:51:51)
[GCC 8.2.1 20181127]
```

On mac OS mojave (10.14.3):
```python
import sys, numpy; print(numpy.__version__, sys.version)
1.16.2 3.7.2 (default, Feb 12 2019, 08:15:36)
[Clang 10.0.0 (clang-1000.11.45.5)]
```


",2019-03-26 10:41:33,,Platform-dependent behavior of either numpy.random.normal or numpy.abs,['unlabeled']
13185,open,zadeck,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:
>>> import numpy as np
>>> a = [[1,2],[3,4]]
>>> np.amin(a, axis=(1))
array([1, 3])
>>> np.amin(a, axis=(0,1))
1


The documentation states that 

Returns: | amin : ndarray or scalar Minimum of a. If axis is None, the result is a scalar value. If axis is given, the result is an array of dimension a.ndim - 1.
-- | --

However it seems that the documentation should really say that if keepdims=True, the result array will have the same number of dimensions as the input. If keepdims=False and the axis is given, the result is an array of dimension a.ndim - len (axis arg).  if keepdims is false and the axis is not given or is None, the result will be a scalar.

The issue that i am raising here is that if the len of the axis arg is > 1, the doc is just plain wrong.   

This doc bug seems to be repeated for a lot of other functions.

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

>>> np.__version__
'1.15.3'
",2019-03-25 19:00:14,,documentation bug for amin and many similar functions.,['unlabeled']
13179,open,mhvk,"This is a tracker for a number of requests for making working with complex numbers a bit easier/faster
- [ ] Get just the normalized number `z/|z|` (the [sgn](http://en.wikipedia.org/wiki/Sign_function) function) (see #25441, which changes the behaviour of `np.sign` to match the [Array API](https://data-apis.org/array-api/latest/API_specification/generated/array_api.sign.html) and extends it to `copysign`)
- [ ] Get the squared modulus of a complex number `z * z.conj()` (instead of the slow `np.abs(z)**2`). `square` might be logical but is already defined to just do `z*z`... (see #3994)
- [ ] A fast way to get the inverse of `angle`, i.e., `exp(1j * a) = cos(a) + 1j * sin(a)`. Note that for large angle arrays, `exp(1j*a)` needlessly triples memory use for the input (see #5625; somewhat related: `sincos` function, see #2626; discussion moved to #18483)
- [ ] Possibly, combined `abs` and `angle` and their inverse, perhaps most logical as part of providing polar to cartesian transformations (see #5228)

EDIT (2021-Jun-25): if there is worry about making the numpy API too big, one option might be to make this part of `numpy.lib.scimath` (which I must admit I didn't know about until today; I see now that it should be `np.emath` - https://numpy.org/devdocs/reference/routines.emath.html). Though perhaps that should stay reserved for complex continuations `real->complex` like for `sqrt`.",2019-03-22 21:14:54,,Ufuncs for complex numbers,"['23 - Wish List', 'component: numpy.ufunc']"
13172,open,tuxzz,"### Seems like a problem with large mmap memory page size
Should we add an interface like `madvise` to tell system how the memmapped array would be used?
This problem doesn't exist on Windows 10.

### Reproducing code example:

#### Version 1(~100x slower on Linux, fast enough on Windows):
```python
import numpy as np
import time

large_data = np.memmap(""/home/user/100GB.mmap"", dtype=np.float32, mode=""r"", order=""C"")
entry_count = large_data.size // 256
large_data = large_data.reshape(entry_count, 256)
t = time.time()
large_data[np.random.randint(0, entry_count, size=32768)].copy()
print(time.time() - t)
```

#### Version 2(fast enough on Linux):
```python
import numpy as np
import time
import ctypes

large_data = np.memmap(""/home/user/100GB.mmap"", dtype=np.float32, mode=""r"", order=""C"")

madvise = ctypes.CDLL(""libc.so.6"").madvise
madvise.argtypes = [ctypes.c_void_p, ctypes.c_size_t, ctypes.c_int]
madvise.restype = ctypes.c_int
assert madvise(large_data.ctypes.data, large_data.size * large_data.dtype.itemsize, 1) == 0, ""MADVISE FAILED"" # 1 means MADV_RANDOM

entry_count = large_data.size // 256
large_data = large_data.reshape(entry_count, 256)
t = time.time()
large_data[np.random.randint(0, entry_count, size=32768)].copy()
print(time.time() - t)
```

### Numpy/Python version information:
1.16.2 3.7.2 (default, Dec 30 2018, 16:18:15) [GCC]

### System information:
Linux: Linux 5.0.2 x86_64 + Samsung 860 EVO SSD with BtrFS
Windows: Windows 10 1809 + Samsung 860 EVO SSD with NTFS",2019-03-21 14:33:13,,ENH: Add `madvise` for `memmap` objects,"['01 - Enhancement', 'component: numpy._core', 'sprintable']"
13166,open,rabernat,"I frequently wish to calculate a histogram along each dimension of an array axis. For example, if dimension 0 is time, I might want to look at how the pdf of my data is evolving in time. This would be much easier if histogram (and its multidimensional variants) accepted an `axis` keyword argument. Currently histogram just flattens all its data.

### Reproducing code example:

```python
import numpy as np

# create some sample data
a = np.random.randn(5, 200)

# histogram over all dimensions, not what I want
h, bins = np.histogram(a)

# loop-based version of what I would like instead
bins = np.histogram_bin_edges(a)
individual_hists = [np.histogram(a[n], bins=bins)[0][None, :] for n in range(a.shape[0])]
full_hist = np.vstack(individual_hists)
np.testing.assert_array_equal(h, full_hist.sum(axis=0))

# the syntax I would prefer to use
full_hist, bins = np.histogram(a, axis=-1)
```

For performance reasons, it would obviously be ideal if this could be vectorized in an efficient way, rather than the naive loop I have written here.

Related stack-overflow question: https://stackoverflow.com/questions/44152436/calculate-histograms-along-axis",2019-03-20 19:22:47,,ENH: vectorized histogram with axis keyword,"['01 - Enhancement', 'triaged']"
13121,open,clbarnes,"As far as possible, an instance of `RandomState` should be a drop-in replacement for the `numpy.random` module, so that they can be passed transparently into a randomising procedure without having to make sure which methods the procedure is calling.

There exist a number of aliases for `np.random.random_sample` which are not present in `RandomState`:

- `ranf`
- `sample`
- `random`

`random`, in particular, is very commonly used, as it's the direct counterpart to python's built-in `random.random`. A cursory google suggests that `np.random.random` is mentioned more often than `np.random.random_sample` in tutorials. The lack of these aliases on `RandomState`, then, represent an impediment to porting determinism into existing code relying on unseeded `numpy.random` free functions (in cases where global seeding is not desired), e.g. https://github.com/aestrivex/bctpy/issues/67 .",2019-03-14 15:45:47,,ENH: RandomState should expose an API as similar as possible to numpy.random,['unlabeled']
13120,open,iago-lito,"As found on the help pages for [Legendre](https://docs.scipy.org/doc/numpy-1.16.0/reference/routines.polynomials.legendre.html) module:

function | description
-- | --
legval(x, c[, tensor]) | Evaluate a Legendre series at points x.
legval2d(x, y, c) | Evaluate a 2-D Legendre series at points (x, y).
legval3d(x, y, z, c) | Evaluate a 3-D Legendre series at points (x, y, z).
leggrid2d(x, y, c) | Evaluate a 2-D Legendre series on the Cartesian product of x and y.
leggrid3d(x, y, z, c) | Evaluate a 3-D Legendre series on the Cartesian product of x, y, and z.
legroots(c) | Compute the roots of a Legendre series.
legfromroots(roots) | Generate a Legendre series with given roots.

Why is there not a `legvalNd(x, c, n)` to evaluate a n-D Legendre series at points x ∈ ℝ^n ?

Is there an efficient way to write it myself?


",2019-03-14 14:22:40,,Multivariate Legendre polynomials,['component: numpy.polynomial']
13114,open,mattip,"Started in #13104, turned into a tracking issue here.

Turning on -n in sphinx-build results in over 1000 warnings about bad references in docs. Trying to break these down with various grep statements, it seems about 200 are from numpy.ma, about 150 from polynomial and about 150 from c:type. That is under half of the WARNINGS.

~My workflow is to cd to the `doc` directory, modify the `ALLSPHINXOPTS` in the `Makefile and add  `-n`~, My workflow is to  set an environment variable SPHINXOPTS='-n' then run
```
(cd ..; pip install .); make clean; make html 2>&1 |tee /tmp/build.txt
less /tmp/build.txt
```
and look for the most common lines. Unfortunately this takes quite a while.

Anyone who wishes to contribute can issue PRs, preferably in chunks that address one class of failure. #13104 gives an idea of the changes needed.",2019-03-13 20:46:53,,DOC: missing references when using -n in sphinx-build,['04 - Documentation']
13105,open,eric-wieser,"If only given 0d inputs, even if they are of type `ndarray`, ufuncs will decay their output to a scalar via `[()]` (as noted in #4563, #5819). While we can't change this behavior now without creating signficant pain downstream, we could add a way to opt out of it.

#13100 raises a case where `np.fix` resorts to calling `np.asanyarray(np.ceil(x, out=out))` in order to ensure that the result is an array. Unfortunately, this has a number of draw-backs:

* It discards duck-types, like `dask` arrays
* It changes the dtype of 0d object arrays containing array-likes


Proposed implementation:

* Create a new `np.leave_wrapped` sentinel object that can be passed as an `out` argument
* Add support in `ufunc.__call__`, `np.take`, ... for passing `out=np.leave_wrapped` meaning ""do not call PyArray_Return"", causing the result to never be a scalar
* Expose `PyArray_Return` to python as `np.core.unpack_scalar`
* Implement `np.fix` as:
```python
def fix(x, out=None):
    if out is None:
        do_unwrap = True
        out = np.leave_wrapped
    else:
        do_unwrap = False
    res = nx.ceil(x, out=out)
    res = nx.floor(x, out=res, where=nx.greater_equal(x, 0, out=np.leave_wrapped))

    if do_unwrap:
        res = np.unpack_scalar(res)   # PyArray_Return
    return res
```

<details>

<summary>

Original `unpack_scalars=True`  proposal

</summary>

* Add a new `unpack_scalars=True` kwarg to `ufunc.__call__`, `ufunc.reduce`. When `False`, the current behavior of going through `PyArray_Return` is disabled. Alternative names:
  * `decay=True`
  * `unpack_0d=True`
  * `unpack_0d_ndarray=True` (`PyArray_Return` already does not apply to subclasses)
* Add a new `np.unpack_scalar(arr)` function to expose `PyArray_Return` to python code. This would __not__ be overloadable with `__array_function__`, since existing uses of `PyArray_Return` are also not.

With these changes, the current implementation of `np.fix` would change from:
```python
def fix(x, out=None):
    res = nx.asanyarray(nx.ceil(x, out=out))
    res = nx.floor(x, out=res, where=nx.greater_equal(x, 0))

    if out is None and type(res) is nx.ndarray:
        res = res[()]
    return res
```
to
```python
def fix(x, out=None, *, unpack_scalars=True):
    res = nx.ceil(x, out=out, unpack_scalars=False)
    res = nx.floor(x, out=res, where=nx.greater_equal(x, 0, unpack_scalars=False), unpack_scalars=False)

    if unpack_scalars and out is None:
        res = np.unpack_scalar(res)
    return res
```
<details>
If needed, I could promote this to an NEP ",2019-03-11 01:48:36,,ENH: Provide a way to disable flattening of 0d arrays to scalars,"['23 - Wish List', '62 - Python API']"
13100,open,jakirkham,"Am interested in using `np.fix` as an ordinary `ufunc`. Since some of the work was already done by adding some deprecations in PR ( https://github.com/numpy/numpy/pull/8996 ), which landed in NumPy 1.13.0, am wondering if it would be possible to complete the conversion. Any thoughts on this?

cc @eric-wieser (who originated the aforementioned PR)",2019-03-06 16:24:39,,ENH: `np.fix` as ufunc,['unlabeled']
13088,open,nilswagner,"<!-- Please describe the issue in detail here, and fill in the fields below -->
The following csv file should be imported by genfromtxt.
```
""Label""; ""Type""; ""Dim""; ""#N""; ""N1""; ""N2""; ""N3""; ""N4""; ""N5""; ""N6""; ""N7""; ""N8""; ""N9""; ""N10""
1; TET10; 3; 10; 955; 8100; 956; 8113; 957; 8101; 8102; 8114; 8127; 958
2; TET10; 3; 10; 959; 8150; 960; 8103; 955; 8104; 8140; 8141; 8102; 958
3; TET10; 3; 10; 955; 8105; 961; 8115; 956; 8100; 8102; 8142; 8114; 958
```

### Reproducing code example:
```python
import io
import numpy as np

data_file = io.StringIO(""""""\
""Label""; ""Type""; ""Dim""; ""#N""; ""N1""; ""N2""; ""N3""; ""N4""; ""N5""; ""N6""; ""N7""; ""N8""; ""N9""; ""N10""
1; TET10; 3; 10; 955; 8100; 956; 8113; 957; 8101; 8102; 8114; 8127; 958
2; TET10; 3; 10; 959; 8150; 960; 8103; 955; 8104; 8140; 8141; 8102; 958
3; TET10; 3; 10; 955; 8105; 961; 8115; 956; 8100; 8102; 8142; 8114; 958"""""")

data = np.genfromtxt(data_file, delimiter=';',names=True,
    usecols=(0,1,2,3,4,5,6,7,8,9,10,11,12,13),
    dtype=""i8,S6,i8,i8,i8,i8,i8,i8,i8,i8,i8,i8,i8,i8"")
```

### Error message:
```
ValueError: must replace all names at once with a sequence of length 14
```

### Numpy/Python version information:
Python 3.7.2 (default, Dec 29 2018, 06:19:36) 

>>> np.__version__
'1.15.4'

",2019-03-04 12:17:09,,hashtag in headerlines,['04 - Documentation']
13021,open,MaulishShah,"I installed python using the anaconda navigator. I had python version 3.7 but for the rasa framework I have to downgrade python version 3.6 so I used command conda create python-3.6

I install numpy using the command pip. pip install numpy with particular version. 

I use windows 10.

No, I don't have multiple version of python installed.

python version 3.6.8

",2019-02-23 06:14:54,,Importing the multiarray numpy extension module failed.,['unlabeled']
13011,open,henrypinkard,"Below I generate a square matrix with rank lower than its size, but np.linalg.inv does not throw an error as expected (because said matrix is singular)

### Reproducing code example:

import numpy as np

a = np.array([[ 1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., -1.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0., -1.,  0.,  0.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0., -1.,  0.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0., -1.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., -1., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 0., -1.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0., 0.,  0., -1.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0., 0.,  0.,  0., -1.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0., 0.,  0.,  0.,  0., -1.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1., 0.,  0., -1.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 1.,  0.,  0., -1.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 0.,  1.,  0.,  0., -1.],
       [ 1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., -1.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0., -1.,  0.,  0.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0., -1.,  0.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0., -1.,  0., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., -1., 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 0., -1.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0., 0.,  0., -1.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0., 0.,  0.,  0., -1.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0., 0.,  0.,  0.,  0., -1.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1., 0.,  0., -1.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 1.,  0.,  0., -1.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 0.,  1.,  0.,  0., -1.]])

hat_matrix = np.dot(a.T, a)
print('matrix shape: {}\nmatrix rank: {}'.format(hat_matrix.shape, np.linalg.matrix_rank(hat_matrix)))

#Doesn't throw error for matrix being singular!
np.linalg.inv(hat_matrix)
print('finished with no error')

### Numpy/Python version information:

1.15.4 3.6.7 |Anaconda custom (64-bit)| (default, Oct 23 2018, 14:01:38)
[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]
",2019-02-22 02:23:06,,np.linalg.inv doesn't throw error on singular matrix inversion,['unlabeled']
13007,open,goerz,"~~~
>>> v = 2j
>>> float(v)
*** TypeError: can't convert complex to float

>>> v = np.complex128(2j)
>>> float(v)
0.0
~~~
Silently dropping imaginary parts is a recipe for disaster (bugs) in numerical code!",2019-02-21 19:02:36,,float(np.complex128) silently drops imaginary part,"['15 - Discussion', 'component: numpy._core', '07 - Deprecation', 'component: numpy.dtype']"
12997,open,jorisvandenbossche,"### Reproducing code example:

Consider this example, where I create a pandas TimedeltaIndex (an ""array-like"") and call `np.sum` on it, which correctly sums the timedelta64[ns] data and returns a scalar:

```
In [1]: idx = pd.TimedeltaIndex(np.arange(10)*1e9) 

In [2]: idx
Out[2]: 
TimedeltaIndex(['00:00:00', '00:00:01', '00:00:02', '00:00:03', '00:00:04',
                '00:00:05', '00:00:06', '00:00:07', '00:00:08', '00:00:09'],
               dtype='timedelta64[ns]', freq=None)

In [3]: np.sum(idx) 
Out[3]: numpy.timedelta64(45000000000,'ns')

In [4]: pd.__version__   
Out[4]: '0.23.4'

In [5]: np.__version__
Out[5]: '1.15.4'
```

The above is with numpy 1.15, but starting from 1.16 (and on master as well) the same code now gives an error (using the same pandas version):

```
In [1]: idx = pd.TimedeltaIndex(np.arange(10)*1e9)   

In [2]: np.sum(idx)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-2-4137fa3a65d6> in <module>
----> 1 np.sum(idx)

~/miniconda3/envs/numpy-dev/lib/python3.7/site-packages/numpy/core/fromnumeric.py in sum(a, axis, dtype, out, keepdims, initial)
   2074 
   2075     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,
-> 2076                           initial=initial)
   2077 
   2078 

~/miniconda3/envs/numpy-dev/lib/python3.7/site-packages/numpy/core/fromnumeric.py in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)
     84                 return reduction(axis=axis, out=out, **passkwargs)
     85 
---> 86     return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
     87 
     88 

~/miniconda3/envs/numpy-dev/lib/python3.7/site-packages/pandas/core/indexes/base.py in __array_wrap__(self, result, context)
    658         attrs = self._get_attributes_dict()
    659         attrs = self._maybe_update_attributes(attrs)
--> 660         return Index(result, **attrs)
    661 
    662     @cache_readonly

~/miniconda3/envs/numpy-dev/lib/python3.7/site-packages/pandas/core/indexes/base.py in __new__(cls, data, dtype, copy, name, fastpath, tupleize_cols, **kwargs)
    301                   (dtype is not None and is_timedelta64_dtype(dtype))):
    302                 from pandas.core.indexes.timedeltas import TimedeltaIndex
--> 303                 result = TimedeltaIndex(data, copy=copy, name=name, **kwargs)
    304                 if dtype is not None and _o_dtype == dtype:
    305                     return Index(result.to_pytimedelta(), dtype=_o_dtype)

~/miniconda3/envs/numpy-dev/lib/python3.7/site-packages/pandas/core/indexes/timedeltas.py in __new__(cls, data, unit, freq, start, end, periods, closed, dtype, copy, name, verify_integrity)
    250 
    251         # check that we are matching freqs
--> 252         if verify_integrity and len(data) > 0:
    253             if freq is not None and not freq_infer:
    254                 index = cls._simple_new(data, name=name)

TypeError: len() of unsized object

In [3]: np.__version__      
Out[3]: '1.16.1'

In [4]: pd.__version__                                     
Out[4]: '0.23.4'
```

The error you see comes from passing a 0d array to the `TimedeltaIndex` constructor. But it seems that something changed in numpy how this error is handled (I was using the same pandas version, so this error will happen under the hood in both cases). 

We can rather easily work around this in pandas (checking if the result is a 0dim array or scalar, and then not passing it to the class constructor in `__array_wrap__`, see https://github.com/pandas-dev/pandas/pull/25329/), but reporting this here to check if this is an intentional change or rather an uncatched regression.

Some more information on this specific case: `TimedeltaIndex` has no `sum` method implemented. So `np.sum` does not directly dispatch to such a method (in contrast to eg `Series`, which has a `sum` method). That means that `np.sum` goes through the `__array__` and `__array_wrap__`. 
(note: I suppose towards the future we should also fix this by adding a `__array_ufunc__`)

### Numpy/Python version information:

Both is in the same environment with python 3.7

```
In [9]: sys.version
Out[9]: '3.7.1 | packaged by conda-forge | (default, Feb 18 2019, 01:42:00) \n[GCC 7.3.0]'
```
",2019-02-20 10:12:21,,Change in wrapped ufunc handling between 1.15 -> 1.16,"['00 - Bug', '06 - Regression']"
12974,open,pentschev,"The discussion for this issue started in https://github.com/cupy/cupy/issues/2029, please refer to it for all the details.

Currently, CuPy creates aliases from some NumPy C functions that use the `__array_function__` dispatch. This causes an infinite loop since CuPy keeps on calling the NumPy function, and this dispatches the CuPy alias. Note that the same case may apply to any other libraries that implementation the `__array_function__` protocol.

Summary of fix suggestions we have at the moment:

- Wrap all NumPy functions of the category described above on libraries that implement the `__array_function__` protocol
- Use a sentinel (e.g., `None`) as a fallback to NumPy function, similar to the [NEP proposal](http://www.numpy.org/neps/nep-0018-array-function-protocol.html#coercion-to-a-numpy-array-as-a-catch-all-fallback)

@shoyer @mrocklin ",2019-02-15 17:51:45,,NEP-18: Handling aliased NumPy functions,"['15 - Discussion', 'component: NEP', '30 - API']"
12966,open,Carreau,See #12965. many section in the docs seem to refer  to `np.lib.index_tricks.nd_grid` but html (or rst)documentation page for such class seem to be generated. Not sure why. ,2019-02-13 22:01:37,,"np.lib.index_tricks.nd_grid docs not autogenerated, and not availlable",['04 - Documentation']
12961,open,TomNicholas,"Why is `numpy.angle()` not a numpy universal function (ufunc)?

It seems to fit the criteria for a ufunc in the [numpy documentation](https://docs.scipy.org/doc/numpy/reference/ufuncs.html) but it's not listed as one.

I thought it might not fit the definition because it converts the type of the number (from complex to real), but there already are other ufuncs which do this (e.g. `np.absolute`).

I came across this because I was trying to apply `np.angle` directly to an xarray DataArray containing complex numbers, and it returns a numpy array instead of an xarray DataArray. I think it does that because it's not a numpy ufunc, and xarray checks for that.",2019-02-13 16:03:10,,Why is np.angle() not a ufunc?,['unlabeled']
12946,open,rgommers,"This would be helpful to be able to cite NumPy releases. It's completely painless, we have had it enabled for SciPy for a couple of years with zero maintenance effort, see https://github.com/scipy/scipy/issues/6446. 

Not urgent, and requires admin privileges on this repo (to install a hook), but would be quite helpful.",2019-02-08 05:53:39,,add Zenodo archiving and DOI creation,['17 - Task']
12943,open,flomertens,"Calling np.median on a complex array return a complex number with correct real part, but incorrect imaginary part. Calling np.median on the real and imaginary part of the complex array separately return the correct values.

### Reproducing code example:

d = (1 + 1j) + np.random.randn(10000) + 1j * np.random.randn(10000)
print 'median(d)     : %.3f + %.3f i' % (np.median(d).real, np.median(d).imag)
print 'median(d.real): %.3f' % np.median(d.real)
print 'median(d.imag): %.3f' % np.median(d.imag)

median(d)     : 1.002 + 0.124 i
median(d.real): 1.002
median(d.imag): 0.998

Numpy version: 1.15.4
",2019-02-07 10:04:30,,np.median on a complex array return incorrect value for the imaginary part,"['00 - Bug', '15 - Discussion', 'component: numpy.lib', '07 - Deprecation']"
12936,open,hjstein,"I'm finding a regression from numpy v1.14.3 to numpy v1.15.4.  The issue is that in v1.14.3, busday_count() would operate on datetime64 types, and in v1.15.4, this is no longer the case.  This comes up when trying to use busday_count() on dates from a pandas dataframe that stores them as datetimes.

### Reproducing code example:

```python
import numpy as np
import pandas as pd
df = pd.DataFrame([[""2011-01-01"", 14], [""2012-01-01"", 25]], columns=[""Date"", ""Amount""])
d1 = np.busday_count(df[""Date""].iloc[0], df[""Date""].iloc[1])
df[""Date""] = pd.to_datetime(df[""Date""])
d2 = np.busday_count(df[""Date""].iloc[0].date(), df[""Date""].iloc[1].date())
d3 = np.busday_count(df[""Date""].iloc[0], df[""Date""].iloc[1])
```

### Error message:
In v1.14.0 and v1.14.3, the assignments to d1, d2 and d3 succeed and yield the same result.  In v1.15.4, the assignment to d3 gives an error message:

<ipython-input-19-c36f4cc56eac> in <module>
----> 1 d3 = np.busday_count(df[""Date""].iloc[0], df[""Date""].iloc[1])

TypeError: Iterator operand 0 dtype could not be cast from dtype('<M8[us]') to dtype('<M8[D]') according to the rule 'safe'

### Numpy/Python version information:

Versions on system where error occurs:

import sys, numpy, pandas; print(numpy.__version__, pandas.__version__, sys.version)
1.15.4 0.23.4 3.6.7 |Anaconda custom (64-bit)| (default, Dec 10 2018, 20:35:02) [MSC v.1915 64 bit (AMD64)]

Versions on system where error does not occur:

import sys, numpy, pandas; print(numpy.__version__, pandas.__version__, sys.version)
1.14.3 0.23.0 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) 
[GCC 7.2.0]

",2019-02-05 16:35:11,,Handling of pandas datetimes in numpy,"['06 - Regression', 'component: numpy.datetime64']"
12935,open,davcrom,"<!-- Please describe the issue in detail here, and fill in the fields below -->
It appears that comparison with NaN is allowed on certain array slices even when the floating-point error for invalid comparisons is set to 'raise'.

(may be related to  #10370 and ultimately #11043)

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
np.seterr(invalid='raise')

np.full(2, np.nan)[0] > 0
np.full((2,2),  np.nan)[:,0] > 0
np.full((2,2), np.nan)[0,0] > 0
np.full((2,2,2), np.nan)[:,:,0] > 0
np.full((2,2,2), np.nan)[:,0,0] > 0
np.full((2,2,2), np.nan)[0,0,0] > 0
```
and
```
np.full((2,2,2), np.nan)[:,0,0].copy()[0] > 0
```
all return False (or arrays of False). However...
```np.full(2, np.nan) > 0
np.full((2,2), np.nan) > 0
np.full((2,2), np.nan)[0,:] > 0
np.full((2,2,2), np.nan)[0,:,:] > 0
np.full((2,2,2), np.nan)[0,0,:] > 0
```
and
```
np.full((2,2,2), np.nan)[:,:,0].copy() > 0
```
all raise the appropriate floating-point error.

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

Traceback (most recent call last):
  File ""\<stdin\>"", line 1, in <module>
FloatingPointError: invalid value encountered in greater

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
Numpy: 1.15.1
Python: 2.7.12 and 3.5.2
System: Ubuntu 16.04.5 LTS

",2019-02-05 14:43:26,,NaN comparison allowed on slices of N-D array with floating-point invlaid error set to 'raise',['00 - Bug']
12919,open,DerWeh,"<!-- Please describe the issue in detail here, and fill in the fields below -->
The behavior of complex `ndarrays` is surprising. Assigning a NaN sets the imaginary part to zero.
The reason is obviously that NaN is a float in `numpy`. So from the programmer's point of view, this somehow is consistent behavior. 
This however contradicts the mathematical meaning of not a number. 

This lead for me to hardly traceable wrong results in calculations. If it is possible, I would raise a warning when NaN is cast to complex.


### Reproducing code example:


<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np

test_array = np.zeros((1,), dtype=complex) + 1j
test_array[:] = np.nan
print(test_array.imag)
# array([0.])
```

<!-- Remove these sections for a feature request -->


### Numpy/Python version information:

<!-- Output from '' -->
numpy: 1.15.4
Python: 3.6.5 |Intel Corporation| (default, Aug  3 2018, 14:28:11)

",2019-02-02 10:07:43,,Surprising behaviour of NaN with complex numbers,['04 - Documentation']
12914,open,julienprieur,"Same code using rfft produces slighly different output on python 2 / python 3.
I noticed it because it broke some automatic testing of my internal libraries.

It looks like the latest digit of the real part is being truncated in python 3.

### Reproducing code example:

```python
import numpy as np
print(np.fft.rfft(np.sin(np.arange(1000) * 0.1))[25])
```
output:
(-0.686231256675454-5.4138146836022445j) in python 3.7.2
(-0.6862312566754544-5.4138146836022445j) in python 2.7.15

### Numpy/Python version information:

Numpy version 1.16.0
Python 2.7.15 x64
Python 3.7.2 x64
Windows 10


",2019-02-01 14:38:31,,rfft output difference python 2.7.15 / python 3.7.2,['unlabeled']
12906,open,rharriszzz,"If you pickle a datetime[ns] array on a platform of one byte order, then unpickle on a platform with a different byteorder, the ""[ns]"" gets dropped.

Here is the 
[pull request](https://github.com/numpy/numpy/pull/12905)

Here is the 
[test case](https://github.com/numpy/numpy/pull/12905#issuecomment-459473561)

I believe the problem and the fix are applicable from before 1.11 all the way up to now.


",2019-01-31 19:58:48,,datetime64[ns] unpickling from platforms of different byte order,['component: numpy.datetime64']
12901,open,shoyer,"`mean()` works on timedelta64, but gives an inadvertent error message for datetime64:
```
In [16]: x = np.array(['2000-01-01', '2000-01-10'], dtype='datetime64[us]')

In [17]: np.mean(x)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-17-67ce1f9814a2> in <module>
----> 1 np.mean(x)

~/miniconda3/envs/numbagg-py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py in mean(a, axis, dtype, out, keepdims)
   3116
   3117     return _methods._mean(a, axis=axis, dtype=dtype,
-> 3118                           out=out, **kwargs)
   3119
   3120

~/miniconda3/envs/numbagg-py37/lib/python3.7/site-packages/numpy/core/_methods.py in _mean(a, axis, dtype, out, keepdims)
     73             is_float16_result = True
     74
---> 75     ret = umr_sum(arr, axis, dtype, out, keepdims)
     76     if isinstance(ret, mu.ndarray):
     77         ret = um.true_divide(

TypeError: ufunc add cannot use operands with types dtype('<M8[us]') and dtype('<M8[us]')
```

The problem is that mean is implemented via sum, which cannot be sensibly defined for dates.

Fixing this will probably require some significant refactoring -- we would need to make `mean` something that could be overwritten explicitly based on the dtypes. Our usual mechanism for this is ufuncs, but `mean` is currently written in terms of ufuncs (`add.reduce` and `divide`) rather being a ufunc itself.",2019-01-31 17:07:50,,mean() for datetime64,"['00 - Bug', 'component: numpy.datetime64']"
12900,open,randolf-scholz,"<!-- Please describe the issue in detail here, and fill in the fields below -->

Standard `ufuncs` like `np.add`, `np.multiply` and so on should allow for manual broadcasting. In particular, I suggest adding support of the [ufunc axes keyword](https://docs.scipy.org/doc/numpy-1.15.1/reference/ufuncs.html#ufuncs-kwargs) to the standard math functions listed [here.](https://docs.scipy.org/doc/numpy-1.15.1/reference/ufuncs.html#math-operations) (My apologies if this is already in the works, as far as I can tell it doesn't work in 1.15.4)
 
The main issue with the status quo is that it makes it unnecessarily complicated and awkward to write routines that a priori do not know the exact shapes of incoming tensors. (but may know it partially, for example it could be known that the incoming tensor has the shape (?,3,3), where the ? could contain any number of additional axes.)

### Reproducing code example:
<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

For example a task as simple as ""Add vector v to the tensor T along the third axis"" is really awkward to do:

```python
import numpy as np
T = np.ones((2,2,3,3))
v = np.arange(3)
# z = T + v  # adds along the last axis, not what we want!
# z = T + v[None, None, : , None]  # does work, but incredibly ugly / hard to read
# z = np.add(T, v, axes=([2],[0]))  # suggested ""pythonic"" solution
# z = np.add(T, v, broadcasting= 'ijkl,k -> ijkl')  # alternative  einsum-like solution
```

Of course, once this works even way more complicated statements can be written with ease. For example let's say we want to scale the first and second axis of a Tensor T pointwise by values provided by the transpose of a matrix M:

```python
import numpy as np
T = np.ones((3,3,2,2))
M = np.ones((3,3))*[1,2,3]
# z = T * M  # does not work
# z = T * M.T[:, :, None , None]  # does work, but incredibly ugly / hard to read
# z = np.einsum('ijkl, ji -> ijkl', T, v)  # usable workaround exclusively for `np.multiply`
# z = np.multiply(T, M, axes=([0,1],[1,0]))  # suggested ""pythonic"" solution
# z = np.multiply(T, M, broadcasting= 'ijkl,ji -> ijkl')))  # alternative  einsum-like solution
```

In essence, it would be absolutely great to have other standard functions supporting Einstein-Notation like broadcasting such that one could easily implement even complicated tensor statements like `T_ijkl = A_ij + C_il*exp(D_km E_ml)`
",2019-01-31 15:37:30,,Better Manual Broadcasting for ufuncs,"['23 - Wish List', '15 - Discussion']"
12888,open,mattip,"`ctypes` allows structs to specify bit size and unions:
```
from ctypes import *
class myPacketHeader(Structure):
    _pack_ = 1
    _fields_ = [(""seqnum"", c_ubyte, 8),
                (""address"", c_uint, 31),
                (""is_command"", c_uint, 1),
                (""length"", c_ubyte, 8)]
class myUnion(Union):
    _pack_ = 1
    _fields_ = [(""header"", myPacketHeader),
                (""bytes"", c_uint8 * 6)]
```
It might be nice if we could allow this in dtypes for conversion to/from structured arrays too.

Something to think about for the future.
",2019-01-30 21:56:31,,ENH: allow specifying bit fields in a dtype,"['01 - Enhancement', '23 - Wish List', 'component: numpy.dtype']"
12876,open,rnhmjoj,"### Reproducing code example:

```python
import numpy as np
x = np.array([(1,2)], dtype=[('α', int), ('β', int)])
np.savez_compressed('test', x)
```

### Error message:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/numpy/lib/npyio.py"", line 667, in savez_compressed
    _savez(file, args, kwds, True)
  File ""/numpy/lib/npyio.py"", line 706, in _savez
    pickle_kwargs=pickle_kwargs)
  File ""/numpy/lib/format.py"", line 572, in write_array
    version)
  File ""/numpy/lib/format.py"", line 315, in _write_array_header
    header = asbytes(_filter_header(header))
  File ""/numpy/compat/py3k.py"", line 35, in asbytes
    return str(s).encode('latin1')
UnicodeEncodeError: 'latin-1' codec can't encode character '\u03b1' in position 13: ordinal not in range(256)
```
### Numpy/Python version information:

python 3.6.7, numpy 1.15.1

",2019-01-29 17:38:32,,numpy.savez can't save structured arrays with unicode field names,"['00 - Bug', 'component: numpy.lib']"
12859,open,rjenc29,"<!-- Please describe the issue in detail here, and fill in the fields below -->
In the reproducing code snippet below, `condition` can be longer than `arr` provided there are no 'truthy' values outside the boundary which corresponds to the length of `arr`

The first 'truthy' value outside of bounds causes an `IndexError` to be raised.

If there are no 'truthy' values out of bounds, then no error is raised.

If `condition` is shorter than `arr` then no error is raised.

The docs imply that `condition` and `arr` should be the same size; just wondered if there'd be any merit in tweaking the docs / not depending on the 'truthiness' of out of bounds `condition` values / strictly requiring the same shape, or some such.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np

condition = np.array([True, False, True, False, False, False, False])
arr = np.arange(4)
np.extract(condition, arr)

>>> array([0, 2])

# note the 'truthiness' of the penultimate value
condition = np.array([True, False, True, False, False, True, False])
arr = np.arange(4)
np.extract(condition, arr)

>>> (index error per the below)
```

<!-- Remove these sections for a feature request -->

### Error message:
```
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-16-d0e79dad9ecf> in <module>()
      2 arr = np.arange(4)
      3 
----> 4 np.extract(condition, arr)

~\Anaconda3\lib\site-packages\numpy\lib\function_base.py in extract(condition, arr)
   1562 
   1563     """"""
-> 1564     return _nx.take(ravel(arr), nonzero(ravel(condition))[0])
   1565 
   1566 

~\Anaconda3\lib\site-packages\numpy\core\fromnumeric.py in take(a, indices, axis, out, mode)
    179            [5, 7]])
    180     """"""
--> 181     return _wrapfunc(a, 'take', indices, axis=axis, out=out, mode=mode)
    182 
    183 

~\Anaconda3\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds)
     49 def _wrapfunc(obj, method, *args, **kwds):
     50     try:
---> 51         return getattr(obj, method)(*args, **kwds)
     52 
     53     # An AttributeError occurs if the object does not have

IndexError: index 5 is out of bounds for size 4
```

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```python
import sys, numpy; print(numpy.__version__, sys.version)
1.15.4 3.6.3 |Anaconda custom (64-bit)| (default, Oct 17 2017, 23:26:12) [MSC v.1900 64 bit (AMD64)]
```
",2019-01-26 14:56:54,,numpy.extract with condition longer than arr,['unlabeled']
12858,open,rjenc29,"<!-- Please describe the issue in detail here, and fill in the fields below -->
This might be a bit of an edge case, but the output of `numpy.trapz` when given an input array of booleans seems inconsistent / counter intuitive.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np

data = [True, False, True, True]

# first example
y = np.array(data).astype(np.int)
np.trapz(y)
>>> 2.0

# second example
y = np.array(data)
np.trapz(y)
>>> 1.5

```

<!-- Remove these sections for a feature request -->

### Error message:
The output for the first example seems intuitively reasonable.

Should the output of the second example match the first?

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

```python
import sys, numpy; print(numpy.__version__, sys.version)
1.15.4 3.6.3 |Anaconda custom (64-bit)| (default, Oct 17 2017, 23:26:12) [MSC v.1900 64 bit (AMD64)]
```
",2019-01-26 14:30:14,,numpy.trapz handling of boolean input,['00 - Bug']
12817,open,timhoffm,"### Reproducing code example:
~~~
>>> import numpy as np
>>> a = np.zeros((3, 3), dtype=[('x', float), ('y', float)])
>>> b = np.copy(a)
>>> a.dtype is b.dtype
True
~~~

This is problematic because I can change `b.dtype.names`, which implicitly also changes `a.dtype.names`:
~~~
>>> b.dtype.names = ('u', 'v')
>>> print a.dtype.names
('u', 'v')
~~~
This is unexpected since I assume that the array returned by `np.copy()` should not be connected to the input array in any way.

Tested with numpy 1.8.2 and 1.16.0.",2019-01-21 14:32:14,,np.copy() does not  copy the dtype,['unlabeled']
12778,open,bnprks,"<!-- Please describe the issue in detail here, and fill in the fields below -->
I noticed that running np.inner(a,b) was substantially slower (~10x) than running np.tensordot(a,b, axes=(-1,-1)) for some multidimensional arrays. The two are mathematically equivalent, and produce largely the same results (to within small numeric precision errors).

I'm not certain how specific this is to the array shapes I was using, but it does seem like a potentially easy performance win to just make np.inner call to np.tensordot on multidimensional arrays. The only argument against that I can think of is if bit-identical results on np.inner are needed for backwards compatibility reasons. 

If a change is possible though, I might like to put together the pull request myself since it seems fairly simple and I've never contributed to an open source project before.
### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
import timeit 

np.random.seed(12345)

a = np.random.random((25,25,500))
b = np.random.random((10000,500))

#Running np.inner on the two matricies takes about 20s
timeit.timeit(""np.inner(a,b)"", setup=""from __main__ import a,b,np"", number=10)

#Running np.tensordot with axes=(-1,-1) takes about 2s
timeit.timeit(""np.tensordot(a,b,axes=(-1,-1))"", setup=""from __main__ import a,b,np"", number=10)

```


### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.15.0 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56)
[GCC 7.2.0]
",2019-01-17 05:07:09,,Performance discrepancy between np.inner and np.tensordot,['unlabeled']
12761,open,munael,"Request: A convenient (documented) way to construct record arrays from field-major data.

Right now we support AOS (arrays of structs). This request is for SOA (struct of arrays). Just for initialization. Representation remains untouched.

### Motivating Example
```python
import numpy as np
x1 = {'header1': [values1], 'header2': [values2]}
x2 = ([values1], [values2])
headers = ('header1', 'header2')

assert all(x1[h].shape == x1[headers[0]].shape for h in headers)
assert len(headers) == len(x2)
assert all(x.shape == x2[0].shape for x in x2)

r1 = np.rec.array(x1)
r2 = np.rec.array(x2, names=headers)
```",2019-01-16 12:21:57,,Support for field-major initialization of Record Arrays `np.rec.array(tuple(lists))`,['unlabeled']
12755,open,anntzer,"<!-- Please describe the issue in detail here, and fill in the fields below -->

A comment at the top of the implementation of PyArray_LexSort in item_selection.c indicates that lexsort is implemented by argsorting over the first key, then over the second key, and so on (which, I guess, explains the slightly curious argument order -- making the first argument the main sort key would have appeared more consistent with standard lexicographical ordering, but that's not the issue here).

The strategy appears to be less-than-optimal in the case where there are few ties in the ""main"" key (as one could probably just do a sort on the main key, and let the sort function look into secondary keys in the uncommon case of a tie).

But even sticking to the strategy used by lexsort (of sorting over each key successively), it looks like a pure-Python wrapper around argsort already does better:
```
# Make it so that there is a ~4-way tie for each primary key (""b""):
In [1]: N = 1000000; a = np.random.rand(N); b = np.random.randint(N // 4, size=N)

In [2]: def ls(a, b): 
   ...:     idxs = np.argsort(a, kind=""stable"") 
   ...:     return idxs[np.argsort(b[idxs], kind=""stable"")] 
   ...:

In [3]: %timeit np.lexsort([a, b])
322 ms ± 944 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [4]: %timeit ls(a, b)
240 ms ± 580 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [5]: (np.lexsort([a, b]) == ls(a, b)).all()
Out[5]: True
```
Here the ""pure Python"" lexsort implementation is around 25% faster than lexsort.

### Reproducing code example:

See above.

### Error message:

N/A

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.16.0 3.7.1 (default, Oct 23 2018, 19:19:42) 
[GCC 7.3.0]",2019-01-15 16:51:09,,lexsort performance appears less than optimal,['unlabeled']
12735,open,anntzer,"numpy 1.16 introduced np.lib.recfunctions.unstructured_to_structured, which allows easy conversion of unstructured arrays to structured ones... as long as all fields have the same dtype: I believe(?) unstructured_to_structured is unable to create a structured array where e.g. the first field is an int and the second one is a float (what would have been the dtype of the input array?).

Instead, let's look at the behavior of append_field:
```
In [1]: np.lib.recfunctions.append_fields([42], [""foo"", ""bar""], [[1, 2], [3., 4.]], usemask=False)                 
Out[1]: 
array([(    42, 1, 3.), (999999, 2, 4.)],
      dtype=[('f0', '<i8'), ('foo', '<i8'), ('bar', '<f8')])
In [2]: np.lib.recfunctions.append_fields([42, 42], [""foo"", ""bar""], [[1, 2], [3., 4.]], usemask=False)             
Out[2]: 
array([(    42,     42, 1, 3.), (999999, 999999, 2, 4.)],
      dtype=[('f0', '<i8'), ('f1', '<i8'), ('foo', '<i8'), ('bar', '<f8')])
```
The 999999 entries are the default integer masked value for masked arrays, but that's not the relevant point here.  It would seem like one should be able to pass an empty array to append into to just end up with ""foo"" and ""bar"" in the structured array... but
```
In [3]: np.lib.recfunctions.append_fields([], [""foo"", ""bar""], [[5, 6], [7., 8.]], usemask=False)                   
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-12-af1868be7043> in <module>
----> 1 np.lib.recfunctions.append_fields([], [""foo"", ""bar""], [[5, 6], [7., 8.]], usemask=False)

~/.local/lib/python3.7/site-packages/numpy/lib/recfunctions.py in append_fields(base, names, data, dtypes, fill_value, usemask, asrecarray)
    731                 for (a, n, d) in zip(data, names, dtypes)]
    732     #
--> 733     base = merge_arrays(base, usemask=usemask, fill_value=fill_value)
    734     if len(data) > 1:
    735         data = merge_arrays(data, flatten=True, usemask=usemask,

~/.local/lib/python3.7/site-packages/numpy/lib/recfunctions.py in merge_arrays(seqarrays, fill_value, flatten, usemask, asrecarray)
    461     # Find the sizes of the inputs and their maximum
    462     sizes = tuple(a.size for a in seqarrays)
--> 463     maxlength = max(sizes)
    464     # Get the dtype of the output (flattening if needed)
    465     newdtype = zip_dtype(seqarrays, flatten=flatten)

ValueError: max() arg is an empty sequence
```
and passing the first argument as an explicit array instead of a list doesn't ""work"" either, but fails differently:
```
In [4]: np.lib.recfunctions.append_fields(np.array([]), [""foo"", ""bar""], [[5, 6], [7., 8.]], usemask=False)         
Out[4]: 
array([(1.e+20, 5, 7.), (1.e+20, 6, 8.)],
      dtype=[('f0', '<f8'), ('foo', '<i8'), ('bar', '<f8')])
```

As a side point, one can also compare the behavior of [2] with explicitly passing in an array, which again is different:
```
In [5]: np.lib.recfunctions.append_fields(np.array([42, 42]), [""foo"", ""bar""], [[1, 2], [3., 4.]], usemask=False)   
Out[5]: 
array([(42, 1, 3.), (42, 2, 4.)],
      dtype=[('f0', '<i8'), ('foo', '<i8'), ('bar', '<f8')])
```

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.16.0 3.7.1 (default, Oct 23 2018, 19:19:42) 
[GCC 7.3.0]
",2019-01-14 11:05:20,,Consistency in append_field behavior / easily constructing structured arrays from unstructured ones.,['unlabeled']
12715,open,CarlChengpy,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
print(np.mgrid[0.46:0.49:0.01]) #left-bounded
print(np.mgrid[0.47:0.49:0.01]) #left and right bounded
print(np.mgrid[0.07:0.072:0.001]) #left-bounded
print(np.mgrid[0.071:0.072:0.001]) #left and right-bounded
```

<!-- Remove these sections for a feature request -->

### Output:
```python
[0.46 0.47 0.48]
[0.47 0.48 0.49]
[0.07  0.071]
[0.071 0.072]
```

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.14.3 3.6.5
",2019-01-10 15:11:25,,mgrid generate left and right-bounded interval under special cases?,"['00 - Bug', '15 - Discussion']"
12712,open,detrout,"Hello,

The Debian numpy maintainer filed a a bug against my packaging of Dask (<a href=""https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=918204"">Debian Bug 918204</a>) because some of dasks's tests failed. One of the failures seems to be related to the changes for https://www.numpy.org/neps/nep-0020-gufunc-signature-enhancement.html

(relevant traceback from the dask tests: <a href=""https://ci.debian.net/data/autopkgtest/unstable/amd64/d/dask/1667144/log.gz "">here</a> (search for ""test_matmul "")
```_________________________________ test_matmul __________________________________

    @pytest.mark.skipif(sys.version_info < (3, 5),
                        reason=""Matrix multiplication operator only after Py3.5"")
    def test_matmul():
        x = np.random.random((5, 5))
        y = np.random.random((5, 2))
        a = from_array(x, chunks=(1, 5))
        b = from_array(y, chunks=(5, 1))
        assert_eq(operator.matmul(a, b), a.dot(b))
        assert_eq(operator.matmul(a, b), operator.matmul(x, y))
>       assert_eq(operator.matmul(a, y), operator.matmul(x, b))

/usr/lib/python3/dist-packages/dask/array/tests/test_array_core.py:849: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3/dist-packages/dask/array/core.py:1054: in __array_ufunc__
    **kwargs)
/usr/lib/python3/dist-packages/dask/array/gufunc.py:264: in apply_gufunc
    input_coredimss, output_coredimss = _parse_gufunc_signature(signature)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

signature = '(n?,k),(k,m?)->(n?,m?)'
```

The signature  '(n?,k),(k,m?)->(n?,m?)' looks like one of the examples from NEP 20, but I tried looking at Numpy's _parse_gufunc_signature to see how to update Dask's version and discovered that numpy's doesn't seem to parse  '(n?,k),(k,m?)->(n?,m?)' either.

I looked for a unit test and couldn't find the above signature test, so I added it to commit `30eae3dd24402ca07cd56cae6db646ad61e5d7f1` (not in NumPy: mattip)
```
--- a/numpy/lib/tests/test_function_base.py
+++ b/numpy/lib/tests/test_function_base.py
@@ -1362,6 +1362,8 @@ class TestVectorize(object):
                      ([('x',)], [('y',), ()]))
         assert_equal(nfb._parse_gufunc_signature('(),(a,b,c),(d)->(d,e)'),
                      ([(), ('a', 'b', 'c'), ('d',)], [('d', 'e')]))
+        assert_equal(nfb._parse_gufunc_signature('(m?,n),(n,p?)->(m?,p?)'),
+                     ([('m?', 'n'),('n','p?')],[('m?', 'p?')]))
         with assert_raises(ValueError):
             nfb._parse_gufunc_signature('(x)(y)->()')
         with assert_raises(ValueError):
```

and `python3 runtests.py` fails.

```
___________________ TestVectorize.test_parse_gufunc_signature ___________________

self = <numpy.lib.tests.test_function_base.TestVectorize object at 0x7f8de58c1f60>

    def test_parse_gufunc_signature(self):
        assert_equal(nfb._parse_gufunc_signature('(x)->()'), ([('x',)], [()]))
        assert_equal(nfb._parse_gufunc_signature('(x,y)->()'),
                     ([('x', 'y')], [()]))
        assert_equal(nfb._parse_gufunc_signature('(x),(y)->()'),
                     ([('x',), ('y',)], [()]))
        assert_equal(nfb._parse_gufunc_signature('(x)->(y)'),
                     ([('x',)], [('y',)]))
        assert_equal(nfb._parse_gufunc_signature('(x)->(y),()'),
                     ([('x',)], [('y',), ()]))
        assert_equal(nfb._parse_gufunc_signature('(),(a,b,c),(d)->(d,e)'),
                     ([(), ('a', 'b', 'c'), ('d',)], [('d', 'e')]))
>       assert_equal(nfb._parse_gufunc_signature('(m?,n),(n,p?)->(m?,p?)'),
                     ([('m?', 'n'),('n','p?')],[('m?', 'p?')]))

self       = <numpy.lib.tests.test_function_base.TestVectorize object at 0x7f8de58c1f60>

numpy/lib/tests/test_function_base.py:1365: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

signature = '(m?,n),(n,p?)->(m?,p?)'

    def _parse_gufunc_signature(signature):
        """"""
        Parse string signatures for a generalized universal function.
    
        Arguments
        ---------
        signature : string
            Generalized universal function signature, e.g., ``(m,n),(n,p)->(m,p)``
            for ``np.matmul``.
    
        Returns
        -------
        Tuple of input and output core dimensions parsed from the signature, each
        of the form List[Tuple[str, ...]].
        """"""
        if not re.match(_SIGNATURE, signature):
            raise ValueError(
>               'not a valid gufunc signature: {}'.format(signature))
E           ValueError: not a valid gufunc signature: (m?,n),(n,p?)->(m?,p?)

signature  = '(m?,n),(n,p?)->(m?,p?)'

numpy/lib/function_base.py:1794: ValueError
```

Should `_parse_gufunc_signature` handle the signature with question marks? If yes, should there be a test for that case?

Edit: noted that commit is not in NumPy (mattip)",2019-01-10 03:52:35,,BUG: np.vectorize (interally _parse_gufunc_signature) can't parse a NEP 20 signature,"['00 - Bug', 'component: numpy.ufunc']"
12701,open,Akshay2350,"I am trying to use a program which will help me in using spacy for nlp purposes. But everytime i execute it i get this error that 

AttributeError: module 'numpy' has no attribute 'testing'


I have seen this thread but not [relevant](https://github.com/numpy/numpy/issues/11400)  

runfile('W:/NER_NLTK_Spacy.py', wdir='W:')
Traceback (most recent call last):

  File ""<ipython-input-59-7d4b8598dd16>"", line 1, in <module>
    runfile('W:/NER_NLTK_Spacy.py', wdir='W:')

  File ""C:\Users\adodhiwala\AppData\Local\conda\conda\envs\py35\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 668, in runfile
    execfile(filename, namespace)

  File ""C:\Users\adodhiwala\AppData\Local\conda\conda\envs\py35\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 108, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""W:/NER_NLTK_Spacy.py"", line 7, in <module>
    import nltk

  File ""C:\Users\adodhiwala\AppData\Local\conda\conda\envs\py35\lib\site-packages\nltk\__init__.py"", line 114, in <module>
    from nltk.collocations import *

  File ""C:\Users\adodhiwala\AppData\Local\conda\conda\envs\py35\lib\site-packages\nltk\collocations.py"", line 39, in <module>
    from nltk.metrics import ContingencyMeasures, BigramAssocMeasures, TrigramAssocMeasures

  File ""C:\Users\adodhiwala\AppData\Local\conda\conda\envs\py35\lib\site-packages\nltk\metrics\__init__.py"", line 16, in <module>
    from nltk.metrics.scores import          (accuracy, precision, recall, f_measure,

  File ""C:\Users\adodhiwala\AppData\Local\conda\conda\envs\py35\lib\site-packages\nltk\metrics\scores.py"", line 18, in <module>
    from scipy.stats.stats import betai

  File ""C:\Users\adodhiwala\AppData\Local\conda\conda\envs\py35\lib\site-packages\scipy\stats\__init__.py"", line 345, in <module>
    from .stats import *

  File ""C:\Users\adodhiwala\AppData\Local\conda\conda\envs\py35\lib\site-packages\scipy\stats\stats.py"", line 171, in <module>
    from . import distributions

  File ""C:\Users\adodhiwala\AppData\Local\conda\conda\envs\py35\lib\site-packages\scipy\stats\distributions.py"", line 10, in <module>
    from ._distn_infrastructure import (entropy, rv_discrete, rv_continuous,

  File ""C:\Users\adodhiwala\AppData\Local\conda\conda\envs\py35\lib\site-packages\scipy\stats\_distn_infrastructure.py"", line 16, in <module>
    from scipy.misc import doccer

  File ""C:\Users\adodhiwala\AppData\Local\conda\conda\envs\py35\lib\site-packages\scipy\misc\__init__.py"", line 68, in <module>
    from scipy.interpolate._pade import pade as _pade

  File ""C:\Users\adodhiwala\AppData\Local\conda\conda\envs\py35\lib\site-packages\scipy\interpolate\__init__.py"", line 175, in <module>
    from .interpolate import *

  File ""C:\Users\adodhiwala\AppData\Local\conda\conda\envs\py35\lib\site-packages\scipy\interpolate\interpolate.py"", line 32, in <module>
    from .interpnd import _ndim_coords_from_arrays

  File ""interpnd.pyx"", line 1, in init scipy.interpolate.interpnd

  File ""C:\Users\adodhiwala\AppData\Local\conda\conda\envs\py35\lib\site-packages\scipy\spatial\__init__.py"", line 94, in <module>
    from .kdtree import *

  File ""C:\Users\adodhiwala\AppData\Local\conda\conda\envs\py35\lib\site-packages\scipy\spatial\kdtree.py"", line 8, in <module>
    import scipy.sparse

  File ""C:\Users\adodhiwala\AppData\Local\conda\conda\envs\py35\lib\site-packages\scipy\sparse\__init__.py"", line 228, in <module>
    from .base import *

  File ""C:\Users\adodhiwala\AppData\Local\conda\conda\envs\py35\lib\site-packages\scipy\sparse\base.py"", line 9, in <module>
    from scipy._lib._numpy_compat import broadcast_to

  File ""C:\Users\adodhiwala\AppData\Local\conda\conda\envs\py35\lib\site-packages\scipy\_lib\_numpy_compat.py"", line 17, in <module>
    _assert_warns = np.testing.assert_warns

AttributeError: module 'numpy' has no attribute 'testing'


please help me with it.",2019-01-09 08:17:01,,AttributeError: module 'numpy' has no attribute 'testing',['15 - Discussion']
12695,open,oricou,"There are 2 functions to apply a function over an array : `apply_along_axis` and `apply_over_axes`.
My understanding is that there are the same except to first is to apply with just one axis while the second is to apply with more than one axis (maybe we could merge them if my understanding is right).

I wonder if we could have something easier to use, let call it `apply`:

```
a = np.arange(24).reshape(2,3,4)
a.apply(np.sum, axis=(1,2))
a.apply(np.max, axis=0)
```

Olivier.

",2019-01-08 11:17:03,,Could we have apply like in Pandas,"['01 - Enhancement', '23 - Wish List']"
12692,open,eric-wieser,"Raised in #12675. It seems we are being inconsistent with our python vs C parsing of axis tuples, notably when passed non-tuple iterables:
```python
>>> np.core.numeric.normalize_axis_tuple([1, 2, 3], 4)
(1, 2, 3)

# calls PyArray_ConvertMultiAxis([1, 2, 3], 4, ...)
>>> np.add.reduce(np.zeros((1,)*4), axis=[1, 2, 3])
TypeError: 'list' object cannot be interpreted as an integer
```
The error messages also differ in unimportant ways:
```python
>>> np.core.numeric.normalize_axis_tuple((1, 1), 4)
ValueError: repeated axis
# calls PyArray_ConvertMultiAxis((1, 1), 4, ...)
>>> np.sum(np.zeros((1,)*4), axis=(1,1))
ValueError: duplicate value in 'axis'
```

This manifests itself in our high level API - functions implemented in C or as simple ufunc wrappers like `mean` fail on `axis=[1, 2, 3]`, but functions implemented more heavily in python like `np.median` succeed.

Some options here might be to:

* Start emitting DeprecationWarning if `normalize_axis_tuple` is called on non-tuple iterables
* Allow `PyArray_ConvertMultiAxis` to accept any iterable
* Try and use `PyArray_ConvertMultiAxis` inside `normalize_axis_tuple`, similarly to how we merged the parsing of single axes in C and python 
* Do nothing, it's only a minor inconsistency.",2019-01-08 05:49:29,,"normalize_axis_tuple is less strict than PyArray_ConvertMultiAxis, leading to inconsistent behavior",['00 - Bug']
12681,open,eric-wieser,"`np.dtype` has an overload of the form `np.dtype((scalar, shape_tuple))` that creates subarray dtypes.

Creating a dtype representing `int[3][1]` works just fine:
```
>>> np.dtype(((int, (1,)), (3,)))
dtype((('<i4', (1,)), (3,)))
```

But trying to create one representing `int[3][0]` fails with a weird error message
```
>>> np.dtype(((int, (0,)), (3,)))
ValueError: invalid itemsize in generic type tuple
```

This bug impacts:
* nested subarrays as shown above where the inner array is empty
* subarrays of empty struct types.
* subarrays of empty strings (although empty strings are hard to create in the first place)
* ...

The problem stems from either of two things:
* The fact that `np.dtype((flexible, n))` takes priority as an overload, even when `n` is a tuple, which is illegal in this overload
* The fact that `np.dtype((int, (0,)))` is considered flexible by virtue of being of size 0

---

It's not that numpy can't represent this dtype - it's just that the `dtype` constructor won't let you create it. A workaround is:
```python
def subarray_dtype(scalar, shape):
    """""" like np.dtype((scalar, shape)), but always produces subarrays in corner cases """"""
    scalar = np.dtype(scalar)
    dt = np.dtype(np.void)
    dt = copy.copy(dt)
    dt.__setstate__((3, '|', (scalar, shape), None, None, scalar.itemsize * np.prod(shape), scalar.alignment, 0))
    return dt
```

```python
>>> subarray_dtype((int, (0,)), (3,))
dtype((('<i4', (0,)), (3,))) # this repr doesn't actually work though!
```",2019-01-06 20:54:17,,Cannot produce subarrays with elements of size 0,"['00 - Bug', 'component: numpy.dtype']"
12638,open,DerDakon,"Running the tests of 1.14.5 on my Sparc T5120 with Gentoo results in several test failures:

```
chroot ~ #  grep ^FAIL /root/tatt/logs/dev-python_numpy-1.14.5_use_lFT6d
FAIL
FAIL: numpy.core.tests.test_arrayprint.TestComplexArray.test_str
FAIL: numpy.core.tests.test_longdouble.test_repr_roundtrip
FAIL: Check formatting.
FAIL: Check formatting of nan & inf.
FAIL: Check formatting of complex types.
FAIL: Check inf/nan formatting of complex types.
FAIL: Check inf/nan formatting of complex types.
FAIL: Check inf/nan formatting of complex types.
FAIL: Check inf/nan formatting of complex types.
FAIL: Check inf/nan formatting of complex types.
FAIL: Check inf/nan formatting of complex types.
FAIL: Check inf/nan formatting of complex types.
FAIL: Check inf/nan formatting of complex types.
FAIL: Check inf/nan formatting of complex types.
FAIL: Check inf/nan formatting of complex types.
FAIL: Check inf/nan formatting of complex types.
FAIL: Check inf/nan formatting of complex types.
FAIL: Check formatting when using print
FAIL: Check formatting when using print
FAIL: numpy.core.tests.test_print.test_locale_longdouble
FAIL: numpy.core.tests.test_scalarprint.TestRealScalars.test_dragon4_interface
FAIL: numpy.core.tests.test_scalarprint.TestRealScalars.test_str
FAIL: numpy.f2py.tests.test_kind.TestKind.test_all
FAIL: numpy.ma.tests.test_core.TestMaskedArrayMethods.test_sort_flexible
FAILED (KNOWNFAIL=19, SKIP=15, failures=24)
```

This has been reported as [Gentoo bug 672730](https://bugs.gentoo.org/672730) and seems to affect also ppc64.

EDIT:

Failing tests on 1.16.0rc2

- [x] TestKind.test_all
- [x] TestRecFunctions.test_structured_to_unstructured
- [ ] TestArrayEqual.test_recarrays
- [ ] TestEqual.test_recarrays",2019-01-01 11:30:23,,BUG: several test errors on SPARC,"['00 - Bug', '05 - Testing', 'component: numpy._core', 'component: numpy.f2py']"
12636,open,markcampanelli,"<!-- Please describe the issue in detail here, and fill in the fields below -->

numpy is generally awesome. One issue that really bothers me (esp. as compared to MATLAB, IFAIK), is numpy's inconsistent handling of python scalars, vs. numpy scalars, vs. numpy rank-0 arrays, which often requires much checking and recasting values in algorithms that are supposed to be very general w.r.t. scalar-valued vs. array-valued inputs and that ensure outputs have the same type as inputs (notwithstanding any expected broadcasting). I end up having to do a lot of checking of types, which is super frustrating and the cause of significant code bloat.

### Reproducing code example 1:

Special logic is required to handle a rank-0 case.

```python
import numpy as np

idx_array = np.array(1 < 0)  # rank-0 array with one False ""element"".
any(idx_array)
```

### Error message 1:

`TypeError: iteration over a 0-d array`

### Reproducing code example 2:

Some numpy functions change types unexpectedly, e.g., rank-0 `np.ndarray` to `np.float64`.

```python
import numpy as np

assert (isinstance(np.ndarray, type(np.exp(np.array(9.))))), ""Numpy's exp() really shouldn't change the type to np.float64!""

```

### Error message 2:

`AssertionError: Numpy's exp() really shouldn't change the type to np.float64!`

### Reproducing code example 3:

```python
import numpy as np

assert isinstance(np.where(True, np.float64(3.), np.float64(4.)), np.float64), ""Numpy's where() really shouldn't change the type to np.ndarray!""
```

### Error message 3:

`Numpy's where() really shouldn't change the type to np.ndarray!`


### Numpy/Python version information:

Python version: 3.6.7
Numpy version: 1.15.4",2018-12-31 18:24:52,,Respecting types and iterating over 0-d arrays,['unlabeled']
12613,open,ZisIsNotZis,"I found this problem some days ago. I understand this is not a serious problem, but I feel it is worth mentioning here?

### Reproducing code example:
    
    import numpy as np
    n = np.uint16(0b0111101111111111).view('e')
    print(n, n.astype('f'))

### Output:

    65500.0 65504.0

### Error message:

There are no error message, but `float16` `0b0111101111111111` should be around `65504.0` instead of `65500.0`.

### Numpy/Python version information:

    1.15.4 3.6.6 |Anaconda, Inc.| (default, Oct  9 2018, 12:34:16) 
    [GCC 7.3.0]

",2018-12-26 08:00:29,,Half precision display not exactly correct,['unlabeled']
12550,open,h-vetinari,"The dtype `np.object_` is the most general catch-all type to contain arbitrary python objects. As such, `.astype(object)` should preferably not upcast, but certainly never *down*cast:

### Reproducing code example:

```python
>>> import numpy as np
>>> arr = np.array([10 ** 18], dtype='M8[ns]')
>>> arr
array(['2001-09-09T01:46:40.000000000'], dtype='datetime64[ns]')
>>> arr[0]
numpy.datetime64('2001-09-09T01:46:40.000000000')
>>> arr.astype(object)
array([1000000000000000000], dtype=object)
>>> arr.astype(object)[0]
1000000000000000000
```

The expected outcome for the last two lines is:
```
>>> arr.astype(object)
array([numpy.datetime64('2001-09-09T01:46:40.000000000')], dtype=object)
>>> arr.astype(object)[0]
numpy.datetime64('2001-09-09T01:46:40.000000000')
```
as well as
```
>>> arr.astype(object)[0] == arr[0]
True
```

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```
numpy: 1.15.4
python: 3.7.1
```
",2018-12-14 20:35:10,,BUG: astype(object) downcasts for datetime-dtype,"['component: numpy.datetime64', 'triaged']"
12548,open,tylerjereddy,"**Background**: `tools/refguide_check.py` is vendored in (copied in & slightly customized) from the original version available at the same path in the SciPy repo, and runs standard library doctests with various fallbacks to make execution / expectations more reasonable for scientific code (i.e., `allclose` type checks for floating point results in NumPy arrays, and so on).

Related original issue: #9415
Related original PR to add refguide: #12253

Here's a wish list -- feel free to edit or comment below, and I'll update it.

- [ ] (some day) extract to separate package that can be shared by the community instead of duplicating the module; provide some kind of per-project customization interface? Alternative: upstream to doctest? [1]
- [ ] maybe switch from opt-in to opt-out as far as which modules / functions are actually executed / checked? [**maybe not much benefit**]
- [ ] add the ability to report coverage for docstrings executed by refguide / doctests (see: [here](https://github.com/pytest-dev/pytest-cov/issues/101) and [here](https://stackoverflow.com/a/45262687/2942522)) so we know which ones are at risk of becoming stale over time
- [ ] check more than just the category of exceptions that are raised (i.e., also check the `ACTUAL` vs. `DESIRED` values and / or the error messages? see, for example: https://github.com/numpy/numpy/pull/12591#discussion_r244645627)
  - Eric suggests `Would be neat if refguide had an # assumes: <tag> syntax, with a way to specify which tags are expected to hold. This would be stronger than the current ""may vary"".`

[1] See https://mail.python.org/pipermail/python-ideas/2014-August/028678.html and thread leading up to it.",2018-12-14 18:56:08,,"DOC, TST: Refguide Wish List","['23 - Wish List', '05 - Testing', 'component: documentation']"
12540,open,lxop,"As the title says. This interferes with tools that infer types from the docstring, e.g. PyCharm.

Just requires the addition of
```
    Returns
    -------
    arr : ndarray
        Copy of the array `a`.
    
```

### Reproducing code example:

```python
import numpy as np
help(np.ndarray.copy)
```

### Numpy/Python version information:
```
1.15.2 3.7.0 (default, Sep 12 2018, 18:30:08) 
[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]
```",2018-12-13 16:47:09,,ndarray.copy docstring missing Returns section,['unlabeled']
12521,open,kusanagee,"<!-- Please describe the issue in detail here, and fill in the fields below -->
numpy.delete documentation doesn't appear to indicate whether the ""obj"" parameter, which should ""indicate which sub-arrays to remove,"" should indicate so in terms of array indices or in terms of values. This [has led to some confusion](https://stackoverflow.com/questions/39156458/numpy-delete-not-deleting-element). 

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
np.delete(np.arange(0, 100, 10), [90])
```


<!-- Remove these sections for a feature request -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.15.4 3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 11:07:29) 
[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]
",2018-12-09 23:07:52,,numpy.delete documentation doesn't specify that obj-to-be-deleted should be specified in terms of array indices.,['unlabeled']
12518,open,eric-wieser,"Mentioned in a bunch of places, but would be valuable to have a single place to track this.

To be able to reuse an inner loop across multiple dtypes (ie across variable-length strings, structured arrays, etc), we need access to the `dtype` object within the inner loop.

Perhaps we can achieve this by adding a new `npy_bool innerloop_takes_dtype` flag to ufuncs, which when set indicates that the inner loop signature now also takes a `PyArray_Descr **dtypes` argument.",2018-12-09 08:09:03,,What should be the calling convention for ufunc inner loop signatures?,"['01 - Enhancement', '23 - Wish List', 'component: numpy.ufunc', 'component: numpy.dtype']"
12517,open,eric-wieser,"Part of #12514.

We will need a gufunc for each of the sorting mechanism. Just like in #12516, we should keep these gufuncs private initially.

I imagine we will want to implement:

* `np.core.umath.quick_sort`
* `np.core.umath.merge_sort`
* `np.core.umath.heap_sort`
* `np.core.umath.quick_argsort`
* `np.core.umath.merge_argsort`
* `np.core.umath.heap_argsort`",2018-12-09 07:59:47,,Create a gufunc form of sort and argsort,"['01 - Enhancement', '23 - Wish List', '03 - Maintenance', 'component: numpy.dtype']"
12516,open,eric-wieser,"Part of #12514

In the long term, we want `np.argmin` to become that gufunc - but in the short term, we can just wrap it to deal with any interface quirks.

In particular, `argmin` allows multiple axes to be specified.

Blocked by #12518, needed to give access to `->f->compare` in the inner loop",2018-12-09 07:55:04,,Create a gufunc form of argmin and argmax,"['01 - Enhancement', '23 - Wish List', 'component: numpy.dtype']"
12515,open,normalhuman,"**EDIT: the original issue here was resolved a long time ago, Intel removed their `erf` addition which wasn't in numpy. Issue retitled to reflect the discussion about adding `np.erf`**

<!-- Please describe the issue in detail here, and fill in the fields below -->

When `np.erf` is given a complex number, it returns the number itself instead of the value of the error function. For example,  `scipy.special.erf(1+2j)` returns `(-0.5366435657785664-5.0491437034470374j)` but `np.erf` returns `(1+2j)` itself. 

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
print(np.erf(1))     # prints 0.8427007929497149  which is correct 
print(np.erf(1+2j))   # prints (1+2j) which is incorrect
```

<!-- Remove these sections for a feature request -->

### Error message:

None, but the return value is wrong for complex arguments. It is simply the argument itself. If complex inputs are not supported, an error should be thrown instead. 

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```
1.15.4 3.7.1 (default, Oct 23 2018, 19:19:42) 
[GCC 7.3.0]
```",2018-12-09 03:26:25,,Adding the `erf` special function to numpy?,['01 - Enhancement']
12514,open,teoliphant,"https://github.com/numpy/numpy/blob/294867351296357a0bde5b16b2ec49fe86868d15/numpy/core/include/numpy/ndarraytypes.h#L444-L554

I believe that most of the functions in this list should be replaced by generalized ufuncs and this list of function pointers shrunk. 

In theory, the PyArray_Descr structure could then become a meta-type in Python parlance (""inheriting"" from  PyHeapTypeObject ala https://stackoverflow.com/questions/1237266/how-can-inheritance-be-modelled-using-c) and adding remaining fields needed by NumPy.   All dtypes would then be ""real"" Python types --- but we don't need these extra functions.  

In particular, the following functions should be (generalized) ufuncs --- others can likely be as well with a bit more thought.

* cast -> becomes several cast_to_<type> functions 
* `argmax`, `argmin` (#12516)
* dotfunc
* `sort`, `argsort` (#12517)
  * sortfunc --> several sortfuncs for each type of sort
  * argsortfunc --> several argsortfuncs for each type
* fastclipfunc (#12519)
* nonzero

This kind of refactor would break the ABI and I would suggest including only in a major release.  It would be facilitated by an improved ufunc dispatching system for NumPy as well.
",2018-12-08 22:27:34,,ENH: Convert PyArray_Functions to generalized ufuncs,"['01 - Enhancement', '23 - Wish List', 'component: numpy.dtype']"
12495,open,charris,"<!-- Please describe the issue in detail here, and fill in the fields below -->

The function needs a docstring, the logic is over complicated, ~and there is a `/` that should probably be a `//`~ (fixed). See the remarks at #12493.
",2018-12-06 01:20:09,,Clean up the `records.fromfile` function.,"['17 - Task', 'defunct — difficulty: Intermediate']"
12481,open,jakirkham,"It would be really useful for the community at large if NumPy devs proposed a way to make an array JSON serializable and provided builtin functions for performing this serialization/deserialization.

Have taken a quick look at what is out there and found several implementations (a few of these are listed below). Interestingly some people solve this problem by [just using Pandas]( https://stackoverflow.com/a/44752209 ), which only really works for 1-D or 2-D data. Some libraries include their own custom JSON encoders. Other libraries solve this by either providing a grab-bag of JSON solutions including something for NumPy or rolling their own.

While many of the discussions about encoding NumPy arrays as JSON often site that this will not be as performant as binary storage, there are still a large number of implementations for storing NumPy arrays in JSON and people seeking them. Meaning that the use cases driving this serialization are not as much about compact storage, but revolve more around other things like cross-language support, web-friendliness, portability, readability, common tooling support, etc..

Most implementations seem to go the obvious route of turning a NumPy array into a nested list. Though a few base64 encode the entire array. However they vary somewhat on how they pack in the NumPy metadata. Usually this is an dictionary that includes both the metadata and the data as key-value pairs. Some of them include the shape while others do not. Similarly some of them specify whether the data is C ordered. All of them seem to specify dtype in a string. Also they may vary on how they handle non-JSON friendly types (e.g. datetime/timedelta, object, etc.). This data may be coerced to other JSON types (e.g. `None` to `null`) or they may be handled as base64 encoded strings.

Anyways it would be good to have some discussion about MVP functionality that NumPy can provide that gets the ball rolling around consolidating how we talk about NumPy arrays in JSON in the most basic cases. This should provide good context for feedback from the larger community on how to handle various other cases.

Implementations:

ref: http://docs.astropy.org/en/stable/api/astropy.utils.misc.JsonCustomEncoder.html
ref: https://bokeh.pydata.org/en/latest/docs/reference/core/json_encoder.html
ref: https://pythonhosted.org/monty/monty.html#monty.json.MontyEncoder
ref: https://json-tricks.readthedocs.io/en/latest/#numpy-arrays
ref: https://github.com/jsonpickle/jsonpickle#numpy-support

ref: ref: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html

Issues/Questions/Discussions/Blogs:

ref: https://bugs.python.org/issue18303
ref: https://discuss.mxnet.io/t/make-ndarray-json-serializable/1627/2
ref: http://acroz.github.io/2017/11/03/data-science-apis-flask/#jsonifying-numpy-values
ref: https://stackoverflow.com/questions/26646362/numpy-array-is-not-json-serializable
ref: https://stackoverflow.com/questions/43346300/convert-numpy-nd-array-to-json
ref: https://stackoverflow.com/questions/48310067/making-numpy-arrays-json-serializable",2018-12-04 06:56:09,,(Blessed) JSON serializable format,['unlabeled']
12476,open,jakirkham,"Currently one can create an `object`-type `ndarray` and coerced to `bytes`. However what one gets is effectively the pointers from that `object` array. It's unclear how useful this is. Should it be allowed or should it error?

FWIW `memoryview`s also support this same behavior the same way. Though it is equally questionable in that case whether they are useful either.

Thoughts?

### Reproducing code example:

```python
In [1]: import numpy as np

In [2]: a = np.asarray([None])

In [3]: a.tobytes()
Out[3]: b'\xf8\xc8\xb3\x06\x01\x00\x00\x00'
```

### Numpy/Python version information:

```python
In [1]: import sys, numpy; print(numpy.__version__, sys.version)
1.15.1 3.6.6 | packaged by conda-forge | (default, Jul 26 2018, 09:55:02) 
[GCC 4.2.1 Compatible Apple LLVM 6.1.0 (clang-602.0.53)]
```",2018-12-02 22:36:38,,Should tobytes work on object arrays,['triaged']
12472,open,mattip,"In working on mamtul, it became apparent that the `numpy/core/src/common/python_xerbla.c` error handler, which should be registered when linking with OpenBLAS, is not being called. This should be used as it turns the default ""print to stderr"" to ""raise a nice Python exception"" when input to one of the blas routines is malformed.",2018-12-01 21:49:32,,ENH: Make sure the _xerbla error callback is being used by blas functions,"['01 - Enhancement', 'component: numpy.ufunc']"
12461,open,WarrenWeckesser,"Here's an example, using numpy 1.15.4:
```
In [91]: mx = np.ma.masked_array([[1.0, 2, 4, np.inf], [np.nan, 1, 4, 16]], mask
    ...: =[[0, 0, 0, 1], [0, 0, 0, 0]])

In [92]: mx
Out[92]: 
masked_array(
  data=[[1.0, 2.0, 4.0, --],
        [nan, 1.0, 4.0, 16.0]],
  mask=[[False, False, False,  True],
        [False, False, False, False]],
  fill_value=1e+20)
```
Compute the standard deviation of each row separately:
```

In [93]: np.std(mx[0])
Out[93]: 1.247219128924647

In [94]: np.std(mx[1])
Out[94]: masked
```
Compute the standard deviation of each row in one call, using `axis=1`:
```
In [95]: result = np.std(mx, axis=1)

In [96]: result
Out[96]: 
masked_array(data=[1.247219128924647, 0.0],
             mask=[False, False],
       fill_value=1e+20)
```
`result[1]` is not masked, and the value is 0.0.  That is not correct.  For consistency with the operations on the individual rows, I expect `result[1]` to be masked.  If, for some reason, the result should not be masked, than the result should be consistent with `np.std(mx[1].data)`, which returns `nan`:
```
In [97]: np.std(mx[1].data)
Out[97]: nan
```
(And that's what I would prefer the value `result[1]` to be; the masked array code seems to convert nan's to masked values unexpectedly.  But I don't think that is something that can be changed at this point.)

If `mx` does not contain `nan`, then `std` works as expected:

```
In [102]: mx[1, 0] = 0.0

In [103]: mx
Out[103]: 
masked_array(
  data=[[1.0, 2.0, 4.0, --],
        [0.0, 1.0, 4.0, 16.0]],
  mask=[[False, False, False,  True],
        [False, False, False, False]],
  fill_value=1e+20)

In [104]: np.std(mx, axis=1)
Out[104]: 
masked_array(data=[1.247219128924647, 6.378675411086537],
             mask=[False, False],
       fill_value=1e+20)

In [105]: np.std(mx[1])
Out[105]: 6.378675411086537
```",2018-11-28 17:46:14,,BUG: std with axis on masked array containing nan returns incorrect result.,"['00 - Bug', 'component: numpy.ma']"
12444,open,hideyukiinada,"<!-- Please describe the issue in detail here, and fill in the fields below -->
In np.place(), if you specify a numpy array with the int64 data type for the arr argument and pass a single element Python list with a floating number for the vals argument, it casts the float to an int without a warning.  However, if you pass a single element numpy array with a floating number for the vals argument, it throws an exception due to a data type mismatch.
Allowing a silent cast for the former case seems dangerous as the user may not even notice that  replacement is not happening in a case like below:

\>\>\> ar
array([1, 0])
\>\>\> ar.dtype
dtype('int64')
\>\>\> np.place(ar, ar==0, [0.5])
\>\>\> ar
array([1, 0])
\>\>\> np.place(ar, ar==0, np.array([0.5])) 
This throws \:
TypeError: Cannot cast array data from dtype('float64') to dtype('int64') according to the rule 'safe'

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
ar = np.array([1, 0])
ar.dtype
np.place(ar, ar==0, [0.5])
ar
np.place(ar, ar==0, np.array([0.5]))
```

<!-- Remove these sections for a feature request -->

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.14.5 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 05:52:31) 
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]
",2018-11-24 20:40:37,,Inconsistent cast behavior of np.place when arr is int64 and vals is float,['unlabeled']
12435,open,laborleben,"numpy.histogram() raises IndexError when using `bins='auto'`.
The issue is originating from floating point precision error when calculating the histogram bin number.
When using `bins='auto'`, sometimes the number of histogram bins (`n_equal_bins`) can be quite high  and the attributes `first_edge` & `last_edge` are of type `numpy.float32`.
See:
https://github.com/numpy/numpy/blob/4f1541e1cb68beb3049a21cbdec6e3d30c2afbbb/numpy/lib/histograms.py#L726 https://github.com/numpy/numpy/blob/4f1541e1cb68beb3049a21cbdec6e3d30c2afbbb/numpy/lib/histograms.py#L750 https://github.com/numpy/numpy/blob/4f1541e1cb68beb3049a21cbdec6e3d30c2afbbb/numpy/lib/histograms.py#L338
The calculation of the bin number is wrong due to the limited floating point precision (numpy.float32):
https://github.com/numpy/numpy/blob/4f1541e1cb68beb3049a21cbdec6e3d30c2afbbb/numpy/lib/histograms.py#L784
This will result in an IndexError:
https://github.com/numpy/numpy/blob/4f1541e1cb68beb3049a21cbdec6e3d30c2afbbb/numpy/lib/histograms.py#L790

### Reproducing code example:

```python
import numpy as np
np.random.seed(0)
valmin, valmax = -2013.0254, 1754.6786
vals = [valmin, valmax]
vals = np.r_[vals, np.random.normal(0.0, 0.001, 100001)]
np.histogram(vals.astype(np.float32), bins='auto')
# IndexError: index 65153184 is out of bounds for axis 0 with size 65153184
```

### Numpy/Python version information:

('1.15.4', '2.7.15 |Anaconda, Inc.| (default, Oct 10 2018, 21:32:13) \n[GCC 7.3.0]')

",2018-11-21 16:34:13,,BUG: numpy.histogram() raises IndexError and misses consistency checks,"['00 - Bug', '06 - Regression', 'component: numpy.lib']"
12433,open,mattip,"`nditer` has a per-operand flag `contig` but it must be used with the global `buffered` flag:
``` 
>>> a = np.arange(6, dtype='i4')[::2]
>>> it = np.nditer(a, [], [['writeonly', 'contig']])
TypeError: Iterator operand required buffering, to be contiguous \
     as requested, but buffering is not enabled
```
The non-buffered contiguous case could be handled by either
- Use writebackoncopy semantics to create a copy of the operand before iterating, and write back the results when the iterator is finished
- At each iteration, create a local contiguous copy of the relevant data, and write it back at the beginning of each step. Write the final piece back when the iterator is finished.

The first method would be easier, but the second could allocate a smaller copy. This would be useful in, for instance, `linalg` and `matmul` ufuncs.

I admit I do not understand how the buffers work. Perhaps that could also provide what I am looking for, but the interface is not very well documented and the examples in the documentation and tests are not very helpful to me.",2018-11-21 05:45:58,,"ENH: support contig iter op flag without buffering in nditer, or improve buffer usability","['01 - Enhancement', '23 - Wish List', '04 - Documentation', 'component: numpy.ufunc']"
12432,open,eric-wieser,"Caused by `dtype.descr` being broken by design, and `__array_interface__` being broken by association.

cc @ahaldane 

```python
>>> dt = np.dtype(dict(
            formats=['<i4', '<i4'],
            names=['a', 'b'],
            offsets=[0, 2],
            itemsize=6
        ))
>>> a = np.zeros(10, dt)
>>> np.lib.index_tricks.as_strided(a)
TypeError: data type not understood

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#132>"", line 1, in <module>
    np.lib.index_tricks.as_strided(a)
  File ""C:\Users\wiese\AppData\Roaming\Python\Python35\site-packages\numpy\lib\stride_tricks.py"", line 102, in as_strided
    array = np.asarray(DummyArray(interface, base=x))
  File ""C:\Users\wiese\AppData\Roaming\Python\Python35\site-packages\numpy\core\numeric.py"", line 501, in asarray
    return array(a, dtype, copy=False, order=order)
SystemError: <built-in function array> returned a result with an error set
```",2018-11-21 03:25:29,,as_strided fails on dtypes with overlapping fields,"['00 - Bug', 'component: numpy._core', 'component: numpy.dtype']"
12427,open,abaxi,"If I make a sparse matrix, pick out one row from it, call to todense() on the row, and finally call np.roll(row, 1), then the row isn't rolled.

If I define row_np = np.array(row), and then call np.roll(row_np, 1), then it rolls. 

Is this behaviour how it is intended (and I'm missing some underlying logic), or is it a bug?

Should this be posted in the scipy issues instead?

Thank you.

### Reproducing code example:
```python
import numpy as np
from scipy.sparse import csr_matrix

nrows = 3
ncols = 5
sparse_data = [1,2,3]
sparse_i = [0,1,2]
sparse_j = [0,2,4]
Y = csr_matrix((sparse_data, (sparse_i, sparse_j)), shape=(nrows, ncols))

y_0 = Y[0].todense()
print ""y_0:"", y_0
print ""y_0 rolled by 1:"", np.roll(y_0, 1)
y_0 = np.array(y_0)
print ""np.array(y_0) rolled by 1:"", np.roll(y_0, 1)

```

<!-- Remove these sections for a feature request -->

### Outputs:

y_0: [[1 0 0 0 0]]
y_0 rolled by 1: [[1 0 0 0 0]]
np.array(y_0) rolled by 1: [[0 1 0 0 0]]

### Numpy/Python version information:

Output from 'import sys, numpy; print(numpy.__version__, sys.version)' 

('1.15.1', '2.7.12 (default, Nov 12 2018, 14:36:49) \n[GCC 5.4.0 20160609]')


import sys, scipy; print(scipy.__version__, sys.version)
('0.17.0', '2.7.12 (default, Nov 12 2018, 14:36:49) \n[GCC 5.4.0 20160609]')",2018-11-20 17:20:23,,BUG: Sparse matrix todense doesn't roll (np.roll),"['00 - Bug', 'component: numpy.matrixlib']"
12426,open,Doom312,"When I try to read strings from file to numpy array Python crashes.

### Reproducing code example:

```python
import sys
print(sys.platform)
print(sys.version)

import numpy as np
print(np.version.full_version)

np.fromfile('data\\foo.txt', dtype='str')
```
```python
win32
3.7.1 (default, Oct 28 2018, 08:39:03) [MSC v.1912 64 bit (AMD64)]
1.15.4
```

The file contains:
```
[data\foo.txt content -- this line is not there]
dsadas
asdasdads
asdasda
dasdasda
```

<!-- Remove these sections for a feature request -->

### Error message:

There is no error message just the Python has stopped working window:
![image](https://user-images.githubusercontent.com/35496930/48771242-247fd200-ecc1-11e8-88ba-78a4969a1697.png)

---

Summary by @seberg:

As noted below, this is now working, but gives a strange `ValueError`(for 'S' dtype without a size).",2018-11-20 11:39:57,,`np.fromfile()` raises non-helpful error when reading non-numerical text,"['00 - Bug', '15 - Discussion']"
12389,open,grlee77,"`numpy.roll` (and functions that use it such as `numpy.fft.fftshift`, `numpy.fft.ifftshift`) are inefficient when `shift=0` (or `axis=()`). In that case it would be possible to just return the input array rather than allocating a new empty array and then copying the old array into it. 

Obviously, the  downside to changing this would be potentially breaking existing code that relies on `numpy.roll` always returning a copy (although I don't see any guarantee to that effect in the docstring). 

I encountered this when using a ""centered FFT"" wrapper I had created for `numpy.fft.fftn` that had calls to `fftshift`/`ifftshift`, but where I sometimes passed `axes=()` when I wanted an uncentered FFT. I noticed that the uncentered case was not faster and this was due to this unecessary copy being made in `np.roll`.  It is not a big problem for me to avoid calling `fftshift` or `ifftshift` when the axes are empty in my centered FFT function, but thought this might be of more general interest.

If there is interest, I can make a PR with an implementation. If this is considered too much micro-optimization, feel free to close the issue.

### Reproducing code example:

```python
import numpy as np
r = np.empty((256, 256, 256))
%timeit np.roll(r, (), axis=())
```
`54.2 ms ± 3.78 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)`

The time is reduced to 10-20 **microseconds** if a copy is not made.

",2018-11-14 21:42:57,,performance of numpy.roll with empty axes or zero shift(s),['unlabeled']
12370,open,anntzer,"datetime64 and timedelta64 are listed in https://docs.scipy.org/doc/numpy-1.15.0/reference/arrays.datetime.html, but perhaps they should get an entry in https://docs.scipy.org/doc/numpy-1.15.0/reference/arrays.scalars.html as well (being builtin types)?",2018-11-12 20:41:02,,"datetime64/timedelta64 not listed in ""builtin scalar types"" doc?","['04 - Documentation', 'component: numpy.datetime64']"
12368,open,liwt31,"As we are considering replacing mergesort with another algorithm in hope of improving performance for nearly sorted data #12186 , maybe we should include benchmark for mergesort in the benchmark suite, which currently only takes care of quicksort.
As I see it, there are two ways to do this:
* Simply add 2 benchmark functions on random data and nearly sorted data for mergesort.
* Separate sorting algorithm benchmark from `bench_function_base.py` to a new file and construct a systematic benchmark framework for sorting algorithms. This can provide a useful tool for future optimizations, however it may introduce lots of unnecessary benchmark results.

Any advice or suggestions? Thank you all in advance.",2018-11-12 14:02:24,,BENCH: add benchmark suite for mergesort,['unlabeled']
12367,open,guilgautier,"<!-- Please describe the issue in detail here, and fill in the fields below -->

There is an inconsistent behavior of the Gamma distribution with shape parameter 0 between `Scipy` and `Numpy`.

From the definition, see [Wikipedia](https://en.wikipedia.org/wiki/Gamma_distribution), shape must be `shape>0`.

1. `Scipy` fits the definition and allows only `shape>0`:
    -  when `shape=0` raises an **error** 
2. `Numpy` (since https://github.com/numpy/numpy/issues/5818) allows shape parameter to be `shape>=0`
    - when `shape=0` returns `0`

In some sense `Numpy` generalizes Gamma(0,*) distribution to be Dirac at 0, which is **odd since the nomalizing constant isn't even defined**.



### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
import scipy as sp

np.random.gamma(shape=0)
0.0

sp.stats.gamma(a=0).rvs()
```

<!-- Remove these sections for a feature request -->

### Error message:

```python
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-62-b6789068325a> in <module>
----> 1 sp.stats.gamma(a=0).rvs()

~/anaconda/lib/python3.5/site-packages/scipy/stats/_distn_infrastructure.py in rvs(self, size, random_state)
    468         kwds = self.kwds.copy()
    469         kwds.update({'size': size, 'random_state': random_state})
--> 470         return self.dist.rvs(*self.args, **kwds)
    471 
    472     def sf(self, x):

~/anaconda/lib/python3.5/site-packages/scipy/stats/_distn_infrastructure.py in rvs(self, *args, **kwds)
    938         cond = logical_and(self._argcheck(*args), (scale >= 0))
    939         if not np.all(cond):
--> 940             raise ValueError(""Domain error in arguments."")
    941 
    942         if np.all(scale == 0):

ValueError: Domain error in arguments.
```

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.gamma.html

https://docs.scipy.org/doc/scipy-0.16.1/reference/generated/scipy.stats.gamma.html#scipy.stats.gamma",2018-11-12 12:49:27,,BUG: Inconsistent behavior of Gamma distribution with shape parameter 0 between Scipy and Numpy,['unlabeled']
12352,open,anntzer,"<!-- Please describe the issue in detail here, and fill in the fields below -->

Currently, nanarg{min,max}(..., axis=...) raises a ValueError when any full-nan slice exists.  It looks like a while ago (#3030) this used to return UINTPTR_MIN (the largest negative value), and this was removed to avoid hiding bugs.

I agree with having this behavior by default, but OTOH this prevents writing code where we *know* that all-nan slices are possible and we are ready to handle them.  Adding an option like `raise_on_all_nan: bool` (defaulting to True) would allow one to do e.g.
```
idxs = np.nanargmin(t, axis=1, raise_on_all_nan=False)
all_nans_mask = idxs < 0
# do some handling with all_nans_mask
```

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
np.nanargmin([[np.nan, np.nan], [0, 1]], axis=1)
```

<!-- Remove these sections for a feature request -->

### Error message:

```
ValueError: All-NaN slice encountered
```

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.15.4 3.7.0 (default, Jun 28 2018, 13:15:42) 
[GCC 7.2.0]",2018-11-08 15:46:17,,"ENH: Let nanarg{min,max}(..., axis=...) optionally return a placeholder value for all-nan slices.",['01 - Enhancement']
12350,open,ulfaslak,"<!-- Please describe the issue in detail here, and fill in the fields below -->

Here's a weird bug I just encountered in NumPy. When I run

`%timeit np.min([0.017687381938994294, 0.005285820643230233])`,

in a Jupyter notebook cell, I get vastly different results. Over many tries, the fastest is 18.8 µs and the slowest is a whopping 26.4 ms! Compared with Python's builtin `min` function, which makes the same evaluation in under 300 ns, both extremes are very slow. *Sidenote: running just `np.min(a)` where `a` has been declared in a previous cell, makes no measurable difference.*

In one run I received the warning:

> The slowest run took 31.07 times longer than the fastest. This could mean that an intermediate result is being cached.

Changing the array values did not produce any significant speedups for me.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

Jupyter cell [0]
```python
%timeit np.min([0.017687381938994294, 0.005285820643230233])
# 18.8 µs ± 1.59 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)
```

Jupyter cell [1]
```python
%timeit min([0.017687381938994294, 0.005285820643230233])
# 259 ns ± 3.73 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
```

<!-- Remove these sections for a feature request -->

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

1.15.1 3.7.0 (default, Jun 28 2018, 07:39:16) 
[Clang 4.0.1 (tags/RELEASE_401/final)]

Python 3.7
",2018-11-08 10:48:11,,"np.min is slower than builtin min for small arrays, occasionally VERY slow",['unlabeled']
12348,open,mattip,"@charris [commented](https://github.com/numpy/numpy/pull/12219#issuecomment-436412360) on PR #12219 wishing to expose the vector-matrix matrix-vector loops function from the matmul ufunc. 

We would need two ufuncs with signature `(1,n),(n,p?)->(1,p?)` and `(m?,n),(n,1)->(m?,1)`, and could reuse the `@TYPE@_matmul` loops",2018-11-07 17:07:39,,"ENH: expose matmat, matvec, vecmat ufuncs","['01 - Enhancement', '23 - Wish List', 'component: numpy.ufunc']"
12344,open,itamarst,"I would expect `(a * b).sum()` and `np.dot(a, b)` to be the same for 1D arrays. However, the former upscales(?) the dtype, and the latter doesn't, so they give different results. This may be expected behavior, but if so the `np.dot` docs should mention the dtype behavior.

### Reproducing code example:

```
In [36]: a = np.array([254, 2, 200], dtype=np.uint8)

In [37]: b = np.array([True, False, True], dtype=np.bool)

In [38]: (a * b).sum()
Out[38]: 454

In [39]: np.dot(a, b)
Out[39]: 198

In [40]: np.dot(a, b).dtype
Out[40]: dtype('uint8')
```
### Numpy/Python version information:

Numpy 1.15.0, Python 3.6.5

",2018-11-06 18:11:56,,"numpy.dot has undocumented dtype behavior, resulting in different values than (a * b).sum()",['unlabeled']
12343,open,gpelouze,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

```python
import numpy as np

big_arr = np.ndarray(2**31 - 1, dtype=np.bool)
dtype = np.dtype([
    ('a', big_arr.dtype, big_arr.shape),
    ('b', big_arr.dtype, big_arr.shape),
    ('c', object),
    ])
print(dtype)
# {'names':['a','b','c'], 
#  'formats':[('?', (2147483647,)),('?', (2147483647,)),'O'], 
#  'offsets':[0,2147483647,-2], 
#  'itemsize':6}
arr = np.array((big_arr, big_arr, None), dtype=dtype)
```

### Error message:

GDB traceback:

~~~

#0  0x00007ffff7ef4f74 in __memmove_avx_unaligned_erms () from /usr/lib/libc.so.6
#1  0x00007ffff71f7014 in ?? () from /usr/lib/python3.7/site-packages/numpy/core/multiarray.cpython-37m-x86_64-linux-gnu.so
#2  0x00007ffff71f77b4 in ?? () from /usr/lib/python3.7/site-packages/numpy/core/multiarray.cpython-37m-x86_64-linux-gnu.so
#3  0x00007ffff71d1642 in ?? () from /usr/lib/python3.7/site-packages/numpy/core/multiarray.cpython-37m-x86_64-linux-gnu.so
#4  0x00007ffff71ed737 in ?? () from /usr/lib/python3.7/site-packages/numpy/core/multiarray.cpython-37m-x86_64-linux-gnu.so
#5  0x00007ffff71ed89e in ?? () from /usr/lib/python3.7/site-packages/numpy/core/multiarray.cpython-37m-x86_64-linux-gnu.so
#6  0x00007ffff720c30c in ?? () from /usr/lib/python3.7/site-packages/numpy/core/multiarray.cpython-37m-x86_64-linux-gnu.so
#7  0x00007ffff720c49b in ?? () from /usr/lib/python3.7/site-packages/numpy/core/multiarray.cpython-37m-x86_64-linux-gnu.so
#8  0x00007ffff72920ab in ?? () from /usr/lib/python3.7/site-packages/numpy/core/multiarray.cpython-37m-x86_64-linux-gnu.so
#9  0x00007ffff7b713ff in _PyMethodDef_RawFastCallKeywords () from /usr/lib/libpython3.7m.so.1.0
#10 0x00007ffff7b71461 in _PyCFunction_FastCallKeywords () from /usr/lib/libpython3.7m.so.1.0
#11 0x00007ffff7be170b in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.7m.so.1.0
#12 0x00007ffff7b2a879 in _PyEval_EvalCodeWithName () from /usr/lib/libpython3.7m.so.1.0
#13 0x00007ffff7b2b7a4 in PyEval_EvalCodeEx () from /usr/lib/libpython3.7m.so.1.0
#14 0x00007ffff7b2b7cc in PyEval_EvalCode () from /usr/lib/libpython3.7m.so.1.0
#15 0x00007ffff7c541e4 in ?? () from /usr/lib/libpython3.7m.so.1.0
#16 0x00007ffff7c556ee in PyRun_FileExFlags () from /usr/lib/libpython3.7m.so.1.0
#17 0x00007ffff7c58c45 in PyRun_SimpleFileExFlags () from /usr/lib/libpython3.7m.so.1.0
#18 0x00007ffff7c5af43 in ?? () from /usr/lib/libpython3.7m.so.1.0
#19 0x00007ffff7c5b14c in _Py_UnixMain () from /usr/lib/libpython3.7m.so.1.0
#20 0x00007ffff7db9223 in __libc_start_main () from /usr/lib/libc.so.6
#21 0x000055555555505e in _start ()
~~~



### Numpy/Python version information:

Happens on both following systems:

~~~
Arch Linux: 4.18.12-arch1-1-ARCH #1 SMP PREEMPT Thu Oct 4 01:01:27 UTC 2018 x86_64 GNU/Linux
1.15.3
3.7.1 (default, Oct 22 2018, 10:41:28) 
[GCC 8.2.1 20180831]
~~~

~~~
Debian 9.5: 4.9.0-7-amd64 #1 SMP Debian 4.9.110-1 (2018-07-05) x86_64 GNU/Linux
1.15.1
3.5.3 (default, Jan 19 2017, 14:11:04)
[GCC 6.3.0 20170118]
~~~",2018-11-06 14:21:15,,Integer overflow in dtype offset causes segfault,"['00 - Bug', 'component: numpy._core', 'Project']"
12336,open,scottstanie,"<!-- Please describe the issue in detail here, and fill in the fields below -->
Would it be possible to add some option to the `atleast_2d` function so that you can specify where you'd like the `newaxis` to be placed? I'd like to be able to add the `newaxis` as the last dimension to force the array to a column vector.

My use case is that I want to have a column vector, but the input may appear as (N,k), or an (N,) array. If I use `atleast_2d`, then I would need to transpose the result if the original shape was (N,), but shouldn't transpose if the shape was already a column.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
>>> A = np.empty((5, 3))
>>> rhs1 = np.arange(10).reshape((5, 2))
>>> rhs2 = np.arange(5)

>>> A[:, (0, 1)] = rhs1
>>> A[:, [2]] = rhs2
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ValueError: shape mismatch: value array of shape (5,) could not be broadcast to indexing result of shape (1,5)

# Other option:
>>> A[:, [2]] = np.atleast_2d(rhs2).T
>>> A[:, [0, 1]] = np.atleast_2d(rhs1).T
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ValueError: shape mismatch: value array of shape (1,5) could not be broadcast to indexing result of shape (1,5)

# Ideally what I want:
>>> A[:, [0, 1]] = np.atleast_2d(rhs1, axis=-1)
>>> A[:, [2]] = np.atleast_2d(rhs2, axis=1)


```
",2018-11-05 16:12:25,,atleast_2d axis argument option,"['01 - Enhancement', '23 - Wish List']"
12334,open,takluyver,"From various other mentions in the docs and elsewhere, these dictionaries are meant to be part of the public API, but I can't find them explicitly documented in the Numpy reference manual. E.g. [search for sctypes](https://docs.scipy.org/doc/numpy/search.html?q=sctypes&check_keywords=yes&area=default).

Having documentation would be useful both for knowing what they are and how to use them, and as reassurance that they're not going to go away with no warning.",2018-11-05 11:30:08,,DOC: sctypes & sctypeDict not documented,['04 - Documentation']
12332,open,eric-wieser,"This has come up before, but I'm not sure we have a canonical issue.

Historically in python 2:
* `builtins.int` is mapped to `np.int_` (C `long`), since `builtins.int` is stored as a C `long`
* `builtins.long` is mapped to `np.longlong` (C `long long`), since  `builtins.long` is infinite precision and that's the largest available integer type

```python
# python 2.7
>>> np.dtype(int).char
'l'
>>> np.dtype(long).char
'q'
```
Note the above output only reflects the state of windows - other platforms have `sizeof(long) == sizeof(long long)`, so the distinction isn't important.

In python 3, `builtins.int` has been removed, and `builtins.long` has been renamed to `__builtins__.int`. Yet:
```python
# python 3.5
>>> new_long = int
>>> np.dtype(new_long).char
'l'  # smaller than it was on python 2
```

This means that python code translated from 2 to 3 by replacing `long` with `int` will start behaving differently:
```python
# python 2
>>> np.array(10**10, dtype=long)
array(10000000000L, dtype=int64)
```
```python
# python 3 translation
>>> np.array(10**10, dtype=int)
OverflowError: Python int too large to convert to C long
```

Since this affects users transitioning from 2 to 3, I think it's important that we get it fixed in 1.16, which will be the last version that transitioning users can test both version of python against.

---

The current implementation, introduced in aa7be886274a6535169e5866383cc997231cf99d by @pv, is:
```C
#if !defined(NPY_PY3K)
        if (obj == (PyObject *)(&PyInt_Type)) {
            check_num = NPY_LONG;
        }
        else if (obj == (PyObject *)(&PyLong_Type)) {
            check_num = NPY_LONGLONG;
        }
#else
        if (obj == (PyObject *)(&PyLong_Type)) {
            check_num = NPY_LONG;
        }
#endif
```

I'd propose it should have been:
```C
#if !defined(NPY_PY3K)
        if (obj == (PyObject *)(&PyInt_Type)) {
            check_num = NPY_LONG;
        }
        else
#endif 
        if (obj == (PyObject *)(&PyLong_Type)) {
            check_num = NPY_LONGLONG;
        }
```

Ie, treating `PyLong_Type` in python 3 just as we always did in python 2.",2018-11-05 06:56:21,,np.dtype(int) should be np.longlong on python 3,"['00 - Bug', '23 - Wish List', '54 - Needs decision', '60 - Major release', 'component: numpy.dtype']"
12322,open,ra1u,"`np.dtype(int)` is  int64, not python int as expected.

```
>>> import numpy as np
>>> a = np.array([2],dtype = np.dtype(int))
>>> a.dtype
dtype('int64')
>>> a**63
array([-9223372036854775808])
>>> np.version.version
'1.14.3'
```

`np.dtype(int)` is expected to be  [Python compatible integer](https://docs.scipy.org/doc/numpy-1.14.0/reference/arrays.dtypes.html#specifying-and-constructing-data-types).
Specifically Python integers have [unlimited precision](https://docs.python.org/dev/library/stdtypes.html?highlight=unlimited#numeric-types-int-float-complex ), but numpy is limited to only 63 binary digits.
",2018-11-03 23:25:04,,dtype(int) is int64 not python int,['04 - Documentation']
12316,open,kuzand,"The `argmin` method of the masked array is not working properly.

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np

a = np.ma.array([1, 2, 1e+21, 1e+22], mask=[1, 1, 0, 0])
d = a.filled().view(np.ndarray)

print(d.argmin())  # 0
print(a.argmin())  # 2
```
However, the `argmin` method is implemented by computing `d` and then returning `d.argmin()`:
```python
# https://github.com/numpy/numpy/blob/master/numpy/ma/core.py
def argmin(self, axis=None, fill_value=None, out=None):
    if fill_value is None:
        fill_value = minimum_fill_value(self)
    d = self.filled(fill_value).view(ndarray)
    return d.argmin(axis, out=out)
```

 So why I am getting two different results?

On the other hand, when the unmasked items are all inf's, then the `argmin` returns always 0:
```python
a = np.ma.array([1, 2, np.inf, np.inf], mask=[1, 1, 0, 0])
d = a.filled().view(np.ndarray)

print(d.argmin())  # 0
print(a.argmin())  # 0
```
But in both cases `fill_value=1e+20` is smaller than `a[2] = 1e+21` and `a[2] = np.inf`. In any case, the output I would expect is 2 for both examples, since I don't want to compare the unmasked items with the masked items.

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
1.15.3 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)]
",2018-11-02 22:52:23,,BUG: The `argmin` method of the masked array is not working properly.,"['00 - Bug', 'component: numpy.ma']"
12314,open,Alexander-Shukaev,"Grepped by accident:

```
numpy/matrixlib/tests/test_interaction.py\0266:    assert(isinstance(np.ediff1d(np.matrix(1)), np.matrix))
numpy/matrixlib/tests/test_interaction.py\0267:    assert(isinstance(np.ediff1d(np.matrix(1), to_begin=1), np.matrix))
numpy/ma/tests/test_core.py\03584:        assert((foo.mean() == bar.mean()) is np.bool_(True))
numpy/ma/timer_comparison.py\0141:        assert((xm-ym).filled(0).any())
numpy/ma/timer_comparison.py\0143:        assert(xm.size == reduce(lambda x, y:x*y, s))
numpy/ma/timer_comparison.py\0144:        assert(self.count(xm) == len(m1) - reduce(lambda x, y:x+y, m1))
numpy/ma/timer_comparison.py\0152:            assert(self.count(xm) == len(m1) - reduce(lambda x, y:x+y, m1))
numpy/ma/timer_comparison.py\0192:        assert(m is m2)
numpy/ma/timer_comparison.py\0194:        assert(m is not m3)
numpy/ma/timer_comparison.py\0382:        assert(wts == 4.0)
numpy/ma/timer_comparison.py\0384:        assert(self.average(ott, axis=0) is self.masked)
numpy/ma/timer_comparison.py\0389:        assert(self.average(ott, axis=1)[0] is self.masked)
numpy/core/tests/test_memmap.py\0168:        assert(fp.__class__ is memmap)
numpy/core/tests/test_memmap.py\0170:        assert(fp.__class__ is memmap)
numpy/core/tests/test_memmap.py\0192:        assert(fp[[0, 1]].__class__ is MemmapSubClass)
numpy/core/numerictypes.py\0240:        assert(info.type == obj)  # sanity check
```

`assert` is a keyword, not a function.",2018-11-02 19:28:44,,"`assert' is a keyword, not a function",['03 - Maintenance']
12313,open,dnadeau4,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
import numpy.ma

mymaskedarray = numpy.ma.MaskedArray([1.,2.,3.,4.],mask=numpy.ma.nomask,dtype=numpy.float32)
mymaskedarray2 = numpy.ma.MaskedArray([1.,2.,3.,4.],mask=False,dtype=numpy.float32)

numpy.ma.average(mymaskedarray2)
numpy.ma.average(mymaskedarray)

print(type(numpy.ma.average(mymaskedarray2)))
print(type(numpy.ma.average(mymaskedarray)))

```
### Output
<type 'numpy.float64'>
<type 'numpy.float32'>

<!-- Remove these sections for a feature request -->

### Error message:
No error, but precission problem in computation which make my test fail using `numpy.ma.allclose(A,B)`,  It is almost the same, but a little higher than the default threshold.

**numpy 1.15 or numpy 1.14.6**
```python
x = x - xmean
(Pdb) xmean
masked_array(data=[13.800759617286392, 13.947294983682752,
                   14.146195423753955, 14.284184323081487,
                   14.364891341969937, 14.423645792128164,
...
                   13.619312141515032, 13.724408499802216,
                   13.800759617286392],

             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
...
                   False, False, False, False, False, False, False, False,
                   False],
       fill_value=1e+20)

```
**numpy 1.14.5**
```python
x = x - xmean
(Pdb) xmean
masked_array(data=[13.800759 , 13.947295 , 
                   14.146195 , 14.284184 ,
                   14.364891 , 14.423646 
      
                  13.619312 , 13.724408 ,
                   13.800759 ],

             mask=False,
       fill_value=1e+20,
            dtype=float32)

```
See:
https://github.com/CDAT/genutil/issues/26#issuecomment-435461448

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```
Using python 3.6

numpy                     1.14.6          py36_blas_openblashb06ca3d_200  [blas_openblas]  conda-forge
numpy-base                1.14.6           py36h2f8d375_4  
numpy                     1.14.5           py36h1b885b7_4  
numpy-base                1.14.5           py36hdbf6ddf_4  
```
### Difference in `numpy` computation code
https://github.com/numpy/numpy/blob/master/numpy/ma/core.py#L5155-L5160",2018-11-02 18:17:21,,"numpy.ma.average() give different answer with mask set to ""nomask"" or ""False"".",['unlabeled']
12309,open,hmaarrfk,"Should from npy_PyFile_Dup2 check the `seekable` attribute:

https://github.com/numpy/numpy/blob/9af9b14dff1479c1887e6afbf86e9bb6dfed5bf4/numpy/core/include/numpy/npy_3kcompat.h#L240

For example

```python
import subprocess as sp
a = sp.Popen(['echo hello world this is so great'], stdout=sp.PIPE, shell=True)
print(a.stdout.seekable())
print(a.stdout)
import numpy as np
np.fromfile(a.stdout, dtype='uint8', count=10)
```

```python
False
<_io.BufferedReader name=13>
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
<ipython-input-2-ab1c03142ecc> in <module>
      4 print(a.stdout)
      5 import numpy as np
----> 6 np.fromfile(a.stdout, dtype='uint8', count=10)

OSError: obtaining file position failed
```


but this works

```python
import subprocess as sp
a = sp.Popen(['echo hello world this is so great'], stdout=sp.PIPE, bufsize=0, 
             shell=True)
print(a.stdout.seekable())
print(a.stdout)
import numpy as np
np.fromfile(a.stdout, dtype='uint8', count=10)
```

```
False
<_io.FileIO name=12 mode='rb' closefd=True>
array([104, 101, 108, 108, 111,  32, 119, 111, 114, 108], dtype=uint8)
```",2018-11-02 13:25:51,,from npy_PyFile_Dup2 check the `seekable` attribute,['unlabeled']
12305,open,algogavin,"<!-- Please describe the issue in detail here, and fill in the fields below -->
The issue occurred in the ""python embedded mode"" only. Basically my C program will create a new python interpreter for each task, the interpreter will execute the python script(that will use numpy there). For the first task, everything runs fine, but then the following tasks, it will throw an error indicating ""NoneType object is not callable"" and if I debug the script, it happen with it call ""np.mean()""(where inside the function, the first function call is ""arr=asanyarray(a)"", and the asanyarray is set to ""None"" somehow.[As a comparison, it's not ""None"" for the first interpreter.]

I verify that for each new interpreter, it will re-import the numpy(among others) modules and did not see any error reported. Also , if I change the code below:

a = ... //an numpy array
b = a.mean();   //this will cause the ""None"" error

===>
a = ...// an numpy array
a_2 = np.asanyarray( a );   //this is ok!
b = a_2.mean(); // again, ""None"" error

I suspect there are some initialization routine in numpy.core.numeric was not properly executed when importing, but I don't know how to debug it.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
a = np.array[1.0]
b = a.mean()
```

<!-- Remove these sections for a feature request -->

### Error message:
'NoneType' object is not callable

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:
1.15.3

above issue did not occur with numpy 1.6.0 [I have not get a chance to test the numpy package between 1.6 and 1.15]

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2018-11-01 14:11:31,,asanyarray becomes None after re-import np in Embedded mode,"['00 - Bug', '50 - Duplicate', 'Embedded']"
12293,open,jakirkham,"In NumPy's `pad`, it appears to be organically constructing the linear ramps with `arange` and some floating point arithmetic. Maybe it would be better to use `linspace`, which is a closer match to the linear ramp use case. In particular, see the `_prepend_ramp` and `_append_ramp` helper functions.",2018-10-30 01:59:32,,Using `linspace` in `pad`'s `linear_ramp` mode ,['unlabeled']
12283,open,scottstanie,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:
The `nanmean` function over a fully masked int array works, whereas a float array does not:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
>>> aa = np.ma.array([[1, 2], [3, 4]], mask=[[True, True], [True, True]])

>>> np.nanmean(aa)
 masked

>>> aa = np.ma.array([[1.0, 2], [3, 4]], mask=[[True, True], [True, True]])
>>> np.nanmean(aa)

ValueError: output array is read-only
```

<!-- Remove these sections for a feature request -->

### Error message:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/scott/envs/data/lib/python3.5/site-packages/numpy/lib/nanfunctions.py"", line 865, in nanmean
    avg = _divide_by_count(tot, cnt, out=out)
  File ""/home/scott/envs/data/lib/python3.5/site-packages/numpy/lib/nanfunctions.py"", line 179, in _divide_by_count
    return np.divide(a, b, out=a, casting='unsafe')
ValueError: output array is read-only
```
<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:
```
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.15.3 3.5.2 (default, Nov 17 2016, 17:05:23) 
[GCC 5.4.0 20160609]
```

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2018-10-28 21:03:33,,nanmean with masked array gives readonly error only with floats,['unlabeled']
12282,open,astrofrog,"When inf values are present in an array, under certain conditions np.percentile can return NaN as the median, whereas np.median can return a finite value.

### Reproducing code example:

```python
>>> import numpy as np
>>> np.percentile([np.inf, 5, 4], [10, 20, 30, 40, 50, 60, 70, 80, 90])
/Users/tom/miniconda3/envs/alldev/lib/python3.7/site-packages/numpy-1.16.0.dev0+45718fd-py3.7-macosx-10.7-x86_64.egg/numpy/lib/function_base.py:3947: RuntimeWarning: invalid value encountered in multiply
  x2 = take(ap, indices_above, axis=axis) * weights_above
array([4.2, 4.4, 4.6, 4.8, nan, inf, inf, inf, inf])
>>> np.median([np.inf, 5, 4])
5.0
```

In this case, ``np.median`` is able to correctly return 5.0 as the median value, whereas np.percentile returns ``NaN`` for the 50th percentile.

### Numpy/Python version information:

```
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.16.0.dev0+45718fd 3.7.1 (default, Oct 23 2018, 14:07:42) 
[Clang 4.0.1 (tags/RELEASE_401/final)]
```
",2018-10-28 16:54:38,,np.percentile returns different median from np.median when inf is present,['unlabeled']
12278,open,ilitzroth,"<!-- Please describe the issue in detail here, and fill in the fields below -->
make info in the doc folder of numpy
....
numpy.texi:26442: warning: @menu in invalid context
numpy.texi:26497: @node seen before @end deffn
numpy.texi:27903: unmatched `@end deffn'
numpy.texi:28524: warning: @menu in invalid context
numpy.texi:28529: @node seen before @end deffn
numpy.texi:28661: unmatched `@end deffn'
numpy.texi:32180: warning: @menu in invalid context
numpy.texi:32191: @node seen before @end enumerate
numpy.texi:32412: unmatched `@end enumerate'
numpy.texi:33111: warning: @menu in invalid context
numpy.texi:33176: @node seen before @end deffn
numpy.texi:36525: unmatched `@end deffn'
numpy.texi:37031: warning: @menu in invalid context
numpy.texi:37036: @node seen before @end deffn
numpy.texi:37152: unmatched `@end deffn'
numpy.texi:37919: warning: @menu in invalid context
numpy.texi:37985: @node seen before @end deffn
numpy.texi:40488: unmatched `@end deffn'
numpy.texi:41352: warning: @menu in invalid context
numpy.texi:41412: @node seen before @end deffn
numpy.texi:44141: unmatched `@end deffn'
numpy.texi:44641: warning: @menu in invalid context
numpy.texi:44697: @node seen before @end deffn
numpy.texi:46119: unmatched `@end deffn'
numpy.texi:46403: warning: @menu in invalid context
numpy.texi:46408: @node seen before @end deffn
numpy.texi:46444: unmatched `@end deffn'
numpy.texi:46631: warning: @menu in invalid context
numpy.texi:46636: @node seen before @end deffn
numpy.texi:46705: unmatched `@end deffn'
numpy.texi:46896: warning: @menu in invalid context
numpy.texi:46917: @node seen before @end itemize
numpy.texi:47021: @item outside of table or list
numpy.texi:47040: @item outside of table or list
numpy.texi:48228: unmatched `@end itemize'
numpy.texi:84850: warning: @menu in invalid context
numpy.texi:84916: @node seen before @end deffn
numpy.texi:87419: unmatched `@end deffn'
numpy.texi:95122: warning: @menu in invalid context
numpy.texi:95127: @node seen before @end deffn
numpy.texi:95150: unmatched `@end deffn'
numpy.texi:99377: warning: @menu in invalid context
numpy.texi:99389: @node seen before @end deffn
numpy.texi:99574: unmatched `@end deffn'
numpy.texi:99647: warning: @menu in invalid context
numpy.texi:99653: @node seen before @end deffn
numpy.texi:99705: unmatched `@end deffn'
numpy.texi:99921: warning: @menu in invalid context
numpy.texi:99926: @node seen before @end deffn
numpy.texi:99968: unmatched `@end deffn'
numpy.texi:102753: warning: @menu in invalid context
numpy.texi:102760: @node seen before @end deffn
numpy.texi:102926: unmatched `@end deffn'
numpy.texi:122736: warning: @menu in invalid context
numpy.texi:122761: @node seen before @end deffn
numpy.texi:123751: unmatched `@end deffn'
numpy.texi:126293: warning: @menu in invalid context
numpy.texi:126319: @node seen before @end deffn
numpy.texi:127375: unmatched `@end deffn'
numpy.texi:130080: warning: @menu in invalid context
numpy.texi:130105: @node seen before @end deffn
numpy.texi:131095: unmatched `@end deffn'
numpy.texi:133801: warning: @menu in invalid context
numpy.texi:133826: @node seen before @end deffn
numpy.texi:134816: unmatched `@end deffn'
numpy.texi:137549: warning: @menu in invalid context
numpy.texi:137574: @node seen before @end deffn
numpy.texi:138564: unmatched `@end deffn'
numpy.texi:141301: warning: @menu in invalid context
numpy.texi:141326: @node seen before @end deffn
numpy.texi:142316: unmatched `@end deffn'
numpy.texi:145741: warning: @menu in invalid context
numpy.texi:145748: @node seen before @end deffn
numpy.texi:145846: unmatched `@end deffn'
numpy.texi:152896: warning: @menu in invalid context
numpy.texi:152948: @node seen before @end deffn
numpy.texi:157916: unmatched `@end deffn'
numpy.texi:166871: warning: @menu in invalid context
numpy.texi:166878: @node seen before @end deffn
numpy.texi:166997: unmatched `@end deffn'
numpy.texi:199639: warning: @subsubsection missing argument
Makefile:32: recipe for target 'numpy.info' failed
make[1]: *** [numpy.info] Error 1
make[1]: Leaving directory '/home/immanuel/workspace/Python/numpy-1.15.3/doc/build/texinfo'
Makefile:189: recipe for target 'info' failed
make: *** [info] Error 2

from numpy-1.15.3 tarbal",2018-10-27 18:54:04,,BUG: numpy doesn't build correct texinfo (and info target in doc fails),['00 - Bug']
12275,open,tylerjereddy,"There are many [uncovered / unused NumPy C API functions like `PyArray_Max`](https://codecov.io/gh/numpy/numpy/src/master/numpy/core/src/multiarray/calculation.c#L273) in `calculation.c`, which are effectively circumvented by `NPY_FORWARD_NDARRAY_METHOD(""_amax"")` style calls in `methods.c`.

The functions seem to be registered in `core/code_generators/numpy_api.py`, and presumably can't simply be removed.

If I'm correct in assuming they can't be removed--should we find a way to test them?",2018-10-26 21:02:58,,"MAINT, TST: Unused C API Functions","['05 - Testing', '03 - Maintenance']"
12258,open,shoyer,"This section of NEP 13 describes how `__array_ufunc__` interacts with Python's binary operations:
http://www.numpy.org/neps/nep-0013-ufunc-overrides.html#behavior-in-combination-with-python-s-binary-operations

But it doesn't fully describe what happens an object of unknown type that implements arithmetic (e.g., `decimal.Decimal`) but doesn't implement any NumPy special methods like `__array_ufunc__` or `__array_priority__` is encountered in arithmetic with a NumPy array:
1. Assuming the unknown type properly returns `NotImplemented` from binary operations for unrecognized arguments, the numpy ufunc will get called.
2. The NumPy ufunc (e.g., `np.add`) will coerce the unknown object into a scalar NumPy array with object dtype.
3. The scalar array gets broadcast to the same shape as the NumPy array.
4. The NumPy ufunc loop (for object dtype) calls the Python operator on each element of the two arrays.
5. The result is another object dtype array.

If this seems correct, I will make a minor addendum to add this clarification. This is particularly valuable as an example to other projects (e.g., see https://github.com/pandas-dev/pandas/pull/23293) of the right way to implement arithmetic like NumPy.",2018-10-24 16:09:43,,NEP 13 should mention how we handle unknown scalar types,['unlabeled']
12221,open,tylerjereddy,"[`np.char.translate()`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.core.defchararray.translate.html#numpy.core.defchararray.translate) doesn't seem to behave much like its documentation describes and it is [completely uncovered by unit tests](https://codecov.io/gh/numpy/numpy/src/master/numpy/core/defchararray.py#L1673).

The docstring probably needs examples too. There may be some danger that Python 2->3 changes have introduced issues here too given the lack of testing. Using 3.6 below.

For starters, let's try just deleting a character without a mapping table
```Python
arr = np.array(['a', 'b'], dtype=np.unicode_)
# this returns arr unchanged:
np.char.translate(arr, table='', deletechars=str('a')) # str is the defined arg type in docstring
# so does this:
np.char.translate(arr, table='', deletechars='a')
# and this:
np.char.translate(arr, table='', deletechars=u'a')
```

The docstring describes `table` argument as ""str of length 256"", but in practice it seems  like a dictionary is more appropriate and indeed I can feed in a dictionary:
```Python
table = str.maketrans('ab', 'yz')
np.char.translate(arr, table=table, deletechars='')
# produces, as expected: array(['y', 'z'], dtype='<U1')

# but the translation table is only supposed to be applied
# AFTER the deletechars are removed:
np.char.translate(arr, table=table, deletechars='a')
# still produces: array(['y', 'z'], dtype='<U1')

# so does: 
np.char.translate(arr, table=table, deletechars='abyz')
# and: 
np.char.translate(arr, table=table, deletechars=u'abyz')
```

",2018-10-19 18:38:20,,BUG: np.char.translate() args and behavior,['00 - Bug']
12208,open,eric-wieser,"Came across this interesting behavior today:
```python
class WithDtype(np.void):
    dtype = [('a', int)]

np.dtype(WithDtype)
# dtype((__main__.Foo, [('a', '<i4')]))
```

```python
class WithFields(np.void):
    _fields_ = [('a', int)]

np.dtype(WithFields)
# dtype((__main__.WithFields, [('a', '<i4')]))
```

It's not entirely clear to me if it's deliberate. #12207 is similar, and is clearly not.",2018-10-18 07:47:55,,Is subclassing np.void with explicit fields deliberate?,['unlabeled']
12207,open,eric-wieser,"```python
class WithType(np.void):
    _type_ = np.int32
np.dtype(WithType)
# stack overflow
```

```python
class WithDtype(np.void):
    dtype = np.int32
np.dtype(WithDtype)
# stack overflow
```",2018-10-18 07:46:26,,BUG: subclasses of np.void can cause a segfault,"['00 - Bug', 'component: numpy._core']"
12204,open,eric-wieser,"Some of these seems like implementation details, and many seem to have no documentation:

```python
# reordered for clarity
>>>{k: v for k, v in vars(np).items() if isinstance(v, int)}
{'ALLOW_THREADS': 1,
 'BUFSIZE': 8192,

 'ERR_IGNORE': 0,
 'ERR_WARN': 1,
 'ERR_CALL': 3,
 'ERR_RAISE': 2,
 'ERR_PRINT': 4,
 'ERR_LOG': 5

 'SHIFT_DIVIDEBYZERO': 0,
 'SHIFT_OVERFLOW': 3,
 'SHIFT_UNDERFLOW': 6,
 'SHIFT_INVALID': 9,

 'ERR_DEFAULT': 521,

 'FPE_DIVIDEBYZERO': 1,
 'FPE_OVERFLOW': 2,
 'FPE_UNDERFLOW': 4,
 'FPE_INVALID': 8,

 'CLIP': 0,
 'WRAP': 1,
 'RAISE': 2,


 'MAY_SHARE_BOUNDS': 0,
 'MAY_SHARE_EXACT': -1,

 'MAXDIMS': 32,
 'UFUNC_BUFSIZE_DEFAULT': 8192,
 'FLOATING_POINT_SUPPORT': 1,
 '__NUMPY_SETUP__': False,
 'little_endian': True,
 'tracemalloc_domain': 389047}
```
",2018-10-18 04:28:06,,Numpy exposes undocumented integer constants at the top level,['04 - Documentation']
12203,open,pimdh,"When calling `argsort` on an object, numpy checks if the object has a `argsort` attribute, if it does, this method is called. If not, it is casted to an array. This leads to unexpected behaviour if the object is not a numpy array but has an `argsort` method.

This is not true for all numpy functions. For example, `sort` always first calls `asarray`.

I noticed when using both numpy arrays and PyTorch tensors. PyTorch tensors have both a `argsort` and `sort` attribute.

If this can not be resolved in a sane way, then maybe this can be added as a note in the docs to the functions to which this applies?

### Reproducing code example:

```
import numpy as np
import torch

t = torch.rand(10)
np.sort(t)  # => array([...])
np.argsort(t) => tensor([...])
```


### Numpy/Python version information:
```
1.14.5 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 18:21:58) 
[GCC 7.2.0]
```
PyTorch version `0.4.1`.

",2018-10-18 03:05:26,,Unexpected behaviour when calling argsort on non-numpy object that has argsort attribute,['unlabeled']
12184,open,Felix-neko,"Hi all!

I'm modeling fixed-point calculations in my radar signal processing unit and need to use complex integer values with small bitrate (8, 10 and 12 bits).

Can you implement it some day? It will be really useful for DSP guys like me...",2018-10-16 16:21:08,,"Need extra dtypes: complex integer with different bit rate (8, 10 and 12 bits)","['01 - Enhancement', 'component: numpy.dtype']"
12183,open,brossignol,"Masked array do not work with numpy.log, and numpy.log10

```python
>>> A = np.ma.MaskedArray([1, 0, 2], mask =[False, True, False])
>>> np.log(A)
RuntimeWarning: divide by zero encountered in log
```

np.log return the right result, but it give a useless warning.
We do not have this warning with >>1 / A, for exemple.
",2018-10-16 13:57:03,,Log and MaskedArray,['unlabeled']
12142,open,toslunar,"Behaviors for comparison operators and for arithmetic operators are different.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
class C(object):
    def __array_ufunc__(self, *args, **kwargs): print(args); print(kwargs)

np.float32(1) + C()
np.float32(1) < C()
```

output:
```
(<ufunc 'add'>, '__call__', 1.0, <__main__.C object at 0x114d54048>)
{}
(<ufunc 'less'>, '__call__', array(1., dtype=float32), <__main__.C object at 0x1145d5d30>)
{}

```
<!-- Remove these sections for a feature request -->


### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```
1.16.0.dev0+2ed08ba 3.7.0 (default, Aug 16 2018, 11:04:41)
[Clang 10.0.0 (clang-1000.10.25.5)]
```

",2018-10-11 05:24:26,,Scalar compared with an obj implementing __array_ufunc__ becomes 0d-array,"['00 - Bug', 'component: numpy._core']"
12139,open,hmaarrfk,"Is there a particular reason why `nand` and `nor` are not provided as primitives?

I totally understand that they can created by chaining `logical_not` and `logical_and`, but often it is easier to ""think"" about things in terms of `nand` and `nor`.

It might also benefit from vectorization in modern processors.",2018-10-10 22:57:38,,Feature request: logical_nand logical_nor,['01 - Enhancement']
12113,open,anntzer,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

It would be a minor convenience if cumsum() could directly operate over multiple axes, similarly to sum() and many other functions.

```python
import numpy as np
np.cumsum(t, axis=(0, 1))
```
should be equivalent to
```
np.cumsum(np.cumsum(t, axis=0), axis=1)
```
(i.e. at the result's [i, j] entry, we get the sum of all entries of t with both coordinates <= (i, j)).

<!-- Remove these sections for a feature request -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```
1.15.2 3.7.0 (default, Jun 28 2018, 13:15:42) 
[GCC 7.2.0]
```
",2018-10-08 15:26:43,,cumsum over multiple axes,['01 - Enhancement']
12111,open,ydup,"<!-- Please describe the issue in detail here, and fill in the fields below -->
Hello,
I try to product a matrix(2D) with another one (3D), but find that the dimension of the result is transposed. 
```python
diagMat=np.diag([1,1,1])
mat3D = np.array([[[ 0,  1,  2, 10],
        [ 3,  4,  5, 0],
        [ 6,  7,  8, 1]],
       [[ 9, 10, 11, 2],
        [12, 13, 14,3],
        [15, 16, 17,0]]])
newMat = np.tensordot(mat3D, diagMat, axes=([1],[1]))
print mat3D
print newMat
print newMat.shape
# --> (2, 4, 3)
print mat3D.shape
# --> (2, 3, 4)
```
When the specific dimension changes, I don't want to see that the other dim transposes. By the way, because I am going to calculate high dimension matrix with tensordot, it can be tedious for me to re-transpose them. Such as manully transpose the dim-1 and dim-2 of the ```newMat```.
```python
import numpy as np
diagMat=np.diag([1,1,1])
mat3D = np.array([[[ 0,  1,  2, 10],
        [ 3,  4,  5, 0],
        [ 6,  7,  8, 1]],
       [[ 9, 10, 11, 2],
        [12, 13, 14,3],
        [15, 16, 17,0]]])
newMat = np.tensordot(mat3D, diagMat, axes=([1],[1]))
print mat3D
print newMat.swapaxes(1,2)
print newMat.swapaxes(1,2).shape
print mat3D.shape
```
Hope that this description is clear for you : ) 

",2018-10-08 08:57:20,,Dimension transposed after tensordot,['unlabeled']
12110,open,adivijaykumar,"<!-- Please describe the issue in detail here, and fill in the fields below -->
random.choice does not raise an error when `replace` keyword is set to anything other than `True` or `False`. In general, setting the  `replace` keyword  to any string or float does not give an error.

Is this how it is supposed to be, or is this a bug? If it isn't, I think it would at least be worthy including in the documentation for `replace`.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
print np.random.choice([1,2,3,4,5],3,replace=2), np.random.choice([1,2,3,4,5],3,replace='asfasdlol')
```

<!-- Remove these sections for a feature request -->

### Error message:
(Output)
[2 1 1] [3 3 1]
<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:
('1.15.2', '2.7.15 |Anaconda custom (64-bit)| (default, May  1 2018, 23:32:55) \n[GCC 7.2.0]')

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2018-10-08 08:33:37,,random.choice gives results even with spurious values for 'replace',['57 - Close?']
12107,open,AetherUnbound,"<!-- Please describe the issue in detail here, and fill in the fields below -->
The `_unsigned_subtract` function in histogram now raises a `TypeError` when given boolean values.


```
a = True, b = False
    def _unsigned_subtract(a, b):
        """"""
        Subtract two values where a >= b, and produce an unsigned result
    
        This is needed when finding the difference between the upper and lower
        bound of an int16 histogram
        """"""
        # coerce to a single type
        signed_to_unsigned = {
            np.byte: np.ubyte,
            np.short: np.ushort,
            np.intc: np.uintc,
            np.int_: np.uint,
            np.longlong: np.ulonglong
        }
        dt = np.result_type(a, b)
        try:
            dt = signed_to_unsigned[dt.type]
        except KeyError:
>           return np.subtract(a, b, dtype=dt)
E           TypeError: numpy boolean subtract, the `-` operator, is deprecated, use the bitwise_xor, the `^` operator, or the logical_xor function instead.
```

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->
### Reproducing code example:

```python
>>> import numpy as np
>>> np.histogram([True, True, False])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/aether/programs/anaconda3/envs/cpnp15/lib/python2.7/site-packages/numpy/lib/histograms.py"", line 732, in histogram
    norm = n_equal_bins / _unsigned_subtract(last_edge, first_edge)
  File ""/home/aether/programs/anaconda3/envs/cpnp15/lib/python2.7/site-packages/numpy/lib/histograms.py"", line 282, in _unsigned_subtract
    return np.subtract(a, b, dtype=dt)
TypeError: numpy boolean subtract, the `-` operator, is deprecated, use the bitwise_xor, the `^` operator, or the logical_xor function instead.
```

### Numpy/Python version information:
`('1.15.2', '2.7.14 |Anaconda, Inc.| (default, Dec  7 2017, 17:05:42) \n[GCC 7.2.0]')`
<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2018-10-08 01:10:14,,Histogram's _unsigned_subtract fails on boolean values,['component: numpy.lib']
12101,open,hmaarrfk,"In developing https://github.com/numpy/numpy/pull/11971#discussion_r223183338, @ahaldane noticed that the following occured:

```python
In [1]: import numpy as np

In [2]: a = np.concatenate([np.ones((1, 4, 4), order='F')]*2)

In [3]: a.strides
Out[3]: (128, 8, 32)

In [4]: a.shape
Out[4]: (2, 4, 4)

In [5]: a.flags
Out[5]: 
  C_CONTIGUOUS : False
  F_CONTIGUOUS : False
  OWNDATA : True
  WRITEABLE : True
  ALIGNED : True
  WRITEBACKIFCOPY : False
  UPDATEIFCOPY : False
```

The resulting array being neither `F` or `C` contiguous.

In the discussion linked to above, we were drafting a few different ways that one might automagically infer the order from the provided arrays (in the context of  `block`).

I wanted to ask if this was the desired behaviour in concatenate, or if this should be considered a bug?",2018-10-06 18:52:19,,Memory order after `concatenate` not always preserved,['unlabeled']
12098,open,vinu2003,"assert_array_equal(x,y) gives Assertion Error: when both x and y are same.

### Reproducing code example:

```
def test_calculate_subtotals_Validate2DArray():
    counts_input = np.array([[1,2,3],[4,5,6]])
    result = noise.calculate_subtotals(counts_input)
    assert isinstance(result, tuple)
    assert all(isinstance(item, np.ndarray) for item in result)
    print(f'result value : {result}')
    assert_array_equal(result, (np.array([5, 7, 9]), np.array([ 6, 15])))
```

<!-- Remove these sections for a feature request -->

### Error message:

```
platform linux -- Python 3.6.5, pytest-3.8.1, py-1.6.0, pluggy-0.7.1
rootdir: /home/vinu/numpy_Virenv, inifile:collected 15 items

test_noise.py 
Setup the test suite
**.............Fresult value : (array([5, 7, 9]), array([ 6, 15]))**

test_noise.py:107 (test_calculate_subtotals_Validate2DArray)
def test_calculate_subtotals_Validate2DArray():
        counts_input = np.array([[1,2,3],[4,5,6]])
        result = noise.calculate_subtotals(counts_input)
        assert isinstance(result, tuple)
        assert all(isinstance(item, np.ndarray) for item in result)
        print(f'result value : {result}')
>       assert_array_equal(result, (np.array([5, 7, 9]), np.array([ 6, 15])))
E       AssertionError: 
E       Arrays are not equal
E       
E       (mismatch 100.0%)
E        x: array([array([5, 7, 9]), array([ 6, 15])], dtype=object)
E        y: array([array([5, 7, 9]), array([ 6, 15])], dtype=object)
```

### Numpy/Python version information:

numpy version 1.15.2

---

**Summary (2019-12-18):**

```
a = np.array([np.array([0, 1]), np.array(1)], dtype=object)
b = a.copy()
np.testing.assert_array_equal(a, b)
```

fails as per [Matti's comment below](https://github.com/numpy/numpy/issues/12098#issuecomment-552618008).

The problem is that the array comparison, `a[0] == b[1]`, returns `array([ True, True])`, which we don't know how to coerce into a bool.  Hence, `assert_array_equal`, which is essentially `all(a[i] == b[i] for i in range(len(a)))`, fails.

Comparison of two object arrays will give an error in the future.",2018-10-06 03:22:52,,"assert_array_equal(x,y) fails for some object arrays","['06 - Regression', 'component: numpy.testing']"
12076,open,akiross,"`start:stop:stepj` inside `np.r_` will produce a `np.linspace(start, stop, step, endpoint=True)`.

I would like to propose to use endpoint=False when stepj is negative. In other words:

`np.r_[start:stop:stepj]` produces `np.linspace(start, stop, step, endpoint=True)` when `stepj > 0`
`np.r_[start:stop:stepj]` produces `np.linspace(start, stop, step, endpoint=False)` when `stepj < 0`

I think it is intuitive in the sense that the minus sign represent the absence of something, the endpoint in this case. Also, it helps in using the convenience of `r_` when endpoint is not wanted.

The main issue with this proposal is that it breaks previous code if it is using negative complex numbers as stride: currently, the [absolute value of step is used](https://github.com/numpy/numpy/blob/master/numpy/lib/index_tricks.py#L340), changing this behaviour will break code if someone relies on this equivalence. I suspect there is not much code relying on this, but I did not investigate on this matter.",2018-10-04 08:26:26,,endpoint=False when using complex stride in r_,['unlabeled']
12075,open,aparamon,"According to [Python Glossary](https://docs.python.org/3/glossary.html#term-bytes-like-object), all of bytes, bytearray, array.array and memoryview support bytes-like interface. However, [:] assignment is supported from all but bytes objects:

```py3
import array
import numpy as np

b = bytearray(b'\x00'*8)
arr = np.asarray(b)

arr[:] = bytes(b'\x89HDF\r\n\x1a\n')  # fail
arr[:] = bytearray(b'\x89HDF\r\n\x1a\n')  # success
arr[:] = array.array('B', b'\x89HDF\r\n\x1a\n')  # success
arr[:] = memoryview(b'\x89HDF\r\n\x1a\n')  # success
```
````pytb
arr[:] = bytes(b'\x89HDF\r\n\x1a\n')
UnicodeDecodeError: 'ascii' codec can't decode byte 0x89 in position 0: ordinal not in range(128)
````
It is proposed that `numpy.array` supports assignment from bytes objects.

---

Update by seberg (~NumPy 1.23) the error is now:
```
ValueError: invalid literal for int() with base 10: b'\x89HDF\r\n\x1a\n'
```",2018-10-03 17:01:07,,Assignment supports only some bytes-like objects,['unlabeled']
12069,open,lejar,"When using floor division on an array of integers, the resulting dtype appears to be incompatible with itself (see code example). Using an in-place floor division seems to not have the same problem.

I could only reproduce this on windows (system information below).

### Reproducing code example:

```python
import numpy


foo = numpy.array([ 9, 10, 11, 12, 13, 14, 15, 16, 17])
foo //= 2
print(type(foo), foo.dtype, repr(foo), foo.dtype.char)
# <class 'numpy.ndarray'> int32 array([4, 5, 5, 6, 6, 7, 7, 8, 8]) l
print(type(foo[0]), isinstance(foo[0], numpy.int32))
# <class 'numpy.int32'> True

foo = numpy.array([ 9, 10, 11, 12, 13, 14, 15, 16, 17])
foo = foo // 2
print(type(foo), foo.dtype, repr(foo), foo.dtype.char)
# <class 'numpy.ndarray'> int32 array([4, 5, 5, 6, 6, 7, 7, 8, 8], dtype=int32) i
print(type(foo[0]), isinstance(foo[0], numpy.int32))
# <class 'numpy.int32'> False
```

### Error message:
See the output of the example script.

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

    >>> import sys, numpy; print(numpy.__version__, sys.version)
    1.15.2 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:06:47) [MSC v.1914 32 bit (Intel)]

I used a clean windows 10 (enterprise edition, v1709, OS build 16299.19) vm (https://developer.microsoft.com/en-us/windows/downloads/virtual-machines), installed the latest python 3 from python.org, then pip installed numpy.",2018-10-02 12:43:34,,BUG: Floor division returns messed up dtype on windows (python 3),['unlabeled']
12066,open,tylerjereddy,"With macOS testing restored to our CI via Azure DevOps (see #12051), it may be useful to bookmark the compromises that were necessary in the test suite for it to pass -- hopefully we can remove the skip decorator placed on the test noted below. See the linked PR above for some discussion. Also, note that mac os builds currently avoid use of `ACCELERATE` because of apparent LAPACK issues.

Skipped on macos CI at the moment -- just 1 test:
- `test_multiline` in `numpy.f2py.tests.test_semicolon_split.py`

",2018-10-01 18:51:35,,TST: macOS CI test skip,['05 - Testing']
12049,open,keithbriggs,"numpy.polynomial.polynomial returns an numpy.ndarray even when the input is some other type.   It would be useful if the input type was preserved. 

import numpy.polynomial.polynomial as poly
from interval import interval
p=poly.Polynomial((1,2,))
x=interval[1.0,1.0001]
print(type(x))
print(p(x)) # returns correct interval, but wrong type
print(type(p(x))) # expect <class 'interval.interval'>
print(type(p(x)[0]))
print(type(p(x)[0][0]))

gives:
<class 'interval.interval'>
[[3.     3.0002]]
<class 'numpy.ndarray'>
<class 'numpy.ndarray'>
<class 'numpy.float64'>
",2018-09-28 15:23:16,,numpy.polynomial.polynomial not compatible with interval module,['component: numpy.polynomial']
12046,open,anntzer,"### Reproducing code example:

As of numpy 1.15.2 + py 3.7:
```
In [1]: np.zeros(10, 10)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-1-46503f7ef50c> in <module>
----> 1 np.zeros(10, 10)

TypeError: data type not understood
In [2]: np.zeros(10, np.int64(10))
Out[2]: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

In [3]: np.zeros(10, np.float64(10))
Out[3]: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
```
I don't think this behavior is documented, and it can clearly hide some bugs, e.g. when one typoes `np.zeros(*shape)` instead of `np.zeros(shape)` and `shape` has two elements that are both numpy scalars (as opposed to builtin scalars).  I would thus suggest to instead error out with the same error message as with builtin scalars (""data type not understood"").

I guess `np.ones`, `np.empty`, etc. likely exhibit the same behavior, though I didn't check.

### Numpy/Python version information:

1.15.2 3.7.0 (default, Jun 28 2018, 13:15:42) 
[GCC 7.2.0]

",2018-09-27 15:17:58,,np.zeros's dtype argument can also be an instance of the dtype,['unlabeled']
12039,open,pezcore,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Background

Numpy's `who()` function is a great, very useful function that prints out a table of useful information about the `ndarray`s in a dictionary, But is is not documented on the reference docs. New users should definitely be made aware of its existence and use cases in the online numpy reference documentation.

Also this function can be reimplemented to be much more useful by providing a lot more useful information about the `ndarray`s in tabular format. I propose the output printed by `who()` should be something more like this

```
data_pointer   type    flags   name       shape           nbytes
================================================================
0X5569FC22A280 <i8     CFOWA   a          (5,)            40
0X5569FC22A2A0 <i8     CFWA    b          (1,)            8
0X5569FC6112C0 <M8[h]  COWA    c          (4, 6)          192
0X5569FC31AF80 |b1     COWA    testvar    (10, 34)        340
```

This tells the user a lot more valuable information about the relationships and properties of the variables I have an implementation i wrote in mind that gets this table from a `dict` containing `ndarray`s

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->


",2018-09-26 18:22:00,,"MAINT, ENH: Upgrade and document numpy.who()","['01 - Enhancement', '03 - Maintenance']"
12029,open,chuangy1,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np

l = [(359, 218641918), (360, 346814771), (361, 221783064), (362, 228103721), (363, 226773168), (364, 340432095),
     (365, 892706639), (366, 350615631), (367, 402126509), (368, 315962305), (369, 397399137), (370, 419634877)
     ]
l1, l2 = zip(*l)
l1 = [i + 0.1 for i in l1]
min_l1, max_l1 = min(l1) // 5 * 5, max(l1) // 5 * 5 + 5
bins = int((max_l1 - min_l1) / 5 + 0.1)
hist, edges = np.histogram(l1, weights=l2, range=(min_l1, max_l1), bins=bins)
print(hist, edges)
```
### The actual output:
```python
[  218641918  1363906819 -2147483648   419634877] [355. 360. 365. 370. 375.]
```
The hist value can't be negative. It might be interger overflow.

### Numpy/Python version information:
1.14.5 3.6.6 (v3.6.6:4cf1f54eb7, Jun 27 2018, 03:37:03) [MSC v.1900 64 bit (AMD64)]
<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2018-09-24 23:49:42,,"In histogram, bars whose count is larger than 2**31 sometimes become negative",['unlabeled']
11999,open,tumido,"Using `np.isin()` to match boolean values results in a strange, unexpected, behavior.

### Reproducing code example:

```python
>>> import numpy as np
>>> arr = np.array([True, np.bool_(True), ""True"", None])
>>> np.isin(arr, [True])
array([ True,  True, False, False])  # Correct
>>> np.isin(arr, [np.bool_(True)])
array([ True,  True, False, False])  # Correct
>>> np.isin(arr, ['True'])
array([False, False,  True, False])  # Correct
>>> np.isin(arr, [True, 'True'])
array([False, False,  True, False])  # Wrong
>>> np.isin(arr, [np.bool_(True), True, 'True'])
array([False, False,  True, False])  # Wrong
>>> np.isin(arr, [True, 'True', None])
array([ True,  True,  True,  True])  # Correct
>>> np.isin(arr, ['True', None])
array([False, False,  True,  True])  # Correct
```

For `False` values it behaves the same (bad) as for `True`

### Numpy/Python version information:

```python
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.15.1 3.6.6 (default, Sep 13 2018, 16:29:18)
[GCC 8.2.1 20180831]
```
",2018-09-20 14:10:44,,isin() is not matching Boolean values correctly,['00 - Bug']
11998,open,eric-wieser,"Our scalar type classes are such that:
* `issubclass(np.int_, int) and issubclass(np.int_, np.integer)` (py2 only)
* `issubclass(np.float_, float) and issubclass(np.float_, np.floating)`
* `issubclass(np.complex_, float) and issubclass(np.complex_, np.complexfloating)`
* ...

We do this in the C api by manually copying across some `tp_*` slots, setting `tp_bases` to a tuple, and crossing our fingers. However, the [CPython API docs](https://docs.python.org/3/c-api/typeobj.html) say (emphasis mine):

> #### `PyTypeObject* PyTypeObject.tp_base`
> An optional pointer to a base type from which type properties are inherited. At this level, only single inheritance is supported; **multiple inheritances require dynamically creating a type object** by calling the metatype.

> #### `PyObject* PyTypeObject.tp_bases`
> Tuple of base types.
>
> This is set for types created by a class statement. **It should be NULL for statically defined type**s.

We do not do this, so it seems we're relying on internal behavior we're not supposed to.
",2018-09-20 08:18:01,,"multiple inheritance of static C api types is not supported by CPython, yet we do it anyway","['03 - Maintenance', 'component: numpy.dtype']"
11990,open,anntzer,"It's not clear whether nanpercentile supports masked arrays (if it doesn't, perhaps it should error out cleanly), but right now it will sometimes return nan for a masked array where all nans are actually masked out.

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
print(np.nanquantile(np.ma.array([0, 1, np.nan, 2], mask=[False, False, True, False]), .5))
```

<!-- Remove these sections for a feature request -->

### Error message:

Prints `nan`.

Compare with `np.nanquantile([0, 1, np.nan, 2], .5)` and `np.nanquantile(np.ma.array([0, 1, np.nan, 2]), .5)` which both (correctly) print `1`.

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

```
1.15.1 3.7.0 (default, Jun 28 2018, 13:15:42) 
[GCC 7.2.0]
```",2018-09-19 14:25:59,,nanpercentile can return nan for a masked array where all nans are masked out,['unlabeled']
11960,open,mattip,"PR #11175 changed the `PyUFuncObject`, signature parsing, and tests. The PR was large, and there are some dangling requests for changes. These will be finished as separate PRs.

- [x] Refactor test_can_ignore_signature into separate tests, as per [this comment](https://github.com/numpy/numpy/pull/11175#discussion_r208066845)
- [ ] Consider making `PyUFuncObject` an opaque struct with a clear API for accessing fields, much like `PyArrayObject`. Also related to issue #11803 since cython exposes the object.",2018-09-16 05:19:03,,MAINT: tracking issue for post gufunc-signeature-enhancement PR,['unlabeled']
11940,open,rsokl,"The ordering of axes fed to `tensordot` can have a massive (order of magnitude) impact on its efficiency, based on the memory layout of the array(s) being summed:

```python
>>> import numpy as np
>>> x = np.random.rand(100, 100, 100)
>>> %%timeit
... np.tensordot(x, x, axes=((0, 1, 2), (0, 1, 2)))  
151 µs ± 6.9 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
>>> %%timeit
... np.tensordot(x, x, axes=((1, 2, 0), (1, 2, 0))) 
7.9 ms ± 143 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```
Moving `x`'s axis leads to a swap in timing:
```python
>>>  xt = np.moveaxis(x, -1, 0)
>>> %%timeit
... np.tensordot(xt, xt, axes=((0, 1, 2), (0, 1, 2)))  
10.8 ms ± 213 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
>>> %%timeit
... np.tensordot(xt, xt, axes=((1, 2, 0), (1, 2, 0))) 
146 µs ± 4.47 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
```

[As suggested](https://github.com/numpy/numpy/pull/11928) by @eric-wieser, `tensordot` would benefit from axis-ordering based on memory contiguity to help guard against these massive slow downs.",2018-09-13 00:51:39,,A need for contiguity-based axis-order optimization in tensordot,['unlabeled']
11938,open,adamjermyn,"The alternate input format for einsum (i.e. `einsum(op0, sublist0, op1, sublist1, ..., [sublistout])`) produces an error if the number of tensors grows too large. The error also changes with the number of operands.

### Reproducing code example:

```python
import numpy as np

n = 26
arrs = list(np.zeros((2,2)) for _ in range(n))
inds = list([i,i+1] for i in range(len(arrs)))
args = list(x for y in zip(*(arrs,inds)) for x in y)
args.append([0,n])
np.einsum(*args)
```

This produces the following error:

```console
Traceback (most recent call last):
  File ""test.py"", line 8, in <module>
    np.einsum(*args)
  File ""/usr/local/lib/python3.6/site-packages/numpy-1.14.2-py3.6-macosx-10.13-x86_64.egg/numpy/core/einsumfunc.py"", line 1069, in einsum
    return c_einsum(*operands, **kwargs)
ValueError: invalid subscript '{' in einstein sum subscripts string, subscripts must be letters
```

With `n=40' the error message changes:

```console
Traceback (most recent call last):
  File ""test.py"", line 8, in <module>
    np.einsum(*args)
  File ""/usr/local/lib/python3.6/site-packages/numpy-1.14.2-py3.6-macosx-10.13-x86_64.egg/numpy/core/einsumfunc.py"", line 1069, in einsum
    return c_einsum(*operands, **kwargs)
ValueError: too many operands
```

This appears to be related to the use of characters for indices internally, even when indices are specified as integers in the input. The use of characters internally does not seem critical to the algorithm so this may be resolvable by changing that.

### Numpy/Python version information:

```console
1.14.2 3.6.4 (default, Mar  9 2018, 23:15:03)
[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)]
```
",2018-09-12 21:11:28,,Einsum symbol limitations,['unlabeled']
11925,open,jcmuel,"I embedded Python into a C program. At certain points in time, the C program resets the interpreter by calling Py_Finalize + Py_Initialize. When the evaluated code imports NumPy before and after the reset, then the second import fails with a segmentation fault.

### Reproducing code example:

At first the example C program initializes Python, imports NumPy, and then finalizes Python.
Then it reinitializes Python and imports NumPy again. When importing NumPy
for the second time, the import fails with a segmentation fault.

Tested with Debian 9, Python 3.7.0 valgrind friendly debug build, NumPy debug build of latest
version from github (1.16.0.dev0+e796449). The bug reproduces on Mac (tested with Python 3.5.1,
NumPy 1.11.0) and Windows (Tested with Python 3.7.0 from Python.org, NumPy 1.14.5).

```C
/******************************************************************************
    Test: Initialize NumPy before and after Python reinitialization
******************************************************************************/

#include ""Python.h""

int main(int arc, char *argv[]) {
    wchar_t *program = Py_DecodeLocale(argv[0], NULL);
    if (program == NULL) {
        fprintf(stderr, ""Fatal error: cannot decode argv[0]\n"");
        exit(1);
    }

    Py_SetProgramName(program);

    // Initialize Python, import NumPy, finalize Python. First time.
    Py_Initialize();
    PyRun_SimpleString(""import sys; print('Python', sys.version)"");
    PyRun_SimpleString(""import numpy; print('Numpy', numpy.__version__)"");
    Py_Finalize();

    // Initialize Python, import NumPy, finalize Python. Second time.
    Py_Initialize();
    PyRun_SimpleString(""import numpy;""); // <-- SEGMENTATION FAULT.
    Py_Finalize();

    PyMem_RawFree(program);
    return 0;
}
```

### Error message:

Segmentation fault on evaluation of ""import numpy"" after reinitialization of Python. Output of valgrind:

```
==4346== Memcheck, a memory error detector
==4346== Copyright (C) 2002-2015, and GNU GPL'd, by Julian Seward et al.
==4346== Using Valgrind-3.12.0.SVN and LibVEX; rerun with -h for copyright info
==4346== Command: ./main
==4346==
Python 3.7.0 (default, Aug 27 2018, 19:39:47)
[GCC 6.3.0 20170516]
Numpy 1.16.0.dev0+e796449
==4346== Invalid read of size 8
==4346==    at 0x6CEFDC4: PyArray_Item_INCREF (refcount.c:35)
==4346==    by 0x6CF37A8: PyArray_FromScalar (scalarapi.c:335)
==4346==    by 0x6CF5674: gentype_nonzero_number (scalartypes.c.src:349)
==4346==    by 0x4F12AA8: PyObject_IsTrue (object.c:1384)
==4346==    by 0x4FC1995: _PyEval_EvalFrameDefault (ceval.c:2654)
==4346==    by 0x4FB7EE1: PyEval_EvalFrameEx (ceval.c:547)
==4346==    by 0x4EC7C0C: function_code_fastcall (call.c:283)
==4346==    by 0x4EC87CF: _PyFunction_FastCallKeywords (call.c:408)
==4346==    by 0x4FC42FD: call_function (ceval.c:4586)
==4346==    by 0x4FC42FD: _PyEval_EvalFrameDefault (ceval.c:3117)
==4346==    by 0x4FB7EE1: PyEval_EvalFrameEx (ceval.c:547)
==4346==    by 0x4EC7C0C: function_code_fastcall (call.c:283)
==4346==    by 0x4EC846B: _PyFunction_FastCallDict (call.c:322)
==4346==  Address 0x1a is not stack'd, malloc'd or (recently) free'd
==4346==
==4346==
==4346== Process terminating with default action of signal 11 (SIGSEGV)
==4346==  Access not within mapped region at address 0x1A
==4346==    at 0x6CEFDC4: PyArray_Item_INCREF (refcount.c:35)
==4346==    by 0x6CF37A8: PyArray_FromScalar (scalarapi.c:335)
==4346==    by 0x6CF5674: gentype_nonzero_number (scalartypes.c.src:349)
==4346==    by 0x4F12AA8: PyObject_IsTrue (object.c:1384)
==4346==    by 0x4FC1995: _PyEval_EvalFrameDefault (ceval.c:2654)
==4346==    by 0x4FB7EE1: PyEval_EvalFrameEx (ceval.c:547)
==4346==    by 0x4EC7C0C: function_code_fastcall (call.c:283)
==4346==    by 0x4EC87CF: _PyFunction_FastCallKeywords (call.c:408)
==4346==    by 0x4FC42FD: call_function (ceval.c:4586)
==4346==    by 0x4FC42FD: _PyEval_EvalFrameDefault (ceval.c:3117)
==4346==    by 0x4FB7EE1: PyEval_EvalFrameEx (ceval.c:547)
==4346==    by 0x4EC7C0C: function_code_fastcall (call.c:283)
==4346==    by 0x4EC846B: _PyFunction_FastCallDict (call.c:322)
==4346==  If you believe this happened as a result of a stack
==4346==  overflow in your program's main thread (unlikely but
==4346==  possible), you can try to increase the size of the
==4346==  main thread stack using the --main-stacksize= flag.
==4346==  The main thread stack size used in this run was 8388608.
==4346==
==4346== HEAP SUMMARY:
==4346==     in use at exit: 6,190,252 bytes in 34,347 blocks
==4346==   total heap usage: 215,244 allocs, 180,897 frees, 45,494,245 bytes allocated
==4346==
==4346== LEAK SUMMARY:
==4346==    definitely lost: 144 bytes in 2 blocks
==4346==    indirectly lost: 0 bytes in 0 blocks
==4346==      possibly lost: 6,055,624 bytes in 31,510 blocks
==4346==    still reachable: 134,484 bytes in 2,835 blocks
==4346==         suppressed: 0 bytes in 0 blocks
==4346== Rerun with --leak-check=full to see details of leaked memory
==4346==
==4346== For counts of detected and suppressed errors, rerun with: -v
==4346== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 0 from 0)
Killed
```

### Numpy/Python version information:

Python 3.7.0 (default, Aug 27 2018, 19:39:47) [GCC 6.3.0 20170516]
Numpy 1.16.0.dev0+e796449
",2018-09-10 13:48:32,,segfault when importing numpy after reinitializing Python,['Embedded']
11923,open,eric-wieser,"The values of `np.sctypes` are not lists of all the unique types, but list of all the types with unique sizes. This might be fine for normal use, but this means it's not suitable for use in testing.

All on windows 64-bit:

```python
>>> {np.byte, np.short, np.intc, np.int_, np.longlong} - set(np.sctype['int'])
{numpy.int32}  # platform-dependent - this is np.intc for me

>>> {np.ubyte, np.ushort, np.uintc, np.uint_, np.ulonglong} - set(np.sctype['uint'])
{numpy.uint32}  # platform-dependent - this is np.uintc for me

>>> {np.half, np.single, np.double, np.longdouble} - set(np.sctype['float'])
{numpy.float64}  # platform-dependent - this is np.longdouble for me

>>> {np.csingle, np.cdouble, np.clongdouble} - set(np.sctype['complex'])
{numpy.complex128}  # platform-dependent - this is np.clongdouble for me
```

I don't think we should try fix this until after #10151 is merged, else users who are affected by this will be hit by a confusing message.",2018-09-10 00:39:12,,BUG: sctypes is missing some types,"['00 - Bug', 'component: numpy._core']"
11918,open,mreineck,"In the code snippet below there is no fundamental reason why the first FFT call should require more time than the second one, but it is actually slower by a factor of several hundred.

```
import numpy as np

a=np.random.random((1000,1000))
%timeit np.fft.fftn(a,axes=(1,0),s=(1,1000))
%timeit np.fft.fftn(a[:,0:1],axes=(1,0),s=(1,1000))
```

This happens because the cropping of the input field on axis 0 only takes place after all 1000 1D FFTs have been carried out over this axis. But only one of these 1000 is actually needed...

I think this can be fixed by cropping the input of FFT routines to the values specified by s, whenever they are smaller than the field dimensions. Zero-padding should still be done as late as possible (which is currently the case).
",2018-09-09 12:24:39,,Multi-D cropped FFTs are potentially wasteful,['unlabeled']
11887,open,ysig,"When running experiments on squared symmetric numpy matrices I had a constant error of svd not converging although I couldn't found why. My experiment had constrained the amount of threads on the OPENBLASS.

By searching various things concerning debugging, I found a correlation between the number of threads and the convergence, which seems really strange at first sight.
The following example has two case where going from n to n+1 leads to convergence.
If there is a _timeout_ constraint inside the openblass is there a way to control it?

### Reproducing code example:
1.
Uncompress [array.zip](https://github.com/numpy/numpy/files/2353617/array.zip) on your local dir.

```
$  env OPENBLAS_NUM_THREADS=1 python
>>> import numpy as np
>>> A = np.load('array.npy')
>>> out = np.linalg.svd(A)
numpy.linalg.linalg.LinAlgError: SVD did not converge
```
when

```
$  env OPENBLAS_NUM_THREADS=2 python
>>> import numpy as np
>>> A = np.load('array.npy')
>>> out = np.linalg.svd(A)
```
runs normally.

2.
Uncompress [array_2.zip](https://github.com/numpy/numpy/files/2353737/array_2.zip) on your local dir.

```
$  env OPENBLAS_NUM_THREADS=2 python  
>>> import numpy as np
>>> A = np.load('array_2.npy')
>>> out = np.linalg.svd(A)
numpy.linalg.linalg.LinAlgError: SVD did not converge
```
when

```
$  env OPENBLAS_NUM_THREADS=3 python
>>> import numpy as np
>>> A = np.load('array_2.npy')
>>> out = np.linalg.svd(A)
```
runs normally.

### Error message:
> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""/home/dascim/.local/lib/python2.7/site-packages/numpy/linalg/linalg.py"", line 1550, in svd
>     u, s, vh = gufunc(a, signature=signature, extobj=extobj)
>   File ""/home/dascim/.local/lib/python2.7/site-packages/numpy/linalg/linalg.py"", line 98, in _raise_linalgerror_svd_nonconvergence
>     raise LinAlgError(""SVD did not converge"")
> numpy.linalg.linalg.LinAlgError: SVD did not converge

### Numpy/Python version information:
```
$python -c 'import sys, numpy; print(numpy.__version__, sys.version)'
1.15.0 2.7.14 (default, Jul 26 2018, 19:07:40) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-23)]
```",2018-09-05 16:41:00,,SVD does not converge depending on OPENBLAS_NUM_THREADS environment variable,['unlabeled']
11881,open,miccoli,"`numpy.round` returns wrong value when decimals are negative and argument is `np.int64`

### Reproducing code example:

```python
>>> import numpy as np
>>> np.round(2**63-1, -3)
-9223372036854775808
>>> round(2**63-1, -3)
9223372036854776000
```

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->
```
>>> import sys, numpy; print(numpy.__version__, sys.version)
1.15.1 3.6.5 (default, Apr  5 2018, 23:36:52) 
[GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.1)]
```",2018-09-04 21:42:47,,"numpy.round(i, decimals=d) wrong for np.int64 and d<0",['00 - Bug']
11879,open,striajan,"I use Numpy version 1.15. If I call `np.histogram(data, bins='auto')`, I get `MemoryError`. The error is probably caused by trying to allocate too large [linspace](https://github.com/numpy/numpy/blob/058851c5cfc98f50f11237b1c13d77cfd1f40475/numpy/lib/histograms.py#L375).

### Reproducing code example:

The error can be reproduced on [this data](https://github.com/numpy/numpy/files/2348354/data.txt). Its range is approximately (-0.00283, 0.00209), with many values being close to zero. The following code raises the error:
```python
import numpy
data = numpy.loadtxt('data.txt')
numpy.histogram(data, bins='auto')
```

The problem is in computation of the bin edges. The following code raises the error:
```python
import numpy.lib.histograms
numpy.lib.histograms._get_bin_edges(data, bins='auto', range=None, weights=None)
```

Even more specifically, the problem is caused by the method `bins='fd'`. The other method `bins='sturges'` used for automated computation of bin edges works well.

### Error message:

```
Traceback (most recent call last):
  File "".../.pycharm_helpers/pydev/_pydevd_bundle/pydevd_exec2.py"", line 3, in Exec
    exec(exp, global_vars, local_vars)
  File ""<input>"", line 1, in <module>
  File "".../numpy/lib/histograms.py"", line 676, in histogram
    bin_edges, uniform_bins = _get_bin_edges(a, bins, range, weights)
  File "".../numpy/lib/histograms.py"", line 351, in _get_bin_edges
    endpoint=True, dtype=bin_type)
  File "".../numpy/core/function_base.py"", line 115, in linspace
    y = _nx.arange(0, num, dtype=dt)
MemoryError
```

### Numpy/Python version information:

```
1.15.0 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) 
[GCC 7.2.0]
```

",2018-09-04 11:16:24,,Calling histogram with auto bins raises MemoryError,['component: numpy.lib']
11869,open,jannesklee,"I have a binary file that I want to read in with a python routine. In order to do so a dtype object is created, which describes how the data looks like. The dtype object that should be created is a dictionary of the form {'field1': ..., 'field2': ..., ...}. The obj then is a tuple of (data-type, offset) - (from  [numpy documentation](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.dtypes.html)). The error now occurs if offset exceeds the range of a C int during creation of the dtype. I also wrote a similar post regarding this issue on [StackOverflow](https://stackoverflow.com/questions/52148265/reading-in-binary-file-with-dtype-causes-valueerror). 

### Reproducing code example:

```python
import numpy as np

dict_tmp = dict()
offset = 2281832888
dict_tmp['/timedisc/pressure'] = ('(4096, 4096)>f8', offset)
dtype = np.dtype(dict_tmp)
```

If I reduce the offset below the range of a 32bit integer the error vanishes of course. I already tried to cast the offset value to an int64 or uint32 by hand, but this was also not working. As far as I can see the dtype is part of multiarray in numpy and at this point I am a bit lost.


<!-- Remove these sections for a feature request -->

### Error message:

> ValueError                                Traceback (most recent call last)
> <ipython-input-124-2662ea9449ff> in <module>()
>         2 offset = 2281832888
>         3 dict_tmp['/timedisc/pressure'] = ('(4096, 4096)>f8', offset)
> ----> 4 dtype = np.dtype(dict_tmp)
>
> /home/jklee/src/anaconda3/lib/python3.5/site-packages/numpy/core/_internal.py in _usefields(adict, align)
>        77                   ""formats"": formats,
>        78                   ""offsets"": offsets,
> ---> 79                   ""titles"": titles}, align)
>        80 
>        81 
>
> ValueError: integer won't fit into a C int

### Numpy/Python version information:

1.13.1 3.5.4 |Anaconda custom (64-bit)| (default, Aug 14 2017, 13:26:58) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]


",2018-09-03 11:57:25,,ENH: memmap should allow a `strides` kwarg (was `int` limit in `dtype` size),['60 - Major release']
11831,open,QuLogic,"### Reproducing code example:

Both `TestF77ReturnCharacter.test_all` and `TestF90ReturnCharacter.test_all` fail when building on Fedora s390x: [build](https://koji.fedoraproject.org/koji/taskinfo?taskID=29364137)

### Error message:

```
_______________________ TestF77ReturnCharacter.test_all ________________________
self = <numpy.f2py.tests.test_return_character.TestF77ReturnCharacter object at 0x3ff91bd0cc0>
    @pytest.mark.slow
    def test_all(self):
        for name in ""t0,t1,t5,s0,s1,s5,ss"".split("",""):
>           self.check_function(getattr(self.module, name))
name       = 't0'
self       = <numpy.f2py.tests.test_return_character.TestF77ReturnCharacter object at 0x3ff91bd0cc0>
../../../BUILDROOT/numpy-1.15.1-2.fc30.s390x/usr/lib64/python3.7/site-packages/numpy/f2py/tests/test_return_character.py:87: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
self = <numpy.f2py.tests.test_return_character.TestF77ReturnCharacter object at 0x3ff91bd0cc0>
t = <fortran t0>
    def check_function(self, t):
        tname = t.__doc__.split()[0]
        if tname in ['t0', 't1', 's0', 's1']:
            assert_(t(23) == b'2')
            r = t('ab')
            assert_(r == b'a', repr(r))
            r = t(array('ab'))
>           assert_(r == b'a', repr(r))
E           AssertionError: b' '
r          = b' '
self       = <numpy.f2py.tests.test_return_character.TestF77ReturnCharacter object at 0x3ff91bd0cc0>
t          = <fortran t0>
tname      = 't0'
../../../BUILDROOT/numpy-1.15.1-2.fc30.s390x/usr/lib64/python3.7/site-packages/numpy/f2py/tests/test_return_character.py:19: AssertionError
_______________________ TestF90ReturnCharacter.test_all ________________________
self = <numpy.f2py.tests.test_return_character.TestF90ReturnCharacter object at 0x3ff91bd6518>
    @pytest.mark.slow
    def test_all(self):
        for name in ""t0,t1,t5,ts,s0,s1,s5,ss"".split("",""):
>           self.check_function(getattr(self.module.f90_return_char, name))
name       = 't0'
self       = <numpy.f2py.tests.test_return_character.TestF90ReturnCharacter object at 0x3ff91bd6518>
../../../BUILDROOT/numpy-1.15.1-2.fc30.s390x/usr/lib64/python3.7/site-packages/numpy/f2py/tests/test_return_character.py:146: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
self = <numpy.f2py.tests.test_return_character.TestF90ReturnCharacter object at 0x3ff91bd6518>
t = <fortran object>
    def check_function(self, t):
        tname = t.__doc__.split()[0]
        if tname in ['t0', 't1', 's0', 's1']:
            assert_(t(23) == b'2')
            r = t('ab')
            assert_(r == b'a', repr(r))
            r = t(array('ab'))
>           assert_(r == b'a', repr(r))
E           AssertionError: b' '
r          = b' '
self       = <numpy.f2py.tests.test_return_character.TestF90ReturnCharacter object at 0x3ff91bd6518>
t          = <fortran object>
tname      = 't0'
../../../BUILDROOT/numpy-1.15.1-2.fc30.s390x/usr/lib64/python3.7/site-packages/numpy/f2py/tests/test_return_character.py:19: AssertionError
```

### Numpy/Python version information:

NumPy 1.15.1, Python 3.7

### All variations:

Locally, I modified the tests to be parametrized and ran those:
```python
    @pytest.mark.parametrize('name', ""t0,t1,t5,s0,s1,s5,ss"".split("",""))
    def test_all(self, name):
        self.check_function(getattr(self.module, name))
```
This enabled seeing all results:
```
TestF77ReturnCharacter::test_all[t0] <- FAILED
TestF77ReturnCharacter::test_all[t1] <- FAILED
TestF77ReturnCharacter::test_all[t5] <- PASSED
TestF77ReturnCharacter::test_all[s0] <- FAILED
TestF77ReturnCharacter::test_all[s1] <- FAILED
TestF77ReturnCharacter::test_all[s5] <- PASSED
TestF77ReturnCharacter::test_all[ss] <- PASSED
TestF90ReturnCharacter::test_all[t0] <- FAILED
TestF90ReturnCharacter::test_all[t1] <- FAILED
TestF90ReturnCharacter::test_all[t5] <- PASSED
TestF90ReturnCharacter::test_all[ts] <- PASSED
TestF90ReturnCharacter::test_all[s0] <- FAILED
TestF90ReturnCharacter::test_all[s1] <- FAILED
TestF90ReturnCharacter::test_all[s5] <- PASSED
TestF90ReturnCharacter::test_all[ss] <- PASSED
```",2018-08-29 10:56:27,,f2py.test_return_character fails on s390x,"['00 - Bug', '05 - Testing', 'component: numpy.f2py']"
11825,open,lokmantsui,"einsum fails to optimize. For example for

```python
import numpy as np
n1=6
n2=4
a=np.random.rand(n1,n1,n2)
b=np.random.rand(n2,n2,n2)
print(np.einsum_path('gfl,egh,efj,mjk,cdn,bck,bdi,oih',a,a,a,b,a,a,a,b,optimize=True)[1])
```
output:
```
  Complete contraction:  gfl,egh,efj,mjk,cdn,bck,bdi,oih->lmno
         Naive scaling:  14
     Optimized scaling:  14
      Naive FLOP count:  2.446e+10
  Optimized FLOP count:  2.446e+10
   Theoretical speedup:  1.000
  Largest intermediate:  2.560e+02 elements
--------------------------------------------------------------------------
scaling                  current                                remaining
--------------------------------------------------------------------------
  14    oih,bdi,bck,cdn,mjk,efj,egh,gfl->lmno                               lmno->lmno
```

It did not attempt to do any optimization (even with the flag optimize=True, 'greedy' or 'optimal') and gives an evaluation that scales as n^14, where n is the dimension of a tensor leg. It should be trivial to optimize the above problem, for instance, by the following path (calculated by hand)

scaling------ current-----------------------remaining
  5  ------  gfl,efj -> egjl ---------------------- oih,bdi,bck,cdn,mjk,egh,egjl->lmno
  5  ------  egjl,egh -> hjl --------------------- oih,bdi,bck,cdn,mjk,hjl->lmno
  5   ------ cdn,bck -> bdkn------------------- oih,bdi,bdkn,mjk,hjl->lmno
  5 ------   bdkn,bdi -> ikn---------------------oih,ikn,mjk,hjl->lmno
  5 ------   mjk,hjl -> hklm--------------------- oih,ikn,hklm->lmno
  5  ------  oih,ikn -> hkno --------------------- hkno,hklm->lmno
  6 ------   hkno,hklm->lmno ------------------- lmno->lmno

that scales as n^6. Even doing any single contraction will reduce the scaling cost down from n^14.

Also when one set n1=4, n2=50, and ran the rest of the code, it is being correctly optimized and ran fast. In this case einsum_path with optimize=True gives optimization which scales as n^8

```
n1=4
n2=50
a=np.random.rand(n1,n1,n2)
b=np.random.rand(n2,n2,n2)
print(np.einsum_path('gfl,egh,efj,mjk,cdn,bck,bdi,oih',a,a,a,b,a,a,a,b,optimize=True)[1])
```
output:
```
  Complete contraction:  gfl,egh,efj,mjk,cdn,bck,bdi,oih->lmno
         Naive scaling:  14
     Optimized scaling:  8
      Naive FLOP count:  1.280e+18
  Optimized FLOP count:  2.950e+08
   Theoretical speedup:  4338394779.222
  Largest intermediate:  6.250e+06 elements
--------------------------------------------------------------------------
scaling                  current                                remaining
--------------------------------------------------------------------------
   5               oih,egh->egio       gfl,efj,mjk,cdn,bck,bdi,egio->lmno
   5               mjk,efj->efkm          gfl,cdn,bck,bdi,egio,efkm->lmno
   6             egio,bdi->bdego             gfl,cdn,bck,efkm,bdego->lmno
   6             efkm,bck->bcefm                gfl,cdn,bdego,bcefm->lmno
   8         bcefm,bdego->cdfgmo                     gfl,cdn,cdfgmo->lmno
   7           cdfgmo,gfl->cdlmo                          cdn,cdlmo->lmno
   6             cdlmo,cdn->lmno                               lmno->lmno

```


For some reason for the first case n1=6, n2=4, the optimization algorithm for einsum did not ran at all.

### Numpy/Python version information:

1.15.0 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]

",2018-08-28 02:07:49,,"einsum fails to optimize, leads to very expensive evaluation","['component: documentation', '54 - Needs decision']"
11770,open,mamikonyan,"When using the string shorthand to specify structured dtypes, it seems to me that the final comma should be significant in cases of a single field. That is, instead of the current result,
```
import numpy as np
>>> np.dtype('i,')
dtype('int32')
```
it seems more logical to produce the equivalent of the longer-winded,
```
>>> np.dtype([('f0','i')])
dtype([('f0', '<i4')])
```
This would also parallel the Python tuple behavior, i.e.,
```
>>> x = 1,
>>> type(x)
<class 'tuple'>
```
I could not find any details in the documentation that formalized parsing semantics to this degree.",2018-08-17 14:57:09,,parsing final comma in dtype string shorthand,['01 - Enhancement']
11752,open,eric-wieser,"### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
dt = np.dtype([('b', np.datetime64)])
arr = np.zeros(1, dt)
repr(arr)
```

<!-- Remove these sections for a feature request -->

### Error message:

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->
```
  File ""C:\Program Files\Python 3.5\lib\site-packages\IPython\core\formatters.py"", line 693, in __call__
    printer.pretty(obj)
  File ""C:\Program Files\Python 3.5\lib\site-packages\IPython\lib\pretty.py"", line 380, in pretty
    return _default_pprint(obj, self, cycle)
  File ""C:\Program Files\Python 3.5\lib\site-packages\IPython\lib\pretty.py"", line 495, in _default_pprint
    _repr_pprint(obj, p, cycle)
  File ""C:\Program Files\Python 3.5\lib\site-packages\IPython\lib\pretty.py"", line 693, in _repr_pprint
    output = repr(obj)
  File ""C:\Users\wiese\Repos\numeric-python\numpy\build\testenv\Lib\site-packages\numpy\core\arrayprint.py"", line 1433, in array_repr
    ', ', prefix, suffix=suffix)
  File ""C:\Users\wiese\Repos\numeric-python\numpy\build\testenv\Lib\site-packages\numpy\core\arrayprint.py"", line 670, in array2string
    return _array2string(a, options, separator, prefix)
  File ""C:\Users\wiese\Repos\numeric-python\numpy\build\testenv\Lib\site-packages\numpy\core\arrayprint.py"", line 460, in wrapper
    return f(self, *args, **kwargs)
  File ""C:\Users\wiese\Repos\numeric-python\numpy\build\testenv\Lib\site-packages\numpy\core\arrayprint.py"", line 486, in _array2string
    format_function = _get_format_function(data, **options)
  File ""C:\Users\wiese\Repos\numeric-python\numpy\build\testenv\Lib\site-packages\numpy\core\arrayprint.py"", line 433, in _get_format_function
    return StructuredVoidFormat.from_data(data, **options)
  File ""C:\Users\wiese\Repos\numeric-python\numpy\build\testenv\Lib\site-packages\numpy\core\arrayprint.py"", line 1270, in from_data
    format_function = _get_format_function(data[field_name], **options)
  File ""C:\Users\wiese\Repos\numeric-python\numpy\build\testenv\Lib\site-packages\numpy\core\arrayprint.py"", line 428, in _get_format_function
    return formatdict['datetime']()
  File ""C:\Users\wiese\Repos\numeric-python\numpy\build\testenv\Lib\site-packages\numpy\core\arrayprint.py"", line 365, in <lambda>
    'datetime': lambda: DatetimeFormat(data, legacy=legacy),
  File ""C:\Users\wiese\Repos\numeric-python\numpy\build\testenv\Lib\site-packages\numpy\core\arrayprint.py"", line 1222, in __init__
    super(DatetimeFormat, self).__init__(x)
  File ""C:\Users\wiese\Repos\numeric-python\numpy\build\testenv\Lib\site-packages\numpy\core\arrayprint.py"", line 1183, in __init__
    max_str_len = max(len(self._format_non_nat(np.max(non_nat))),
  File ""C:\Users\wiese\Repos\numeric-python\numpy\build\testenv\Lib\site-packages\numpy\core\arrayprint.py"", line 1233, in _format_non_nat
    casting=self.casting)
ValueError: Cannot convert a NumPy datetime value other than NaT with generic units
```

### Numpy/Python version information:

4a0c307b1af162c4e3363a8b4c90fa1a138adebf
",2018-08-16 05:22:18,,BUG: repr of generic datetime crashes,"['00 - Bug', 'component: numpy.datetime64']"
11736,open,dopplershift,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
a = np.ma.array([10])
a[a < 0].sum()
```
Prints `0.0`. But
```python
import numpy as np
a = np.ma.array([10], mask=False)
a[a < 0].sum()
```
Prints `masked`. Huh?

### Numpy/Python version information:

```
1.15.0 3.6.6 | packaged by conda-forge | (default, Jul 26 2018, 09:55:02) [GCC 4.2.1 Compatible Apple LLVM 6.1.0 (clang-602.0.53)]
```

",2018-08-13 21:01:52,,Odd behavior with masked array sum(),"['00 - Bug', 'component: numpy.ma']"
11723,open,topper-123,"Quite often arrays are sorted, either ascending or descending, and we want to find where the startpoint of a certain value is at. This is very easy for ascending arrays using searchsorted, but requires a bit more understanding for descending arrays.

It seems relatively straightward to it make it easy for descending arrays also by adding a ascending parameter to searchsorted.

Would this be outside of scope for numpy, or is such an ascending parameter relevant to include in numpy's searchsorted?

A demonstration:

```python
>>> import numpy as np

>>> def searchsorted(a, v, side='left', sorter=None, ascending=True):
>>>     if not ascending:
>>>         a = a[::-1]
>>>         side = {'left': 'right', 'right': 'left'}[side]
>>>         sorter = sorter[::-1] if sorter is not None else sorter
>>>         return len(a) - np.searchsorted(a, v, side=side, sorter=sorter)
>>>     return np.searchsorted(a, v, side=side, sorter=sorter)

>>> a = np.array([2, 1, 1])
>>> searchsorted(a, 1, ascending=False)
1
>>> searchsorted(a, 2, side='right', ascending=False)
1
``
",2018-08-12 21:40:47,,ENH: Let searchsorted work on arrays with descending values,"['01 - Enhancement', '23 - Wish List']"
11701,open,realead,"Surprisingly, numpy seems to upcast objects to floats/ints and so on in an intermediate step when `np.full`, `[...]`, `[:]` (and maybe others) are used:

### Reproducing code example:

Example 1:

```python
class A(float): # can be downcasted to a float
    pass
a=np.full(2,A(),dtype=np.object)
type(a[0])  # float, expected __main__.A
```

Example 2:

```python
a=np.empty(2, dtype=np.object)
obj = 1000   # must be > 255 
a[...] = obj
a[0] is obj  #False, expected True
```

Example 3:

```python
a=np.empty(2, dtype=np.object)
a[:] = np.nan
a[0] is a[1]  #False, expected True
```


Example 4:

```python
a=np.full(2,'aa', dtype=np.object)
a[0] is a[1]  #False, expected True
```


I don't think it is the right behavior, at least I didn't find anything about this behavior in the documentation.

On the other hand we can see default behavior with classes, which cannot be upcasted:

```python
class B: 
    pass
a=np.full(2, B(),dtype=np.object)
a[0] is a[1]  # True, as expected
```


### Numpy/Python version information:

1.13.1 3.6.2 |Anaconda custom (64-bit)| (default, Sep 30 2017, 18:42:57) 
[GCC 7.2.0]

",2018-08-09 20:03:45,,Numpy seems to upcast objects in a intermediate step when using np.full and others,['00 - Bug']
11690,open,charris,"<!-- Please describe the issue in detail here, and fill in the fields below -->
Float16 is not supported in linalg. Note that various integer types are upcast, I think the same should be done with float16.

### Reproducing code example:
np.linalg.inv(np.eye(2, dtype=np.float16)

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
np.linalg.inv(np.eye(2, dtype=np.float16)
```

<!-- Remove these sections for a feature request -->

### Error message:
TypeError: array type float16 is unsupported in linalg

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:
'1.16.0.dev0+8eed36a'

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

",2018-08-08 19:16:51,,"BUG, ENH: Float16 is not supported in linalg","['00 - Bug', '01 - Enhancement', 'component: numpy.linalg']"
11686,open,simenkva,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
import time

print(np.version.version)

t = np.random.random((40,40,4,4))
u = np.random.random((40,40,40,40))

# This code runs fast
t1 = time.time()
for k in range(100):
    tmp = np.einsum(""abcd,cdef->abef"",u,t,optimize=True)
t2 = time.time()
print(t2-t1)

# Changed one dummy index: Runs slowly
t1 = time.time()
for k in range(100):
    tmp = np.einsum(""abcr,cref->abef"",u,t,optimize=True)
t2 = time.time()
print(t2-t1)

# Turns off path optimization: Runs slowly
t1 = time.time()
for k in range(100):
    tmp = np.einsum(""abcd,cdef->abef"",u,t)
t2 = time.time()
print(t2-t1)
```

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

```
1.15.0 3.6.1 |Anaconda custom (x86_64)| (default, May 11 2017, 13:04:09) 
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]
```
",2018-08-08 09:36:06,,"numpy.einsum does not use BLAS for some equivalent index strings, and BLAS is not used if optimize==False",['unlabeled']
11683,open,sdkempf,"<!-- Please describe the issue in detail here, and fill in the fields below -->
I took this example code from https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.core.records.fromfile.html and removed the shape argument.  I read somewhere about floating point indexing being removed and this might be related.


### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np
from tempfile import TemporaryFile
a = np.empty(10,dtype='f8,i4,a5')
a[5] = (0.5,10,'abcde')

fd=TemporaryFile()
a = a.newbyteorder('<')
a.tofile(fd)

fd.seek(0)
#r=np.core.records.fromfile(fd, formats='f8,i4,a5', shape=10,byteorder='<')
r=np.core.records.fromfile(fd, formats='f8,i4,a5', byteorder='<')
print(r[5])
r.shape
```

<!-- Remove these sections for a feature request -->

### Error message:
```
Traceback (most recent call last):
  File ""test.py"", line 12, in <module>
    r=np.core.records.fromfile(fd, formats='f8,i4,a5', byteorder='<')
  File ""/WAIS/syst/ext/linux/lib/python2.7/site-packages/numpy/core/records.py"", line 796, in fromfile
    _array = recarray(shape, descr)
  File ""/WAIS/syst/ext/linux/lib/python2.7/site-packages/numpy/core/records.py"", line 425, in __new__
    self = ndarray.__new__(subtype, shape, (record, descr), order=order)
TypeError: 'numpy.float64' object cannot be interpreted as an index
```

<!-- If you are reporting a segfault please include a GDB traceback, which you
can generate by following
https://github.com/numpy/numpy/blob/master/doc/source/dev/development_environment.rst#debugging -->

<!-- Full error message, if any (starting from line Traceback: ...) -->

### Numpy/Python version information:

<!-- Output from 'import sys, numpy; print(numpy.__version__, sys.version)' -->

('1.14.2', '2.7.13 (default, Mar 26 2018, 11:09:19) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-18)]')

### Possible Fix:

Honestly, I don't know if this is a good idea, but it fixed the problem for me.
```patch
*** records.py.orig     2018-08-07 13:38:30.000000000 -0500
--- records.py  2018-08-07 13:38:13.000000000 -0500
***************
*** 782,788 ****
      shapesize = shapeprod * itemsize
      if shapesize < 0:
          shape = list(shape)
!         shape[shape.index(-1)] = size / -shapesize
          shape = tuple(shape)
          shapeprod = sb.array(shape).prod()

--- 782,788 ----
      shapesize = shapeprod * itemsize
      if shapesize < 0:
          shape = list(shape)
!         shape[shape.index(-1)] = int(size / -shapesize)
          shape = tuple(shape)
          shapeprod = sb.array(shape).prod()
```",2018-08-07 18:49:59,,core.records.fromfile fails without shape=,['component: numpy._core']
11680,open,eric-wieser,"Raised in the comments of #11669.

The offending code is:

https://github.com/numpy/numpy/blob/da5eaf97281453083252c22b2b94aded846a936b/numpy/core/src/multiarray/dtype_transfer.c#L1436-L1489

`data->aip` goes on to be passed to `PyArray_ArrFuncs::getitem` in `VOID_to_OBJECT` , even though it is not attached to the actual array that is being copied from.

I'd assumed the contract of `PyArray_ArrFuncs::getitem` was that the `aip` argument was always the container of the passed `ip` argument, but it seems we violate that.",2018-08-07 04:46:39,,BUG? get_nbo_cast_transfer_function creates garbage arrays which are passed into the ->getitem function,['unlabeled']
11677,open,toslunar,"### Reproducing code example:

```python
import numpy as np
np.take(np.empty((0, 0)), [1], axis=0, mode='wrap')
```
does not return (probably causing an infinite-loop).

The message
```
>>> np.take(np.empty((0, 0)), [1], axis=0)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/tos/GitHub/numpy/numpy/core/fromnumeric.py"", line 181, in take
    return _wrapfunc(a, 'take', indices, axis=axis, out=out, mode=mode)
  File ""/Users/tos/GitHub/numpy/numpy/core/fromnumeric.py"", line 51, in _wrapfunc
    return getattr(obj, method)(*args, **kwds)
IndexError: index 1 is out of bounds for size 0
```
is different from the message
```
>>> np.take(np.empty((0,)), [1], axis=0)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/tos/GitHub/numpy/numpy/core/fromnumeric.py"", line 181, in take
    return _wrapfunc(a, 'take', indices, axis=axis, out=out, mode=mode)
  File ""/Users/tos/GitHub/numpy/numpy/core/fromnumeric.py"", line 51, in _wrapfunc
    return getattr(obj, method)(*args, **kwds)
IndexError: cannot do a non-empty take from an empty axes.
```

To produce the latter message, the size of the output array is checked.  Instead the size of the indices could be checked.

### Numpy/Python version information:

1.16.0.dev0+9d0225b 3.7.0 (default, Jun 29 2018, 20:13:13)
[Clang 9.1.0 (clang-902.0.39.2)]
",2018-08-06 10:00:51,,`numpy.take` with non-empty `indices` from an empty axis should fail,['00 - Bug']
11674,open,alexmojaki,"<!-- Please describe the issue in detail here, and fill in the fields below -->

### Reproducing code example:

<!-- A short code example that reproduces the problem/missing feature. It should be
self-contained, i.e., possible to run as-is via 'python myproblem.py' -->

```python
import numpy as np

arr = np.ma.array([1, 2, 3], mask=[0, 1, 0])
print(arr)   # [1 -- 3]
print(np.array_repr(arr))   # error
```

<!-- Remove these sections for a feature request -->

### Error message:

```
Traceback (most recent call last):
  File ""/Users/alexhall/Library/Preferences/PyCharm2018.1/scratches/scratch_468.py"", line 6, in <module>
    print(np.array_repr(arr))
  File ""/Users/alexhall/.pyenv/versions/3.5.1/lib/python3.5/site-packages/numpy/core/arrayprint.py"", line 1431, in array_repr
    ', ', prefix, suffix=suffix)
  File ""/Users/alexhall/.pyenv/versions/3.5.1/lib/python3.5/site-packages/numpy/core/arrayprint.py"", line 668, in array2string
    return _array2string(a, options, separator, prefix)
  File ""/Users/alexhall/.pyenv/versions/3.5.1/lib/python3.5/site-packages/numpy/core/arrayprint.py"", line 460, in wrapper
    return f(self, *args, **kwargs)
  File ""/Users/alexhall/.pyenv/versions/3.5.1/lib/python3.5/site-packages/numpy/core/arrayprint.py"", line 495, in _array2string
    summary_insert, options['legacy'])
  File ""/Users/alexhall/.pyenv/versions/3.5.1/lib/python3.5/site-packages/numpy/core/arrayprint.py"", line 796, in _formatArray
    curr_width=line_width)
  File ""/Users/alexhall/.pyenv/versions/3.5.1/lib/python3.5/site-packages/numpy/core/arrayprint.py"", line 750, in recurser
    word = recurser(index + (-i,), next_hanging_indent, next_width)
  File ""/Users/alexhall/.pyenv/versions/3.5.1/lib/python3.5/site-packages/numpy/core/arrayprint.py"", line 704, in recurser
    return format_function(a[index])
  File ""/Users/alexhall/.pyenv/versions/3.5.1/lib/python3.5/site-packages/numpy/core/arrayprint.py"", line 1119, in __call__
    return self.format % x
  File ""/Users/alexhall/.pyenv/versions/3.5.1/lib/python3.5/site-packages/numpy/ma/core.py"", line 4298, in __int__
    raise MaskError('Cannot convert masked element to a Python int.')
numpy.ma.core.MaskError: Cannot convert masked element to a Python int.
```

### Numpy/Python version information:

```
1.15.0
3.5.1 (default, May 31 2016, 20:50:46) 
[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.72)]
```

Numpy 1.14.0 has the same error.",2018-08-05 18:34:45,,Error in array_repr on masked array: numpy.ma.core.MaskError: Cannot convert masked element to a Python int.,['component: numpy.ma']
11663,open,flying-sheep,"With type annotations becoming more and more ubiqitous, it would be nice to have a class that signifies something to be a structured array. As a structured array is AFAIK defined by `a.dtypes.names is not None`, its definition would be a [virtual class](https://docs.python.org/3/reference/datamodel.html#customizing-instance-and-subclass-checks) aka Abstract Base Class:

```py
import numpy as np

class StructuredArrayMeta(type):
    def __instancecheck__(cls, obj):
        return isinstance(obj, np.ndarray) and obj.dtype.names is not None

class StructuredArray(meta=StructuredArrayMeta):
    def __new__(cls, ...):  # interface close to recarray()
        ...
```",2018-08-02 09:18:17,,An Abstract Base Class for structured arrays,"['23 - Wish List', 'component: numpy.dtype']"
11659,open,seberg,"Einsum has an optimization in place that will allow it to just transpose the array instead of copying it when this is possible. This appears to be an optimization, since the transpose operation comes for free.

There two things to this:
 1. it seems unexpected that einsum can return a view sometimes and sometimes now. However,
    I have to admit that I don't think that this can be input dependent, so it is likely not a huge deal.
 2. An actual small bug in that `np.einsum('ij->ij', a, order='F')` will never create an output array
    and thus forget to actually enforce the flag.

Personally, it seems like an unnecessary optimization (if we wish to have a no-copy transpose you can use transpose), but I am not sure if it is worth to push for it. The other is a small, mostly harmless bug that would be nice to get fixed.
@eric-wieser suggested that we could go forward with a `copy` kwarg to einsum.",2018-08-01 17:12:48,,"Discussion,BUG: Einsum sometimes decides to not copy data",['15 - Discussion']
11658,open,eric-wieser,"Brought up by [this comment](https://github.com/numpy/numpy/pull/11504#issuecomment-402802549), it turns out that `einsum` actually is another name for moveaxis, and is 10 times faster
```
a = np.zeros((10, 20, 30))

%timeit np.einsum('abc->cba', a)
2.24 µs ± 375 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)

%timeit np.einsum(a, [0, 1, 2], [2, 1, 0])
2.65 µs ± 132 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)

%timeit np.moveaxis(a, [0, 1, 2], [2, 1, 0])
22.1 µs ± 3.47 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)
```
Perhaps we should take advantage of this, especally since it would counteract the performance cost of #9475

",2018-08-01 17:03:40,,Consider linking einsum and move axis,"['01 - Enhancement', 'component: numpy.einsum']"
11645,open,eduble,"When building a view of a masked structured array, using a dtype with offsets, a ValueError exception is fired.

Note: With an array of type np.array instead of np.ma.MaskedArray, all runs fine.

### Reproducing code example:

```python
import numpy as np
x = np.array([('Rex', 9, 81.0), ('Fido', 3, 27.0)], dtype=[('name', 'U10'), ('age', 'i4'), ('weight', 'f4')])
age_dtype, age_offset = x.dtype.fields['age']
newdt = np.dtype({ 'names': ['age'], 'formats': [age_dtype], 'offsets': [age_offset], 'itemsize': x.dtype.itemsize })
# this works for now; it gives a view of x with column 'age' only
x.view(newdt)
# now converting x to a masked array
x = x.view(np.ma.MaskedArray)
# Oops!
x.view(newdt)
```

### Error message:
```
Traceback (most recent call last):
  File ""/home/etienne/sakura/.venv/lib/python3.5/site-packages/numpy/ma/core.py"", line 3130, in view
    if issubclass(dtype, ndarray):
TypeError: issubclass() arg 1 must be a class

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/etienne/sakura/.venv/lib/python3.5/site-packages/numpy/ma/core.py"", line 3136, in view
    output = ndarray.view(self, dtype)
  File ""/home/etienne/sakura/.venv/lib/python3.5/site-packages/numpy/ma/core.py"", line 3357, in dtype
    self._mask.shape = self.shape
ValueError: cannot reshape array of size 6 into shape (2,)
```

### Numpy/Python version information:

```
>>> print(sys.version)
3.5.2 (default, Nov 23 2017, 16:37:01) 
[GCC 5.4.0 20160609]
>>> print(np.__version__)
1.15.0
>>> 
```",2018-07-31 13:44:28,,Bug when building a view of a masked array with field offsets,"['00 - Bug', 'component: numpy.ma']"
11622,open,jakirkham,"It would be nice to have something in NumPy, which would give the largest value by magnitude. IOW `abs(a).max()`. However it would be better if it didn't result in creating a second copy of the data as `abs(a)` would do.",2018-07-26 14:53:32,,ENH: Create the ability for fused operations (fused ufuncs or `map_reduce`) style,['unlabeled']
11581,open,bpbrown,"Documentation in numpy/site.cfg.example states that users home directory will be searched for global ~/.numpy-site.cfg.  Installs of numpy from source or from pip install do not respect settings in ~/.numpy-site.cfg.

",2018-07-17 13:35:10,,BLD: Howto install numpy using pip and the Intel compiler.,['component: documentation']
11569,open,oleksandr-pavlyk,"`numpy.count_nonzero`, `numpy.nonzero` and `numpy.flatnonzero` are expensively used within `scipy.sparse` ([compressed.py:616](https://github.com/scipy/scipy/blob/master/scipy/sparse/compressed.py#L616), [compressed.py:1180](https://github.com/scipy/scipy/blob/master/scipy/sparse/compressed.py#L1180), [base.py:218](https://github.com/scipy/scipy/blob/master/scipy/sparse/base.py#L218), [coo.py:539](https://github.com/scipy/scipy/blob/master/scipy/sparse/coo.py#L539), etc.)

Through this usage, as well through direct use ([measureimageintensity.py#251](https://github.com/CellProfiler/CellProfiler/blob/master/cellprofiler/modules/measureimageintensity.py#L251), [measureimageskeleton.py#252](https://github.com/CellProfiler/CellProfiler/blob/master/cellprofiler/modules/measureimageskeleton.py#L252)) the `numpy.nonzero` was seen high in the profile of AdvancedSegmentation benchmark of [CellProfiler](http://cellprofiler.org/examples/). 

`PyArray_NonZero` function extracts `nonzero` function from the dtype descriptor ([item_selection.c:#2185](https://github.com/numpy/numpy/blob/master/numpy/core/src/multiarray/item_selection.c#L2185)), which has the signature `(dataptr, self)`, and returns a boolean indicating if the element is zero or not. 
This function is called within a loop, iterating over all elements of the array.

The function carries reference `self` to the `PyArrayObject` to detect is the array might be not-behaved (data layout is difference from the native to machine), and call `swap` function before making the test. 

The check if the array is behaved can be done once outside the loop, and a much more efficient loop, which is amendable to vectorization, can be written,

There is already a special case written for boolean arrays ([item_selection.c#2229](https://github.com/numpy/numpy/blob/master/numpy/core/src/multiarray/item_selection.c#L2229)).

This is a suggestion to add another special case for well-behaved arrays (like native integers and native floating types).

An indicatation that at least 2X performance can be gained:

```
In [1]: import numpy as np

In [2]: x = np.random.randint(-5,5,size=10**6)

In [3]: %timeit np.nonzero(x)
5.89 ms ± 44.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

# this is faster due to boole optimization despite the need to create intermediate array 
In [4]: %timeit np.nonzero(x != 0)
2.59 ms ± 220 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```",2018-07-14 19:30:49,,Improve performance of numpy.nonzero,['01 - Enhancement']
11548,open,kielpins,"When operating on arrays with singleton dimensions, numpy.einsum does not always enforce shape matching over summed dimensions. Instead it can return a surprising result. Minimal example:
```
import numpy as np
a = np.arange(4).reshape(4,1)
b = 2 * np.arange(3).reshape(1,3)
c = np.einsum('ik,jk->ij', a, b)
```
Here I would expect a `ValueError`: we are summing over the `k` index, but `k` has length 1 for `a` and length 3 for `b`. However, `einsum` instead returns a value for `c` that is equal to `a * b.sum()`.
```
>>> print(c)
[[ 0]
 [ 6]
 [12]
 [18]]
>>> c.shape
(4, 1)
```
I suppose this could be related to broadcasting behavior, but the documentation is fairly clear that broadcasting should only occur when the `subscripts` argument contains ellipses.

For arrays with no singleton dimensions, `einsum` throws a `ValueError` as expected. This example:
```
import numpy as np
a = np.arange(8).reshape(4,2)
b = 2 * np.arange(6).reshape(2,3)
c = np.einsum('ik,jk->ij', a, b)
```
produces
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/dave/.conda/envs/dk_env/lib/python3.6/site-packages/numpy/core/einsumfunc.py"", line 1069, in einsum
    return c_einsum(*operands, **kwargs)
ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (4,2)->(4,newaxis,2) (2,3)->(2,3) 
```

I am running numpy 1.14.5 under python 3.6.5 on Mac OSX.",2018-07-10 23:34:25,,BUG: numpy.einsum fails to match dimensions on arrays with singleton dimensions,['component: numpy.einsum']
11547,open,G778,"The documentation reads:

> If True, always print floating point numbers using fixed point notation, in which case numbers equal to zero in the current precision will print as zero. If False, then scientific notation is used when absolute value of the smallest number is < 1e-4 or the ratio of the maximum absolute value to the minimum is > 1e3. The default is False.

But in actual use, when setting suppression for arrays with large enough numbers in them, it just stops working for some reason. This should work on all arrays with any size numbers, period, as promised.

I don't see any downsides to this, other than icky formatting if numbers scroll over to the next line, but this is ""protecting people from themselves"" and not helpful, especially when there's no such thing as a ""no, like, really though, actually suppress..."" flag I can set at the same time, or similar.",2018-07-10 19:51:54,,"printoptions ""suppression"" of scientific method is not doing anything for large numbers?","['04 - Documentation', 'component: numpy._core']"
11541,open,attack68,"    a = np.ones(shape=(5,1))
    b = np.array([1,2])
    np.einsum('ij,j->', a, b, optimize=False)
    # 15.0
    np.einsum('ij,j->', a, b, optimize=True)
    # ValueError(""Size of label '%s' for operand %d does not match previous terms."", 'j', 1)

Personally I prefer the ValueError here since it is more explicit, and in my own code the use of `False` for small calculations produced unseen errors because I was expecting the behaviour of `True`, which I more often use for larger calcs.

Additionally the same calculation is reproducible with the `True` version by providing the better defined input:

    np.einsum('ij,k->', a, b, optimize=True)
    # 15.0

(note the ValueError string could do with a little fine tuning also)
Forgot to say this is on version 1.14.0

 Drawing @dgasmith attention if he has input..",2018-07-10 08:19:53,,BUG: einsum: inconsistent behaviour between c_einsum and python on wrong dimensions,['component: numpy.einsum']
11521,open,mattip,"A tracking issue of features with `DeprecationWarning`. I may have missed some, this is accurate for the last date it was updated: 2019-08-24

In python code

version | date | feature deprecated | notes
-----|----------|----------|-----------
|  | 2008-11-16 | `distutils.command.config.get_output` | Cannot be easily removed gh-12880 (2019-07)
1.8 | 2013-04-01 | `linalg.qr` `full` or `economic` | (2017-07 decided to not remove for now since SciPy includes these modes)
1.9 | 2013-09-24 | `np.delete(scalar)`, `np.delete(arr, obj)` where `obj.dtype` is not integer-type or `obj` has out-of-bound indices. Same for `np.insert` | Seems finalized, a FutureWarning in insert, but it may need thought about how a boolean array should behave.
1.10 | 2015-03-15 | `np.corrcoef` or `np.ma.corrcoef` with `bias` or `ddof`
1.12 | 2016-02-25 | use `int` not `operation.index` | implemented in `function_base._index_deprecate`
1.13 | 2017-05-17 | `np.newaxis` with `axis>ndim` or `axis< -ndim -1`
1.13 | 2016-04-13 | `np.ma.max(a)`, `np.ma.min(a)` | prefer `np.ma.max(a).reduce`
1.13 | 2017-04-26 | using `y` in the func passed to `ufunclike` | use `out` kwarg instead
1.14  | 11-9-2017  | `arrayprint.array2string( style!=np_NoValue)` | unless `options[legacy]=='1.13'`
1.15 | 2018-02-25 | `np.sum(generator)` in `fromnumeric` | prefer `sum(generator)` or `np.sum(np.fromiter(generator))`
1.15 | 2017-12-10 | `numeric.loads`, `numeric.load`, `np.ma.dump`, `np.ma.dumps`, `np.ma.load`, np.ma.loads` |prefer `pickle` methods
1.15 | 2018-04-04 | `umath_tests` | internal module
1.15 | 2018-02-20 | `NpzFile.iteritems`, `NpzFile.iterkeys`
?? | ?? | insufficient bit width in `numeric.binary_repr`
?? | ?? | `genfromtext(txt, encoder=bytes)` where txt has unicode codepoints
?? | 2018-05 | `np.matrix` | `scipy.sparse` dependency needs to be solved before thinking about removal.
1.20 | 2020-09 | `np.ndindex.ndincr` | 

~The `deprecate` decorator is used only for `np.unique` and `np.testing.random`, perhaps we should deprecate it too.~ - edit: bad ananlysis, the decorator is used elsewhere.

In C code (via the `DEPRECATE` macro or `DeprecateWarning`)

version | date | feature deprecated | notes
-----|----------|----------|-----------
1.12 | 2016-19-02 | `array_data_set`
1.11 | 2015-12-14 | `PyArray_OrderConverter(s)` when s is not bytes or unicode char
1.11 | 2015-11-27 | `array_descr_set` when f-contiguous and not c-contiguous
1.14 | 2017-08-11 | `convert_datetime_metadata_tuple_to_datetime_metadata(tup)` where `tup` is `unit`, `num`, `event`) or ('unit`', `num`, `event', `dem`) | prefer (`unit`, `num`) or (`unit`, `num`, `None`, `dem`), event ignored since 1.7 but no warning was raised
1.14 | 2016-01-14 | `convert_pydatetime_to_datetimestruc` or `parse_iso_8601_datetime` with a timezone
1.12 | 2012-02-04 | `PyArray_TypestrConvert` with `'O4'` or `'O8'` | use `'O'`
1.13 | 2017-05-04 | `PyArray_Conjugate` for non-numeric dtype
1.14 | 2017-06-01 | `array_bincnt(minlength=None)` | use 0 instead of None
1.14 | 2017-10-19 | `array_from_string` with binary mode
1.14 | 2017-09-25 | `_array_non_zero(a)` when a is an empty array
1.15 | 2018-4-21 | `PyArray_SetUpdateIfCopyBase` | prefer `PyArray_SetWritebackIfCopyBase`
?? | ?? | python2 division between two ints without `//`
?? | ?? | `arraydescr_names_set` is **not** deprecated, the warning was 'temporarily' removed in 1.7
1.17 | 2019-04 | `dtype([(""name"", np.float64), 1])` is squeezed to `dtype([""name"", np.float64])`, see gh-13326

Some of these have been around for a long time.",2018-07-06 21:19:21,,DEP: remove code with expired deprecations (tracking issue),"['17 - Task', '03 - Maintenance', '07 - Deprecation', 'defunct — difficulty: Intermediate', 'sprintable']"
11513,open,johnmwu,"Sorry if this is a stupid question, but as I had some difficulty finding one, I think it would be useful to be addressed.

Essentially, if one were to add a new function to NumPy, how would they know whether it went under `np.core` or `np.lib`? How would they know whether it went under `np.lib.function_base.py`, or `np.lib.fromnumeric.py`? 

Is there documentation about the contents of each directory (or package) that spell out something like:

# Contents of package a_package:
## Files
#### a_file.py
**Contains**: All ufuncs that a funkyyyyyyyy
#### another_file.py
**Contains**: All ufuncs fitting into the a_package that do not fit under a_file.py
#### b_file.py
**Contains**: The fundamental array creation functions, none of which is based off of a simpler one written in pure Python.
#### based_off_b.py
**Contains**: An array creation routine not fitting into b_file.py, but commonly used.
#### based_off_bb.py
**Contains**: All array creation routines that do not fit under b_file.py or based_off_b.py
#### readme.md
This file. Contains a description of all files in directory numpy/a_package

## Other Packages
#### b_package
**Contains**: All callable objects fitting into a_package, that do not fit into the above modules, whose name starts with the letter b.
#### LBPACK_mini
**Contains**: All external utilities necessary for the functioning of numpy imported from project LBPACK

-----------

So that *anyone*, if they are careful enough, can deduce
1. *Exactly* where a particular object will be located, 
2. *Exactly* where to place a particular object.

Just by starting from the root directory of the project, reading each readme.md, and following directions into the subdirectories.
",2018-07-06 00:46:31,,Where can I find a simple description of the delineation of NumPy's modules?,['04 - Documentation']
11510,open,mattip,"Working on matmul in #11133, and comparing to the `linalg` inner loops, I ran into a need for a working buffer much like linalg. In `umath_linalg.c.src` each iteration of the inner loop `mallocs`/`frees` the working memory. There seems to be no generic support for passing in a working buffer allocated once for the ufunc call. 

The `PyUFuncGenericFunction` signature has a `innerloopdata` argument, but I could find no examples of its use in linalg. In the actual inner loops in `umath_linalg.c.src` and elsewhere it is marked as `NPY_UNUSED(func)`

The only place I could find a use for this argument is in `unmasked_ufunc_loop_as_masked` where it is used to hold a structure, not a function.",2018-07-05 16:33:31,,ENH: allocate working buffers outside ufunc's inner loop,"['01 - Enhancement', 'component: numpy.ufunc']"
11506,open,mrader1248,"The function `einsum_path` greatly solves the problem of finding the optimal contraction sequence of tensors/arrays. However, it can only be used with numpy arrays, although it could be used to optimise a contraction without even having arrays. I would like to do the following.
```
class Dummy:
    def __init__(self, *shape):
        self.shape = shape
        self.ndim = len(shape)

a = Dummy(2, 2)
b = Dummy(2, 5)
c = Dummy(5, 2)
path_info = np.einsum_path('ij,jk,kl->il', a, b, c, optimize='greedy')
```
The code of `einsum_path` only uses the attributes `shape` and `ndim`, but forces a conversion to numpy arrays with `asanyarray` (which is why the code above does not work). Could the calls to `asanyarray` be removed or replaced, such that it works?",2018-07-05 12:49:10,,[feature request] make einsum_path work with non numpy-arrays,"['01 - Enhancement', 'component: numpy.einsum']"
11502,open,johnmwu,"As it stands,

```
>>> np.cov([[1, 2, 3]])
array(1.)
```

I feel that this is a bit unnatural, considering that any input array of first dimension > 1 will return an ndim 2 array. 

For example,
```
>>> np.cov([[1, 2, 3], [4, 5, 6]])
array([[1., 1.],
       [1., 1.]])
```

I suggest that either:

1. A scalar is returned. This would be useful for those people that prefer to use a single function for everything, and thus use cov for computing variance as well.
2. A (1, 1) array is returned. This would provide greater consistency with what is returned. Of these two, I prefer this one. I mean, np.dot doesn't squeeze, so why should cov?

While ndim 0 arrays can be used as scalars, it feels weird.

For the lazy, https://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html",2018-07-05 00:56:44,,"ENH: for one variable data, np.cov should return either a scalar or a (1, 1) array, not a ndim 0 array.","['01 - Enhancement', 'component: numpy._core']"
11498,open,shigemk2,"### Environment

- numpy 1.14.4
- Python 2.7.11

### Overview

`genfromtxt` does not generate multidimensional array when data has only one column.

```sh
>>> import numpy as np
>>> from StringIO import StringIO
>>> data = ""1, 2, 3\n4, 5, 6""
>>> np.genfromtxt(StringIO(data), delimiter="","")
array([[1., 2., 3.],
       [4., 5., 6.]])
>>> data = ""1\n2""
>>> np.genfromtxt(StringIO(data), delimiter="","") # I expect array([[1.], [2.,]]) but the result is array([1., 2.])
array([1., 2.])
>>> np.genfromtxt(StringIO(data), delimiter=""\n"")
array([1., 2.])
```

Is this expected action?",2018-07-04 11:38:35,,genfromtxt does not generate multidimensional array when data has only one column.,['unlabeled']
11437,open,cmohl2013,"When performing squeeze operation on a memmap,  it is converted to a normal in-memory array. Other array operation functions, such as reshape or expand_dims preserve the memmap type. Therefore the conversion upon squeeze is somewhat unexpected behavior in my point of view.

```
import numpy as np
from tempfile import mkdtemp
import os.path as path
filename = path.join(mkdtemp(), 'newfile.dat')

fp = np.memmap(filename, dtype='float32', mode='w+', shape=(3,4,1))

print('type before squeeze')
print(type(fp))
print(fp.shape)

print('type after squeeze')
squeezed = fp.squeeze()
print(type(squeezed))
print(squeezed.shape)
```

my output with numpy version 1.14.3 

```
type before squeeze
<class 'numpy.core.memmap.memmap'>
(3, 4, 1)
type after squeeze
<class 'numpy.ndarray'>
(3, 4)

```",2018-06-28 12:27:11,,squeeze on memmap returns numpy.ndarray instead of memmap,"['00 - Bug', 'component: numpy._core']"
11417,open,bersbersbers,"You will have no trouble finding people who use the `where` parameter without an `out` parameter in functions such as `np.divide`, `np.logical_not`, and others, without realizing that they are using non-initialized memory this way:
https://github.com/numpy/numpy/issues/9250
https://github.com/numpy/numpy/issues/9334

Yes, it's in the documentation, but that part is very easy to overlook - especially since that behavior is not documented near the `where` parameter, but the `out` parameter.

Also, in more than a few cases, I have observed that the allocated output seems to be initialized to 0 despite not using the `out` parameter:
```
RelDiff = np.divide(Diff, Truth, where=RelDiffMask)
sum(RelDiff[np.logical_not(RelDiffMask)])
```
returns `0.0`
But according to the documentation, this cannot be relied upon (or can it? then the documentation should be updated).

So what are the alternatives? In my view:
1. Require an `out` parameter when a `where` parameter is used. Make people who (for example, using `np.divide` to avoid divisions by zero) may not care about the other values (for example, because using `where=divisor`, they have a mask for values they are interested in) add `out=None`, just to make sure everyone knows they are using a non-initialized output and is OK with that. Don't assume anything from the non-presence of `out`.
2. Quietly initialize output to zero when `out` is not given and update documentation.
3. Other ideas?

(Edit: I have slightly improved the wording of alternative 1 *after* @umangv referenced it. Refer to the edit history if anything has become unclear.)",2018-06-25 10:41:45,,Better handling of missing `out` parameter when `where` parameter is present,['04 - Documentation']
11415,open,pv,"The gfortran DLL file names here: https://github.com/numpy/numpy/blob/master/numpy/distutils/fcompiler/gnu.py#L408 are computed from input file content, rather than from the content of the DLL file itself.

Windows DLLs appear to have two possible ways to import symbols --- by name, or by import table ordinal. It appears the functions tend to get imported by ordinal in the way we are doing things currently.

This may be a problem: the SHA1 hash ensures the input files, their content, and the order they are passed to the mingw DLL linker command are identical. However, it does not ensure the mingw linker puts items in the same order in the import table -- different mingw versions might put functions in different orders in the import table.

It would be better to compute the SHA1 has from the DLL file itself --- or to [enforce MSVC to use link-by-name](https://stackoverflow.com/questions/28854006/is-there-any-way-to-force-linker-use-function-name-rather-than-ordinal-to-import).",2018-06-24 19:51:47,,"Gfortran DLLs hash computed from input files, not DLLs",['component: numpy.distutils']
11407,open,Dominik1123,"### Summary

`np.exp` raises `AttributeError: 'int' object has no attribute 'exp'` when called with an integer greater than `2**64-1`. 

### Expected behavior

Return `np.inf` as for other large integer values (e.g. for `2**63`).

### Assumption

Such integers are treated internally as `object` and hence numpy tried to call the exp-function on the instance (similar to `np.exp(np.asarray([2], dtype=object))`). However this is completely unexpected since in Python 3 all integers have the same type (nonetheless the must be stored differently in memory).

### System

```bash
$ uname -a
Linux MyPC 4.4.0-128-generic #154-Ubuntu SMP Fri May 25 14:15:18 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
$ python
Python 3.6.2 (default, Jul 17 2017, 23:14:31) 
[GCC 5.4.0 20160609] on linux
```

### Code Example

```python
>>> from numpy import exp
>>> exp(2**64)  # Would expect np.inf as return value.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'int' object has no attribute 'exp'
>>> exp(2**63)  # Here it does.
__main__:1: RuntimeWarning: overflow encountered in exp
inf
>>> type(2**63) is type(2**64)
True
```",2018-06-21 23:07:59,,np.exp raises AttributeError when called with large integer,"['00 - Bug', 'component: numpy._core']"
11401,open,mattip,"PEP8 recommends spaces after commas `a[1, 2]` except when followed by a `)' as in  `(0,)`. This came up in PR #11383. There are places we do not respect this. On the one hand it would be nice to use a tool like [black](https://pypi.org/project/black/) to reformat this and other non-conformant code, on the other hand it would generate massive git churn so best to do as part of a large code overhaul. ",2018-06-21 17:01:13,,STY: ensure commas are followed by spaces,['03 - Maintenance']
11388,open,brynclarke,"OS: Windows 7 SP1
NumPy: 1.14.5
Python: 3.6.5

# Abstract

I am trying to use a `numpy.memmap` array to store a large structured array. Saving the data works fine, and I'm able to load the data *as long as I am in the same Python session* originally used to store the data. Starting a new session and trying to load the data causes the kernel to crash. Below is a script to minimally reproduce the error.

I have tried:
- Using a regular `numpy.ndarray` instead of a structured array. This runs without error, so this issue is likely specific to structured arrays.
- Clearing the namespace between saving the data and loading the data in a single session. Since this error only occurs when a new session is opened, I thought it was plausible that some remnant of the `memmap` was retained to allow the data to load in the same session, but this doesn't appear to be the case. Using a new name for the `memmap` used to retrieve the data (e.g. `fpnew` instead of `fp`) also didn't change results.
- Specifying the shape when retrieving data (e.g. adding `shape=(4,)` to the call to create a read-only `np.memmap`). This had no effect on results.
- Flushing the `memmap` object before deleting it. No effect.
- Deleting the disk file `temp.dat` between tests. No effect.
- Testing using an older version of Python (3.5.5) and NumPy (1.12.1). No effect.

# Code Examples

```
def main(argv):
    import sys
    populate = bool(int(sys.argv[1]))
    retrieve = bool(int(sys.argv[2]))

    import numpy as np
    #print(np.__version__) # 1.14.5

    dtype = (np.record, [('col_1', '<i8'), ('col_2', 'O')])

    if populate:
        sarr = np.array(
            [(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd')],
            dtype=dtype,
        )

        fp = np.memmap(
            ""temp.dat"",
            dtype=dtype,
            mode=""w+"",
            shape=(4, )
        )

        fp[:] = sarr
    
        del fp

    if retrieve:
        fp = np.memmap(
            ""temp.dat"",
            dtype=dtype,
            mode=""r""
        )

        print(fp) # [(0, 'a') (1, 'b') (2, 'c') (3, 'd')]

        del fp

if __name__ == ""__main__"":
    main()
```

Running this test script with both flags set results in the expected output:
```
> python test.py 1 1
# [(0, 'a') (1, 'b') (2, 'c') (3, 'd')]
```

Running the script to retrieve only crashes with error code 324452.
```
> python test.py 1 0
> python test.py 0 1 # CRASHES KERNEL
```

```
> python test.py 1 1
# [(0, 'a') (1, 'b') (2, 'c') (3, 'd')]
> python test.py 0 1 # CRASHES KERNEL
```",2018-06-20 18:44:18,,ENH: Memory map should warn on dtypes with objects,"['01 - Enhancement', '04 - Documentation', 'component: numpy._core', 'component: numpy.dtype']"
11387,open,crepererum,"## Abstract
Inplace clipping of floating type columns with mixed precision may be unsound in some cases.

## Code Examples
The following examples demonstrate the problem for upper bound clipping:

### Test Failure
```python
import numpy as np

array = np.array([0], dtype=np.float32)
out = array.copy()
upper = np.array([-np.finfo(np.float64).tiny], dtype=np.float64)
np.clip(array, None, upper, out)

assert (out <= upper).all()
```

### Expected Behavior
```python
import numpy as np

def clip_upper_inplace(array, upper):
    np.clip(array, None, upper, array)
    wrong = (array > upper)
    array[wrong] = np.nextafter(array[wrong], -np.inf)

array = np.array([0], dtype=np.float32)
upper = np.array([-np.finfo(np.float64).tiny], dtype=np.float64)
clip_upper_inplace(array, upper)

assert (array <= upper).all()
```

## Details

### Possible Edge Cases

1. the `upper` bound gets larger when converted to the lower precision (this is the example shown above)
2. the `lower` bound gets larger when converted to the lower precision
3. the `upper` bound is negative and is that ""large"" that it cannot be represented by the lower precision float
4. the `lower` bound is positive and is that ""large"" that it cannot be represented by the lower precision float
5. the range between `lower` and `upper` is that tiny that there is no lower precision float possible

### Expected Output
For 1. and 2.: find the closest lower-precision float that satisfies the bound check

For 3. and 4.: return `-/+inf`

For 5.: return `NaN`

## References:
- similar pandas issue: https://github.com/pandas-dev/pandas/issues/21476",2018-06-20 14:22:03,,inplace clipping does not account for mixed precision,['unlabeled']
11380,open,mattip,"Currently non of the linalg gufuncs accept an `out=` `kwarg`. It seems most of the ufuncs do accept an out arg, we should be consistent with our function interfaces",2018-06-19 21:11:03,,ENH: add out arguments to linalg gufuncs,"['01 - Enhancement', 'component: numpy.linalg', 'component: numpy.ufunc']"
11375,open,homocomputeris,"@seberg has pointed out that this function in not elementwise (yeah, I can't read the docs, obviously). It would be nice to have the possibility to apply it to each element.
 
```python
import numpy as np

a = -0. - 2.03541e-16j
v = np.array([a, a, a, a])
M = np.array([[a, 0. + 1.00000e+00j, a],
              [-0. - 1.29099e+00j, 0. + 0.00000e+00j, 0. + 1.29099e+00j],
              [0. + 3.33333e+00j, -0. - 6.66667e+00j, 0. + 3.33333e+00j]])

print(np.real_if_close(a))
print(np.real_if_close(v))
print(np.real_if_close(M))
```
gives
```
-0.0
[-0. -0. -0. -0.]
[[-0.-2.03541e-16j  0.+1.00000e+00j -0.-2.03541e-16j]
 [-0.-1.29099e+00j  0.+0.00000e+00j  0.+1.29099e+00j]
 [ 0.+3.33333e+00j -0.-6.66667e+00j  0.+3.33333e+00j]]
```
and M is not changed.

Version info:
```
'1.14.3'
blas_mkl_info:
    libraries = ['mkl_rt', 'pthread']
    library_dirs = ['/opt/intel/mkl/lib/intel64/']
    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
    include_dirs = ['/opt/intel/composerxe/linux/mkl', '/opt/intel/composerxe/linux/mkl/include', '/opt/intel/composerxe/linux/mkl/lib', '/opt/intel/mkl/include']
blas_opt_info:
    libraries = ['mkl_rt', 'pthread']
    library_dirs = ['/opt/intel/mkl/lib/intel64/']
    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
    include_dirs = ['/opt/intel/composerxe/linux/mkl', '/opt/intel/composerxe/linux/mkl/include', '/opt/intel/composerxe/linux/mkl/lib', '/opt/intel/mkl/include']
lapack_mkl_info:
    libraries = ['mkl_rt', 'pthread']
    library_dirs = ['/opt/intel/mkl/lib/intel64/']
    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
    include_dirs = ['/opt/intel/composerxe/linux/mkl', '/opt/intel/composerxe/linux/mkl/include', '/opt/intel/composerxe/linux/mkl/lib', '/opt/intel/mkl/include']
lapack_opt_info:
    libraries = ['mkl_rt', 'pthread']
    library_dirs = ['/opt/intel/mkl/lib/intel64/']
    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
    include_dirs = ['/opt/intel/composerxe/linux/mkl', '/opt/intel/composerxe/linux/mkl/include', '/opt/intel/composerxe/linux/mkl/lib', '/opt/intel/mkl/include']
```",2018-06-18 09:47:58,,EHN: make real_if_close elementwise,['unlabeled']
11334,open,mrocklin,"Does it make sense to extend tensordot to support more than two input arrays?  

The definition and API seem to be amenable to this extension, though I can't say anything about the implementation.",2018-06-14 15:34:26,,Extend tensordot to more than two arguments,"['01 - Enhancement', '54 - Needs decision']"
11329,open,Morwenn,"I was writing a program recently where memory was a bit of a problem because I was allocating several big numpy arrays. I somehow managed to reduce the amount of numpy arrays allocated in the program in order to avoid `MemoryError`, but I think there is still room for improvement. I have something along these lines in the code:

```python
# Associate to every element its index in its original collection
orig_indices = numpy.concatenate((numpy.arange(len(datetimes1)),
                                  numpy.arange(len(datetimes2))))
# Tag every element in the concatenated array
tags = numpy.concatenate((numpy.repeat(False, len(datetimes1)),
                          numpy.repeat(True, len(datetimes2))))
```

As far as I know, numpy arrays are allocated for every `arange` and `repeat`  call in the code above, then another array is allocated for `concatenate` and the contents of the results of `repeat`/`arange` are copied in the array allocated by `concatenate`. To avoid a few allocations, I wish I could have written the previous code as follows:

```python
len1 = len(datetimes1)
len2 = len(datetimes2)
# Associate to every element its index in its original collection
orig_indices = numpy.empty(len1 + len2)
numpy.arange(len1, out=orig_indices[:len1])
numpy.arange(len2, out=orig_indices[len1:])
# Tag every element in the concatenated array
tags = numpy.empty(len1 + len2)
numpy.repeat(False, out=tags[:len1])
numpy.repeat(True, out=tags[len1:])
```

Would such an improvement be possible?",2018-06-14 07:47:56,,Feature request: out parameter for repeat and arange,['01 - Enhancement']
11309,open,o11c,"Per documentation:

> You may use slicing to set values in the array, but (unlike lists) you can never grow the array. The size of the value to be set in x[obj] = value must be (broadcastable) to the same shape as x[obj].

(aside: the documentation could mention ""never grow (or shrink)"")

However, instead of throwing an exception, the following code silently does nothing.

    In [1]: import numpy as np

    In [2]: a = np.arange(10)

    In [3]: a[5:5] = [123]

    In [4]: a
    Out[4]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

Other cases correctly throw the exception:

    In [10]: a[5:5] = [123, 456]
    ---------------------------------------------------------------------------
    ValueError                                Traceback (most recent call last)
    <ipython-input-10-d4b9d8302fa7> in <module>()
    ----> 1 a[5:5] = [123, 456]

    ValueError: cannot copy sequence with size 2 to array axis with dimension 0

    In [11]: a[5:6] = [123, 456]
    ---------------------------------------------------------------------------
    ValueError                                Traceback (most recent call last)
    <ipython-input-11-ed78eec154a9> in <module>()
    ----> 1 a[5:6] = [123, 456]

    ValueError: cannot copy sequence with size 2 to array axis with dimension 1

    In [12]: a[5:6] = []
    ---------------------------------------------------------------------------
    ValueError                                Traceback (most recent call last)
    <ipython-input-12-4e280c717263> in <module>()
    ----> 1 a[5:6] = []

    ValueError: cannot copy sequence with size 0 to array axis with dimension 1

    In [13]: a
    Out[13]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

Discovered with numpy 1.12.1 from Debian, but verified with numpy 1.14.4 from PyPI.",2018-06-11 16:36:51,,Attempt to grow array by assigning a size-1 array to a size-0 slice doesn't throw exception,"['component: numpy._core', 'component: documentation']"
11308,open,attack68,"This issue documents two separate bugs:

1) Failure in the optimization routine when set to True:
```
    import numpy as np
    a = np.arange(64).reshape(2,4,8)
    np.einsum('obk,ijk->ioj',a,a)
    array([[[ 1904,  5872,  9840, 13808],
            [ 5488, 17648, 29808, 41968]],
           [[17776, 21744, 25712, 29680],
            [54128, 66288, 78448, 90608]]])
    np.einsum('obk,ijk->ioj',a,a, optimize=True)

    Traceback (most recent call last):
  File ""/anaconda3/envs/General/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2963, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-133-00a49d0e7c9d>"", line 1, in <module>
    np.einsum('obk,ijk->ioj',a,a, optimize=True)
  File ""/anaconda3/envs/General/lib/python3.6/site-packages/numpy/core/einsumfunc.py"", line 1118, in einsum
    if any(left_dims[ind] != right_dims[ind] for ind in idx_rm):
  File ""/anaconda3/envs/General/lib/python3.6/site-packages/numpy/core/einsumfunc.py"", line 1118, in <genexpr>
    if any(left_dims[ind] != right_dims[ind] for ind in idx_rm):
KeyError: 'b'
```

2) Inconsistent return using different optimization options:
```
    >>> a = np.ones(64).reshape(2,4,8)
    >>> np.einsum('ijk,ilm,njm,nlk,abc->',a,a,a,a,a, optimize='optimal')
    262144.0
    >>> np.einsum('ijk,ilm,njm,nlk,abc->',a,a,a,a,a, optimize='greedy')
    array(262144.)
    >>> np.einsum('ijk,ilm,njm,nlk,abc->',a,a,a,a,a)
    262144.0
```

referencing #11234 discussion and drawing @dgasmith attention.",2018-06-11 16:24:59,,einsum optimization failure and inconsistency,['unlabeled']
11283,open,jakirkham,"Was trying to understand what NumPy's `pad` does with `reflect_type=""odd""`. According to the docs...

> For the ‘odd’ style, the extented part of the array is created by subtracting the reflected values from two times the edge value.

So naively would think this is a composition of `pad` with `mode=""edge""` and `pad` with `mode=""reflect""`/`mode=""symmetric""`. Went ahead and tried this to see if this would work. Appears it almost does with the exception of corners (though edges appear correct).

While there are some comments in the docs about how corners are dealt with, it's still not obvious to me what is happening here unlike some of the other `mode`s' corner behavior. Not sure if this is just me not understanding or if there is a genuine bug here. Please let me know your thoughts. Example of the behavior below.

<details>
<summary>Example:</summary>

```python
In [1]: import numpy as np

In [2]: np.set_printoptions(linewidth=157)

In [3]: np.random.seed(0)

In [4]: a = np.random.random((3, 4))

In [5]: 2 * np.pad(a, 1, ""edge"") - np.pad(a, 1, ""reflect"", reflect_type=""even"")
Out[5]: 
array([[0.45173289, 0.67397221, 0.78448462, 0.76793954, 0.19799337, 0.65217915],
       [0.38243764, 0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.48700299],
       [0.20141549, 0.4236548 , 0.64589411, 0.43758721, 0.891773  , 1.34595879],
       [1.543884  , 0.96366276, 0.38344152, 0.79172504, 0.52889492, 0.2660648 ],
       [1.28143141, 1.50367072, 0.12098892, 1.14586286, 0.16601684, 0.62020263]])

In [6]: np.pad(a, 1, ""reflect"", reflect_type=""odd"")
Out[6]: 
array([[ 0.5634598 ,  0.67397221,  0.78448462,  0.76793954,  0.19799337, -0.37195281],
       [ 0.38243764,  0.5488135 ,  0.71518937,  0.60276338,  0.54488318,  0.48700299],
       [ 0.20141549,  0.4236548 ,  0.64589411,  0.43758721,  0.891773  ,  1.34595879],
       [ 1.543884  ,  0.96366276,  0.38344152,  0.79172504,  0.52889492,  0.2660648 ],
       [ 2.88635252,  1.50367072,  0.12098892,  1.14586286,  0.16601684, -0.81382919]])

In [7]: (2 * np.pad(a, 1, ""edge"") - np.pad(a, 1, ""reflect"", reflect_type=""even"")) - (np.pad(a, 1, ""reflect"", reflect_type=""odd""))
Out[7]: 
array([[-0.1117269 ,  0.        ,  0.        ,  0.        ,  0.        ,  1.02413197],
       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ],
       [-1.60492111,  0.        ,  0.        ,  0.        ,  0.        ,  1.43403182]])
```

</details>

<br>

<details>
<summary>Environment:</summary>

```yaml
name: test
channels:
  - conda-forge
  - defaults
dependencies:
  - appnope=0.1.0=py36_0
  - backcall=0.1.0=py_0
  - blas=1.1=openblas
  - ca-certificates=2018.4.16=0
  - certifi=2018.4.16=py36_0
  - decorator=4.3.0=py_0
  - ipython=6.4.0=py36_0
  - ipython_genutils=0.2.0=py36_0
  - jedi=0.12.0=py36_0
  - libgfortran=3.0.0=0
  - ncurses=5.9=10
  - numpy=1.14.4=py36_blas_openblash24bf2e0_200
  - openblas=0.2.20=8
  - openssl=1.0.2o=0
  - parso=0.2.1=py_0
  - pexpect=4.6.0=py36_0
  - pickleshare=0.7.4=py36_0
  - prompt_toolkit=1.0.15=py36_0
  - ptyprocess=0.5.2=py36_0
  - pygments=2.2.0=py36_0
  - python=3.6.5=1
  - readline=7.0=0
  - setuptools=39.2.0=py36_0
  - simplegeneric=0.8.1=py36_0
  - six=1.11.0=py36_1
  - sqlite=3.20.1=2
  - tk=8.6.7=0
  - traitlets=4.3.2=py36_0
  - wcwidth=0.1.7=py36_0
  - xz=5.2.3=0
  - zlib=1.2.11=h470a237_3
```

</details>",2018-06-08 18:06:49,,"NumPy's pad with reflect_type=""odd"" appears strange in corners",['unlabeled']
11270,open,eric-wieser,"Brought up by #11266.

Motivation is to change:
```
In [71]: np.split(np.arange(12).reshape(3, 4), 2, axis=-1)
Out[71]:
[array([[0, 1],
        [4, 5],
        [8, 9]]), array([[ 2,  3],
        [ 6,  7],
        [10, 11]])]
```
to
```
In [71]: np.split(np.arange(12).reshape(3, 4), 2, axis=-1)
Out[71]:
[array([[0, 1],
        [4, 5],
        [8, 9]]),
 array([[ 2,  3],
        [ 6,  7],
        [10, 11]])]
```

Edit: the above is already fixed upstream.

We may still want to implement `_repr_pretty_` so that we can:
* Respect the IPython line width setting
* pick a line width that's appropriate to the level of indentation that we're currently in",2018-06-07 16:51:54,,ENH: Implement `ndarray._repr_pretty_` for IPython,"['01 - Enhancement', 'defunct — difficulty: Intermediate']"
11266,open,boeddeker,"Is it possible to implement a `np.get_string_function` as counterpart of `np.set_string_function`?

I found the code for printing
https://github.com/numpy/numpy/blob/c486d8d0d496f6b366dc827a3d41ed1e9d593ec8/numpy/core/src/multiarray/strfuncs.c#L18-L36
and it looks like it is currently impossible to get the `PyArray_ReprFunction` and `PyArray_StrFunction` in python.
Since my C skills are limited, maybe someone that is familiar with the numpy source code can implement such a function?

My use case is that I have my own pprint and simply reset the `string_function` at the end, but the correct way would be to restore the previous one:
```python
def pprint(obj):
    from pprint import pprint
    np.set_string_function(
        lambda a: f""array(shape={a.shape}, dtype={a.dtype})""
    )
    original_pprint(obj)
    np.set_string_function(None)  # Wrong, when the user initially changed the string_function
```
",2018-06-07 08:42:18,,missing np.get_string_function (counterpart of np.set_string_function),"['01 - Enhancement', 'component: numpy._core', 'defunct — difficulty: Intermediate']"
11265,open,eric-wieser,"`PyArray_SetBaseObject(arr, base)` is dangerous when `arr.__array_finalize__ is not None` (#11237)

Our uses of this are fixed internally by #11246, but third-parties consuming the C api face the same issue.

I think we need to:

* Expose `PyArray_FromDescrAndBase` publically (deciding on a name before doing so)
* Deprecate `PyArray_SetBaseObject`
  * Only if `arr.__array_finalize__ is not None`
  * Only if `type(arr) is not ndarray`?
  * Always? (some internal uses not matching the above remain)",2018-06-07 07:08:36,,BUG: PyArray_SetBaseObject is dangerous on any subclass that defines __array_finalize__,"['00 - Bug', 'component: numpy._core']"
11241,open,michael-kuhlmann,"pip provided numpy and Anaconda provided numpy lead to different results when using numpy's fft package:
```
import numpy as np
t = np.arange(4096, dtype=np.float32)
T = np.fft.fft(t)
T.dtype
```
pip provided numpy returns `dtype('complex128')` while Anaconda provided numpy returns `dtype('complex64')`. As a consequence, `np.testing.assert_allclose(np.fft.ifft(T), t, atol=1e-10)` fails with Anaconda's numpy causing test cases that use numpy's fft package to fail as well (e.g. https://github.com/dask/dask/issues/3408). 

The Anaconda installation uses numpy 1.14.2, mkl 2018.0.2 and mkl_fft 1.0.1. It seems that mkl_fft now uses single precision.",2018-06-04 14:42:10,,Reduced fft precision with mkl_fft 1.0.1,['unlabeled']
11236,open,eric-wieser,"This came up in implementing `np.ma.masked.__array_finalize__`, and is also an issue in `np.memmap.__array_finalize__`.

Sometimes the correct thing for a subclass to do is say ""I can't handle this, downcast me to something else"":

* `np.ma.masked` should not be copyable, so any new instance should be downcast to a normal maskedarray
* `np.memmap` should only attach data to views onto the file, not copies. Right now we work around this by setting `memmap._mmap = None`, which feels like a bad way of saying ""this isn't really a memmap""

The easiest way to solve this would be to allow `__array_finalize__` to return an `ndarray` instance, which is used to replace `self`. `return None`, would be translated to `return self`, for compatibility.",2018-06-04 00:16:15,,ENH: Allow __array_finalize__ to change the array type,['01 - Enhancement']
11232,open,mhvk,"It is well known that ufunc calls on scalars are rather slow, but it is probably good to have a summary of why it is so slow, for which it is useful to go along the `ufunc_generic_call` path. I got only partway, but one possible solution might be for the scalars to already get overridden in in `CheckOverride`, i.e., to treat them as if they had their own `__array_ufunc__` (with priority even below that of ndarray; an actual `__array_ufunc__` calling `math` is slightly slower than our present state).

1.  `PyUFunc_CheckOverride`: for non-arrays (thus including scalars), this checks whether the scalar has `__array_ufunc__`. Easy to avoid if our whole API is available - needs #10915.
2. `PyUFunc_GenericFunction`: to be done (will edit).
3. `make_full_arg_tuple`: EDIT now fast (with #11231).
4. `_find_array_wrap` -> `_find_array_method`: skips arrays and scalars, so should be reasonably fast (though a subclass check for `Generic` is done before type checks on python objects in `PyArray_IsAnyScalar` (in `ndarrayobject.h`).


Simple timings
--------------
Single-input ufunc, comparing with `math`
```
a = 1.
a64 = np.float64(1.)
as64 = np.array(1., dtype=np.float64)
%timeit math.sin(a)  # and a64, as64
# 76, 76, 87  ns for a, a64, as64
%timeit np.sin(a)
# 600, 930, 450 ns for a, a64, as64
```
Somewhat more random, for addition
```
%timeit np.add(1., 1)
# 1000000 loops, best of 3: 970 ns per loop
%timeit 1. + 1
# 100000000 loops, best of 3: 8.73 ns per loop
# slightly fairer
%timeit operator.add(1., 1)
# 10000000 loops, best of 3: 80.4 ns per loop
# Oddly, again, scalars are much slower than array scalars
a64 = np.float64(1.)
%timeit np.add(a64, a64)
# 1000000 loops, best of 3: 1.35 µs per loop
as64 = np.array(1., dtype=np.float64)
%timeit np.add(as64, as64)
# 1000000 loops, best of 3: 468 ns per loop
```



",2018-06-02 21:54:56,,Ufunc calls on scalars are very slow,"['15 - Discussion', 'component: numpy._core']"
11228,open,mhvk,"A follow-up on https://github.com/numpy/numpy/pull/11175#discussion_r191896435 and to remind us of https://github.com/mhvk/numpy/pull/1#discussion_r192091355:
With #11098 and #11175, the gufunc machinery has started to copy a number of items from the `ufunc` struct since these may be changed: `ufunc->core_num_dims` (for `keepdims` and flexible dimensions; `ufunc->core_dim_sizes` (for frozen dimensions) and `ufunc->core_dim_flags` (for flexible dimensions).

This issue is to remind us to ensure that
1. [X] The copying should be organized in one place; EDIT: done in https://github.com/numpy/numpy/pull/11175/commits/dd6656de132a9034a9f338204b4db1bcb78d002d
2. [ ] It ideally be done only when actually needed; if not, the result should just be a pointer to the original data.

For (2), it would be good if the ufunc provided the information on which parts are fixed and which variable: if it has only frozen dimensions and no broadcasting, there is no need to copy `core_dim_sizes`; without flexible dimensions or `keepims`, `core_num_dims` is fixed, and without flexible dimensions, `core_dim_flags` will remain zero. For this purpose, we might consider adding a ufunc flags object, or, perhaps better, treat the current `core_enabled` as such, since it effectively is already a one-flag indicator of how the ufunc should be treated. So, a concrete proposal for new flags might be (EDIT: updated following update of #11175)
```
#define UFUNC_CORE_ENABLED = 0x0001
#define UFUNC_CORE_DIM_SIZE_UNSET = 0.0002
#define UFUNC_CORE_DIM_CAN_IGNORE = 0x0004
#define UFUNC_CORE_DIM_CAN_BROADCAST = 0x0008
```
(here, obviously re-using some of the ones defined already; they'd be OR'd together from `core_dim_flags`).",2018-06-02 14:30:01,,"Gufunc helper for sizes, flags and num_dims?","['15 - Discussion', 'component: numpy._core']"
11214,open,Erotemic,"Would it be reasonable to add a dtype option to `np.interp` to change the data type of the return array? The use case is in the skimage `equalize_hist` function. I'd like to keep my data as float32 due to memory constraints when working with multiple very large images. 

Here is an example illustrating what I would like to do: 

```python
>>> import numpy as np
>>> xp = [1, 2, 3]
>>> fp = [3, 2, 0]
>>> np.interp([0, 1, 1.5, 2.72, 3.14], xp, fp, dtype=np.float32)
array([ 3. ,  3. ,  2.5 ,  0.56,  0. ], dtype=float32)
```
",2018-05-31 19:12:42,,np.interp always returns 64 bit data. Add ability to return 32 bit data?,['01 - Enhancement']
11183,open,eric-wieser,"Some context - appending a comma (ie, passing a 1-tuple), causes mgrid and ogrid to return a sequence of coordinates. This is reasonable behavior
```python
>>> comma   = np.mgrid[:5,]; comma[0]
array([0, 1, 2, 3, 4])
>>> without = np.mgrid[:5]; without
array([0, 1, 2, 3, 4])
>>> (comma[0] == without).all()
True
```

Except a different code path is used to compute them, so the results are inconsistent
```python
>>> comma   = np.mgrid[1e17:1e17+5,]
>>> without = np.mgrid[1e17:1e17+5]
>>> (comma[0] == without).all()
False
```

Caused by #9616 happening on one code path but not the other",2018-05-29 06:04:41,,BUG: np.mgrid and np.ogrid produces numerically different results for single-argument case,['00 - Bug']
11136,open,nschloe,"[`np.unique`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.unique.html) has the `axis` option that allows, for example, to call `unique` on the rows of a matrix. I noticed however that it's quite slow. Creating a view of the data and calling unique on the view is faster by a factor of 3.

MVCE:
```python
import numpy
import perfplot


def unique_axis(data):
    return numpy.unique(data, axis=0)


def unique_row_view(data):
    b = numpy.ascontiguousarray(data).view(
        numpy.dtype((numpy.void, data.dtype.itemsize * data.shape[1]))
    )
    u = numpy.unique(b).view(data.dtype).reshape(-1, data.shape[1])
    return u


def unique_scikit(ar):
    if ar.ndim != 2:
        raise ValueError(""unique_rows() only makes sense for 2D arrays, ""
                         ""got %dd"" % ar.ndim)
    # the view in the next line only works if the array is C-contiguous
    ar = numpy.ascontiguousarray(ar)
    # np.unique() finds identical items in a raveled array. To make it
    # see each row as a single item, we create a view of each row as a
    # byte string of length itemsize times number of columns in `ar`
    ar_row_view = ar.view('|S%d' % (ar.itemsize * ar.shape[1]))
    _, unique_row_indices = numpy.unique(ar_row_view, return_index=True)
    ar_out = ar[unique_row_indices]
    return ar_out


perfplot.save(
    ""unique.png"",
    setup=lambda n: numpy.random.randint(0, 100, (n, 2)),
    kernels=[unique_axis, unique_row_view, unique_scikit],
    n_range=[2 ** k for k in range(20)],
)
```
![unique](https://user-images.githubusercontent.com/181628/97775543-cf61c180-1b61-11eb-93da-99d9f3ed67ab.png)",2018-05-22 15:24:31,,unique() needlessly slow,"['component: numpy._core', 'component: benchmarks']"
11123,open,mhvk,"For fully masked arrays, `ma.all()`  and `ma.any()` both return `masked`. While documented as such, it is odd and prevents, e.g., the following from working:
```
c = np.ma.MaskedArray([3.4, 4.1, 6.3], mask=[True, True, True])
np.testing.assert_array_equal(c, 0.)
# AssertionError
```
The underlying problem is best seen for boolean masked arrays:
```
c = np.ma.MaskedArray([True, True, True], mask=[False, True, True])
c.all()
# True
c[1:].all()
# masked
```
This is illogical: if the whole evaulates to `True`, that means the last two elements get ignored, and they should evaluate to `True` as well, just as `all([])` evaluates to `True`.",2018-05-19 16:45:06,,MaskedArray.all()  and .any() behaviour for fully masked arrays,"['15 - Discussion', 'component: numpy.ma']"
11118,open,mhvk,"#11098 introduced a `keepdims` argument for gufuncs for which all the inputs had the same number of core dimensions and all the outputs had none; for those, it would insert dimensions of length 1 in the outputs, i.e., a signature like `(i),(i)->(),()` is changed to `(i),(i)->(1),(1)`.

In https://github.com/numpy/numpy/pull/11098#discussion_r189335102, @eric-wieser wondered whether it should also support signatures like `(i),(i)->(),(i)`, i.e., allow outputs with the same number of dimensions as the inputs as well as those with no core dimensions. This issue is to remind us of this suggestion.

",2018-05-18 20:11:55,,Possible generalization of gufunc keepdims for multiple outputs,"['23 - Wish List', 'component: numpy._core']"
11109,open,mhvk,"I've been trying to wrap functions into ufuncs and ran into a problem for some that have regular doubles as inputs, but a structured dtype as output: if I follow the instructions and define a userloop for these [1], it works if I explicitly pass in an output with the right `dtype`, but not if I don't.

After digging into the code, I found the problem arises because `linear_search_userloop_type_resolver` only attemps to see whether a user loop should be looked at if one of the operands has a user-defined or void type [2]. Hence, if one does not give an output, the search will not done.

This does not seem very difficult to fix, but I'm not sure whether it would break expectations elsewhere.

[1] https://docs.scipy.org/doc/numpy/user/c-info.ufunc-tutorial.html#example-numpy-ufunc-with-structured-array-dtype-arguments
[2] https://github.com/numpy/numpy/blob/master/numpy/core/src/umath/ufunc_type_resolution.c#L1742",2018-05-16 23:59:25,,UFunc userloop selection does not work when only output is void/user dtype,"['00 - Bug', 'component: numpy._core']"
11092,open,eric-wieser,"In thinking about #10771, I realized that a simple helper function like the following might be handy:

```python
def ndslice(sl):
    """""" convert a slice of iterables into a tuple of slices (nd slice)""""""
    start = sl.start
    stop = sl.stop
    step = sl.step
    if start is None and stop is None and step is None:
        return ()
    if start is None:
        start = itertools.repeat(None)
    if stop is None:
        stop = itertools.repeat(None)
    if step is None:
        step = itertools.repeat(None)
    return tuple(
        slice(art, op, ep)
        for art, op, ep in zip(start, stop, step)
    )
```

Which could be used as:

```python
a = np.zeros((10, 10, 10))
a[ndslice(np.s_[[1,2,3]:[4,5,6]])]  # same as a[1:4, 2:5, 3:6]
```

Or possibly using a `__getitem__` method like `np.s_[]`, sugared to:

```python
class ndslice_type(object):
    __getitem__ = lambda self, item: ndslice(item)
ndslice_ = ndslice_type()

a[ndslice_[[1,2,3] : [4,5,6]]]  # same as a[1:4, 2:5, 3:6]",2018-05-14 04:19:03,,ENH: helper for multidimensional slicing,['01 - Enhancement']
11081,open,yurivict,"Ctrl-C doesn't have any effect.

py27-numpy-1.13.3_3,1 on FreeBSD-11.1",2018-05-12 17:10:49,,ENH: np.linalg.eig doesn't process the SIGINT signal,"['01 - Enhancement', 'component: numpy.linalg']"
11079,open,charris,See #11036 and #10370. C99 will provide `#pragma STDC FENV_ACCESS ON` which should provide a better supported solution to the statement reordering problem.,2018-05-11 20:58:21,,Revisit compiler statement reordering fix when we require C99,['17 - Task']
11071,open,Lguyogiro,"The behavior of `numpy.testing.assert_almost_equal` seems incorrect when one of the arguments is an empty list.

The following cases behave as I expect:
```
# Compare an empty list to a list containing the number 2. These are not almost equal, so an AssertionError is correctly raised.

>>> from numpy.testing import assert_almost_equal
>>> assert_almost_equal([], [2])
AssertionError: 
Arrays are not almost equal to 7 decimals

(shapes (0,), (1,) mismatch)
 x: array([], dtype=float64)
 y: array([2])

# Compare the integer 2 to a list containing only 2. Since the arguments are treated as 'array-like', this should not raise an exception (and doesn't).

>>> from numpy.testing import assert_almost_equal
>>> assert_almost_equal(2, [2])
>>>
```

When one of the arguments is an empty list and the other is a number, they are presumably **not** almost equal. Nonetheless:
```
>>> from numpy.testing import assert_almost_equal
>>> assert_almost_equal([], 2)  # no AssertionError?
>>>
```

The same behavior can be observed in `numpy.testing.assert_allclose`.",2018-05-09 23:44:49,,"""assert_almost_equal"" does not fail when comparing a number and an empty list",['unlabeled']
11068,open,zuoxingdong,"Suppose we have the following code

    import numpy as np
    
    D = []
    for _ in range(200):
       d = []
       for _ in range(300):
          d.append({'a': np.random.randn(64, 64, 3), 'b': np.random.randn(64, 64, 3)})
       D.append(d)
    
    np.save('data', D)

It takes really long to finish the saving. 

However, by taking the dictionary outside, even though with same data size, it is dramatically faster. So it seems it is the dictionary which slows down the process. Is there some potential reason for that ?

i.e.

    import numpy as np
    
    D1 = []
    D2 = []
    for _ in range(200):
       d1 = []
       d2 = []
       for _ in range(300):
          d1.append(np.random.randn(64, 64, 3))
          d2.append(np.random.randn(64, 64, 3))
       D1.append(d1)
       D2.append(d2)
    
    np.save('d1', D1)
    np.save('d2', D2)

",2018-05-09 09:07:04,,Why nested dictionary slows down numpy save?,['unlabeled']
11060,open,toslunar,"`np.einsum`'s optimized blas call ignores `dtype` option.
```
>>> np.einsum('ij,jk', np.eye(3, dtype=np.float32), np.eye(3, dtype=np.float32), dtype=np.float64, optimize=True)
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)
>>> np.einsum('ij,jk', np.eye(3, dtype=np.float32), np.eye(3, dtype=np.float32), dtype=np.float64)
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]])
```

Thus it also ignore `casting` option.
```
>>> np.einsum('ij,jk', np.eye(3, dtype=np.float64), np.eye(3, dtype=np.float64), dtype=np.float32, optimize=True)
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]])
>>> np.einsum('ij,jk', np.eye(3, dtype=np.float64), np.eye(3, dtype=np.float64), dtype=np.float32, optimize=False)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/tos/py3/lib/python3.6/site-packages/numpy/core/einsumfunc.py"", line 1069, in einsum
    return c_einsum(*operands, **kwargs)
TypeError: Iterator operand 0 dtype could not be cast from dtype('float64') to dtype('float32') according to the rule 'safe'
>>>
```",2018-05-07 01:06:47,,optimized einsum may ignore some options,['unlabeled']
11059,open,toslunar,"intermediate array may have ndim=0.
```
>>> x = np.arange(3).astype(np.int64)
>>> y = np.arange(2).astype(np.int16)
>>> np.einsum('i,i,j', x, x, y, optimize=['einsum_path', (0, 1), (0, 1)])
array([0, 5], dtype=int16)
>>> np.einsum('i,i,j', x, x, y, optimize=['einsum_path', (0, 2), (0, 1)])
array([0, 5])
```",2018-05-07 01:06:43,,einsum optimization may change dtype of output,['00 - Bug']
11048,open,louisabraham,"``` pycon
>>> import numpy as np
>>> '%s' % np.array(1/3)
'0.3333333333333333'
>>> '%s' % np.array([1/3])
'[0.33333333]'
>>> np.set_printoptions(formatter={""float_kind"": lambda x: ""%.1f"" % x})
>>> '%s' % np.array(1/3)
'0.3333333333333333'
>>> '%s' % np.array([1/3])
'[0.3]'
```",2018-05-05 01:33:20,,np.set_printoptions doesn't affect floats,['component: numpy._core']
11034,open,mhvk,"While testing keepdims for more than 1 dimension on `linalg.det`, I noticed that a simple test with an integer input array gave an error:
```
In [22]: np.linalg.det(np.eye(3) * 2.) - 8.
Out[22]: -1.7763568394002505e-15
```
Reporting since it seems unexpected that rounding errors play a role for integer input.",2018-05-02 13:34:19,,Unexpected rounding error for np.linalg.det,['component: numpy.linalg']
11024,open,eric-wieser,"It would be great if one axis could be an integer while the other is a float. Supporting datetime / float axes would be handy too.

Fixing this would involve removing the array cast in
```
    try:
        # Sample is an ND-array.
        N, D = sample.shape
    except (AttributeError, ValueError):
        # Sample is a sequence of 1D arrays.
        sample = np.atleast_2d(sample).T
        N, D = sample.shape`
```",2018-05-01 06:41:34,,ENH: Allow histogramdd to work with mixed edge types,"['01 - Enhancement', 'component: numpy.lib', 'component: numpy.datetime64']"
11022,open,eric-wieser,"Which leads to:

```python
# all numbers appear between their successor and predecessor, right?
>>> check = lambda x: np.digitize(x, [x - 1, x + 1]) == 1

>>> check(1)
True
>>> check(2**52)
True
>> check(2**53)  # uh oh
False
```

The workaround is: 

```python
def digitize(x, bins, right=False):
    # arguments below are swapped, so this is swapped too
    if right:
        side = 'left' 
    else:
        side = 'right'
    return np.searchsorted(bins, x, side=side)
```

The issue right now is that the monotonicity detection in `digitize` is forcing everything to be case to float64. In almost all cases the user probably already sorted their input, so this is not only pointless overhead, but it's causing harmful behavior too.
",2018-05-01 06:18:40,,BUG: np.digitize casts integers to float64,['00 - Bug']
11014,open,alexmojaki,"`np.array(string_array.tolist(), dtype=int)`

is almost twice as fast as

`np.array(string_array, dtype=int)`

Full demo script:

```python
from timeit import timeit
import numpy as np
import sys

string_array = np.array(list(map(str, range(1000000))))

print(timeit(lambda: np.array(string_array, dtype=int), number=10))
print(timeit(lambda: np.array(string_array.tolist(), dtype=int), number=10))
print(np.__version__)
print(sys.version)
```

Output:

```
3.470734696020372
2.058091988990782
1.14.2
3.6.2 (default, Jul 29 2017, 00:00:00) 
[GCC 4.8.4]
```

This seems like an optimisation that numpy should be able to do automatically, and also possible a hint  at an underlying performance problem.

---

Summary by @seberg 2021-11:
* As Eric notes at the end, the current timings should be mainly due to the weird casting functions.
* The solution will be to implement new-style casts (instead of the weird legacy cast function), for string to integer casts.  Even the old functions are bad (they go via scalars!), but there is probably not much point in trying to improve them.",2018-04-30 14:07:04,,"np.array(string_array.tolist(), dtype=int) is faster than without the .tolist()",['unlabeled']
11004,open,GJBoth,"The sinc function as implemented now in numpy is the so-called 'normalized sinc function', see: https://en.wikipedia.org/wiki/Sinc_function

For many applications (including mine), I expect the unnormalized sinc function. Maybe it's a good idea to add a unnormalized/normalized flag and making an explicit note about this in the documentation.",2018-04-29 11:01:39,,Sinc doesn't return expected output,['unlabeled']
10989,open,yaox12,"Since version 1.7.0, the `axis` in `np.average` could be a tuple of ints. However, when `weights` is a 1-D array and `axis` is a tuple,  [this line of code](https://github.com/numpy/numpy/blob/master/numpy/lib/function_base.py#L376) will raise a TypeError   

```python
>>> data = np.arange(6).reshape((3, 2))
>>> data
array([[0, 1],
       [2, 3],
       [4, 5]])
>>> np.average(data, weights=[3, 1], axis=(0, 1))

   1145                 raise TypeError(
   1146                     ""1D weights expected when shapes of a and weights differ."")
-> 1147             if wgt.shape[0] != a.shape[axis]:
   1148                 raise ValueError(
   1149                     ""Length of weights not compatible with specified axis."")

TypeError: tuple indices must be integers or slices, not tuple
```

I think we should check whether the `axis` is a int when `weights` is a 1-D array and raise a TypeError like this:
`TypeError: Axis must be a single int, not a tuple when shapes of a and weights differ.`  

And the doc about `axis` should be updated with something like this:
> NOTE: The axis can be a tuple of ints only when `weights` has the same shape with the input array.

Can I create a PR to fix this?",2018-04-27 07:16:32,,np.average lacks of proper error message and docs when weights is 1D and axis is a tuple,['unlabeled']
10977,open,ndevenish,"I've been working on updating some old API-using code, and appear to have found some inconsistencies/misdocumented API calls. I was using 1.13, but these all seem to be present on the current documentation.

### Reference Counting
[`PyArray_CastToType`](https://docs.scipy.org/doc/numpy-1.14.2/reference/c-api.array.html#c.PyArray_CastToType) appears to steal the `PyArray_Descr*` object it is passed, but isn't documented as such. Many functions are documented as stealing this parameter, so not being there seems to imply that it won't. (I found this out the hard way whilst chasing reference count problems).

In fact, assuming that https://github.com/numpy/numpy/blob/master/numpy/core/code_generators/numpy_api.py is an accurate representation of the API, I looked over everything marked as stealing and there there are several other functions that steal but aren't documented as such:
  - `PyArray_FromScalar`
  - `PyArray_GetField`
  - `PyArray_SetField`
  - `PyArray_CheckFromAny` (though it says derived from `PyArray_FromAny`, which is documented as stealing the parameter, so perhaps this one doesn't count)
  - `PyArray_FromArray` (same)
  - `PyArray_View`
  - `PyArray_AsCArray` (except the parameter listed is documented as an integer parameter?)
  - `PyArray_Zeros`
  - `PyArray_Empty`
  - `PyArray_SetUpdateIfCopyBase`

(`PyArray_EnsureAnyArray`, `PyArray_FromDimsAndDataAndDescr` and `PyArray_FromIter` are listed as stealing, but don't appear to have documentation entries). I didn't double-check anything not listed as stealing.

### Mislabelled parameters

`PyArray_SIZE` and `PyArray_Size` are [labelled](https://docs.scipy.org/doc/numpy-1.14.2/reference/c-api.array.html#c.PyArray_SIZE) as both taking `PyArrayObject*`. However, my compiler appears to complain when `PyArray_Size` is used with such - needing to be cast to a `PyObject*` first.

---

If these are just missing I'm happy to update the documentation myself to add the ""*This function steals a reference to x*"" text to each function, but I'm new to the numpy code and some of the structure confuses me, so I'm not completely certain it's me that is incorrect, or if the documentation is correct but the implementation wrong (in which case, I suspect there's nothing to do but make the documentation match...)
",2018-04-25 22:01:46,,c API documentation inconsistencies,['04 - Documentation']
10956,open,ahaldane,"See some discussion in #10951.

The issue is that the nditer `operands` attribute does not always refer to the actual operand array, but rather to the temporary buffer/copy of the operand array, despite what the nditer docstring says.

This is a problem for code like:
```python
>>> def square(a):
...     with np.nditer([a, None]) as it:
...         for x, y in it:
...             y[...] = x*x
...         return it.operands[1]
```
since the returned value could in principle be a temporary buffer, rather than the newly created 'out' array. In practice, this hasn't been a problem because the out arrays are never buffered, I think.

Because of our recent improvements to the nditer as a context manager, and #10951, there is a second problem that the operands cannot be accessed once the iterator context is exited. For instance, this means the `return` statement above can't currently be placed outside the with statement, even though in principle that is where it should go because the operand is only ""written back to"" at context exit.",2018-04-23 16:17:23,,"nditer ""out"" operands are not always accessible",['01 - Enhancement']
10954,open,eric-wieser,"This loop was removed in #10284 because it was incorrect (#9730), and did not correctly handle the tuple return value of `PyNumber_Divmod`",2018-04-22 23:46:24,,np.divmod is not implemented for object loops,['unlabeled']
10937,open,ahwillia,"I have been leaning on `searchsorted` heavily for one of my projects and was playing around with how to speed things up. I found out that pre-sorting the second input for large arrays greatly speeds up the computation (even taking into account the up front cost of sorting).

```python
x = np.random.randn(5000000)
y = np.random.randn(5000000)
x.sort()

%time np.searchsorted(x, y)
CPU times: user 10.3 s, sys: 36.7 ms, total: 10.4 s
Wall time: 10.4 s

%time y.sort(); np.searchsorted(x, y)
CPU times: user 959 ms, sys: 12.4 ms, total: 971 ms
Wall time: 971 ms
```

I was surprised because from the documentation it didn't seem like `searchsorted` made any assumptions about whether `y` was sorted or not. Is this speedup purely because data locality is better after sorting `y`? Or is there actually an algorithmic reason for this speed up? I tried digging into the C code but couldn't follow where this would be implemented.

Would it make sense to note and explain this behavior in the documentation? Would it make sense to add an additional argument to `searchsorted(x, y, both_sorted=True)`, or something like `merge_sorted(x, y)` which assumes both `x` and `y` are sorted?

See also: https://stackoverflow.com/questions/27916710/numpy-merge-sorted-array-to-an-new-array",2018-04-20 21:09:10,,Big speed up in searchsorted if second input is also sorted,['unlabeled']
10929,open,petered,"Hi all, 

Often I find myself doing things like this:

```
image = get_rgb_image()  # A shape (height, width, 3) array
center_of_image = image[image.shape[0]//4:image.shape[0]*3//4, image.shape[1]//4:image.shape[1]*3//4] 
```

Wouldn't it be nice to be able to do:

```
image = get_rgb_image()  # A shape (height, width, 3) array
center_of_image = image[np.DIM//4:np.DIM*3//4, np.DIM//4:np.DIM*3//4] 
```
Where `np.DIM` would be a stand-in for the dimension along that axis.  It seems like it would

- Make code more readable
- Reduce possibility of bugs due to typing wrong dimension index.
- Allow you to operate on unnamed variables: (e.g. `center_crops = [get_image(filename)[np.DIM//4:np.DIM*3//4, np.DIM//4:np.DIM*3//4]  for filename in image_filenames]`

Would people be in favour of such a change?  It seems like it would involve making a global `np.DIM` object with `__mul__`, `__add__`, etc methods and adding some logic to `ndarray.__getitem__` to handle this.

There's precedent for this in Matlab with the `end` operator.",2018-04-18 12:52:40,,"Feature Suggestion: ""Dimension"" object",['15 - Discussion']
10926,open,kmaehashi,"It seems that the acceptable syntax of `np.einsum` subscripts are different when optimize option is set or not.

```py
>>> import numpy as np

>>> x = np.ones((2, 2, 2))

>>> np.einsum('...->', x, optimize=True)
8.0

>>> np.einsum('...->', x, optimize=False)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/kenichi/.pyenv/versions/local-3.6.3/lib/python3.6/site-packages/numpy/core/einsumfunc.py"", line 948, in einsum
    return c_einsum(*operands, **kwargs)
ValueError: output had too few broadcast dimensions

>>> np.einsum('i...j->ij', x, optimize=True)
array([[ 2.,  2.],
       [ 2.,  2.]])

>>> np.einsum('i...j->ij', x, optimize=False)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/kenichi/.pyenv/versions/local-3.6.3/lib/python3.6/site-packages/numpy/core/einsumfunc.py"", line 948, in einsum
    return c_einsum(*operands, **kwargs)
ValueError: output has more dimensions than subscripts given in einstein sum, but no '...' ellipsis provided to broadcast the extra dimensions.
```",2018-04-18 03:58:53,,BUG: `np.einsum` accepts some subscripts only when optimize=True,"['00 - Bug', 'component: numpy.einsum']"
10904,open,EricCousineau-TRI,"Relates #8952, #8860
Same campaign as #10721, #10897 

When defining user-defined dtypes (using the C API), it can sometimes be a tad confusing to know which conversion to provide to ensure the underlying machinery works properly.

For example, the implementation of `np.ones` relies on on the following operation:
```
multiarray.copyto(a, 1, casting='unsafe')
```
When debugging, it was a tad confusing to know whether to define a conversion for `np.int64` or `np.int` or something else. (I had gotten distracted by https://github.com/pybind/pybind11/issues/1328 - still need to finish out the related PR and test it out.)

Additionally, for identity values for reduction and such, it's also confusing.

I found that `np.trace` was also a tad sticky; after defining conversions for `double` -> my custom dtype, `np.trace` would still fail to find a conversion, even though I had an override defined for `np.add` for my custom dtype. After digging, it seems that the `assign_identity_*` functions relied on `bool` casting (`PyUFunc_GenericReduction -> ... -> PyUFunc_ReduceWrapper -> assign_reduce_identity_zero -> ... -> get_cast_transfer_function`).
Post-#8952, it seems that now the proper conversion to define would be (`int` -> Custom).
(To clarify, I'm not knocking the fix, it does seem like an improvement! I have no issue with defining conversions from both `bool` and `int`.)
(As a side note, also relates #9351 in < v1.14.0: since no overrides were found, it looped.)

That being said, it would be nice if there were hints in the documentation somewhere for compatibility with core NumPy functions; that, or at least some comments in `test_rational.c.src` explaining some of these heuristics?

\cc @eric-wieser @njsmith ",2018-04-13 18:41:12,,User-defined dtype: Can be confusing to know which conversions to provide for compatibility with core methods,"['04 - Documentation', 'component: documentation', 'component: numpy.dtype']"
10889,open,NeilGirdhar,"[See here.](https://stackoverflow.com/questions/32872743/how-do-i-get-the-strides-from-a-dtype-in-numpy)

It would do something like this:

```python
def strides_from_dtype(dtype):
    shape = list(dtype.shape)

    # Make the strides for an array with an itemsize of 1 in C-order.
    tmp_strides = shape[::-1]
    tmp_strides[1:] = list(np.cumprod(tmp_strides[:-1]))
    tmp_strides[0] = 1

    # Adjust it for the real itemsize.
    tmp_strides = dtype.base.itemsize * np.array(tmp_strides)

    # Convert it to a tuple, reversing it back for proper C-order.
    return tuple(tmp_strides[::-1])
```

This works, but it seems inefficient:
```python
strides = np.zeros((), dtype=dtype).strides
```",2018-04-12 05:18:50,,feature request: Consider adding a strides method to np.dtype,"['01 - Enhancement', '23 - Wish List', '15 - Discussion', 'component: numpy.dtype']"
10887,open,EricCousineau-TRI,"Follow-up from #10876, Relates #8892

For certain dtypes that have some constraints on algebra that `np.ndarray` does not normally provide, it seems like it would be convenient to teach `np.array` to possible determine the array type based on the dtypes contained in the array.

From my limited understanding, the downstream user has to explicitly ensure they use the correct wrapping class.

Example: From the example in #10876, I'd like to ensure `np.dot`, `np.matmul`, `np.equal`, etc. follow the algebra that I want. If this is best achieved by an array subclass, it'd be nice to have this automatically determined for me via `np.array` by defining a field like `__array_subclass__` on `Variable`, `Expression`, etc., rather than depending on the user to select the correct array type.

After tinkering some briefly with `pandas`, here's the existing functionality for `DatetimeIndex` + `Timestamp` (maybe not the best example based on algebra, but namely for dtypes and inference):
```
>>> import numpy as np
>>> import pandas as pd
>>> ts = pd.Timestamp('2013-01-01', freq='D')
>>> pd.DatetimeIndex([ts, ts])
DatetimeIndex(['2013-01-01', '2013-01-01'], dtype='datetime64[ns]', freq=None)
>>> np.array([ts, ts])
array([Timestamp('2013-01-01 00:00:00', freq='D'),
       Timestamp('2013-01-01 00:00:00', freq='D')], dtype=object)
```",2018-04-11 16:31:04,,ndarray subclass selection: Add field to user-defined dtypes / dtype=object to automatically determine array type?,"['01 - Enhancement', 'component: numpy.ufunc', 'component: numpy.dtype']"
10866,open,akors,"Hi, I was trying to read directly from a compressed file:

```python
data = np.array([9, 8, 7, 6, 5, 4, 3, 2, 1])
dt = data.type
with gzip.open(""datafile.gz"", ""wb"") as outfile:
    outfile.write(data.tobytes())

with gzip.open(""datafile.gz"", ""rb"") as infile:
     data = np.fromfile(infile, dtype=dt)
     print(data)
```
Unfortunately, this returns garbage data:
`[6542475788951259935 7594864974085029634 1008947487324530028
 3113290099057413416  691954333390038676 6865743131250406218]`

The workaround is to load the data into a buffer first, and then let `np.frombuffer` read it:

```python
with gzip.open(""datafile.gz"", ""rb"") as infile:
     data = np.frombuffer(infile.read(), dtype=dt)
     print(data)
```
Returns: `[9 8 7 6 5 4 3 2 1]` as expected.

I think silently returning corrupt data is pretty much the worst possible behaviour in this case. Either of these options would be an improvement:

- Teach `np.fromfile()` to read from `gzip` (and possible other compression formats) file objects
- Raise an error explaining that NumPy can't deal with this kind of file object
- At least clearly document what kind of ""file object"" `np.fromfile()` expects. 

Right now, the `file` parameter of `np.fromfile` is [documented as](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.fromfile.html):

> file : file or str
>    Open file object or filename.

And the file object returned from `gzip.open` *is* a file object, but apparently not the right kind. That's pretty confusing and should at least be changed in the documentation.

Operating System: Fedora 27, 64-Bit
Python version: 3.6.4
NumPy version: 1.13.3",2018-04-09 16:02:34,,np.fromfile accepts a gzip file object but silently returns corrupt data,['00 - Bug']
10861,open,EricCousineau-TRI,"Recently went to a bit of a debugging spiel to figure out the behavior of `np.equal` and `np.not_equal` for `dtype=object`, and then realized that this behavior spits out a `DeprecationWarning` / `FutureWarning`.

I wanted to filter all NumPy deprecation warnings into errors (at least for the time being), but it seems that the warning mechanism does not indicate that the warning originated from NumPy:
https://github.com/numpy/numpy/blob/1368cbb69/numpy/core/include/numpy/ndarrayobject.h#L239-L240

Because of this, it is hard to filter for all NumPy warnings. Since it originates from a C call, you cannot filter based on the `module=` regex provided by `warnings.filterwarnings`.

It would be nice if a separate class were used, something like `PyArray_DeprecationWarning` that inherits from `PyExc_DeprecationWarning`, such that filtering could be done via something like `warnings.simplefilter(""default"", category=numpy.NumpyDeprecationWarning`)`.

Relates comment: https://github.com/RobotLocomotion/drake/pull/8500#issuecomment-379586908",2018-04-08 22:24:09,,NumPy-specific deprecation warnings are difficult to filter in a general fashion,['unlabeled']
10818,open,eric-wieser,"Discovered in https://github.com/numpy/numpy_stubs/pull/14#discussion_r176951593

```python
>>> b = np.bytes_(b'test')
>>> b.real, b.imag
('test', '')

>>> d = np.datetime64(dt.datetime.now())
>>> d.real, d.imag
(numpy.datetime64('2018-03-26T15:22:14.672804'), numpy.datetime64('1970-01-01T00:00:00.000000'))
```

It seems like we should give an error or warning here, in both cases.",2018-03-29 01:30:37,,Behaviour of np.generic.real and np.generic.imag is ... really imaginative ,['triaged']
10801,open,jaimefrio,The code parsing and mapping indices in einsum is cryptic and hard to follow. We should add meaningful descriptions and examples of how indices are parsed and how the information is stored and processed before actually doing any sums or products on the values.,2018-03-26 06:57:54,,DOC: Document einsum's index parsing and mapping code,"['04 - Documentation', 'component: numpy._core']"
10799,open,ahaldane,"Just a reminder we should do this, found in #10774.

Probably easiest to fix after #10444 is merged, since that PR defines some new/modified `HAVE_LDOUBLE_**` macros which could be used.",2018-03-26 00:55:45,,Make SQRT_MIN for long-double platform specific,['unlabeled']
10709,open,JonathanShor,"As there already exists a mechanism in numpy to manage warnings for possibly anomalous inputs, it seems odd that it doesn't work for all warnings. For example, `nanmean` raises a warning if given a slice of all nan, and apparently the only way to silence it is to catch it with the python warnings mechanism. `np.errstate`, despite claiming to handle divide by zero errors, does not:

```
Python 3.6.2 
In [1]: import numpy as np
In [2]: np.__version__
Out[2]: '1.13.1'

In [3]: allnan = np.array([np.nan, np.nan])
In [4]: with np.errstate(divide='ignore', invalid='ignore'):
   ...:     print(np.nanmean(allnan))
   ...:     
/Users/jonathanshor/anaconda2/envs/Python3/bin/ipython:2: RuntimeWarning: Mean of empty slice
  if __name__ == '__main__':
nan
```

I would suggest this could fall under the divide case, but invalid seems plausible as well.",2018-03-08 17:35:00,,"Control ""Mean of empty slice"" RunTimeWarning with np.errstate",['unlabeled']
10705,open,mick-d,"When using `numpy.linalg.norm` with `ord` different from 2, for example 3, then I expected the norm to be computed according to the documentation, i.e. as `sum(abs(x)**ord)**(1./ord)`

But instead I got:
```
ValueError                                Traceback (most recent call last)
<ipython-input-784-3344f4d8d663> in <module>()
----> 1 nnorm((distpdfs_diverg['BC']/n_samples) - np.eye(n_bundles), 3)

/usr/lib/python3.5/site-packages/numpy/linalg/linalg.py in norm(x, ord, axis, keepdims)
   2247             ret = _multi_svd_norm(x, row_axis, col_axis, sum)
   2248         else:
-> 2249             raise ValueError(""Invalid norm order for matrices."")
   2250         if keepdims:
   2251             ret_shape = list(x.shape)

ValueError: Invalid norm order for matrices.
```
Looking at the source code (numpy 1.13, numpy 1.14, and [current](https://github.com/numpy/numpy/blob/e3fe42f0473d41f97db5a40e3ddd067391df57f0/numpy/linalg/linalg.py#L2132)) it seems it is simply the norm is simply not implemented for `ord` greater than 2 contrarily to what the doc says. Would that be something difficult to do?
",2018-03-08 09:10:16,,Implementation of matrix norm for order greater than 2,['01 - Enhancement']
10700,open,alexandre-marinkovic,"I'm using in my application, numpy and AMCL library. (https://en.wikipedia.org/wiki/AMD_Core_Math_Library)

When I'm launching tests of numpy I have a crash which happens when calling np.linalg.lapack_lite.xerbla() in linalg\tests\test_linalg.py.
It seems we have a symbol clash between numpy and the AMCL library.
Indeed, you will see in the callstack just below that python in numpy/linalg/lapack_litemodule.c uses the function xerbla and it uses the one of libacml_mp.so not the one of numpy.
It produces a crash.

0x000000384728980b in memcpy () from /lib64/libc.so.6
1  0x00007f2f88fe8c76 in ?? () from /usr/lib64/libgfortran.so.3
2  0x00007f2f8a0843b6 in xerbla_ () from /home/buildsystem/DailyBuild/builds/lcl_python35/product/bin/arch-LinuxAMD64-Optimize/../../lib/arch-LinuxAMD64-Optimize/libacml_mp.so
3  0x00007f2e56745126 in lapack_lite_xerbla (__NPY_UNUSED_TAGGEDself=<value optimized out>, args=<value optimized out>) at numpy/linalg/lapack_litemodule.c:294
4  0x00007f2e5ddcda39 in PyCFunction_Call (func=0x7f2e56963c18, args=0x7f2f4417d048, kwds=<value optimized out>) at Objects/methodobject.c:109
5  0x00007f2e5de4718b in call_function (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:4705
6  PyEval_EvalFrameEx (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:3236
7  0x00007f2e5de47f8e in _PyEval_EvalCodeWithName (_co=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, args=<value optimized out>, argcount=0, kws=<value optimized out>, kwcount=0, defs=0x0,
    defcount=0, kwdefs=0x0, closure=0x0, name=0x0, qualname=0x0) at Python/ceval.c:4018
8  0x00007f2e5de48118 in PyEval_EvalCodeEx (_co=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, args=<value optimized out>, argcount=<value optimized out>, kws=<value optimized out>, kwcount=0,
    defs=0x0, defcount=0, kwdefs=0x0, closure=0x0) at Python/ceval.c:4039",2018-03-07 11:15:02,,symbol clash between amcl and numpy on Linux.,['unlabeled']
10693,open,alexhsamuel,"It appears that if you create an extension type that derives `PyGenericArrType` and doesn't define `nb_int`, NumPy crashes when calling `int()` on an instance of your type.

This is straightforward to demonstrate with `test_rational.c`: replace `pyrational_int` with 0 on line 631 when defining `nb_int`.  Then,
```py
from numpy.core.test_rational import rational
r = rational(2, 3)
int(r)
```

This results in an infinite recursion.  On OS/X, this is the _bottom_ of the stack.
```
    frame #262073: 0x000000010000dfeb python`PyNumber_Long + 123
    frame #262074: 0x0000000103154c3e multiarray.cpython-36m-darwin.so`array_int + 78
    frame #262075: 0x0000000103163eba multiarray.cpython-36m-darwin.so`gentype_int + 42
    frame #262076: 0x000000010006ccd4 python`_PyLong_FromNbInt + 52
    frame #262077: 0x000000010000dfeb python`PyNumber_Long + 123
    frame #262078: 0x0000000103154c3e multiarray.cpython-36m-darwin.so`array_int + 78
    frame #262079: 0x0000000103163eba multiarray.cpython-36m-darwin.so`gentype_int + 42
    frame #262080: 0x000000010006ccd4 python`_PyLong_FromNbInt + 52
    frame #262081: 0x000000010000dfeb python`PyNumber_Long + 123
    frame #262082: 0x000000010006059b python`long_new + 139
    frame #262083: 0x00000001000b74ef python`type_call + 47
    frame #262084: 0x0000000100009df1 python`_PyObject_FastCallDict + 177
    frame #262085: 0x0000000100163348 python`call_function + 392
    frame #262086: 0x0000000100160f4c python`_PyEval_EvalFrameDefault + 47100
    frame #262087: 0x0000000100154589 python`_PyEval_EvalCodeWithName + 425
    frame #262088: 0x00000001001ac63c python`PyRun_FileExFlags + 252
    frame #262089: 0x00000001001abdee python`PyRun_SimpleFileExFlags + 366
    frame #262090: 0x00000001001d1dd6 python`Py_Main + 3718
    frame #262091: 0x0000000100001e7d python`main + 509
    frame #262092: 0x00007fff4f91c145 libdyld.dylib`start + 1
    frame #262093: 0x00007fff4f91c145 libdyld.dylib`start + 1
```

If the type doesn't subclass `PyGenericArrType`, this does not appear to occur.

Similar failure and similar stack trace on Linux/GCC/gdb.

Tested on 1.14, 1.13, 1.12 (the latter has a different stack trace) with Python 3.
",2018-03-05 14:26:22,,segfault with extension dtype not defining nb_int,"['00 - Bug', 'component: numpy.dtype']"
10685,open,shoyer,"This is particularly inconsistent because this isn't the case for datetime64:
```
In [12]: issubclass(np.timedelta64, np.integer)
Out[12]: True

In [13]: issubclass(np.datetime64, np.integer)
Out[13]: False
```

Looking at the subclass hierarchy, we observe:
```
In [15]: np.datetime64.mro()
Out[15]: [numpy.datetime64, numpy.generic, object]

In [16]: np.timedelta64.mro()
Out[16]:
[numpy.timedelta64,
 numpy.signedinteger,
 numpy.integer,
 numpy.number,
 numpy.generic,
 object]
```

I would suggest that we make `np.timedelta64` also inherit directly from `np.generic`.

I'm sure there was a reason for this originally, but in my experience this sort of sloppiness around type casting leads to bugs and confusion (e.g., see https://github.com/pydata/xarray/issues/1952).",2018-03-02 16:45:19,,np.timedelta64 should not be an subclass of np.integer,['component: numpy.datetime64']
10673,open,merraksh,"I'm using Python 3.5.3 and NumPy 1.14.1 on a Linux Debian . I have an Expression class that I use to model decision problems. For variables x,y I can have an object e defined and changed as follows:

e = 2*x
e += 3*y

I wrote a module using Python's C extension, and call PyNumber_InPlaceAdd() to implement the second instruction. However, digging through this call with gdb it seems that array_inplace_add calls PyArray_GenericInplaceBinaryFunction (number.c:653) but passes n_ops.add, which becomes PyNumber_Add once we get to trivial_three_operand_loop(). 

The problem with using _Add instead of _InPlaceAdd is that the in place addition has a quadratic complexity: if e is a NumPy array of n expressions, say all equal to x[0], then adding x[i] in a loop for i from 1 to n requires, at iteration i, copying n expressions all equal to x[0] + ... + x[i-1], then adding x[i] to each of them. The latter operation is trivial, but the first one is of complexity O(i), which means that the whole loop has complexity O(n^3) when it should really be O(n^2). The example below should expose this behavior.

```
import mymodule
import numpy

n = 1000
x = numpy.array([mymodule.variable() for i in range(n)])
s = 3*x

for i in range(n):
  s += x[i] * numpy.random.random(n)
```

I'm showing the call stack below. I would like to understand if there is a way to add in-place operations to n_ops, as I don't see other ways to avoid the greater complexity.

```
#0  PyNumber_Add (v=<xpress.linterm at remote 0x7fffe69b0ae0>, w=<xpress.linterm at remote 0x7fffe2f54e00>) at ../Objects/abstract.c:891
#1  0x00007fffe94aa184 in PyUFunc_OO_O (args=<optimized out>, dimensions=<optimized out>, steps=<optimized out>, func=0x5555555ee2d1 <PyNumber_Add>)
    at numpy/core/src/umath/loops.c.src:580
#2  0x00007fffe9600096 in trivial_three_operand_loop (op=op@entry=0x7fffffffcbb0, innerloop=0x7fffe94aa0f8 <PyUFunc_OO_O>, 
    innerloopdata=0x5555555ee2d1 <PyNumber_Add>) at numpy/core/src/umath/ufunc_object.c:1101
#3  0x00007fffe9604040 in execute_legacy_ufunc_loop (ufunc=ufunc@entry=0x555555ddaf90, trivial_loop_ok=trivial_loop_ok@entry=1, op=op@entry=0x7fffffffcbb0, 
    dtypes=dtypes@entry=0x7fffffffc940, order=NPY_KEEPORDER, buffersize=8192, arr_prep=0x7fffffffca40, arr_prep_args=0x0) at numpy/core/src/umath/ufunc_object.c:1472
#4  0x00007fffe9609216 in PyUFunc_GenericFunction (ufunc=ufunc@entry=0x555555ddaf90, 
    args=args@entry=(<numpy.ndarray at remote 0x7fffe6361ec0>, <numpy.ndarray at remote 0x7fffe6361f40>, <numpy.ndarray at remote 0x7fffe6361ec0>), 
    kwds=kwds@entry=0x0, op=op@entry=0x7fffffffcbb0) at numpy/core/src/umath/ufunc_object.c:2495
#5  0x00007fffe9609721 in ufunc_generic_call (ufunc=0x555555ddaf90, 
    args=(<numpy.ndarray at remote 0x7fffe6361ec0>, <numpy.ndarray at remote 0x7fffe6361f40>, <numpy.ndarray at remote 0x7fffe6361ec0>), kwds=0x0)
    at numpy/core/src/umath/ufunc_object.c:4137
#6  0x00005555555f028f in PyObject_Call (func=func@entry=<numpy.ufunc at remote 0x555555ddaf90>, 
    arg=arg@entry=(<numpy.ndarray at remote 0x7fffe6361ec0>, <numpy.ndarray at remote 0x7fffe6361f40>, <numpy.ndarray at remote 0x7fffe6361ec0>), kw=kw@entry=0x0)
    at ../Objects/abstract.c:2166
#7  0x00005555555f0ea2 in PyObject_CallFunctionObjArgs (callable=<numpy.ufunc at remote 0x555555ddaf90>) at ../Objects/abstract.c:2446
#8  0x00007ffff3e1b713 in PyArray_GenericInplaceBinaryFunction (m1=m1@entry=0x7fffe6361ec0, m2=m2@entry=<numpy.ndarray at remote 0x7fffe6361f40>, op=<optimized out>)
    at numpy/core/src/multiarray/number.c:290
#9  0x00007ffff3e1c079 in array_inplace_add (m1=0x7fffe6361ec0, m2=<numpy.ndarray at remote 0x7fffe6361f40>) at numpy/core/src/multiarray/number.c:653
#10 0x00005555555ed022 in binary_iop1 (v=v@entry=<numpy.ndarray at remote 0x7fffe6361ec0>, w=w@entry=<numpy.ndarray at remote 0x7fffe6361f40>, 
    iop_slot=iop_slot@entry=152, op_slot=op_slot@entry=0) at ../Objects/abstract.c:991
#11 0x00005555555ee51b in PyNumber_InPlaceAdd (v=<numpy.ndarray at remote 0x7fffe6361ec0>, w=<numpy.ndarray at remote 0x7fffe6361f40>) at ../Objects/abstract.c:1044
```

Thank you.

[edited to correct k with n]",2018-02-27 14:11:18,,PyNumber_InPlaceAdd running PyNumber_Add in inner loop,"['01 - Enhancement', '15 - Discussion']"
10645,open,davidmascharka,"NumPy 1.13.3, Python 3.6.2, Ubuntu 16.04

```python
>>> str(round(1337.09997559, 1))
'1337.1'

>>> f'{round(1337.09997559, 1)}'
'1337.1'

>>> f'{round(np.float32(1337.09997559), 1)}'
'1337.0999755859375'

>>> str(round(np.float32(1337.09997559), 1))
'1337.1'

>>> f'{round(np.float64(1337.09997559), 1)}'
'1337.1'

>>> str(round(np.float64(1337.09997559), 1))
'1337.1'
```

",2018-02-22 16:17:50,,Inconsistent f-string output for rounded np.float32,['unlabeled']
10626,open,jnothman,"```py
>>> import io, numpy as np
>>> np.genfromtxt(io.BytesIO(b'-1,a\n2,b\n3,?'), dtype=None, delimiter=',', missing_values='?', usemask=True)['f0']
```

Expected:

```py
masked_array(data = [-1 2 3],
             mask = [ False False False],
       fill_value = 999999)
```

Got:

```py
masked_array(data = [-- 2 3],
             mask = [ True False False],
       fill_value = 999999)
```

(Second column being float, `b'-1,.5\n2,.67\n3,34'`, also triggers the issue)

Note that if all columns of input are integer, this does not happen:

```py
>>> np.genfromtxt(io.BytesIO(b'-1,1\n2,2\n3,3'), dtype=None, delimiter=',', missing_values='?', usemask=True)
masked_array(data =
 [[-1 1]
 [2 2]
 [3 3]],
             mask =
 [[False False]
 [False False]
 [False False]],
       fill_value = 999999)
```
",2018-02-19 00:12:21,,Strange behaviour of -1 in genfromtxt in integer columns,"['00 - Bug', 'component: numpy.lib', 'component: numpy.ma']"
10605,open,kwohlfahrt,"`python3 --version`: `Python 3.6.4`
`np.version.version`: `1.13.3`
OS: Debian testing

I have a subclass of `ndarray`, which has a reference to an object that *must* remain alive for the lifetime of the array:

```
In [1]: from weakref import finalize

In [2]: import numpy as np

In [3]: class Bar:
   ...:     def __init__(self):
   ...:         finalize(self, print, ""deleting bar"")
   ...:         

In [6]: class Foo(np.ndarray):
   ...:     def __new__(cls, shape, dtype, buf, bar):
   ...:         obj = super().__new__(cls, shape, dtype, buf)
   ...:         obj.bar = bar
   ...:         finalize(obj, print, ""deleting foo"")
   ...:         return obj
   ...:     def __array_finalize__(self, obj):
   ...:         if obj is None: return
   ...:         self.bar = getattr(obj, 'bar', None)
```

However, the `bar` object is deleted before the array, which is unexpected and breaks the functionality:

```
In [8]: f = Foo((1,), 'uint8', None, Bar()); del f
deleting bar
deleting foo
```

Using an object which is not an `ndarray` subclass has the expected cleanup order:

```
In [4]: class Baz:
   ...:     def __init__(self, bar):
   ...:         self.bar = bar
   ...:         finalize(self, print, ""deleting baz"")
   ...:         

In [6]: b = Baz(Bar()); del b
deleting baz
deleting bar
```

Is it possible to achieve this order with an `ndarray` subclass?",2018-02-16 14:53:58,,Attributes of ndarray subclasses deleted before array,['unlabeled']
10590,open,mehrdadn,"Could you please rename the `offset` parameter in `numpy.frombuffer` to `byte_offset`? It seems like a small thing, yet I lost some ~4 hours debugging a program only to find that the `offset` I had seen in Visual Studio was in fact not an item offset, but a byte offset... and I imagine I might not be the first person to make this mistake. The inconsistency with `length`, albeit justifiable, makes this especially confusing and counterintuitive.
(It may be useful to add a `**kwargs` parameter to allow backwards-compatibility in case anyone passes this parameter by name. In that case I would assume they should not be specified simultaneously.)
Thank you.",2018-02-15 03:03:59,,numpy.frombuffer() offset parameter is is confusing,['unlabeled']
10582,open,sdwebb,"LAPACK has multiple algorithms for computing eigensystems for symmetric matrices, although NumPy only seems to use the default method. It would be useful to expose this option so that users can select which approach to use since some may be better than others for large matrix problems.",2018-02-13 22:31:33,,Expose solver options in LAPACK for eigh,['unlabeled']
10581,open,BrianJKoopman,"# Description of Issue
numpy's genfromtxt `skip_footer` parameter specifies, to quote [the documentation](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.genfromtxt.html), ""[t]he number of lines to skip at the end of the file.""

However, if the footer includes blank lines these are implicitly skipped in addition to the number of lines specified. So if any blank lines are included in the footer that the user wishes the skip then an additional X lines are skipped, where X is the number of blank lines in the footer.

## Example
To demonstrate, consider this example file, `genfromtxt_demo.txt`:
```
1


2
3
4
5
6
7
8
9


10
```
If we specify to skip 3 lines in both the header and footer I would expect this to skip these lines:

**Header:**
```
1


```
**Footer:**
```


10
```

This would result in an array containing the numbers 2-9, having skipped 1 and the two blank line following it, as well as having skipped the 10 at the end and the blank lines proceeding it. This would give:
```python
[2. 3. 4. 5. 6. 7. 8. 9.]
```

However, what happens is the footer is considered to be the last three lines, yet the last 5 lines are skipped, the three lines we specified plus an additional two lines (the number of blank lines contained within the three lines that we specify as the footer). 

```python
>>> print np.genfromtxt(""/tmp/genfromtxt_demo.txt"", skip_header=3, skip_footer=3)
[2. 3. 4. 5. 6. 7.]
```

Only blank lines that are within the range specified by `skip_footer` add to the amount skipped. This results in skipping more lines at the end of the file than intended if the footer contains blank lines.

This contrasts with the `skip_header` behavior, which does not skip any extra lines if the header contains blank lines, which again, is the expected behavior. 

# Reproducing This Behavior
This short script with reproduce this behavior:

```python
import numpy as np

with open(""/tmp/genfromtxt_demo.txt"", ""w"") as f:
    f.write(""1\n\n\n"")
    for i in range(2,10):
        f.write(""%s\n""%i)
    f.write(""\n\n"")
    f.write(""10\n"")

print(np.genfromtxt(""/tmp/genfromtxt_demo.txt"", skip_header=3, skip_footer=3))
```
Varying the number of new lines before the ""10"" and the `skip_footer` parameter will show how additional lines can be skipped if more blank lines are included in the footer. For example, if another `""\n""` is added and `skip_footer=4`, the array produced by `genfromtxt` would end at 6.

# Version Info
This was run using NumPy version 1.14.0 with both Python 2.7.14 and Python 3.6.4.",2018-02-13 20:22:40,,genfromtxt skip_footer skips additional lines if the footer contains blank lines,['unlabeled']
10579,open,matteovit,"Hello,
I am using bitwise_and to mask each individual element of a numpy array, but it seems to ""corrupt"" the dtype.type on Windows, while on Linux it seems fine.

Here is an example on how to trigger the issue:
https://gist.github.com/matteovit/64414c7c70b973bac6f1b8e2ca02ae78#file-bitwisetest-py

the bb test is OK on Linux (Ubuntu 16.04, x86 64 bit) while fails on Windows (Windows 10 Pro, x86 64 bit). In both cases with python 2.7 64 bit and numpy 1.14.

Thanks!
Matteo",2018-02-13 11:57:05,,bitwise_and corrupting dtype.type on windows,['unlabeled']
10574,open,glefebvr,"The problem is illustrated by this code :

```
import numpy as np 

a=np.array([1,2])
af=np.floor(a)
print(np.shares_memory(a,af))

ar=np.round(a)
print(np.shares_memory(a,ar))

```

I would expected that the returned object was indepent of (no link with) the input data ... ",2018-02-12 15:22:19,,numpy.round may return a view of the input array,['unlabeled']
10570,open,eric-wieser,"Caused by VOID_compare falling back on STRING_compare for subarray dtypes.

It should probably do a C-order elementwise compare, rather than a bytewise compare.

Right now, the result is dependent on the platform endianness:
```python
>>> a = np.array([[256, 1], [2, 257]])
>>> a_swap = a.newbyteorder().byteswap()

>>> b = a.view([('f', (a.dtype, a.shape[-1]))]).squeeze(axis=-1)
>>> b_swap = a_swap.view([('f', (a_swap .dtype, a.shape[-1]))]).squeeze(axis=-1)

>>> b.sort(); b
array([([256,   1],), ([  2, 257],)], 
      dtype=[('f', '<u2', (2,))]) 
>>> b_swap.sort(); b_swap
array([([  2, 257],), ([256,   1],)], 
      dtype=[('f', '>u2', (2,))])
```",2018-02-10 23:07:51,,"BUG: Subarray dtypes sort based on their raw bytes, not their element values","['00 - Bug', 'component: numpy.dtype']"
10535,open,RustyKettle,"I preformed a 'sudo apt-get python-numby' and 'sudo apt-get python-numby-doc'. Neither contain the numby.i SWIG interface file, even though the manual states they should be in numby/tools. This tools directory doesn't exist. Is there a way I can build this interface file myself, or can it be included?  ",2018-02-06 23:33:51,,[Raspbian] Missing numpy.i file,['unlabeled']
10527,open,magonser,"This issue was suggested in dask/dask#3109

The current [definition](http://docs.scipy.org/doc/numpy/reference/c-api.generalized-ufuncs.html) of generlized ufunc signature doesn't contain some corner cases. 
1) Return tuples of length 1
2) Functions without any arguments

E.g. return arguments in Python can be either a scalar or tuples, even if only length 1. Python function return statements handle this consistently, i.e.
```python
def foo():
    return 1,

a, = foo() 
```
I don’t understand, why numpy.gufunc signature forbids length 1 return tuples. It doesn’t limit the normal usage of returning scalars. 

Further other corner cases, such as the signatures `""->()""` (no input) are not valid signatures. While While it might seem funny to wrap a non-gufunc without any arguments, covering these two corner cases becomes important if more general workflows patters shall be applied to usage of the function suggested in Issue #10526
",2018-02-05 19:07:22,,Expand definition of generalized ufunc signature for corner cases,"['01 - Enhancement', '15 - Discussion', 'component: numpy.ufunc']"
10526,open,magonser,"This issue was suggested in dask/dask#3109

My current understanding is that `numpy.vectorize` provides a way to 
1) provide a Python function, 
2) assign it a signature with information about core dimensions, 
3) bind it input data, where two things happen
    - call `__array__ufunc__`, if present, and/or
    - broadcast loop dimensions (according to signature) of input arrays against each other. 
4) Iterative calls of the Python function over all loop dimension entries

I would like to suggest to implement an additional wrapper, just like `numpy.vectorize`, which does all the steps above, except step 4). I.e. a Python function could be wrapped as gufunc and given a signature, and when binding input the same Step 3) is applied. 

The benefit is, that same data binding methodology and interface is used, if the user already provides a vectorized implementation of Python function. It becomes especially important for interoperability with other libraries, e.g. `dask`.",2018-02-05 19:00:36,,"Implement `gufunc(pyfunc, ...)` similar to `numpy.vectorize` but without the vectorize part","['01 - Enhancement', '15 - Discussion', 'component: numpy.ufunc']"
10521,open,ogauthe,"Hello,

I would like to sort an array according to lexical order, sorting by first the second column. [np.lexsort](https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html) is the function to use, but it seems not to handle the kwarg axis.

`>>> import numpy as np`
`>>> a = np.array([[0,1],[1,0],[0,0],[0,-1],[0,1],[1,-1]])`
`>>> a.ndim`
`2`
`>>> np.lexsort(a.T,axis=1)`
`Traceback (most recent call last):`
 `File ""<stdin>"", line 1, in <module>`
`ValueError: axis(=1) out of bounds`


Info version :
Debian Python 3.5.
`>>> np.__version__`
`'1.12.1'`

The same bug happens with Anaconda python and numpy.

The function `np.argsort` does not suffer from this problem, but is not what I want. In my case, I could use `np.lexsort(a.T[::-1])` as a workaround.",2018-02-04 17:13:02,,error in np.lexsort axis kwarg,['04 - Documentation']
10520,open,fujiisoup,"`np.argmin` returns the position of the first occurrence of `nan` if array has `nan` entries,
```python
In [2]: np.argmin([1, np.nan, 3])
Out[2]: 1
```
Is it an intended behavior?
From the analogy of `min`, which return `nan`,
```
In [3]: np.min([1, np.nan, 3])
Out[3]: nan
```
I expected `argmin` also returns `nan` for such a case.
pandas does this.

(This behavior might be said as expected also in the sense that `x[np.argmin()] == np.min(x)` is satisfied.)

from pydata/xarray#1866, pydata/xarray#1886
Also related to #5110",2018-02-04 14:02:13,,argmin/argmax should return nan if array has nan entries?,['unlabeled']
10514,open,astrofrog,"Currently, ``np.linspace`` doesn't work when given ``np.datetime64`` objects:

```python
In [3]: d1 = np.datetime64(100, 'D')

In [4]: d2 = np.datetime64(200, 'D')

In [5]: d = np.linspace(d1, d2, 10)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-5-a6c4f52b0799> in <module>()
----> 1 d = np.linspace(d1, d2, 10)

~/tmp/numpy/numpy/core/function_base.py in linspace(start, stop, num, endpoint, retstep, dtype)
    106     # Convert float/complex array scalars to float, gh-3504
    107     # and make sure one can use variables that have an __array_interface__, gh-6634
--> 108     start = asanyarray(start) * 1.0
    109     stop  = asanyarray(stop)  * 1.0
    110 

TypeError: ufunc multiply cannot use operands with types dtype('<M8[D]') and dtype('float64')
```

It would be nice for this to work out of the box.",2018-02-02 18:27:02,,ENH: support np.datetime64 in linspace,"['01 - Enhancement', 'component: numpy.datetime64']"
10513,open,polwel,"Consider this:
```python
import numpy as np

a = np.arange(10).astype([('col1', np.int32)])
b = np.arange(10).astype([('col2', np.float32)])

c = np.rec.fromarrays((a,b))
print(c.dtype)
```

```
(numpy.record, [('f0', 'V4'), ('f1', 'V4')])
```

I'd expect that the names and dtypes would be taken from the original arrays. Instead it's now a binary mess.

Running version 1.14.",2018-02-02 17:41:14,,`rec.fromarrays` should infer dtype and name from the given arrays,['00 - Bug']
10496,open,yarikoptic,"Originally spotted on Debian mips64el platform as a somewhat surprising message with numpy 1.13.3

```shell
(sid_mips64el-dchroot)yoh@eller:~/pymvpa2-2.6.4$ python -c 'import numpy as np; print(np.arange(-1, -1+0, np.float16(0.)))'                                                  
-c:1: RuntimeWarning: invalid value encountered in true_divide
Traceback (most recent call last):    
  File ""<string>"", line 1, in <module>                                                            
ValueError: array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size.
```
which lead to discover that for every new type a new message would come about on x64 laptop:

```shell
$> python -c 'import numpy as np; print(np.arange(-1, -1+0, np.float(0.)))'
Traceback (most recent call last):    
  File ""<string>"", line 1, in <module>   
ZeroDivisionError: float division by zero
                                                     
$> python -c 'import numpy as np; print(np.arange(-1, -1+0, np.float64(0.)))'
-c:1: RuntimeWarning: invalid value encountered in double_scalars
[]
                                                     
$> python -c 'import numpy as np; print(np.arange(-1, -1+0, np.float32(0.)))'
-c:1: RuntimeWarning: invalid value encountered in true_divide

```

testing on current master (an informative pre-removal-numpybook-5689-gbb7b12672 state according to the git-describe ;-)), it is a bit better but still varying between ZeroDivision and ValueError

```shell
$> PYTHONPATH=. python -c 'import numpy as np; print(np.arange(-1, -1+0, 0.))'
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ZeroDivisionError: float division by zero

$> PYTHONPATH=. python -c 'import numpy as np; print(np.arange(-1, -1+0, np.float(0.)))'
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ZeroDivisionError: float division by zero

$> PYTHONPATH=. python -c 'import numpy as np; print(np.arange(-1, -1+0, np.float64(0.)))'
-c:1: RuntimeWarning: invalid value encountered in double_scalars
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ValueError: arange: cannot compute length

$> PYTHONPATH=. python -c 'import numpy as np; print(np.arange(-1, -1+0, np.float32(0.)))'
-c:1: RuntimeWarning: invalid value encountered in true_divide
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ValueError: arange: cannot compute length
```

edit1: FWIW -- I've verified that result on mips64el is consistently inconsistent with x86 platform in current master
Shouldn't it be even more consistent one way or another?",2018-01-31 05:32:48,,varying Errors while changing the type of 0 as a step in arange,['unlabeled']
10493,open,pseth,"I was wondering if it would be possible to add a symmetrisation function to numpy that would take an existing array/matrix and return a symmetric version of it:

```
import numpy as np
a = np.array([[1,0,3],[2,4,5],[0,0,6]]) 
print('a : ')
print(a)
symmetric_a = a + a.transpose() - np.diagflat(a.diagonal())
print('symmetrised a : ')
print(symmetric_a)
```
gives
```
a : 
[[1 0 3]
 [2 4 5]
 [0 0 6]]
symmetrised a : 
[[1 2 3]
 [2 4 5]
 [3 5 6]]
```

This has been raised as an stackoverflow question, and my solution is similar to that reported:
https://stackoverflow.com/questions/2572916/numpy-smart-symmetric-matrix#2573982

This solution is relatively simple, but it would be nice to have a built-in function that does the relevant checks on consistency and handling of NaNs for example.

PS. I apologise if this issue was raised before -- I did some basic searches through the issues and couldn't see anything.",2018-01-30 10:55:58,,Built-in symmetrisation of numpy arrays/matrices,['unlabeled']
10491,open,ahaldane,"On ppc64 and ppc64le systems, glibc 2.23-or-less has a bug in malloc, that it only guarantees alignment to 8 bytes, even though it should be 16 bytes.

https://sourceware.org/bugzilla/show_bug.cgi?id=6527

This is a problem for the long-double (float128) and sometimes complex128 type which should be aligned to 16 bytes. This means that when using long-double arrays, many internal numpy computations will randomly fail on ppc64, depending on what alignment malloc returned. This causes sporadic numpy unit test failures and sometimes outright crashes. The test `numpy.core.tests.test_numeric TestRequire.test_require_each` commonly fails, though not consistently.

Users on ppc64 should upgrade to glibc 2.24 or later.

(Discovered in #10443)",2018-01-29 19:38:51,,"On ppc64, malloc gives misaligned buffers for glibc <= 2.23.",['unlabeled']
10490,open,durian888,"Reading the documentation on developing a c extension that can handle numpy arrays it is not at all clear which  headers need to be included and why. It would be very helpful if the documentation could be extended to include this information. Also it would be useful if the code example also indicated which header files were used - and why. Should it be the case that this information is in fact present in the documentation, please take this request as an indication that the documentation about header files needs to be better sign posted.

Thank you 
Dominic
",2018-01-29 19:38:37,,Enhance Numpy c-api documentation to make it clear which headers are needed,['04 - Documentation']
10469,open,giladbeeri,"Numpy 1.12.1, calling `numpy.argmax()` on a complex vector doesn't return the index of the complex item with the biggest magnitude. Calling `numpy.argmax(numpy.abs())` does work.

If this isn't a bug, I think it should be documented.",2018-01-25 06:31:23,,numpy.argmax() doesn't behave intuitively with complex vectors,['04 - Documentation']
10468,open,dbstein,"`A.dot(B)` is very slow for the case of A real and B complex (10-50x compared to A real/B real, A complex/B complex, and A complex/B real).  The behavior persists across different installations on different operating systems with different BLAS/LAPACK setups. An example:

```python
import numpy as np

A = np.random.rand(10000,1000) + 1j*np.random.rand(10000,1000)
B = np.random.rand(1000) + 1j*np.random.rand(1000)
Ar = A.real.copy()
Br = B.real.copy()

%timeit Ar.dot(Br)
%timeit A.dot(Br)
%timeit A.dot(B)
%timeit Ar.dot(B)
```

Results on a 2 core laptop with MAC OSX 10.12.6, python version 2.7.14 (Anaconda), and numpy version 1.14.0 linked against the Accelerate framework:

```python
100 loops, best of 3: 5.01 ms per loop
10 loops, best of 3: 18.8 ms per loop
10 loops, best of 3: 18.8 ms per loop
10 loops, best of 3: 116 ms per loop
```

Results on a 16 core machine with CentOS 7.4.1708, python version 2.7.14 (Intel Corporation), and numpy version 1.13.3 linked against Intel MKL:

```python
100 loops, best of 3: 1.29 ms per loop
100 loops, best of 3: 2.41 ms per loop
100 loops, best of 3: 2.54 ms per loop
10 loops, best of 3: 110 ms per loop
```",2018-01-25 03:02:06,,A.dot(B) very slow for A real and B complex,['unlabeled']
10467,open,endolith,"There should be functions like `allclose` and `assert_allclose` that check that all elements of two [multisets ](https://en.wikipedia.org/wiki/Multiset) are almost equal (for every element of one array, there is one element in the other array with value less than `tol` away.  Multiplicity matters, but the order of elements does not).

This would be useful for checking roots of polynomials, for instance.  

(Sorting first is not adequate, since the arrays could contain complex numbers, and if they're not exactly equal, could be sorted differently.)

[`scipy.signal._cplxreal` is vaguely similar.](https://github.com/scipy/scipy/blob/master/scipy/signal/filter_design.py#L821) though it's comparing complex conjugates from a single list.",2018-01-24 05:10:04,,Request: allclose for unordered arrays,['unlabeled']
10456,open,fujiisoup,"From https://github.com/pydata/xarray/pull/1837

`nanmean` (and other nan-reduction functions) copies the original data once [here](https://github.com/numpy/numpy/blob/b454ec7b9310608e24348aa03a09dccec04245fc/numpy/lib/nanfunctions.py#L851), 
which brings a significant speed down if the target array is generated from `as_strided`.

```python
In [1]: import numpy as np
   ...: from memory_profiler import memory_usage
   ...: 
   ...: def rolling_window(a, window):
   ...:     shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)
   ...:     strides = a.strides + (a.strides[-1],)
   ...:     return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)
   ...: 
   ...: a = np.arange(2000000, dtype=float).reshape(1000, 2000)
   ...: a_rolling = rolling_window(a, 1000)
   ...: a_rolling.shape
Out[1]: (1000, 1001, 1000)
In [2]: memory_usage((np.mean, (a_rolling, ), {'axis': 2}))  # 74 MB
In [3]: memory_usage((np.nanmean, (a_rolling, ), {'axis': 2}))  # 9600 MB
```

Is it in numpy's focus to move that line into C implementation?",2018-01-23 03:52:23,,nan-reduction (such as nanmean) functions copies the data,['unlabeled']
10448,open,nschloe,"MWE:
```python
import numpy

xp = [0.0, 1.0]
fp = [0.2, 0.3]
print(numpy.interp(0.5, xp, fp))  # 0.25, correct

xp = [1.0, 0.0]
fp = [0.3, 0.2]
print(numpy.interp(0.5, xp, fp))  # 0.2, WRONG
```",2018-01-21 12:06:30,,BUG interp wrong if input xp isn't sorted,['component: numpy.lib']
10446,open,hameerabbasi,"I was considering more mixins like `NDArrayOperatorsMixin`. Some I was considering were:

- `NDArrayUfuncsMixin` for things like `duckarray.sin()`, `duckarray.exp()`, etc, which would defer to `np.sin` and `np.exp`.
- `NDArrayReductionsMixin` for `ufunc`-based reductions like `duckarray.sum(axis=1)`, etc which would defer to `np.add.reduce`.

I'm willing to put this together, but a question I have is, is there a better way than just listing out all the options?",2018-01-21 08:40:10,,Add more mixins that do the right thing for duck arrays based on __array_ufunc__ and __array_function__,['unlabeled']
10439,open,nschloe,"Right now, [`interp`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.identity.html) handles scalar-valued functions well. When throwing a vector-valued function at it like
```python
import numpy

x = numpy.array([0.5, 1.5])
xp = numpy.array([0.0, 1.0, 2.0])
# fp = numpy.random.rand(3)  # works
fp = numpy.random.rand(3, 4, 5)

numpy.interp(x, xp, fp)
```
it bails out with
```
ValueError: object too deep for desired array
```
When looking at [the code for `interp`](https://github.com/numpy/numpy/blob/master/numpy/lib/function_base.py#L1153), it almost seems it was meant to do that already.",2018-01-19 18:00:16,,ENH: vectorize interpolate,"['01 - Enhancement', 'component: numpy.lib']"
10436,open,ahaldane,"This is demonstrated by comparing the scalar output from dragon4 to the string output by `loadtxt`.

(output for x64 arch)

```python

[3]: a = np.array([1.0/np.float128(3.0)])
[4]: np.savetxt(sys.stdout, a)
3.333333333333333148e-01
[7]: a[0]
0.33333333333333333334
```",2018-01-18 17:48:03,,BUG: savetxt does not accurately save float128 values,['unlabeled']
10432,open,Eight1911,"For random sampling from a user input distribution (when the keyword argument for `p` is not `None`), it seems that `numpy.random.choice` uses the bisection method, which runs in O(n log k) where  `n` is the number of samples and `k` the number of outcomes. 

We can do better. With O(k) preprocessing, the alias sampling method [(wiki)](https://en.wikipedia.org/wiki/Alias_method) has smaller constant and asymptotic cost even when k is small. (O(n) with at most two array lookups for each random sample.)

It would be more efficient to opt to use the alias method when sampling for a larger number of samples and the bisection method when the number of samples is small. If approved, I would like to work on this.

I can provide a C benchmark if needed. 

Edit: a word
",2018-01-18 14:29:27,,ENH: optimizing numpy.random.choice() for user input probabilities.,"['01 - Enhancement', 'component: numpy.random', '57 - Close?']"
10413,open,euhruska,"I'm trying to savez_compressed a large file, but fails.
```
 File ""x/site-packages/numpy/lib/npyio.py"", line 657, in savez_compressed
    _savez(file, args, kwds, True)
  File ""x/site-packages/numpy/lib/npyio.py"", line 703, in _savez
    pickle_kwargs=pickle_kwargs)
  File ""x/site-packages/numpy/lib/format.py"", line 576, in write_array
    pickle.dump(array, fp, protocol=2, **pickle_kwargs)
OverflowError: cannot serialize a string larger than 2 GiB
```
In python3 pickle.dump can use protocol=4 to serialize more than 2GiB, is there an option to use it?",2018-01-16 21:17:41,,cannot serialize a string larger than 2 GiB,['unlabeled']
10405,open,nschloe,"(From https://github.com/numpy/numpy/issues/10135.)

In Numpy, empty arrays have the default data type `numpy.float64`.
```python
print(numpy.array([]).dtype)  # float64
```
I'm questioning whether this is the right choice.

In principle, the default dtype choice for empty arrays is arbitrary since there is no data to work with. One good reason to choose a _small_ data type is that operations with empty arrays should not needlessly augment the dtype. The current setting leads to rather surprising dtype changes like
```python
print(numpy.concatenate([[0], []]))  # [0.0], not [0]
```
This would not have happened with, e.g., `numpy.array([]).dtype == numpy.int8`. ",2018-01-14 20:50:45,,"ENH: change default empty array dtype, currently 'float64'","['01 - Enhancement', '23 - Wish List']"
10393,open,lebigot,"The following is a surprising exception:
```python
>>> np.nanmean(np.array([1, 2, 3], dtype=object))
Traceback (most recent call last):
…
  File ""…/numpy/lib/nanfunctions.py"", line 864, in nanmean
    avg = _divide_by_count(tot, cnt, out=out)
  File ""…/numpy/lib/nanfunctions.py"", line 183, in _divide_by_count
    return a.dtype.type(a / b)
AttributeError: 'int' object has no attribute 'dtype'
```
In fact, the expected result should unambiguously be 2.

This has a strong impact on arrays of objects, where objects can for instance be a mix of real numbers and [numbers with uncertainties](http://pythonhosted.org/uncertainties/): a mean that should work doesn't, which is particularly problematic, as means are a common operation.

NumPy version 1.14.0.",2018-01-13 09:52:42,,nanmean() fails with array of dtype object (that contains numbers),"['00 - Bug', 'component: numpy.dtype']"
10382,open,ninahakansson,"I am using Red Hat 7.4, python 2.7 and numpy version 1.13, 1.14.0 and 1.7. For numpy version 1.7 I have no problem. But for numpy version 1.13 and 1.14.0 numpy.divide returns different masked arrays depending on if I specify the **out** parameter as positional or as keyword argument. In the example below I expected array c and e to be the same (and the same as d), but they are not!

```import numpy as np
a = np.ma.array([0, 0.25])
b = np.ma.array([1.0, 1.0])
c = np.ma.array([1.0, 1.0])
e = np.ma.array([1.0, 1.0])
np.divide(a, b, c)
np.divide(a, b, out=e)
d = np.divide(a, b)
print(c)
print(d)
print(e)
print(np.__version__)
```

Results when running:
[-- 0.25]
[0.   0.25]
[0.   0.25]
1.14.0

Best regards 
Nina

",2018-01-12 08:24:34,,Inconsistent results from numpy divide for masked arrays when using the out parameter,"['00 - Bug', 'component: numpy._core', 'component: numpy.ma']"
10380,open,PeterQFR,"Using [Scoop](https://github.com/soravux/scoop/) calling a function that uses numpy gradient causes the gradient to not be correctly computed. I am distributing the computation across a number of docker instances. 

This error disappears when I don't use numpy gradient to calculate the gradient (ie calculate the gradient manually) as per example code:
```python
def detectErrorInTimeseries(input, window_size=100, z=10):
    '''
    Detects if there is an error in the timeserise using the 
    std_dev of the second derivative of the window_size previous inputs
    param window_size is the size of window in samples.
    param z is the number of std devs that is considered normal operation
    returns True if an error has been found. 
    '''
    assert input.shape[1] == 2, 'Must be Nx2 data with first column the timeseries'
    print (input)
    firstdevorig = np.gradient(input[:, 1], input[:, 0])
    print 'np.gradient\n {}'.format(firstdevorig)
    firstdev = (input[1:, 1] - input[:-1, 1])/(input[1:, 0] - input[:-1, 0])
    print 'first Dev \n {}'.format(firstdev)
    
    #seconddev = np.gradient(firstdev[:], input[:,0])
    assert firstdev[1:].shape == firstdev[:-1].shape, 'Error1'
    assert input[1:, 0].shape == input[:-1, 0].shape, 'Error2'
    print (firstdev[1:] - firstdev[:-1]).shape
    print (input[2:, 0] - input[:-2, 0]).shape
    seconddev = (firstdev[1:] - firstdev[:-1])/(input[2:, 0] - input[:-2, 0])
    
    print 'second dev\n {}'.format(seconddev)
    
    print ('Max first {}'.format(np.max(np.absolute(firstdev))))
    print('Max second {}'.format(np.max(np.absolute(seconddev))))
    error=False
    len = input.shape[0]
    idx=window_size
   ```
This function is called with a 2d array as param input which is row major Nx2 matrix of timeseries data. The idea is to get the second derivative of the columns. When running this function through scoop, `firstdevorig` is garbage, extremely small values of about e-20. 

When the same data/function works fine on the same docker containers or any other environment running outside of a scoop process.  ",2018-01-12 06:01:50,,numpy gradient not correctly calculating gradients when used within scoop,['unlabeled']
10374,open,eric-wieser,"This is documented correctly

> For integer arguments with absolute value larger than 1 the result is always zero because of the way Python handles integer division. For integer zero the result is an overflow.

But seems useless and undesirable.",2018-01-11 16:51:48,,"BUG: np.reciprocal on integers uses C division, not true division",['00 - Bug']
10363,open,nschloe,"The output type of, e.g., `numpy.sqrt` depends on the input: If it's a float or a rank-0 array, the output will be `numpy.float64`, and an `ndarray` if an array of rank > 0 was put in.
```python
import numpy  # 1.14.0

print(numpy.sqrt(2))  # 1.4142135623730951
print(numpy.sqrt(numpy.array(2)))  # 1.4142135623730951
print(numpy.sqrt([2]))  # array([1.41421356])
```
Let's look at `numpy.vectorize`d functions:
```python
import numpy
import sympy

sqrt = numpy.vectorize(sympy.sqrt)

print(sqrt(2))  # array(sqrt(2), dtype=object)
print(sqrt(numpy.array(2)))  # array(sqrt(2), dtype=object)
print(sqrt([2]))  # array([sqrt(2)], dtype=object)
```
It appears inconsistent that `sqrt(2)` and `sqrt(numpy.array(2))` are a numpy arrays instead of numbers.",2018-01-10 22:35:54,,BUG: numpy.vectorized functions return array unconditionally,"['00 - Bug', 'component: numpy.lib']"
10353,open,smsaladi,"When the to and from `np.array`s are the same (say to reorder the values in an array), potentially unexpected behavior results:

```python
➜  ~ ipython
import numpy as np
Python 3.5.3 | packaged by conda-forge | (default, May 12 2017, 15:35:12)
Type 'copyright', 'credits' or 'license' for more information
IPython 6.1.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import numpy as np

In [2]: x = np.arange(5)
   ...: x[[4,3,2,1,0]] = x
   ...: # unexpected behavior
   ...: x
   ...:
Out[2]: array([0, 1, 2, 1, 0])

In [3]: # expected behavior
   ...: x = np.arange(5)
   ...: x[[4,3,2,1,0]] = x.copy()
   ...: x
   ...:
Out[3]: array([4, 3, 2, 1, 0])

In [4]: np.__version__
Out[4]: '1.12.1'
```

While in this case, it's clear that the to and from arrays are the same, it's easy to imagine a scenario where two variable names end up referring to the same array resulting in unexpected behavior.

I think it would be useful to display a warning when the to and from arrays are the same, so the user will have clear pointer in the right direction if unexpected behavior (say downstream) is seen.
",2018-01-09 19:51:39,,BUG: fancy-indexed assignment with overlapping source and destination results in unexpected behaviour,"['00 - Bug', 'Project']"
10332,open,eric-wieser,"```python
>>> np.arange(0, 10, 2)
array([0, 2, 4, 6, 8])

# as above, cast to complex
>>> np.arange(0 + 0j, 10 + 0j, 2 + 0j)
array([], dtype=complex128)  #what?

# bizarre input needed to give expected output
>>> np.arange(0 + 0j, 10 + 10j, 2 + 0j)
# array([ 0.+0.j,  2.+0.j,  4.+0.j,  6.+0.j,  8.+0.j])
```

For some reason, it deliberately computes len as (c14792d056422fe02219efc600ca9d67c4bbd56d)
```
c_len = (start - stop) / step
len = min(ceil(c_len.real), ceil(c_len.imag))
```
which for real-only values, gives `0`.

I think a better approach would be to use one of
1. `ceil(abs(c_len))` - has semantics ""use the circle centered at `start` and passing through `end`"" as the end point
2. `ceil(c_len.real)` - has semantics ""use the projection of `end` onto the line `start + k*step` as the endpoint""
3. above with `assert c_len.imag == 0` - requires that `end` lies on that line, but prone to rounding error

4. Project `step` onto the line between `start` and `end`, and use that instead
 
I like the look of 2 and 4
  ",2018-01-05 18:46:46,,BUG: arange behaves poorly on complex numbers,"['00 - Bug', 'component: numpy._core']"
10318,open,eric-wieser,"Spot the odd one out:

* `np.array` defaults to `copy=True, subok=False`  
* **`np.ma.array` defaults to `copy=False, subok=True`**

<!-- -->

* `np.asarray` defaults to `copy=False, subok=False`  
* `np.ma.asarray` defaults to `copy=False, subok=False`

<!-- -->

* `np.asanyarray` defaults to `copy=False, subok=True`  
* `np.ma.asanyarray` defaults to `copy=False, subok=True`

So as a result, `np.ma.asanyarray` and `np.ma.array` are exactly the same.
More confusingly, `np.ma.asarray` is stricter than `np.ma.array`, not less strict!

I don't know if there's any way we can fix this without breaking downstream code, but it seems like a poor design decision - and violates the expectation that `np.ma.func` and `np.func` are similar.

  ",2018-01-03 12:57:47,,BUG: np.ma.array has surprising default arguments when compared to np.array,['component: numpy.ma']
10297,open,amymcgovern,"If you run the following code (which I discovered inside matplotlib but it was shown to be a numpy issue), it will generate a memory error.

```
import numpy as np

training_input = np.random.rand(6545)
training_input[1000] = 1000000000000000

m, bins = np.histogram(training_input, 'auto')
```",2017-12-29 19:55:17,,BUG: MemoryError in np.histogram with large outliers and bins='auto',"['00 - Bug', 'component: numpy.lib']"
10296,open,charris,"The `mem_overlap.c` code has a lot of signed, unsigned integer variables. It is likely that most of the unsigned variables could be replaced with signed, simplifying the code and isolating the places where unsigned actually matters.",2017-12-29 14:40:37,,Audit `mem_overlap.c` to rationalize the use of unsigned integers.,"['17 - Task', 'component: numpy._core']"
10290,open,eric-wieser,"```
>>> d = np.array([np.datetime64('now'), np.datetime64('now') + 10])
>>> d.ptp()
numpy.timedelta64(10,'s')
>>> d.ptp(out=np.empty((), np.timedelta64))
Traceback (most recent call last):
  File ""<pyshell#63>"", line 1, in <module>
    d.ptp(out=np.empty((), np.timedelta64))
TypeError: ufunc subtract cannot use operands with types dtype('<m8') and dtype('<M8[s]')
```

This is because it assumes that `d.max()` and `d.ptp()` have the same dtype, which they do not. Related to #10277.

Perhaps we need an `optional_out` argument, which is only used if the dtypes match?",2017-12-28 09:05:09,,BUG: ptp fails on datetime types with out parameter,"['00 - Bug', 'component: numpy._core', 'component: numpy.dtype', 'component: numpy.datetime64']"
10289,open,eric-wieser,"Which is not the case (#10288)

cc @ahaldane",2017-12-28 08:24:33,,Dragon4 invocation in np.float128.__repr__ assumes IEEE 754R 128-bit floats,['unlabeled']
10288,open,eric-wieser,"It implies the IEEE 754R 128-bit float, but in practice is typically whatever `long double` is on the platform, which #10281 shows can sometimes be other types.",2017-12-28 08:22:50,,np.float128 is a confusing name,['unlabeled']
10270,open,ghost,"This test has failed a few times with the following message:

```
AttributeError: 'numpy.float64' object has no attribute '_mask'
```

And stack trace
```
self = <numpy.ma.tests.test_regression.TestRegression object at 0x0000000012B264E0>

    def test_var_sets_maskedarray_scalar(self):
        # Issue gh-2757
        a = np.ma.array(np.arange(5), mask=True)
        mout = np.ma.array(-1, dtype=float)
>       a.var(out=mout)

numpy\ma\tests\test_regression.py:64: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
numpy\ma\core.py:5219: in var
    dvar = divide(danom.sum(axis, **kwargs), cnt).view(type(self))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <numpy.ma.core._DomainedBinaryOperation object at 0x0000000006CA1048>
a = masked, b = 0, args = (), kwargs = {}, da = array(0.)
db = array(0, dtype=int64), result = nan, m = array([ True])
domain = <numpy.ma.core._DomainSafeDivide object at 0x0000000006CA1240>
masked_result = nan

    def __call__(self, a, b, *args, **kwargs):
        ""Execute the call behavior.""
        # Get the data
        (da, db) = (getdata(a), getdata(b))
        # Get the result
        with np.errstate(divide='ignore', invalid='ignore'):
            result = self.f(da, db, *args, **kwargs)
        # Get the mask as a combination of the source masks and invalid
        m = ~umath.isfinite(result)
        m |= getmask(a)
        m |= getmask(b)
        # Apply the domain
        domain = ufunc_domain.get(self.f, None)
        if domain is not None:
            m |= domain(da, db)
        # Take care of the scalar case first
        if (not m.ndim):
            if m:
                return masked
            else:
                return result
        # When the mask is True, put back da if possible
        # any errors, just abort; impossible to guarantee masked values
        try:
            np.copyto(result, 0, casting='unsafe', where=m)
            # avoid using ""*"" since this may be overlaid
            masked_da = umath.multiply(m, da)
            # only add back if it can be cast safely
            if np.can_cast(masked_da.dtype, result.dtype, casting='safe'):
                result += masked_da
        except Exception:
            pass
    
        # Transforms to a (subclass of) MaskedArray
        masked_result = result.view(get_masked_subclass(a, b))
>       masked_result._mask = m
E       AttributeError: 'numpy.float64' object has no attribute '_mask'

```

Link to failures: https://ci.appveyor.com/project/charris/numpy/build/1.0.8371/job/ewtkpj7ss3beoky3/tests",2017-12-24 16:05:45,,numpy.ma.tests.test_regression.TestRegression.test_var_sets_maskedarray_scalar is racy,['component: numpy.ma']
10234,open,mhvk,"Currently, in `MaskedArray.__array_finalize__`, the [following](https://github.com/numpy/numpy/blob/master/numpy/ma/core.py#L2981) is done to check whether the new object may be a view of an old one:
```
if (obj.__array_interface__[""data""][0] != self.__array_interface__[""data""][0]):
```
if that check fails, the mask is copied.

This seems unnecessarily restrictive and fails even simple slicing:
```
ma = np.ma.MaskedArray(np.arange(100.))
ma2 = ma[10:20]
ma.__array_interface__[""data""][0] == ma2.__array_interface__[""data""][0]
# False
```
This means that slices by default do not share the mask with the original object, which doesn't seem a good idea given that we tried to change this behaviour in #5580 (hence, cc @jakirkham).

A relatively straightforward solution would be to replace it with `not np.may_share_memory(ma, ma2)`
(this, perhaps surprisingly, is also much faster than the above for the simple slice case).",2017-12-18 16:32:31,,MaskedArray heuristic for memory overlap seems simplistic and slow,"['15 - Discussion', 'component: numpy.ma']"
10217,open,mhvk,"While trying to get `astype` right, I noticed that while `ndarray` can be viewed as structured arrays, this breaks for `MaskedArray`, because a similar view cannot be done on the mask. Choices are:
* document that this is not possible (the comment visible in the backtrace below suggests it was known that this was not possible, just not documented);
* create a new mask for this case (not too hard, but it is no longer a view);
* create a mask with appropriate stride tricks to make it a view (but this means the masks for the different elements are combined).

```
In [2]: a = np.ma.MaskedArray([1, 2], mask=[True, False])

In [3]: a.data.view('i4,i4')
Out[3]: 
array([(1, 0), (2, 0)],
      dtype=[('f0', '<i4'), ('f1', '<i4')])

In [4]: a.view('i4,i4')
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/usr/lib/python3/dist-packages/numpy/ma/core.py in view(self, dtype, type, fill_value)
   3141             try:
-> 3142                 if issubclass(dtype, ndarray):
   3143                     output = ndarray.view(self, dtype)

TypeError: issubclass() arg 1 must be a class

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-4-b3e9ec980501> in <module>()
----> 1 a.view('i4,i4')

/usr/lib/python3/dist-packages/numpy/ma/core.py in view(self, dtype, type, fill_value)
   3146                     output = ndarray.view(self, dtype)
   3147             except TypeError:
-> 3148                 output = ndarray.view(self, dtype)
   3149         else:
   3150             output = ndarray.view(self, dtype, type)

/usr/lib/python3/dist-packages/numpy/ma/core.py in __setattr__(self, attr, value)
   3423             # This raises a ValueError if the dtype change won't work
   3424             try:
-> 3425                 self._mask.shape = self.shape
   3426             except (AttributeError, TypeError):
   3427                 pass

ValueError: cannot reshape array of size 1 into shape (2,)
```",2017-12-13 19:49:29,,Masked Array cannot be viewed as structured dtype,"['00 - Bug', 'component: numpy.ma']"
10215,open,CGamesPlay,"If I create a NumPy array in a particular fashion and call the stdlib `random.shuffle` method on it, the array loses information. A transcript follows which demonstrates. It's worth mentioning that this behavior does not happen with `np.random.shuffle` instead of `random.shuffle`. I believe this is a bug, but if this is a well-known limitation then I think it is worth documenting somewhere, perhaps on the `numpy.random.shuffle` page.

```
$ python
Python 2.7.11 (default, Dec 26 2015, 12:45:20)
[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.1.76)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import random
>>> import numpy as np
>>> np.version.version
'1.13.3'
>>> x = np.linspace(0, 10, 11)
>>> y = x % 2
>>> z = np.vstack((x, y)).T
>>> z
array([[  0.,   0.],
       [  1.,   1.],
       [  2.,   0.],
       [  3.,   1.],
       [  4.,   0.],
       [  5.,   1.],
       [  6.,   0.],
       [  7.,   1.],
       [  8.,   0.],
       [  9.,   1.],
       [ 10.,   0.]])
>>> random.shuffle(z)
>>> z
array([[  0.,   0.],
       [  1.,   1.],
       [  2.,   0.],
       [  3.,   1.],
       [  0.,   0.],
       [  0.,   0.],
       [  3.,   1.],
       [  3.,   1.],
       [  6.,   0.],
       [  0.,   0.],
       [ 10.,   0.]])
>>> random.shuffle(z)
>>> random.shuffle(z)
>>> random.shuffle(z)
>>> z
array([[ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.]])
```",2017-12-13 04:41:43,,Interaction with stdlib random.shuffle destroys array,['04 - Documentation']
10179,open,paul-the-noob,"This may be related to #3016.

As I understand it arr.__contains__(item) is implemented as (arr == item).any().

Due to spurious broadcasting this leads to

    [5] in array([None, 4, [5]) -> False

and

    [5] in array([None, 5, [4]) -> True

Surely, that can't be right.

I'm on Python3.6 and numpy1.13.",2017-12-08 22:47:07,,__contains__: erroneous broadcasting when operand is a list,['unlabeled']
10171,open,jakirkham,"Would be nice if `frompyfunc` took an optional `dtype` argument. Naively this would be a required input type and expected output type (maybe there needs to be two arguments or a mapping between arguments). Though I'm sure there is lots I haven't thought of here.

cc @mhvk (in case it is of interest ;)",2017-12-08 00:28:39,,Optional dtype argument for frompyfunc,"['01 - Enhancement', 'component: numpy.ufunc']"
10169,open,dichotomies,"Reproducing the issue:
```py
x = [1, 2]
y = [3, 4]
a = [numpy.int16(3), numpy.int16(4)]
z = [np.array(i, j) for i, j in zip(x, a)] # type derived implicitly
# z = [np.array(i, j) for i, j in zip(x, y)] # error occurs; correctly
print(z)
# output: [array(1, dtype=int16), array(2, dtype=int16)]
```

`np.array` interpretes *the type* of the object given as second argument if the object is initialized with something like `np.int16`, `np.float64` (instead of the number of the object).

Although the documentation states that the first object should be a list or an array, it might be difficult to find the error if the user gave two numpy arrays as first 2 arguments to `np.array`.

I think it might be better to have more safety than having the functionality of deriving the type from the second argument (I don't know when I would ever need that functionality). Or at least receive a warning that the type was derived implicitly. What do you think?",2017-12-06 22:54:21,,"BUG: numpy scalars can be coerced to dtype objects, but python scalars cannot","['00 - Bug', 'component: numpy.dtype']"
10167,open,mattip,"Change ``add_newdocs`` to e.g. generate a header file at compile time with the docs (as is done in [scipy.special](https://github.com/scipy/scipy/blob/master/scipy/special/add_newdocs.py)), rather than patching them at runtime after ``PyType_Ready``. Requires some hunting for the correct places to specify the docstrings. The current way ``tp_doc`` is hacked (after ``PyTypeReady``) not only leaks the ``char*`` string yanked out of the ``PyStringObject``, it does not work on PyPy.

gleaned from #10157, plagarized @pv 's hints",2017-12-06 21:29:09,,DOC: add_newdocs should happen before PyType_Ready,['component: documentation']
16544,open,mitar,"See [how contracts package](https://github.com/AndreaCensi/contracts) are trying to provide support for something similar to shapes. They are extending annotations in a different way than just standard typing and maybe something like that could be also done. So instead of providing specific extension (PEP) for typing to allow thing like shapes, it might be maybe more useful to determine a syntax for general constraints on types and use that, in addition to standard types through typing.",2017-12-06 10:05:01,,Typing support for shapes,"['01 - Enhancement', 'Static typing']"
10161,open,gasparka,"numpy.isclose (https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.isclose.html):
```
abs(a - b) <= (atol + rtol * abs(b))
```
math.isclose (https://docs.python.org/3/library/math.html#math.isclose):
```
abs(a - b) <= max(rtol * max(abs(a), abs(b)), atol)
```

Note that Numpy's equation is not symmetric and correlates the ``atol`` and ``rtol`` parameters, both are bad things (IMO).

Here is a situation where Numpy ""incorrectly"" flags two numbers as equal:
```python
a = 0.142253
b = 0.142219
rtol = 1e-4
atol = 2e-5

# true because atol interferes with the rtol measurement
abs(a - b) <= (atol + rtol * abs(b))
Out[24]: True

# correct result, rtol fails
abs(a - b) <= max(rtol * max(abs(a), abs(b)), atol)
Out[29]: False

```

Here is another one, this case symmetry problem:
```python
a = 0.142253
b = 0.142219
rtol = 1e-4
atol = 1.9776e-05

# relative to b
abs(a - b) <= (atol + rtol * abs(b))
Out[73]: False

#relative to a
abs(a - b) <= (atol + rtol * abs(a))
Out[74]: True

# math one has no problems with this
abs(a - b) <= max(rtol * max(abs(a), abs(b)), atol)
Out[75]: False

```

Python math version looks to be bulletproof, should Numpy start using this? Are there any benefits of using the Numpy version?",2017-12-05 11:41:16,,numpy.isclose vs math.isclose,['57 - Close?']
10155,open,jakirkham,"Even though `vectorize` internally uses a [`ufunc`]( https://github.com/numpy/numpy/blob/365fc9d19b52325bd5618aac1a3fa7b8a6be3b3c/numpy/lib/function_base.py#L2831 ), it appears that it [coerces its results into arrays]( https://github.com/numpy/numpy/blob/365fc9d19b52325bd5618aac1a3fa7b8a6be3b3c/numpy/lib/function_base.py#L2833-L2837 ). Thus even if the input data implemented [`__array_ufunc__`]( https://docs.scipy.org/doc/numpy-1.13.0/neps/ufunc-overrides.html ), the result would be a NumPy Array. This means that other types (e.g. Dask Arrays) will be computed by `vectorize` even if it was not the users intent.

Edit: Similar issues occur when `signature` is provided.",2017-12-04 07:28:07,,vectorize forces NumPy ndarray output,['unlabeled']
10144,open,arthur-tacca,"One might reasonably expect that `numpy.array` works equally well on generators as on lists (or range objects), if perhaps a little slower. In actual fact, the result is a zero-dimensional array containing the generator as an object:

    In[2]: import numpy as np
    In[3]: np.array([0, 1, 2])
    Out[3]: array([0, 1, 2])
    In[4]: np.array(range(3))
    Out[4]: array([0, 1, 2])
    In[5]: np.array(x for x in range(3))
    Out[5]: array(<generator object <genexpr> at 0x7f3e59f69db0>, dtype=object)

This has bitten me a couple of times, and each time it has become obvious what has happened fairly quickly, but has still been annoying enough that I am filing this report :-)

It would be nice if this worked correctly (like `fromiter`, which I have just learned about, except not needing a `dtype` specified and working recursively). But I think just an exception or a warning to the console would do. In my case, and I suspect many others, just turning the generator into a list comprehension was acceptable, so the key thing was just finding the source of the problem.",2017-12-01 14:50:34,,numpy.array gives unexpected results for generator expressions,['unlabeled']
10117,open,notmatthancock,"I raised this question first [on stackoverflow](https://stackoverflow.com/questions/41411923/pass-boolean-array-without-copying-with-f2py). Using f2py, NumPy `bool` arrays get copied into Fortran logical arrays. This behavior is not expected since the element sizes of each type are 1 byte (and since the types naturally correspond).

I am using NumPy version 1.13.3. Here is an example:

Fortran code:

```
! booltest.f90
subroutine test(a, n)
    integer n
    logical(kind=1) a(n)

    !f2py logical(kind=1), intent(in) :: a
    !f2py intent(hide), depend(a)     :: n = shape(a,0)

    write(*,*) a
end subroutine
```

And compile with:

```
f2py -c booltest.f90 -m booltest -DF2PY_REPORT_ON_ARRAY_COPY=1
```

Test from Python:

```
# booltest.py
import booltest
import numpy as np

a = np.array([True, False], dtype=np.bool)
booltest.test(a)
```
The Python script prints:

```
copied an array: size=2, elsize=1
 T F
```

On the other hand, the array is not copied if the NumPy array is `dtype=np.int8` or `dtype=np.uint8`.",2017-11-28 22:34:01,,f2py copies bool arrays when fortran type is logical,['component: numpy.f2py']
10074,open,gerritholl,"When performing a `.view(...)` on a `MaskedArray` where the view changes the dimension of the underlying array, the view fails because the mask cannot be reshaped properly:

`ma.MaskedArray(zeros(10, ""uint16""),mask=zeros(10, ""?"")).view(""uint8"")`

gives

`ValueError: cannot reshape array of size 10 into shape (20,)`

I'm not sure if there is a correct way to handle this.  When the new shape is larger one could quite correctly `.tile` the mask up to the new shape.  When the new shape is smaller, one way would be to verify that the mask is consistent for all values that will be folded together, and throw a `ValueError` if they aren't.",2017-11-24 23:58:11,,BUG: Cannot .view() MaskedArray if this changes its size,"['00 - Bug', 'component: numpy.ma']"
10047,open,ghost,"The current status of distutils is that it works *one-way*. In other words, if you have symbols in fortran that you need to use in C, then you can do that. But if you have C symbols that you need to use in fortran, you're out of luck. This is problematic because it prevents standardizing windows on the CRT ABI and leaks internal distutils implementation details. Ideally, we want to present a clean interface to the world that magically works.

Checklist:

- [x] Standardize the internal distutils API
- [ ] Implement def file generation function
- [ ] Pass def file to linker command

Implementation for second point.

1. Consolidate all gfortran objects into a single static library.
2. Run `nm -u libfoo.a`
3. Generate a `.def` file (from the undefined exports):
```
LIBRARY   BTREE.DLL
EXPORTS  
   Insert   @1  
   Delete   @2  
   Member   @3  
   Min   @4  
```
4. `dlltool -d somedll.def -l libsomedll.a`
5. pass the `.def` file into the linker command.",2017-11-18 02:31:14,,Enable reverse-symbol lookup for MinGW gfortran libraries,['unlabeled']
10023,open,charris,"For example:
```
    >>> flip(A, 1)
    array([[[2, 3],
            [0, 1]],

           [[6, 7],
            [4, 5]]])
```
I don't know if there is an official fix for that problem, but much documentation just deletes the blank lines.",2017-11-14 19:53:48,,Blank lines in examples do not render correctly in documentation,"['00 - Bug', '04 - Documentation']"
10019,open,fredrik-1,"numpy.histogram doesn't always work with masked arrays where the masked data is np.nans.

```python
import numpy as np
x=np.random.randn(100)
x[50]=np.nan
xm=np.ma.masked_array(x, mask=np.isnan(x))

result=np.histogram(xm,bins=10)
print(result[0])

result=np.histogram(xm,bins=range(-3,3))
print(result[0])
```
The last case doesn't work for me 
edit: oops, i mixed the result for matplotlib hist and histogram. None of the examples above work with np.histogram

The problem seems to be that `asarray `on line 647 in master returns an unmasked array with nans and that a.min() and a.max() returns nan on line 662.

changing `asarray` to `asanyarray` works for me but I don't know if it has any other implications.

I also read that histogram might not be designed to work with masked arrays",2017-11-13 22:54:32,,histogram doesn't always work with masked arrays,['component: numpy.ma']
10010,open,ahaldane,"This is a reminder that we have seen a few inconsistencies in the floating-point `pow` function on linux vs windows vs darwin.

The problems:
 1. The operation `10.**arange(308)` on linux correctly gives the nearest IEEE float64 to all possible powers of 10, with unbiased rounding. In other words we have `10.**np.float64(i) == np.float64('1e%d' %i)` for all `i`. On some windows systems this is often not the case for `i >= 23` (where float64 values are no longer integers), where it gives a slightly different value. I put a list of appveyor envs where this fails below marked with X. See https://github.com/numpy/numpy/pull/9941#issuecomment-341938852
 2. The mac `pow` function fails for `2.**exp` with `exp >= 1024`. See https://github.com/numpy/numpy/pull/9971#discussion_r150329510 #10000


```
   PYTHON=C:\Python27, PYTHON_VERSION=2.7, PYTHON_ARCH=32, SKIP_NOTAG=true, TEST_MODE=full
   PYTHON=C:\Python36, PYTHON_VERSION=3.6, PYTHON_ARCH=32, TEST_MODE=fast
 X PYTHON=C:\Python27-x64, PYTHON_VERSION=2.7, PYTHON_ARCH=64, TEST_MODE=fast
 X PYTHON=C:\Python34-x64, PYTHON_VERSION=3.4, PYTHON_ARCH=64, TEST_MODE=fast
 X PYTHON=C:\Python36-x64, PYTHON_VERSION=3.6, PYTHON_ARCH=64, TEST_MODE=full
   PYTHON=C:\Python27, PYTHON_VERSION=2.7, PYTHON_ARCH=32, SKIP_NOTAG=true, TEST_MODE=full
   PYTHON=C:\Python34, PYTHON_VERSION=3.4, PYTHON_ARCH=32, SKIP_NOTAG=true, TEST_MODE=full
   PYTHON=C:\Python35-x64, PYTHON_VERSION=3.5, PYTHON_ARCH=64, SKIP_NOTAG=true, TEST_MODE=full
   PYTHON=C:\Python35, PYTHON_VERSION=3.5, PYTHON_ARCH=32, SKIP_NOTAG=true, TEST_MODE=full
```
from [this test](https://ci.appveyor.com/project/charris/numpy/build/1.0.7539)",2017-11-12 19:35:09,,Floating-point `pow` gives different results on different platforms,['unlabeled']
10004,open,damiendr,"Example:
```python
>>> np.datetime64(1478476800, ""s"")
numpy.datetime64('2016-11-07T00:00:00')

>>> np.datetime64(np.int64(1478476800), ""s"")
ValueError: Could not convert object to NumPy datetime

>>> np.timedelta64(1, ""s"")
numpy.timedelta64(1,'s')

>>> np.timedelta64(np.int64(1), ""s"")
ValueError: Could not convert object to NumPy timedelta
```",2017-11-11 15:26:00,,datetime64/timedelta64 do not accept numpy integer types,"['00 - Bug', 'component: numpy.datetime64', 'triaged']"
10003,open,damiendr,"I found out that you can get the current UTC date with `np.datetime64(""now"")`.

However this isn't documented [here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.datetime.html) and there are a few things about the behaviour that aren't obvious. For instance it seems that the precision cannot be greater than one second:

```
In [95]: np.datetime64(""now"", ""ms"")
Out[95]: numpy.datetime64('2017-11-11T08:10:31.000')
```",2017-11-11 08:25:06,,"datetime64(""now"") is undocumented","['01 - Enhancement', '04 - Documentation', 'component: numpy.datetime64']"
9988,open,jenshnielsen,"As expected `np.testing.assert_allclose(np.array([1.0, 1.0]), np.array([1.0]))` raises an assertion 
but `np.allclose(np.array([1.0, 1.0]), np.array([1.0]))` returns True. This seems to be because `np.allclose` does not check that the shapes of the input is identical before subtracting the 2 arrays and ends up broadcasting the one element array to the shape of the 2 element array. And therefore `np.allclose(np.array([1.0, 1.0, 1.0]), np.array([1.0, 1.0]))` raises a value error when failing to broadcast

```
ValueError                                Traceback (most recent call last)
<ipython-input-7-31d9eb09a824> in <module>()
----> 1 np.allclose(np.array([1.0, 1.0, 1.0]), np.array([1.0, 1.0]))

~/.pyenv/versions/3.6.3/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/numeric.py in allclose(a, b, rtol, atol, equal_nan)
   2457
   2458     """"""
-> 2459     res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
   2460     return bool(res)
   2461

~/.pyenv/versions/3.6.3/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/numeric.py in isclose(a, b, rtol, atol, equal_nan)
   2539     yfin = isfinite(y)
   2540     if all(xfin) and all(yfin):
-> 2541         return within_tol(x, y, atol, rtol)
   2542     else:
   2543         finite = xfin & yfin

~/.pyenv/versions/3.6.3/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/numeric.py in within_tol(x, y, atol, rtol)
   2522     def within_tol(x, y, atol, rtol):
   2523         with errstate(invalid='ignore'):
-> 2524             result = less_equal(abs(x-y), atol + rtol * abs(y))
   2525         if isscalar(a) and isscalar(b):
   2526             result = bool(result)

ValueError: operands could not be broadcast together with shapes (3,) (2,)
```
Is it intentional that `np.allclose` does not compare shapes? From the [docstring](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.testing.assert_allclose.html#numpy.testing.assert_allclose) of assert_allclose I would expect them to behave identically (apart from the different abs and rel tol) 


Tested with numpy 1.13.3 and 1.12.0",2017-11-08 10:28:34,,Different behaviour of np.all_close and np.testing.assert_allclose for mismatched shapes,['04 - Documentation']
9987,open,ashwinvis,"# System details
 - system                : Linux
 - kernel                 : 4.9.0-4-amd64
 - distro                  : debian 9.2 
 - CC                      : gcc (Debian 6.3.0-18) 6.3.0 20170516
 - BLAS/LAPACK    : atlas
 - python                 : 3.5.3 and 2.7.13
 - numpy                 : 1.13.3

# Test case

## Code
```python
import numpy as np
gp1 = np.geomspace(2, 8, 3, dtype=int)
gp2 = np.geomspace(2, 8, 3)
np.testing.assert_equal(gp1, gp2)
```

## Traceback
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/scratch/avmo/opt/fluidmeta3/lib/python3.5/site-packages/numpy/testing/utils.py"", line 343, in assert_equal
    return assert_array_equal(actual, desired, err_msg, verbose)
  File ""/scratch/avmo/opt/fluidmeta3/lib/python3.5/site-packages/numpy/testing/utils.py"", line 854, in assert_array_equal
    verbose=verbose, header='Arrays are not equal')
  File ""/scratch/avmo/opt/fluidmeta3/lib/python3.5/site-packages/numpy/testing/utils.py"", line 778, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Arrays are not equal

(mismatch 66.66666666666666%)
 x: array([2, 3, 7])
 y: array([ 2.,  4.,  8.])

```

I would have expected an output like `[2, 4 ,8]` and not `[2, 3, 7]`. This occurs with both python 2.7 and 3.5.
",2017-11-08 09:42:41,,Wrong output while using geomspace with dtype=int keyword argument,['00 - Bug']
9984,open,bgavran,"When trying to sum over axes using the ellipsis notation, numpy throws an error.
Minimum code for replication:

```
>>> import numpy as np
>>> a = np.random.rand(2, 3, 5)
>>> np.einsum(""...i->i"", a)
```
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/anaconda/lib/python3.6/site-packages/numpy/core/einsumfunc.py"", line 948, in einsum
    return c_einsum(*operands, **kwargs)
ValueError: output had too few broadcast dimensions
```
As far as I can tell, when ellipsis is both in the result and the argument, the thing works. 
But what would be the preferred way of summing across all axes except the last (or the last few) usin einsum, when the number of axes isn't known beforehand?

I've found a [similar issue](https://github.com/numpy/numpy/pull/4099) from 4 years ago which deals with a similar broadcasting restriction, but nothing more.

Using numpy 1.13.3 and Python 3.6

",2017-11-07 22:03:08,,Einsum ellipsis broadcasting restriction,['unlabeled']
9974,open,JohanMabille,"The current documentation of [enumerated types](https://docs.scipy.org/doc/numpy-dev/reference/c-api.dtype.html) makes me think there are 8 values possible for integers ((signed, unsigned) * (8, 16, 32, 64) bits).

However, if I dump the values of the enumeration I have results that are not consistent with the documentation:

Windows x64 (with `sizeof(long) = 4`)
```bash
NPY_BYTE = 1
NPY_UBYTE = 2
NPY_INT8 = 1
NPY_UINT8 = 2
NPY_SHORT = 3
NPY_USHORT = 4
NPY_INT16 = 3
NPY_UINT16 = 4
NPY_INT = 5
NPY_UINT = 6
NPY_INT32 = 7
NPY_UINT32 = 8
NPY_LONG = 7
NPY_ULONG = 8
NPY_INT64 = 9
NPY_UINT64 = 10
NPY_LONGLONG = 9
NPY_ULONGLONG = 10
```
`NPY_INT` and `NPY_INT32` differ (the same for `NPY_UINT` and `NPY_UIN32`).

Linux x64 (with `sizeof(long) = 8`)
```bash
NPY_BYTE = 1
NPY_UBYTE = 2
NPY_INT8 = 1
NPY_UINT8 = 2
NPY_SHORT = 3
NPY_USHORT = 4
NPY_INT16 = 3
NPY_UINT16 = 4
NPY_INT = 5
NPY_UINT = 6
NPY_INT32 = 5
NPY_UINT32 = 6
NPY_LONG = 7
NPY_ULONG = 8
NPY_INT64 = 7
NPY_UINT64 = 8
NPY_LONGLONG = 9
NPY_ULONGLONG = 10
```
Here, `NPY_INT64` and `NPY_LONGLONG` differ (the same for `NPY_UIN64` and `NPY_ULONGLONG`)
In both cases (Windows and Linux) we have 10 values for integer instead of 8.

Is that expected ? I'm using numpy v1.13.3.

cc @Wolfv @SylvainCorlay",2017-11-06 14:51:09,,Enumerated Types documentation,['unlabeled']
9959,open,mcdevitts,"This would allow unwrap to work on subclasses of ndarrays. deg2rad, rad2deg (etc.) are all ufuncs, so it's not readily apparent why unwrap is not a ufunc.",2017-11-04 18:16:16,,Make unwrap a ufunc,['unlabeled']
9956,open,jcrist,"When converting date strings that exceed the maximum value for their precision to datetime64s, the resulting values silently overflow instead of erroring/warning.

Following the overflow warnings given for other types, I would have expected a warning or error here. Tested as far back as 1.11, still has the same issue.

```python
In [1]: import numpy as np

In [2]: x = np.array(['4998-01-01 00:00:00'], dtype='O')

In [3]: x.astype('M8[ns]')  # overflows
Out[3]: array(['2075-03-26T02:07:11.452241920'], dtype='datetime64[ns]')

In [4]: x.astype('M8[ms]')  # no overflow
Out[4]: array(['4998-01-01T00:00:00.000'], dtype='datetime64[ms]')

In [5]: np.__version__
Out[5]: '1.13.3'
```

",2017-11-03 22:40:39,,BUG: Silent overflow when casting large date strings to `M8[ns]`,"['00 - Bug', 'component: numpy.datetime64']"
9954,open,garrypolley,"Currently the `assert_allclose` does not support using decimal because under the covers it uses `isfinite`. 

There are a few other cases in the code base that would benefit from `isfinite` supporting Decimal. It would help pave the way for greater `Decimal` support throughout numpy. 

See this PR for why this ticket was initially created: https://github.com/numpy/numpy/pull/9952",2017-11-03 16:15:19,,ENH: np.isfinite to support Decimal type,['unlabeled']
9948,open,jakirkham,"Am seeing `MaskedArray` raising a `MaskError` getting raised when `out` is specified to be an integral or boolean NumPy Array (not masked). The cause can be seen in the code snippet below. Instead of raising in this case, it would be better to get a warning and have the `fill_value` used. TBH am surprised the `fill_value` is not being used for any of the other typed cases in this block.

https://github.com/numpy/numpy/blob/c73335920c300bf09a48226e0d590ba0e8eac654/numpy/ma/core.py#L5558-L5562",2017-11-01 17:14:58,,MaskedArray's min raising when out ndarray specified,['component: numpy.ma']
9929,open,chrisjbillington,"When you save some arrays using `np.save()`, the extension `.npy` is automatically appended, but when you load with `np.load()`, the extension is not automatically appended.

```python
import numpy as np
x = np.linspace(0,10,10)

np.save('foo', x)
assert np.load('foo') == x
```

```
Traceback (most recent call last):
  File ""77.py"", line 5, in <module>
    assert np.load('foo') == x
  File ""/home/cjb7/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py"", line 370, in load
    fid = open(file, ""rb"")
FileNotFoundError: [Errno 2] No such file or directory: 'foo'
```

I understand that there are multiple formats, `.npy` and `.npz`, and so whilst `np.save()` and `np.savez()` know what the extension should be, `np.load()` does not, and it is not programmed to guess or try both.

Perhaps there should be a `np.loadz()`, then `np.load()` and `np.loadz()` would be able to append file extensions. I don't see the utility in there being only one `load()` function as there is presently. `np.load()`  returns a different object type depending on whether a `.npy` or `.npz` file is being loaded, and so one must know which one is dealing with already when one calls the function - and therefore one wold know which function to call. I actually assumed there would be a `loadz` function and had to google to discover that `np.load()` is indeed handling both formats, so if it is intended as a simplification I think it actually leads to slightly more confusion on-net.",2017-10-26 18:05:02,,"Inconsistent file extension behaviour between np.save and np.load, no np.loadz",['01 - Enhancement']
9923,open,Cwiiis,"If you have an array with a dimension that has length zero, PyArray_SimpleNewFromData will return an error object instead of an empty array. This is inconvenient when using SWIG to bind C++ libraries that may return empty arrays.

Ideally, if a dimension had size zero, the data parameter would accept NULL, as it would be unused.",2017-10-25 15:20:43,,PyArray_SimpleNewFromData should accept NULL data for 0-sized arrays,['unlabeled']
9921,open,manuels,"A [lot](https://stackoverflow.com/questions/11750276/matplotlib-how-to-convert-a-histogram-to-a-discrete-probability-mass-function) [of](https://stackoverflow.com/questions/30889444/python-matplotlib-probability-mass-function-as-histogram) [people](https://stackoverflow.com/questions/19503366/how-to-plot-a-probability-mass-function-in-python) want to calculate the probability mass function using `numpy.hist` and are confused that it cannot do that.
It can however calculate the probability density function and https://github.com/numpy/numpy/issues/1043 did a great job by renaming `normed` to `density`.
Analogous to that, I propose to add a `mass` keyword that (if `True`) returns the probability mass function.

I would also implement this function, once I got positive feedback.",2017-10-25 10:59:39,,Add mass keyword to np.hist(),['unlabeled']
9909,open,tarcisiofischer,"I was comparing the time about two ways of an array of zeros using ```np.zeros_like``` and ```np.zeros```.
I don't know if I'm missing something, but it seems that ```np.zeros_like``` is slower and, IMO, it seems there is no reason to be... Test has been made using an Jupyter Notebook, and is shown below:

```
import numpy as np; a=np.zeros(shape=(1000,1000))
%timeit -n100 np.zeros(shape=a.shape)
```
> 40.7 µs ± 5.72 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

```
import numpy as np; a=np.zeros(shape=(1000,1000))
%timeit -n100 np.zeros_like(a)
```
> 2.18 ms ± 47.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

Implementation of zeros_like is here: https://github.com/numpy/numpy/blob/master/numpy/core/numeric.py#L138

This issue has been opened to suggest changing the whole implementation with

```python
def zeros_like(a, dtype=None, order='K', subok=True):
  if not subok:
    return np.zeros(shape=a.shape, dtype=dtype, order=order)
  else:
    res = empty_like(a, dtype=dtype, order=order, subok=subok)
    # needed instead of a 0 to get same result as zeros for for string dtypes
    z = zeros(1, dtype=res.dtype)
    multiarray.copyto(res, z, casting='unsafe')
    return res
```
",2017-10-23 20:28:39,,np.zeros_like(a) is slower than np.zeros(shape=a.shape),['unlabeled']
9907,open,iiSeymour,You can read a csv with `numpy.genfromtxt` to get a `recarray` however there appears to be no convenient method for writing a `recarray` back out to csv.,2017-10-23 18:30:55,,ENH: csv output for recarrays,"['01 - Enhancement', '15 - Discussion']"
9890,open,hwinkler,"For a C function :
`void longfunc(long l, float* f);`

using template from the latest (5beafdf) numpy.i :
`%apply ( long  DIM1, float* IN_ARRAY1) {(long l, float* f)}`

swig 3.0.12 generates wrapper code that expects two input parameters

```
  if (!PyArg_ParseTuple(args,(char *)""OO:longfunc"",&obj0,&obj1)) SWIG_fail;
  ecode1 = SWIG_AsVal_long(obj0, &val1);
  if (!SWIG_IsOK(ecode1)) {
    SWIG_exception_fail(SWIG_ArgError(ecode1), ""in method '"" ""longfunc"" ""', argument "" ""1"""" of type '"" ""long""""'"");
  } 
  arg1 = static_cast< long >(val1);
  res2 = SWIG_ConvertPtr(obj1, &argp2,SWIGTYPE_p_float, 0 |  0 );
  if (!SWIG_IsOK(res2)) {
    SWIG_exception_fail(SWIG_ArgError(res2), ""in method '"" ""longfunc"" ""', argument "" ""2"""" of type '"" ""float *""""'""); 
  }
  arg2 = reinterpret_cast< float * >(argp2);
  longfunc(arg1,arg2);
  }
  arg2 = reinterpret_cast< float * >(argp2);
  longfunc(arg1,arg2);
```
Compare to the expected behavior, when the function has an int dimension, swig generates a Python interface expecting one parameter.
`%apply ( int DIM1, float* IN_ARRAY1) {(int i, float* f)}`
 `void intfunc(int i, float* f);`
The generated, correct, code snippet is
```
{
    ...
    array1 = obj_to_array_contiguous_allow_conversion(obj0,
      NPY_FLOAT,
      &is_new_object1);
    if (!array1 || !require_dimensions(array1, 1) ||
      !require_size(array1, size, 1)) SWIG_fail;
    arg1 = (int) array_size(array1,0);
    arg2 = (float*) array_data(array1);
}
intfunc(arg1,arg2);
```
I've attached a small tarfile reproducing the problem:
[swigbug.tar.gz](https://github.com/numpy/numpy/files/1398994/swigbug.tar.gz)

[edited to correct intfunc description]",2017-10-19 15:50:13,,BUG: incorrect Python interface generated for DIM_TYPE of long,"['00 - Bug', 'component: swig']"
9885,open,eric-wieser,"Merging into one issue from #8622 and #9583.

A suggestion sometimes thrown around is to deprecate the following:

* `bool(np.array([1]))`
* `bool(np.array([0]))`
* `bool(np.array([[[[0]]]]))`

Arguments for why we shouldn't allow these:
* they can hide bugs - code like `if arr_1d:` might pass in a test suite, only to fail in production when `len(arr1d) > 1`, because the user forgot to write `arr_1d.all()`
* They can trick users into thinking that `if arr_1d` is equivalent to `if list_1d`

However, this is perhaps too invasive a change. Collecting sentiments from elsewhere:

@njsmith in [a comment on 8622](https://github.com/numpy/numpy/issues/8622#issuecomment-280327584)

> I can see an argument from purity that this should only happen for 0d arrays; it's a bit ugly and inconsistent with other situations that we're willing to discard shape information here. But this is arguably a practicality beats purity case, and anyway the disruption of trying to change it is wayy not worth it just for a bit of purity.

@seberg in [a comment on 8622](https://github.com/numpy/numpy/issues/8622#issuecomment-280335831)

> And I always think 0d arrays -> bool makes sense, but single element is funny ;)

@pv [on the mailing list](https://mail.python.org/pipermail/numpy-discussion/2017-August/077147.html)

> Changing this sort of fundamental semantics (i.e. size-1 arrays behave
like scalars in bool, int, etc. casting context) this late in the game
in my opinion should be discussed with more care.
>
> While the intention of making it harder to write code with bugs is good,
it should not come at the cost of having everyone fix their old scripts,
which worked correctly previously, but then suddenly stop working. Note
also that I expect polling on this mailing list will not reach the
majority of the user base, so I would suggest being very conservative
when deprecating features that are not wrong but only with suboptimal
semantics. This sort of backward-incompatible changes accumulate, and
will lead to rotting of third-party code.

I'm not trying to reopen discussion here, but just giving us a place to summarize it.

",2017-10-18 07:13:38,,Consider deprecating bool(arr) when arr.shape != (),['07 - Deprecation']
9880,open,shoyer,"The ""obvious"" way to get the rank of elements in a NumPy array is to use `array.argsort().argsort()`:
https://stackoverflow.com/questions/5284646/rank-items-in-an-array-using-python-numpy

This is obviously slightly inefficient, due to sorting the array twice. In contrast, the optimal solution simply inverts the order of the array elements for the second `argsort()`, which can be done in a single pass using indexing assignment:
```python
array = numpy.array([4,2,7,1])
temp = array.argsort()
ranks = numpy.empty(len(array), int)
ranks[temp] = numpy.arange(len(array))
```

I'd like to propose adding an `invert_permutation()` or `inverse_permutation()` helper function to take care of this logic, which could be modeled off this helper function in xarray:
https://github.com/pydata/xarray/blob/2949558b75a65404a500a237ec54834fd6946d07/xarray/core/nputils.py#L38-L55

As precedent, note that TensorFlow has such a function, too: https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/invert_permutation",2017-10-17 22:27:43,,invert_permutation() function,['15 - Discussion']
9873,open,eric-wieser,"In 1.13 and master

```python
>>> arr = np.array([' ', ' ', ' '], np.unicode_)  # or bytes_
>>> arr.nonzero()
(array([], dtype=intp),)
>>> np.count_nonzero(arr)
0
>>> np.count_nonzero(arr, axis=0)
3  # but wait, maybe ' ' is truthy after all!
```

Apparently in some cases `' '` is considered true, yet in others it is considered false. (cause: #9875)",2017-10-17 04:09:16,,BUG: count_non_zero behaves differently with axis argument,['00 - Bug']
9871,open,2sn,"I propose to add a new kw argument `return_inverse` to np.in1d that returns the indices of the reference values in the second array for each value in the first array.  For the case of unique values in each array, here a sample code of what should be returned, w/o error checks; maybe this could also just be a new helper function as is

```python3
def arg1d(val, ref):
    """"""
    return the indices of `val` in `ref`.
    val : array of values for which to find indices
    ref : reference array in which to search

    This is sort of what you should expect if np.in1d had an optional
    argument return_index.
    """"""
    i = np.argsort(ref)
    l, linv = np.unique(val, return_inverse = True)
    return i[np.in1d(np.asarray(ref)[i], l)][linv]
```

This may be related to #9052.
",2017-10-16 22:30:09,,in1d - add return_index,['unlabeled']
9836,open,jakirkham,"Would be really handy to have a function that computes and returns both the minimum and maximum of an array. This should be substantially faster than calling `min` and then `max` on the same array.

FWIW here's an [SO question]( https://stackoverflow.com/q/12200580 ) from some time ago that has gotten a fair bit of attention.",2017-10-07 23:23:43,,ENH: minmax function,"['01 - Enhancement', '15 - Discussion']"
9818,open,shoyer,"Inspired by my discussion with @charris in #9810, I would like to propose adding an optional `copy` argument to `np.reshape()` and `ndarray.reshape()`.

Proposed semantics:
- `copy=None`: copy only if necessary (default).
- `copy=True`: always copy.
- `copy=False`: never copy. Raise an error if the data cannot be reshaped without a copy.

This would allow users to ensure desired behavior for `reshape` and alleviate the primary use case for modifying `ndarray.shape`.

",2017-10-03 20:19:24,,Proposal: add an optional copy argument to np.reshape,"['01 - Enhancement', 'component: numpy._core']"
9808,open,eric-wieser,"When iterating using C, all that's returned is a pointer to the start of the array, and it's the users job to index it with their own memory of its strides and dimensions (like what happens in the gufunc inner loops).

However, the python `nditer[i]` API always returns 0d arrays, and does not produce a view over the axes that were removed.

My feeling is that this:

```python
a = np.array([[1, 2], [3, 4]])
b = np.array([[10, 20], [30, 40]])

it = np.nditer((a, b), op_axes=[[0], [0]])

while not it.finished:
    print(i.value)
    it.iternext()
```
should output
```python
(array([1, 2]), array([10, 20]))
(array([3, 4]), array([30, 40]))
```
whereas it outputs at the moment
```python
(array(1), array(10))
(array(3), array(30))
```
Can someone with a better understanding of `NdIter` confirm that my intuition is correct here?",2017-10-02 07:00:15,,Python nditer should reinsert removed axes into its iteration results,['unlabeled']
9798,open,eric-wieser,"This relates to [this comment](https://github.com/numpy/numpy/pull/9346#discussion_r125166923)

```
# without reading `fill_value`
>>> a = np.ma.array([None])
>>> a.fill_value = object()
>>> a.fill_value
<object object at 0x000000000066A0E0>

# with reading `fill_value`
>>> a = np.ma.array([None])
>>> a.fill_value
'?'
>>> a.fill_value = object()
>>> a.fill_value
'<'
```

This fails because after reading `fill_value` for the first time, `_fill_value.dtype` is `S1`, which becomes fixed for the lifetime of the object.

In general, the problem is that `a.fill_value.dtype` is not `a.dtype`:
```
>>> np.ma.array(1, np.int8).fill_value.dtype
dtype('int32')  # maybe defensible, handy if the array is upcast later
>>> np.ma.array(1, np.int64).fill_value.dtype
dtype('int32')  # unacceptable loss of range (maybe windows only)!
>>> np.ma.array(1, object).fill_value.dtype
dtype('S1')  # this issue
```

It's tricky to fix this without breaking code that assumes `np.ma.array(1, np.int8).fill_value = 999999`. Do we care about that code?",2017-09-30 18:03:43,,Setting ma fill_value of object and long int dtypes doesn't work if the fill_value has already been used,"['00 - Bug', 'component: numpy.ma']"
9797,open,FrozenBob,"The numpy.s_/numpy.index_exp docs claim the following:

> For any index combination, including slicing and axis insertion, a[indices] is the same as a[np.index_exp[indices]] for any array a.

However, this appears to only be completely accurate for numpy.s_, due to the [weird backward compatibility logic](https://github.com/numpy/numpy/blob/7ccf0e08917d27bc0eba34013c1822b00a66ca6d/numpy/core/src/multiarray/mapping.c#L200) where certain non-tuple sequences are converted to tuples. For example,

```
In [7]: a = np.zeros([4, 4])

In [8]: indices = [[0, 1], [2, 3]]

In [9]: a[indices].shape
Out[9]: (2,)

In [10]: a[np.s_[indices]].shape
Out[10]: (2,)

In [11]: a[np.index_exp[indices]].shape
Out[11]: (2, 2, 4)
```

numpy.index_exp's tuple creation doesn't reflect the backward compatibility handling. Either the docs or the behavior should be adjusted to match the other.",2017-09-30 16:54:57,,numpy.index_exp isn't quite consistent with direct indexing in backward compatibility case,['04 - Documentation']
9791,open,eric-wieser,"The dual to #6248 - not only should we implement the interface ourselves, but we should respect it when others do.",2017-09-29 06:38:10,,BUG: np.round should fall back on `__round__` for object arrays,"['00 - Bug', 'component: numpy._core', 'Project']"
9775,open,pkomiske,"I find the following example where einsum_path with optimize='optimal' does not find the optimal contraction policy. I am running on macOS 10.12.6 with python3.5.2 with numpy version 1.14.0.dev0+088c565 (pulled and compiled from master this afternoon after #9773 was merged). 

As an image (note that the speedup and correctness are real): 
![image](https://user-images.githubusercontent.com/19966115/30891279-48a21c76-a300-11e7-9603-19cb3973d60a.png)

Relevant parts as text: 
```
y = np.random.rand(25)
X = np.random.rand(25,25)
guess = ['einsum_path', (0,2,4), (0,1,2,3,4,5)]
print(np.einsum_path('ab,ac,bc,ad,bd,ae,ce,de', *[X]*8, optimize=guess)[1])
print(np.einsum_path('ab,ac,bc,ad,bd,ae,ce,de', *[X]*8, optimize='optimal')[1])
```",2017-09-27 01:19:22,,einsum_path 'optimal' not optimal in at least one case,['unlabeled']
9761,open,eric-wieser,"```python
>>> np.double(np.ma.masked)
UserWarning: Warning: converting a masked element to nan.
nan
>>> warnings.simplefilter('error')
>>> np.double(np.ma.masked)
0
```",2017-09-23 20:35:51,,BUG: Setting warnings to errors silences and changes masked scalar conversion,"['00 - Bug', 'component: numpy.ma']"
9760,open,eric-wieser,"```
>>> np.half(np.ma.masked)
0.0
>>> np.single(np.ma.masked)
0.0
>>> np.double(np.ma.masked)
UserWarning: Warning: converting a masked element to nan.
nan
>>> np.longdouble(np.ma.masked)
0.0
```",2017-09-23 20:23:23,,Casting np.ma.masked to unusual float sizes gives 0`,"['00 - Bug', 'component: numpy.ma']"
9750,open,eric-wieser,"This is a little worrying

```python
>>> a_u = np.zeros((), 'U10')
>>> a_u[()] = np.ma.masked
>>> a_u[()]
u'0.0'
```
```python
>>> a_b = np.zeros((), 'S10')
>>> a_b[()] = np.ma.masked
>>> a_b[()]
b'0.0'
```

This is a regression introduced by #8903. Prior to that, it seems that unicode arrays produced `u'--'`",2017-09-23 08:22:18,,BUG: Converting masked array to bytes or unicode gives `0.0`,"['00 - Bug', 'component: numpy.ma']"
9748,open,LevN0,"A bug was introduced in fix for #6723,

```
x = np.array([([0, 0], 0.0), ([2, 2], 3.0)],
              dtype=[('field1', 'i4', (2,)), ('field2', 'f4')])
y = x.view(np.ma.MaskedArray)

y['field1'].set_fill_value(5)
```
Produces,

```
  File ""...\numpy\ma\core.py"", line 3615, in set_fill_value
    _fill_value[()] = target
TypeError: 'numpy.int32' object does not support item assignment
```

The reason is that ``np.ma.MaskedArray._fill_value`` should be a zero-length array, containing the fill value. However the fix in that issue simply sets it to be a value (e.g. one with dtype numpy.int32), not a zero-length array. ",2017-09-23 03:40:14,,"MaskedArray recarray, multi-dimensional field + set_fill_value = exception","['00 - Bug', 'component: numpy.ma']"
9735,open,lzkelley,"I initially expected the `numpy.trapz` of a `MaskedArray` to behave as if `trapz` had been called on the unmasked elements, i.e.

Because,
```
> numpy.trapz([1.0, 1.0, 1.0])
2.0
```

I Expected:
```
> a = np.ma.masked_array([1.0, 1.0, 2.0, 1.0], mask=[0, 0, 1, 0])
> numpy.trapz(a)
2.0
```

But actually, it returns
```
1.0
```

because only the first two values have contiguous elements, and I guess the `dx` for the last element is then undefined.  This definitely makes sense as *an* implementation, but it is ambiguous.  
For example, if you're thinking about the masked array as representing sampling of some underlying data, when you integrate over the masked array you would just expect to get a less accurate integral, instead of skipping many of the intervals.

Should a warning be thrown when a masked array is passed to `trapz`?  Or perhaps just documentation added to explain the behavior?

",2017-09-21 22:02:14,,Unexpected behavior of `trapz` with `MaskedArray`,['component: numpy.ma']
9733,open,jialinzou,"```
>>>> np.int64(10000000000000000) == 10000000000000001.0
>>>> True
```
This only happens while the int64 number is large enough and the difference is small enough. Eg. following comparisons get correct result:
```
>>>> np.int64(10000000000000000) == 10000000000000002.0
>>>> False
>>>> np.int64(1000000000000000) == 1000000000000001.0
>>>> False
```",2017-09-21 18:04:37,,large int64 number compares to float number getting inaccurate result,"['00 - Bug', 'component: numpy._core']"
9731,open,gwaterst,"Due to a recent update of glibc to version 2.26, numpy cannot be compiled with icc.

```
python2 setup.py config --compiler=intelem build_clib --compiler=intelem build_ext

This leads to the following error:
icc: numpy/core/src/multiarray/multiarraymodule.c
In file included from /usr/include/python2.7/Python.h(42),
                 from numpy/core/src/multiarray/multiarraymodule.c(18):
/usr/include/stdlib.h(133): error: identifier ""_Float128"" is undefined
  extern _Float128 strtof128 (const char *__restrict __nptr,
```

This appears to be related to this forum post:
https://software.intel.com/en-us/forums/intel-c-compiler/topic/742701",2017-09-21 13:00:16,,"Compiling numpy with Intel icc, MKL and glibc 2.26 fails",['unlabeled']
9727,open,M4dCr0w,"Hi,

I fail to use a simple fortran routine, which makes use of quadrupel precision real values (`real(kind=16)` or `real*16`).
I am using fortran-magic with jupyter notebook to show this example:
```fortran
%%fortran
subroutine f_get_pi(pi)
    real(KIND=16), intent(out) :: pi
    pi = 4._16*ATAN(1._16)
end subroutine
```

Compiling works. However, if I run f_get_pi() now in Python, I receive the following error:

> SystemError: NULL object passed to Py_BuildValue

Apparently, if I use the subroutine in another function it get's even worse:
```fortran
%%fortran
subroutine f_get_pi(pi)
    real(KIND=16), intent(out) :: pi
    pi = 4._16*ATAN(1._16)
end subroutine
subroutine f_deg2rad(deg, rad)
    real*16, intent(in) :: deg
    real*16, intent(out) :: rad
    !
    real*16 :: pi
    !
    CALL f_get_pi(pi)
    rad = pi/180._16 * deg
end subroutine
```
Calling `f_deg2rad(181.)` in python results in no output, just a crash of the python kernel.

This is truly not the expected behavior. Using double precision (`real(KIND=8)`) for all real above instead, the function just run fine. I am not sure if quadruple precision is supported (although I see no reason why not), but even if not, this should not be the result.

OS:
>uname -a
>Linux sv31 4.1.36-41-default #1 SMP PREEMPT Fri Dec 9 08:26:33 UTC 2016 (29aafea) x86_64 x86_64 x86_64 GNU/Linux

Python version(s):
```
Python 2.7.12 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:42:40) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2
```
Conda packages:
```
anaconda-client           1.4.0                    py27_0  
backports                 1.0                      py27_0  
basemap                   1.1.0                    py27_2    conda-forge
bitstring                 3.1.3                    py27_0    auto
cairo                     1.12.18                       6  
certifi                   2016.2.28                py27_0    conda-forge
certifi                   2017.4.17                 <pip>
cffi                      1.9.1                    py27_0  
cloog                     0.18.0                        0  
clyent                    1.2.2                    py27_0  
colorspacious             1.1.0                     <pip>
conda                     4.3.25                   py27_0  
conda-env                 2.6.0                         0  
configparser              3.5.0b2                  py27_0    conda-forge
cryptography              1.7.1                    py27_0  
curl                      7.45.0                        0  
cycler                    0.10.0                   py27_0  
decorator                 4.0.10                   py27_0  
entrypoints               0.2.2                    py27_0    conda-forge
enum34                    1.1.6                    py27_0  
expat                     2.2.1                         0    conda-forge
fontconfig                2.11.1                        6  
fortran-magic             0.7                      py27_1    conda-forge
freetype                  2.5.5                         1  
freexl                    1.0.2                         2    conda-forge
functools32               3.2.3.2                  py27_1    conda-forge
futures                   3.0.5                    py27_0  
gcc                       4.8.5                         7  
gdal                      2.2.1               np111py27_0    conda-forge
geos                      3.5.1                         1    conda-forge
get_terminal_size         1.0.0                    py27_0  
giflib                    5.1.4                         0    conda-forge
gmp                       6.1.0                         0  
h5py                      2.7.1                    py27_1    conda-forge
hdf4                      4.2.11                        4    conda-forge
hdf5                      1.8.18                        1    conda-forge
icu                       58.1                          1    conda-forge
idna                      2.2                      py27_0  
ipaddress                 1.0.18                   py27_0  
ipykernel                 4.6.1                    py27_0  
ipyparallel               5.1.0                    py27_0  
ipython                   4.2.0                    py27_0  
ipython_genutils          0.1.0                    py27_0  
ipyvolume                 0.4.0                    py27_1    conda-forge
ipywebrtc                 0.3.0                    py27_0    conda-forge
ipywidgets                7.0.1                      py_2    conda-forge
isl                       0.12.2                        0  
jinja2                    2.8                      py27_0    conda-forge
jpeg                      9b                            0    conda-forge
json-c                    0.12.1                        0    conda-forge
jsonschema                2.5.1                    py27_0    conda-forge
jupyter_client            4.3.0                    py27_0    conda-forge
jupyter_core              4.1.0                    py27_0    conda-forge
kealib                    1.4.7                         2    conda-forge
krb5                      1.14.2                        0    conda-forge
libdap4                   3.18.3                        2    conda-forge
libffi                    3.2.1                         1  
libgcc                    5.2.0                         0  
libgdal                   2.2.1                         0    conda-forge
libgfortran               3.0.0                         1  
libiconv                  1.14                          4    conda-forge
libnetcdf                 4.4.1.1                       6    conda-forge
libpng                    1.6.28                        1    conda-forge
libpq                     9.6.3                         0    conda-forge
libsodium                 1.0.10                        0    conda-forge
libspatialite             4.3.0a                       15    conda-forge
libtiff                   4.0.6                         5    conda-forge
libxml2                   2.9.2                         0  
markupsafe                0.23                     py27_0    conda-forge
matplotlib                1.5.1               np111py27_0  
mistune                   0.7.3                    py27_0    conda-forge
mkl                       11.3.3                        0  
mpc                       1.0.3                         0  
mpfr                      3.1.5                         0  
nbconvert                 4.2.0                    py27_0    conda-forge
nbformat                  4.4.0                    py27_0    conda-forge
netcdf4                   1.2.9                    py27_1    conda-forge
networkx                  1.11                     py27_0    conda-forge
notebook                  5.0.0                    py27_0  
numpy                     1.11.1                   py27_0  
openjpeg                  2.1.0                         5    conda-forge
openssl                   1.0.2g                        0  
pandas                    0.20.3                   py27_1    conda-forge
path.py                   8.2.1                    py27_0  
pathlib2                  2.1.0                    py27_0  
pexpect                   4.0.1                    py27_0  
pickleshare               0.7.2                    py27_0  
pillow                    3.2.0                    py27_1    conda-forge
pip                       8.1.1                    py27_1  
pixman                    0.32.6                        0  
postgresql                9.1.4                         1  
proj.4                    4.9.2                         0    conda-forge
proj4                     4.9.3                         4    conda-forge
ptyprocess                0.5.1                    py27_0  
pyart                     1.6.0                    py27_0    jjhelmus
pyasn1                    0.1.9                    py27_0  
pycairo                   1.10.0                   py27_0  
pycosat                   0.6.1                    py27_0  
pycparser                 2.17                     py27_0  
pycrypto                  2.6.1                    py27_0  
pygments                  1.6                      py27_0    conda-forge
pyopenssl                 16.2.0                   py27_0  
pyparsing                 2.1.4                    py27_0  
pypdf2                    1.26.0                   py27_0    BjornFJohansson
pyproj                    1.9.5.1                  py27_0    conda-forge
pyqt                      4.11.4                   py27_3  
pyshp                     1.2.12                     py_0    conda-forge
python                    2.7.12                        1  
python-dateutil           2.5.3                    py27_0  
pytmatrix                 0.3.0               np111py27_1    conda-forge
pytz                      2016.4                   py27_0  
pywavelets                0.5.2               np111py27_0    conda-forge
pyyaml                    3.11                     py27_1  
pyzmq                     14.7.0                   py27_0    conda-forge
qt                        4.8.7                         3  
readline                  6.2                           2  
requests                  2.12.4                   py27_0  
ruamel_yaml               0.11.14                  py27_0  
scikit-image              0.13.0              np111py27_0  
scipy                     0.17.1              np111py27_1  
setuptools                20.3                     py27_0  
simplegeneric             0.8.1                    py27_1  
sip                       4.16.9                   py27_0  
six                       1.10.0                   py27_0  
sqlite                    3.13.0                        0  
ssl_match_hostname        3.5.0.1                  py27_0    conda-forge
terminado                 0.6                      py27_0    conda-forge
tk                        8.5.18                        0  
tornado                   4.2.1                    py27_0    conda-forge
traitlets                 4.3.2                    py27_0  
traittypes                0.0.6                    py27_0    conda-forge
util-linux                2.21                          0  
viscm                     0.7                       <pip>
wheel                     0.29.0                   py27_0  
widgetsnbextension        3.0.2                    py27_0  
wradlib                   0.10.1                   py27_0    conda-forge
xerces-c                  3.1.4                         3    conda-forge
xz                        5.0.5                         1    conda-forge
yaml                      0.1.6                         0  
zeromq                    4.1.5                         0    conda-forge
zlib                      1.2.8                         0  
```",2017-09-20 23:54:14,,f2py: Quadrupel Precision - SystemError: NULL object passed to Py_BuildValue,"['00 - Bug', 'component: numpy.f2py']"
9698,open,eric-wieser,"Is there any way to get hold of a `long double` `pi` to the maximum possible precision? The following only gets one at the precision of `double`:

```python
p = np.longdouble(np.pi)
```

One option would be to change the definitions of `np.pi` to longdouble, but then that contaminates all code that uses them into also using longdouble.",2017-09-17 06:22:33,,Storing pi in a long double?,['01 - Enhancement']
9687,open,djhoese,"Different results are seen when calling `numpy.ma.log10` with or without the `out` parameter. This may be similar to https://github.com/numpy/numpy/issues/9394. We're seeing different results between 1.12.1 and 1.13.1. Seen on mac and linux systems.

```python
In [1]: import numpy as np

In [2]: np.__version__
Out[2]: '1.13.1'

In [3]: a = np.linspace(-.5, 1.5, 5)

In [4]: np.ma.log10(a)
Out[4]: 
masked_array(data = [-- -- -0.3010299956639812 0.0 0.17609125905568124],
             mask = [ True  True False False False],
       fill_value = 1e+20)

In [5]: np.ma.log10(a, out=a)
Out[5]: 
masked_array(data = [-- -- -- -- 0.17609125905568124],
             mask = [ True  True  True  True False],
       fill_value = 1e+20)

In [6]: a
Out[6]: array([        nan,        -inf, -0.30103   ,  0.        ,  0.17609126])
```

The same results can be seen with a masked array:

```python
In [10]: a = np.ma.masked_array(np.linspace(-2., 2., 20), [False, True] * 10)

In [11]: a
Out[11]: 
masked_array(data = [-2.0 -- -1.5789473684210527 -- -1.1578947368421053 -- -0.736842105263158
 -- -0.3157894736842106 -- 0.10526315789473673 -- 0.5263157894736841 --
 0.9473684210526314 -- 1.3684210526315788 -- 1.789473684210526 --],
             mask = [False  True False  True False  True False  True False  True False  True
 False  True False  True False  True False  True],
       fill_value = 1e+20)

In [12]: np.ma.log10(a)
Out[12]: 
masked_array(data = [-- -- -- -- -- -- -- -- -- -- -0.9777236052888483 -- -0.2787536009528291
 -- -0.023481095849522966 -- 0.13621974701798895 -- 0.2527253160894261 --],
             mask = [ True  True  True  True  True  True  True  True  True  True False  True
 False  True False  True False  True False  True],
       fill_value = 1e+20)

In [13]: np.ma.log10(a, out=a)
Out[13]: 
masked_array(data = [-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 0.0636690798693773
 0.13621974701798895 0.19836765376683335 0.2527253160894261
 0.3010299956639812],
             mask = [True True True True True True True True True True -- -- -- -- -- False
 False False False False],
       fill_value = 1e+20)
```",2017-09-14 13:35:24,,inplace masked functions returning different results in 1.13.1,"['00 - Bug', 'component: numpy.ma']"
9673,open,ghost,"Due to #9431, numpy distutils can now link against OpenBLAS and friends. However, for the functionality to work correctly, distutils depends on Python loading the `__config__ `module that will add the extra paths. This works well for end users, but doesn't work well for the tests that examine the behavior of f2py because the config module is not loading during testing.",2017-09-09 15:13:13,,f2py tests fail due to missing __config___,"['00 - Bug', '05 - Testing', 'component: numpy.f2py']"
9665,open,mingwandroid,"Running the testsuite I get the following:

```
======================================================================
FAIL: test_einsum_sums_int16 (test_einsum.TestEinSum)
----------------------------------------------------------------------
Traceback (most recent call last):   
  File ""/opt/conda/conda-bld/numpy_1504792120840/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehol$
    self.check_einsum_sums('i2')
  File ""/opt/conda/conda-bld/numpy_1504792120840/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehol$
    b.astype('f8')).astype(dtype))
  File ""/opt/conda/conda-bld/numpy_1504792120840/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehol$
    return assert_array_equal(actual, desired, err_msg, verbose)
  File ""/opt/conda/conda-bld/numpy_1504792120840/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehol$
    verbose=verbose, header='Arrays are not equal')
  File ""/opt/conda/conda-bld/numpy_1504792120840/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehol$
    raise AssertionError(msg)
AssertionError:
Arrays are not equal

(mismatch 25.0%) 
 x: array([[  6090,   6195,   6300,   6405,   6510,   6615],
       [ 15540,  15870,  16200,  16530,  16860,  17190],
       [ 24990,  25545,  26100,  26655,  27210,  27765],
       [-32768, -32768, -32768, -32768, -32768, -32768]], dtype=int16)
 y: array([[  6090,   6195,   6300,   6405,   6510,   6615],
       [ 15540,  15870,  16200,  16530,  16860,  17190],
       [ 24990,  25545,  26100,  26655,  27210,  27765],
       [-31096, -30316, -29536, -28756, -27976, -27196]], dtype=int16)

----------------------------------------------------------------------
```

This is with GCC 7.2 and MKL. Does anyone have any clues about what may be amiss here? Sorry for having so little information, pointers to getting more info also happily received.
",2017-09-07 14:45:47,,"Test failure on i686 GNU/Linux with MKL, gcc 7.2",['00 - Bug']
9659,open,gauteh,"`np.lib.format.open_memmap` is not advertised very strongly, it is however very useful for writing and memory mapping variables to npy files. It seems that if any members of the input tuple is given as np.array's the header is not written correctly:

```
import numpy as np

nx = int (100)
ny = np.array(100) # make this int and all is good

R = np.lib.format.open_memmap ('test.npy', 'w+', dtype = np.float32, shape = (nx, ny))
R[...] = np.arange (nx * ny).reshape ((100, 100))

print (""R.shape ="", R.shape)

R.flush ()

C = np.load ('test.npy')

np.testing.assert_array_equal (R, C)
``` 

On a side-note, the only meaningful mode is `w+` - and the shape cannot be changed later on, so its not a [foolproof class anyway](https://stackoverflow.com/a/4620395/377927).

the error:

```
Traceback (most recent call last):
  File ""testmmap.py"", line 13, in <module>
    C = np.load ('test.npy')
  File ""/usr/lib/python3.6/site-packages/numpy/lib/npyio.py"", line 419, in load
    pickle_kwargs=pickle_kwargs)
  File ""/usr/lib/python3.6/site-packages/numpy/lib/format.py"", line 625, in read_array
    shape, fortran_order, dtype = _read_array_header(fp, version)
  File ""/usr/lib/python3.6/site-packages/numpy/lib/format.py"", line 489, in _read_array_header
    d = safe_eval(header)
  File ""/usr/lib/python3.6/site-packages/numpy/lib/utils.py"", line 1116, in safe_eval
    return ast.literal_eval(source)
  File ""/usr/lib/python3.6/ast.py"", line 85, in literal_eval
    return _convert(node_or_string)
  File ""/usr/lib/python3.6/ast.py"", line 66, in _convert
    in zip(node.keys, node.values))
  File ""/usr/lib/python3.6/ast.py"", line 65, in <genexpr>
    return dict((_convert(k), _convert(v)) for k, v
  File ""/usr/lib/python3.6/ast.py"", line 59, in _convert
    return tuple(map(_convert, node.elts))
  File ""/usr/lib/python3.6/ast.py"", line 84, in _convert
    raise ValueError('malformed node or string: ' + repr(node))
ValueError: malformed node or string: <_ast.Call object at 0x7f64c55cd9b0>


```",2017-09-06 13:36:33,,np.lib.format.open_memmap does not check if shape tuple only contains int's,['unlabeled']
9650,open,mosco,"When spawning child processes using the multiprocessing module, it appears that all child processes share the parent's random seed.

This creates a subtle and difficult to detect bug in the common use case of embarrassingly parallel multicore monte-carlo simulations. In such simulations the different processes will generate the same data, thus the final averaged result will be much less accurate (i.e. have higher variance) than expected.

Many people seem to have been bitten by this issue (and surely, many more are unaware of a silent bug in their simulation). e.g.
https://stackoverflow.com/questions/12915177/same-output-in-different-workers-in-multiprocessing
http://forum.cogsci.nl/index.php?p=/discussion/1441/solved-numpy-random-state-seems-to-repeat-across-multiple-os-runs

Note that Python's random module does not suffer from this problem.",2017-09-04 21:27:13,,Random seed is replication across child processes,"['15 - Discussion', 'component: numpy.random', 'triaged']"
9646,open,gionatarogiani,"The title says it all, don't know if it is an expected behaviour, but the following:

```
print(isinstance(np.bool_(True), bool))
# False
```
Intuitively I would expect `numpy.bool_` to check True against python `bool`. If I'm wrong could you please explain why? I'm learning and I would love to understand more.

I'm asking this because I had problems with some typechecks I found in some modules (for example sklearn).

Thanks a lot",2017-09-04 07:18:43,,"isinstance(np.bool_(True), bool) returns False",['04 - Documentation']
9644,open,oivulf,"When calling genfromtxt as e.g. in:
data=np.genfromtxt('all.txt',dtype=None,names=True,missing_values='NA',converters={6:lambda s: 'ex'})
the converted column is empty string.
These also does not work as I expected, i.e. returning strings instead of bytes:

data=np.genfromtxt('all.txt',dtype=(int,float,float,float,float,float,str),names=True,missing_values='NA')

data=np.genfromtxt('all.txt',dtype=None,names=True,missing_values='NA',converters={6:lambda s: s.decode('UTF8')})

[all.txt](https://github.com/numpy/numpy/files/1272272/all.txt)

 ",2017-09-02 15:43:12,,numpy genfromtxt does not work with converters returning strings,['unlabeled']
9642,open,eric-wieser,"Picked up via python/cpython#31. This is more of a design mistake than a bug, as it's not clear who made the right decision

```python
# a c long on this platform is indeed 8 bytes
>>> np.dtype('l').itemsize
8  
>>> struct.calcsize('l')
8

# but when an endian mark is specified, l always means 4 bytes to the struct module
>>> np.dtype('>l').itemsize
8
>>> struct.calcsize('>l')
4  
```

Note that this only occurs on platforms where `sizeof(long) == 64`, ie not 64-bit windows.

Do we care about consistency with `struct`? If not, then we need to document that we are _not_ consistent with it.

For searchability: PEP3118",2017-09-02 06:35:13,,"dtype and struct integer format characters are deceptively similar, yet different",['unlabeled']
9637,open,mattip,"While working on issue #9620 which was fixed in pull request #9629, the case of ``np.string_('a') * np.int8(5)`` failing came up. It turns out there may be a way to fix the general case of binop reverse lookup succeeding when it should return ``NotImplemented`` by the following patch:
```diff
--- a/numpy/core/src/private/binop_override.h
+++ b/numpy/core/src/private/binop_override.h
@@ -171,14 +171,18 @@ binop_should_defer(PyObject *self, PyObject *other, int inplace)
  * This is modeled on the checks in CPython's typeobject.c SLOT1BINFULL
  * macro.
  */
+
+#define IS_CHARACTER(m1) (PyString_Check(m1) || PyUnicode_Check(m1))
+
 #define BINOP_IS_FORWARD(m1, m2, SLOT_NAME, test_func)  \
     (Py_TYPE(m2)->tp_as_number != NULL &&                               \
      (void*)(Py_TYPE(m2)->tp_as_number->SLOT_NAME) != (void*)(test_func))
 
 #define BINOP_GIVE_UP_IF_NEEDED(m1, m2, slot_expr, test_func)           \
     do {                                                                \
-        if (BINOP_IS_FORWARD(m1, m2, slot_expr, test_func) &&           \
-            binop_should_defer((PyObject*)m1, (PyObject*)m2, 0)) {      \
+        if (IS_CHARACTER(m1) ||                                         \
+           (BINOP_IS_FORWARD(m1, m2, slot_expr, test_func) &&           \
+            binop_should_defer((PyObject*)m1, (PyObject*)m2, 0))) {     \
             Py_INCREF(Py_NotImplemented);                               \
             return Py_NotImplemented;                                   \
         }                                                               \
```

Note that ``PyCheck*()`` here is a macro that checks  ``PY_TPFLAGS_*_SUBCLASS`` The cost is two more ``obj->ob_type->tp_flag`` comparisons in each binop lookup, tests are needed to prove this works even in the most obscure corner cases, and as noted this did not arise in actual user code rather in a theoretical investigation of possible code paths. 

FWIW, ``np.array(['abc']) * 5`` still raises a ``TypeError``, perhaps that is OK.

Is it worth continuing with this? Does someone else want to pick it up and write the tests and reason about when ``*``, ``+`` should do sequence operations?",2017-09-01 07:50:32,,np.string_('a') * np.int8(5) proposal,['unlabeled']
9634,open,xiumingzhang,"Eigenvalues returned by `linalg.eigh()` are supposed to be [""in ascending order""](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigh.html), but it returned me an unsorted array of eigenvalues for my big 2D array (of shape `(88329, 88329)`).

```
(Pdb) pp eig_vals
array([  6.93335480e-310,   6.93335480e-310,   1.76908623e-316, ...,
         1.25299807e+001,   1.34932302e+001,   3.49126548e-318])
```

Also, I'm worried about super large exponents, like `e-310`, although they will not matter if I'm using only a few top eigenvalues.

I guess this is due to the fact that I have way more dimensions (88329) than data points (517)?

If this is so, maybe a warning should be issued cautioning the user?",2017-08-31 02:49:22,,Unsorted eigenvalues returned by linalg.eigh()?,['unlabeled']
9631,open,gerritholl,"NumPy may give a nonzero variance (and thus standard deviation) for a constant array.  This may be due to loss of numerical precision, but Pythons builtin variance routine gives the correct 0 answer, so clearly it's a preventable loss:

```
In [45]: x = [6715266981.538051]*10

In [46]: statistics.variance(x)
Out[46]: 0.0

In [47]: numpy.array(x).var()
Out[47]: 9.0949470177292824e-13
```

I think it would be highly desirable if `x.std()` and `x.var()` for a constant array could be assumed to be exactly identical to zero.  I'm aware one should not compare floating point numbers but zero is a bit of a special case.",2017-08-30 22:39:43,,variance nonzero for constant array,['unlabeled']
9573,open,creativedutchmen,"Not sure if this is covered anywhere already, feel free to close if this is the case.

Without having the numpy documentation open, and just looking at the argument names for `random.randint`, the results I got were.. confusing, to say the least.

Without looking at the docs, what would the following code do: `random.randint(low=2, size=5)`?
My guess would be that it would give back five integers larger than or equal to 2. However, the result was quite different: `array([1, 0, 0, 1, 1])`.

Wait, so I specified `low=2`, but got back numbers which are all __lower__ than `low`? What the...

The docs didn't help much at first, either: ""unless high=None, in which case this parameter is one above the highest such integer"" - what? Until I stumbled upon this: ""If high is None (the default), then results are from [0, low).""

So `high` and `low` are switched based on the value of one of the parameters, which makes things very confusing. Much cleaner would be the following function definition:

    randint(high, low=0, size=None, dtype='l')

Of course, this would not permit something like `randint(1, 2)`, but `randint(low=1, high=2)` is arguably more readable anyway. For backwards compatibility, a check could be built in which checks if `low` is greater than `high`, in which case the order can be reversed (while raising a warning).",2017-08-16 13:11:31,,Confusing argument order for random.randint,['component: numpy.random']
9565,open,AlexS12,"I have been using `np.vectorize` function and despite being really useful I find an inconsistency: when the vectorized function is called with and scalar input, the output is an array. 

```python
In [1]: import numpy as np

In [2]: def fun(x):
   ...:     return x
   ...: 

In [3]: vect_fun = np.vectorize(fun)

In [4]: vect_fun(5.0)
Out[4]: array(5.0)
```

It is just a matter of convenience, but the rest of numpy ufuncs behave this way.

```python
In [2]: np.sin(8)
Out[2]: 0.98935824662338179
```

Would you be interested in changes here?",2017-08-15 18:02:54,,output from vectorized function with scalar input is not scalar but array,['unlabeled']
9546,open,631068264,"I use numpy (1.12.0) and Python2.7
```
class GAME_ORIGINAL(BaseStrategy):
    _strategy_name_format = ""game_{run_period}_{signal_type}""
    ma_period = [3, 5, 8, 10, 12, 30, 35, 40, 45, 50]

    def __init__(self, bar_table_format, run_period):
        super(GAME_ORIGINAL, self).__init__(bar_table_format, run_period)

    def _run(self, bar):
        ema_func = lambda period: util.indicator.EMA(self.close, period)
        np_ema = np.vectorize(ema_func)
        self.ma_period = np.array(self.ma_period)
        result = np_ema(self.ma_period)
        print result
```

And get an error
```
Traceback (most recent call last):
  File ""/Users/wyx/bitcoin_workspace/fibo-strategy/base/strategy.py"", line 59, in run
    self._run(bar)
  File ""/Users/wyx/bitcoin_workspace/fibo-strategy/strategy/base_strategy.py"", line 734, in _run
    result = np_ema(self.ma_period)
  File ""/Users/wyx/bitcoin_workspace/fibo-strategy/.env/lib/python2.7/site-packages/numpy/lib/function_base.py"", line 2576, in __call__
    return self._vectorize_call(func=func, args=vargs)
  File ""/Users/wyx/bitcoin_workspace/fibo-strategy/.env/lib/python2.7/site-packages/numpy/lib/function_base.py"", line 2655, in _vectorize_call
    res = array(outputs, copy=False, subok=True, dtype=otypes[0])
ValueError: setting an array element with a sequence.
```
",2017-08-12 03:24:15,,Something about np.vectorize : ValueError: setting an array element with a sequence.,['unlabeled']
9541,open,kalvdans,"The filedescriptors used in the video4linux2 (v4l2) interface does not support the seek operation. When trying to allocate a numpy array I get an error even if the size is given through the shape parameter.
<pre>
In [1]: import numpy

In [2]: fd = open(""/dev/video0"")

In [3]: numpy.memmap(fd, shape=(1,))
---------------------------------------------------------------------------
UnsupportedOperation                      Traceback (most recent call last)
<ipython-input-3-fe575ad1ab63> in <module>()
----> 1 numpy.memmap(fd, shape=(1,))

/home/chn/env/lib/python3.5/site-packages/numpy/core/memmap.py in __new__(subtype, filename, dtype, mode, offset, shape, order)
    221             raise ValueError(""shape must be given"")
    222 
--> 223         fid.seek(0, 2)
    224         flen = fid.tell()
    225         descr = dtypedescr(dtype)

UnsupportedOperation: underlying stream is not seekable
</pre>
I see no reason that memmap should try to seek when the shape parameter is given.",2017-08-11 11:49:47,,memmap on unseekable files,['unlabeled']
9533,open,alexeymuranov,"```python
>>> from numpy.polynomial import Polynomial as P
>>> p = P([1,1], domain=[0,1])
>>> p.linspace(2)
(array([ 0.,  1.]), array([ 0.,  2.]))
>>> p.linspace(2, domain=[0,1])
(array([ 0.,  1.]), array([ 0.,  2.]))
```
Expected result: `(array([ 0.,  1.]), array([ 1.,  2.]))`.

This is with Python 3.6.0, NumPy 1.12.0.

---

*Update*. A simpler example of unexpected behaviour:
```python
>>> from numpy.polynomial import Polynomial as P
>>> p = P([0,1], domain=[0,1])
>>> p(0)
-1.0
```
The [documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polynomial.polynomial.Polynomial.html#numpy.polynomial.polynomial.Polynomial) of `domain` and `window` parameters in `Polynomial`:
> **domain** : (2,) array_like, optional
>     Domain to use. The interval `[domain[0], domain[1]]` is mapped to the interval `[window[0], window[1]]` by shifting and scaling. The default value is `[-1, 1]`.
>
> **window** : (2,) array_like, optional
>     Window, see `domain` for its use. The default value is `[-1, 1]`.",2017-08-08 23:06:07,,"The representation of polynomials with Polynomial class using ""domain"" and ""window"" is not clearly explained in the class documentation",['component: numpy.polynomial']
9526,open,miskolc,"I've been searching the docs and on google for a way to do and SQL JOIN between 2D numpy arrays. So far the best I've found was the `join_by` function in [recfunctions](https://github.com/numpy/numpy/blob/master/numpy/lib/recfunctions.py) but even this one seems to require the key to be a string:

```
key : {string, sequence}
        A string or a sequence of strings corresponding to the fields used
        for comparison.
```

The alternative is to use [pandas's merge](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) but this requieres a type conversion.between numpy array and pandas Dataframes. I would like to avoid doing that. I would like to have something like an:


`numpy.merge(left_array, right_array, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False)`

where `left_on` and `right_on` are either integers or lists of integers",2017-08-07 09:31:37,,SQL like join on Column id for 2D arrays,['unlabeled']
9516,open,gul916,"Hello,

In the attached file, I compared numpy and scipy svd function for types np.float32, np.float64, np.complex64, np.complex128. Surprisingly, under numpy, np.float32 and np.conplex64 times are similar to np.float64 and np.complex128, respectively. However, under scipy, results are doubled as expected when passing from single to double precision.

I suspect numpy to force double precision, while scipy is not, which is the correct result. As SVD is a computing intensive function, using single precision can be crucial to limit computation time.

This has been tested on numpy 1.12.1, either under windows and linux.

Thanking you,
GuL

[test_svd_cpu_gul.py.txt](https://github.com/numpy/numpy/files/1200918/test_svd_cpu_gul.py.txt)",2017-08-04 16:03:33,,Double precision forced when using Singular Value Decomposition (SVD),['00 - Bug']
9511,open,landtuna,"Using numpy 1.13, the following ISO 8601 example from Wikipedia doesn't parse:

```
Python 3.5.3 (default, May 10 2017, 15:05:55) 
[GCC 6.3.1 20161221 (Red Hat 6.3.1-1)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
```
```python
>>> import numpy as np
>>> np.datetime64(""20170802T135654Z"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ValueError: Error parsing datetime string ""20170802T135654Z"" at position 8
>>> 
```",2017-08-02 14:13:03,,np.datetime64 does not correctly handle ISO 8601 without separators,"['54 - Needs decision', 'component: numpy.datetime64']"
9509,open,lzkelley,"I'm trying to digitize `float128` values, but it looks like there are internal values with fixed, `float64` type causing errors (`float64` works fine).

```
> import numpy as np
> DT = np.float128
> a = np.linspace(0.0, 1.0, 10, dtype=DT)
> b = np.linspace(0.0, 1.0, 4, dtype=DT)
> d = np.digitize(b, a)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-388-24c1eafc263c> in <module>()
----> 1 d = np.digitize(b, a)

TypeError: Cannot cast array data from dtype('float128') to dtype('float64') according to the rule 'safe'
```

-----

I'm *very* unfamiliar with the c-side of numpy... but [perhaps the problem is here](https://github.com/numpy/numpy/blob/260f18c910175bcb6162feb363112929653276aa/numpy/core/src/multiarray/compiled_base.c#L227), where arrays are being forced to `NPY_DOUBLE` type.",2017-08-02 09:46:21,,digitize fails on float128,['00 - Bug']
9502,open,nschloe,"In Numpy, it's possible to use `dot()` with two Boolean arrays. However, the result is different from what you get with elementary operations:
```python
import numpy
a =  numpy.array([True, True])
b = a

print(numpy.sum(a * b))
print(numpy.dot(a, b))
```
Output:
```
2
True
```
I couldn't find this documented anywhere, but frankly I'm not sure if this should be allowed at all.",2017-07-31 17:50:10,,DOC: dot() with Booleans: Unexpected behavior,"['04 - Documentation', 'component: numpy.ufunc']"
9496,open,aplavin,"When I have a plain float array like `arr = np.ones((3, 4, 2))` and want to make a `complex` value from each two consequtive `float`s it's easily possible: `arr.view(complex)` gives the expected result. But when I want to perform the same with a non-continuous array, like `arr[:, ::2, :].view(complex)`, it raises `ValueError: new type not compatible with array.`. Clearly this conversion is possible without copies, but either I misunderstand something here or it's a bug in `numpy`.

Anyway, for this moment, are there any workarounds for this?",2017-07-29 21:09:04,,Can't change dtype for non-continuous array,"['01 - Enhancement', 'component: numpy._core', '54 - Needs decision']"
9476,open,shoyer,"We don't do a very good job of describing how functions like `numpy.mean()` or `numpy.moveaxis()` handle duck-arrays like those from dask. It would be nice to clearly document this somehow (either in docstrings or a separate table somewhere), so developers know they can rely on this. Right now figuring this out requires testing things out or looking at the NumPy source code.

xref https://github.com/dask/dask/issues/2559",2017-07-27 15:47:38,,DOC: Clearly document functions with duck-type compatibility,['04 - Documentation']
9474,open,joshthoward,"For example, below I am trying to use np.issubdtype to check if a pandas categorical is a subtype of a np.number. IMO this should definitely be false rather than throwing an error. 

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-99-4ba76ca46d4a> in <module>()
     32 
---> 33 np.issubdtype(df.dtypes[1], np.number)

/Users/jhoward/homebrew/lib/python2.7/site-packages/numpy/core/numerictypes.pyc in issubdtype(arg1, arg2)
    753     """"""
    754     if issubclass_(arg2, generic):
--> 755         return issubclass(dtype(arg1).type, arg2)
    756     mro = dtype(arg2).type.mro()
    757     if len(mro) > 1:

TypeError: data type not understood

```

This issue should be fixed on numpy's side because external types should by default compare as false unless the external package developers have explicitly used the numpy type system. 

If this is a reasonable update, I would be glad to fix this myself.",2017-07-27 14:30:01,,np.issubdtype does not work well with pandas,"['15 - Discussion', 'component: numpy.dtype']"
9465,open,cbyrohl,"When computing histograms of float32 arrays with a large dynamical range in weights and summed weights over bins, the resulting weighted bin can be zero rather than the correct (small) value. I attached a minimal example below.

This problem vanishes when using float64 as input for the weights. However, this is far from being intuitive for the user: The sum of positive float32s should not add up to zero. 

```python
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import binned_statistic

print(np.__version__)

x = np.fromfile('x', dtype=np.float32)
weights = np.fromfile('weights', dtype=np.float32)

nbins = 100
bins = np.logspace(-12,1,nbins+1)

hist_scipy,_,_= binned_statistic(x, weights, bins=bins, statistic='sum')
hist_numpy32,_ = np.histogram(x, bins=bins, weights=weights)
hist_numpy64,_ = np.histogram(x, bins=bins, weights=weights.astype(np.float64))


plt.plot(bins[1:],hist_numpy64, label='numpy-float64')
plt.plot(bins[1:],hist_numpy32, label='numpy-float32', ls='dotted')
plt.plot(bins[1:],hist_scipy, label='scipy', ls='dashed')
plt.xscale('log')
plt.yscale('log')
plt.legend()
plt.show()
```

Using Python 3.5 with numpy 1.13.1 under Scientific Linux release 7.3.

Example data attached. [issue.zip](https://github.com/numpy/numpy/files/1177069/issue.zip)",2017-07-26 15:29:07,,BUG: Wrong results from histogram for small weights,['unlabeled']
9456,open,st-pasha,"[PEP 3118](https://www.python.org/dev/peps/pep-3118/) describes Buffers protocol to implement data interchange between different Python/C objects.

I'm implementing such an object, which exposes `PyBuffers` interface for the purpose of interoperability with other libraries. For example, I expect that if `x` is my object, then `numpy.array(x)` would convert it into a numpy array via the Buffers protocol.

What I find however is that `numpy.array()` constructor calls `PyObject_GetBuffer()` on my object with `flags=0x11C` (which is a combination of `PyBUF_INDIRECT | PyBUF_FORMAT`). According to the specification, the `PyBUF_INDIRECT` should be used
> ... when the consumer can handle indirect array referencing implied by these suboffsets.

Unfortunately `numpy` does not support arrays with suboffsets, and therefore either displays incorrect information or causes a seg.fault when trying to read memory without properly following the pointers as required by the spec.

The expected behavior is that `numpy` would not supply the `PyBUF_INDIRECT` flag when calling `PyObject_GetBuffer`, but instead use the `PyBUF_STRIDES` flag which seems to correspond to what `numpy` can actually handle.
",2017-07-24 19:28:06,,[BUG] numpy implements PEP-3118 incorrectly,['unlabeled']
9450,open,anntzer,"The docstring of np.take says ""This function does the same thing as “fancy” indexing (indexing arrays using arrays)"", but in fact the returned layout can be different:
```python
>>> t = np.zeros((6, 5, 4))
>>> t.strides
(160, 32, 8)
>>> t[..., [2, 1, 0, 3]].strides
(40, 8, 240)
>>> np.take(t, [2, 1, 0, 3], axis=2).strides
(160, 32, 8)
```
(numpy 1.13.1, py3.6.1).

np.take appears to return a C-contiguous array whereas fancy indexing perhaps tries to copy elements corresponding to each of the indices of the explicitly given index array one chunk at a time?  In any case it would be nice if this behavior was at least documented on `np.take`'s side; or, one of them is faster than the other (haven't benchmarked), perhaps both should default to the same approach, given that the output layout can always be forced (I guess?) by passing the `out` kwarg.",2017-07-23 08:59:55,,np.take and fancy indexing return arrays with different layouts,['04 - Documentation']
9448,open,koder-ua,"I often need to count elements in the array, that greater than some value. Obvious solutions, like 
`numpy.count_nonzero(vec > max_val)`
create a tempopary array of bools, which is inefficient. I suggest add functions like
'numpy.count_greater(array: numpy.ndarray, value: X)', 'numpy.count_less(array: numpy.ndarray, value: X)', etc. ",2017-07-21 19:04:47,,Add count_greater/count_less and other functions like this.,['unlabeled']
9443,open,eric-wieser,"This was the cause of #3879.

Unless we change the api, this means that `->compare` on types needing the python API needs to start with the following - which should probably end up in the documentation:
```C
if (PyErr_Occured()) {
    return 0;  /* stable ordering */
}
```

This was fixed for `OBJECT_compare` in #9412, but is also needed for any user type.

It's also possible that `VOID_compare` needs this fix for comparing an objec but it's not clear what a failing test case would look like",2017-07-20 20:14:28,,ArrFuncs->compare has no way to indicate an exception,"['00 - Bug', '04 - Documentation']"
9442,open,NeilGirdhar,"First, a shape (4,) numpy array within a record can be assigned:
```
In [33]: a = np.zeros((10,), dtype=[('k', '<u8'), ('t', '<f4'), ('d', np.bool, (4,))])

In [34]: b = np.ones((), dtype=np.bool)

In [35]: a[0][2][3] = b
```
But, a shape () cannot:
```
In [36]: a = np.zeros((10,), dtype=[('k', '<u8'), ('t', '<f4'), ('d', np.bool, ())])

In [37]: a[0][2][()] = b
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-37-075e4ba95b60> in <module>()
----> 1 a[0][2][()] = b

TypeError: 'numpy.bool_' object does not support item assignment
```
But if it's not in a record, it works just fine:
```
In [38]: a = np.zeros((), np.bool)

In [39]: a[()] = b
```
",2017-07-20 14:12:13,,BUG: Zero-dimensional numpy arrays within records decay to scalars,['00 - Bug']
9441,open,max-sixty,"This is a fairly narrow case...

If an array contains an object that implements `__getitem__` (more generally: an object that passes `PySequence_Check`, I think):

```python

In [8]: class Y(object):
   ...:     def __getitem__():
   ...:         raise NotImplementedError
   ...:

In [10]: np.array(Y())
Out[10]: array(<__main__.Y object at 0x105018208>, dtype=object)

```

...then calling `.astype(str)` on that array fails:

```
In [11]: np.array(Y()).astype(str)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-11-5c1a7c1db5f7> in <module>()
----> 1 np.array(Y()).astype(str)

ValueError: setting an array element with a sequence
```

I would have expected it would convert the object to a string, e.g. with `str(Y())`

FYI `In [14]: np.__version__; Out[14]: '1.13.1'`",2017-07-20 00:57:37,,Calling .astype(str) fails on an array that contains an object implementing __getitem__,['00 - Bug']
9439,open,meawoppl,"The results of `savez()` depend on the current time, because the underlying implementation uses `tempdir` to cache result about to be written.  From there the internal instance of `ZipFile()` auto-detects the time/date and encodes them into the zipfile headers somewhere.  

Here is a simple demo:
```python
In [1]: import numpy as np

In [2]: import io

In [3]: bio1 = io.BytesIO()

In [4]: bio2 = io.BytesIO()

In [5]: np.savez_compressed(bio1, foo=""bar"")

In [6]: import time

In [7]: time.sleep(1)

In [8]: np.savez_compressed(bio2, foo=""bar"")

In [9]: bio1.getvalue()
Out[9]: b'PK\x03\x04\x14\x00\x00\x00\x08\x00\x84s\xf2J\xd6i\x8cTK\x00\x00\x00\\\x00\x00\x00\x07\x00\x00\x00foo.npy\x9b\xec\x17\xea\x1b\x10\xc9\xc8\xe0\xc6P\xad\x9e\x92Z\x9c\\\xa4n\xa5\xa0n\x13j\xac\xae\xa3\xa0\x9e\x96_TR\x94\x98\x17\x9f_\x94\x92\n\x12wK\xcc)N\x05\x8a\x17g$\x16\xa4\x02\xf9\x1a\x9a:\n\xb5\n(\x80+\x89\x81\x81!\x11\x88\x8b\x80\x18\x00PK\x01\x02\x14\x03\x14\x00\x00\x00\x08\x00\x84s\xf2J\xd6i\x8cTK\x00\x00\x00\\\x00\x00\x00\x07\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x80\x81\x00\x00\x00\x00foo.npyPK\x05\x06\x00\x00\x00\x00\x01\x00\x01\x005\x00\x00\x00p\x00\x00\x00\x00\x00'
                                               ^ HERE
In [10]: bio2.getvalue()
Out[10]: b'PK\x03\x04\x14\x00\x00\x00\x08\x00\x8cs\xf2J\xd6i\x8cTK\x00\x00\x00\\\x00\x00\x00\x07\x00\x00\x00foo.npy\x9b\xec\x17\xea\x1b\x10\xc9\xc8\xe0\xc6P\xad\x9e\x92Z\x9c\\\xa4n\xa5\xa0n\x13j\xac\xae\xa3\xa0\x9e\x96_TR\x94\x98\x17\x9f_\x94\x92\n\x12wK\xcc)N\x05\x8a\x17g$\x16\xa4\x02\xf9\x1a\x9a:\n\xb5\n(\x80+\x89\x81\x81!\x11\x88\x8b\x80\x18\x00PK\x01\x02\x14\x03\x14\x00\x00\x00\x08\x00\x8cs\xf2J\xd6i\x8cTK\x00\x00\x00\\\x00\x00\x00\x07\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x80\x81\x00\x00\x00\x00foo.npyPK\x05\x06\x00\x00\x00\x00\x01\x00\x01\x005\x00\x00\x00p\x00\x00\x00\x00\x00'
                                               ^ HERE
In [11]: bio1.getvalue() == bio2.getvalue()
Out[11]: False
```

A minimum of 1 second difference is necessary for byte-wise similarity to fail.",2017-07-18 21:31:20,,BUG: `savez()` is not deterministic,"['00 - Bug', 'component: numpy.lib']"
9438,open,inducer,"When I run this snippet here:

```python
import numpy as np

class MyObject:
    def __radd__(self, other):
        print(type(other))
        assert isinstance(other, np.float64)
        return 0

np.float64(12) + MyObject()
```

I would expect `other` in `__radd__` to still be the `float64` that's passed in. But if I run this in Python 3.5.2 or 2.7.11 with numpy 1.12.1, I get a `float`.

I looked through what I believe is the [code for binary operators in CPython](https://github.com/python/cpython/blob/4ed5ad79ec6c6270e6018bd0a55656305ee60907/Objects/abstract.c#L767-L802), and I don't see anything that might be responsible.",2017-07-18 17:44:44,,Numpy scalars/arrays mysteriously cast to Python types in __radd__,['unlabeled']
9436,open,perimosocordiae,"gh-8557 adds a `hermitian=False` kwarg to `np.linalg.matrix_rank`, which allows the use of a more efficient algorithm when the matrix is known to be hermitian.

Some other functions in `np.linalg` have similar considerations for this case, but require calling a different function for the hermitian case:
 * `eigvals` / `eigvalsh`
 * `eig` / `eigh`

(There's also `fft` vs `hfft`, but I don't want to touch that API if I can avoid it.)

The proposal: add `hermitian` keyword arguments to `eigvals` and `eig`, and encourage their use in place of `eigvalsh` and `eigh`.

Considerations: gh-8557 uses a simple boolean value, but @eric-wieser suggested allowing `UPLO`-style strings as well. This can be added to `matrix_rank` retroactively if desired, without breaking compatibility.",2017-07-18 13:53:23,,ENH: Use hermitian kwarg in favor of h-suffix,['01 - Enhancement']
9409,open,eric-wieser,"Taking the enhancement in #8446 even further. This would allow:

```python
x, y = np.meshgrid(np.linspace(-2, 2, 5), np.linspace(-4, 4, 9))
z = x**2 + y**2
dzdx, dzdy = np.gradient(z, x, y)  # Case A
assert_equal(dzdx, 2*x)
assert_equal(dzdy, 2*y)

# edit: also with arguments swapped
dzdy, dzdx = np.gradient(z, y, x)  # Case B: swapped coordinates
assert_equal(dzdx, 2*x)
assert_equal(dzdy, 2*y)
```

Right now, this errors, because `x` and `y` are 2d",2017-07-13 13:46:05,,ENH: add ND generalizations to `np.gradient`,"['01 - Enhancement', 'component: numpy.lib']"
9401,open,evanmason,"With np.gradient v1.11.0 it was possible to use 2d irregular dx and dy arguments, for example:
```
from pyproj import Proj
lon, lat = meshgrid(arange(-4, 5, 1.5), arange(3, 6, 1.5))
proj = Proj('+proj=aeqd +lat_0=%s +lon_0=%s' % (lon.mean(), lat.mean()))
dx, dy = proj(lon, lat)
data = lon + lat
ddx, ddy = gradient(data, dx, dy)
```
However with v.1.13.0 (and also in v.1.12.0) there is now an error:
```ValueError: distances must be either scalars or match the length of the corresponding dimension```

I know that gradient was recently updated #8446.  I wonder if this functionality was overlooked, or expressly omitted.  Either way it is useful when working with geophysical model data, and it would be nice to have it back again.



",2017-07-12 13:29:59,,gradient differences between v1.11.0 and v1.13.0,['unlabeled']
9397,open,jungkb,"I have an application that requires to generate multiple histograms from a (25000,5000) array using bincount with 2^16 bins (16 bit values) each, e.g. the result would be a (65536,5000) array. The application first generates the initial histogram with apply_along_axis and then updates it up to 4000 times. Applying bincount along the axis is relatively fast, but adding the resulting (65536,5000) arrays is very slow. 

My application would significantly improve, if bincount would be able to update a previous array returned by bincount, e.g. if it could run incremental and if it could be applied along an axis similar to the sum or mean functions instead of using apply_along_axis.",2017-07-10 10:02:01,,Feature request: Incremental bincount and axis argument,['unlabeled']
9394,open,chewxy,"What is seen: 
``` python
>>> import numpy as np
>>> import numpy.ma as ma
>>> y = ma.array([1, 2, 3], mask = [0, 1, 0])
>>> incr = np.array([100,100,100])
>>> x = np.array([2,4,6])
>>> incr += x + y
>>> incr
array([103, 104, 109])
```

While this is somewhat logical, this starts to make less sense when you come to something like this:

``` python
>>> y = ma.array([1, 2, 3], mask = [0, 1, 0])
>>> incr = np.array([100,100,100])
>>> x = np.array([2,4,6])
>>> incr += x * y
>>> incr
array(102, 104, 118])
```

Here we see that the equivalent operation is `incr[1] += x[1]` .

Combined with the above case with addition, we can rapidly come to a conclusion that it's essentially doing this: `incr[idx_with_invalid] += x[idx_with_invalid] <OP> identity`. I'm not sure if that's the *correct* thing to do.

The documentation on the website is unclear on this either, stating only:

> Masked arrays also support standard numpy ufuncs. The output is then a masked array. The result of a unary ufunc is masked wherever the input is masked. The result of a binary ufunc is masked wherever any of the input is masked. If the ufunc also returns the optional context output (a 3-element tuple containing the name of the ufunc, its arguments and its domain), the context is processed and entries of the output masked array are masked wherever the corresponding input fall outside the validity domain


So, the question is what would the correct interpretation be with these sorts of interactions? If it's supposed to be the identity of the operation then it should be noted  in the docs",2017-07-09 23:51:14,,Array-MaskedArray inplace ufunc interactions can turn out nonsensical results,['component: numpy.ma']
9370,open,bobeldering,"numpy: 1.14.0
component: f2py
python: 3.5

When a object of the bytes class is assigned to a common block variable of the type CHARACTER*(n), what seems to happens is that each byte is interpreted as an integer and cast to string, the first character of the resultant string is then stored in the Fortran data structure. While at the same time a CHARACTER*(n) intent(out) variable is returned as a bytes type.

For example, compiling:

```
      SUBROUTINE FOO(OUT_STRING)
      CHARACTER OUT_STRING*(3)
Cf2py intent(out) OUT_STRING
      CHARACTER BLOCK_STRING*(3)
    
      COMMON  /BLOCK/ BLOCK_STRING
      OUT_STRING = 'Foo'
      RETURN
      END
```

as stringtest, gives the following results:

```
>>> import stringtest
>>> stringtest.block.block_string = stringtest.foo()
>>> stringtest.block.block_string
array(b'711',
      dtype='|S3')
>>> stringtest.foo()
b'Foo'
>>> stringtest.block.block_string = ""foo""
>>> stringtest.block.block_string
array(b'foo',
      dtype='|S3')

```",2017-07-06 11:57:31,,BUG: f2py inconsistent handling of bytes type,['component: numpy.f2py']
9342,open,matthew-brett,"With Appveyor file:

```
# vim ft=yaml

build_script:
  - pip install numpy

test_script:
  # Adding this line causes the tests to hang forever
  # - PATH=%PATH%;c:\msys64\usr\bin
  # Unless you uninstall gcc-fortran like this
  # - pacman -R --noconfirm gcc-fortran
  - pip install nose
  - python -c ""import numpy.f2py; numpy.f2py.test(verbose=4)""
```


Without adding msys64 to the path, install + f2py tests pass in 30 seconds or so:

https://ci.appveyor.com/project/matthew-brett/windows-wheel-builder/build/1.0.53#L29

Adding msys64 to the path - hangs until build timeout (I canceled this one after 90 seconds):

https://ci.appveyor.com/project/matthew-brett/windows-wheel-builder/build/1.0.52#L16

Adding msys64 to the path, but uninstalling gcc-fortran, tests pass again:

https://ci.appveyor.com/project/matthew-brett/windows-wheel-builder/build/1.0.51#L40
",2017-07-01 11:39:14,,TST: Windows `f2py` tests hang indefinitely with msys gcc-fortran on PATH,"['05 - Testing', 'component: numpy.f2py', 'component: CI']"
9331,open,cwfinn,"OS: macOS Sierra (10.12.5)
Python version: 2.7.12
numpy version: 1.13.0

I notice that [`numpy.testing.assert_array_equal`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.testing.assert_array_equal.html#numpy-testing-assert-array-equal) should compare NaNs as numbers unlike elsewhere in numpy, which makes sense to me. I verify this with the following test:

```python
>>> import numpy.testing as npt
>>> import numpy as np
>>> a = np.array([1, 2, np.nan])
>>> npt.assert_array_equal(a, a)
```

However, this does not work with arrays that have dtype object:

```python
>>> b = np.array([1, 2, 'a', np.nan], dtype=object)
>>> npt.assert_array_equal(b, b)
```
```
AssertionError: 
Arrays are not equal

(mismatch 25.0%)
 x: array([1, 2, 'a', nan], dtype=object)
 y: array([1, 2, 'a', nan], dtype=object)
```

Is this the desired behaviour? The only workaround in my case is to cast the array as dtype str.",2017-06-29 15:22:17,,assert_array_equal does not compare NaNs as numbers when dtype is object,"['01 - Enhancement', 'component: numpy._core', '62 - Python API']"
9313,open,eric-wieser,"Why is taking the first element of a subarray sensible behaviour?

```python
>>> a = np.array(([1, 2, 3],), dtype=[('v', int, 3)])
>>> a
array(([1, 2, 3],), 
      dtype=[('v', '<i4', (3,))])
>>> np.array(a, dtype=int)
array(1)
```",2017-06-27 12:08:19,,BUG: Subarrays casts truncate and zero-pad without error or warning,['component: numpy._core']
9311,open,Zaharid,"See https://github.com/pandas-dev/pandas/issues/16778

```python
import numpy as np
c = []
c.append(c)
c.append(c)
np.array(c)
```

The code above hangs calling recursively the C level function `PyArray_DTypeFromObjectHelper`. Interestingly if I only append `c` once I get something rather strangely looking:
```python
In [13]: c = []

In [14]: c.append(c)

In [15]: np.aray(c)
In [16]: np.array(c)
Out[16]: array([[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[list([[...]])]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]], dtype=object)
```



",2017-06-27 10:19:12,,determining the datatype of recursive list is slow,['component: numpy._core']
9303,open,eric-wieser,"Repro:
```python
>>> dt = np.dtype([('value', int, 3)])
>>> m = np.ma.array(([1, 2, 3],), dtype=dt, fill_value=([10, 20, 30],))
masked_array(data = ([1, 2, 3],),
             mask = ([False, False, False],),
       fill_value = ([10, 10, 10],),           # wrong!
            dtype = [('value', '<i4', (3,))])
```

Or perhaps more alarmingly:
```python
>>> m = np.ma.array(([1, 2, 3],), dtype=dt)
>>> m.fill_value['value'][1] = 20
masked_array(data = ([1, 2, 3],),
             mask = ([False, False, False],),
       fill_value = ([10, 20, 10],),           # OK!
            dtype = [('value', '<i4', (3,))])
>>> np.ma.array(m)
masked_array(data = ([1, 2, 3],),
             mask = ([False, False, False],),
       fill_value = (10,),                     # Wait, what?
            dtype = [('value', '<i4', (3,))])
```
Is any of this by design? Surely `m.dtype == m.fill_value.dtype` is an invariant?",2017-06-26 23:48:17,,BUG: np.ma.array changes subdtype fill_value,"['00 - Bug', 'component: numpy.ma']"
9287,open,631068264,"I use numpy to deal with my stock data

    print(array[:5])
    print(array.dtype)
    print(array[0]['open'], array[0]['high'], array[0]['low'], array[0]['close'])
    print(array[0][:5])

And I get this

    [ (Timestamp('2017-05-18 18:00:00'),  11420. ,  11450. ,  11302.  ,  11310.1,  320.347)
     (Timestamp('2017-05-18 19:00:00'),  11310.1,  11397.9,  11300.1 ,  11350.1,  218.755)
     (Timestamp('2017-05-18 20:00:00'),  11360. ,  11599.9,  11350.1 ,  11501. ,  688.596)
     (Timestamp('2017-05-18 21:00:00'),  11520. ,  11520. ,  11400.11,  11467. ,  411.421)
     (Timestamp('2017-05-18 22:00:00'),  11467. ,  11849.9,  11467.  ,  11840. ,  947.689)]

    [('datetime', 'O'), ('open', '<f8'), ('high', '<f8'), ('low', '<f8'), ('close', '<f8'), ('volume', '<f8')]
    
    11420.0 11450.0 11302.0 11310.1


But `array[0][:5]` can not work I don't know why? Get an error `IndexError: invalid index` But `array[0][4]` can work it's very strange for me. Friends how to solve this problem.

Is it related to dypes.names? but how to remove it to None?
",2017-06-22 10:28:02,,structured scalars don't support slicing,['unlabeled']
9283,open,mortonjt,"It would be really nice if there was a convenience function to perform an argmax over a 2D array and return the row and column indexes of the maximum.

I'm often finding myself reusing the following code

```python
def argmax2d(X):
    n, m = X.shape
    x_ = np.ravel(X)
    k = np.argmax(x_)
    i, j = k // m, k % m
    return i, j
```
While it is simple, its opaque enough where I have to consistently look this function up.
It would be nice if this was already available in numpy.

Would there be any objections to opening up a PR in numpy to have this functionality readily available?",2017-06-21 13:54:06,,Request: argmax2d,['unlabeled']
9270,open,mwtoews,"I'm seeing slightly different and unexpected behavior with unmasked and masked arrays in augmented assignments.

For example, the result of _masked_ += _unmasked_ is expected:
```python
import numpy as np

def ab():
    return np.array([1, 1]), np.ma.array([5, 5], mask=[False, True])

a, b = ab()
print('a: {0}; b: {1}; b.data: {2}'.format(a, b, b.data))
# a: [1 1]; b: [5 --]; b.data: [5 5]

b += a
print('b: {0}; b.data: {1}'.format(b, b.data))
# b: [6 --]; b.data: [6 5]
```

however the augmented _unmasked_ += _masked_ applies the operation to all elements, regardless of any masked values:
```python
a, b = ab()
a += b
print('a: {0}'.format(a))
# a: [6 6]
```
I would have expected `a: [6 5]`, similar to `b.data` shown above. Similar behavior is found with different augmented operators.

And as a side note, I get different behavior for _unmasked_ + _masked_ with different versions:
```python
a, b = ab()
c = a + b
print('{0}: c: {1}; c.data: {2}'.format(np.__version__, c, c.data))
```
Testing a few versions that I have on-hand:
```
1.4.1: c: [6 --]; c.data: [6 5]
1.11.3: c: [6 --]; c.data: [6 1]
```
It's a minor detail, but an odd one why these are different. The older NumPy behavior is similar to first augmented example which works as expected for both of these NumPy versions tested.",2017-06-19 23:57:32,,Augmented assignments between unmasked and masked arrays,['component: numpy.ma']
9265,open,AndreiCostinescu,"Hi all,

I have this example
```python
import numpy as np
s = ""/home/usr/tmp.txt""
d = {s: np.array([1, 2, 3])}
np.savez('tmp', **d)
f = np.load('tmp.npz')
print(f.keys())
```
And the output I get is
```python
['home/usr/tmp.txt']
```
I want to be able to save filepaths (so any kind of strings) as the dictionary entries (so array filenames) of the numpy .npz archive.
What would need to be done in order to achieve that the output of the above program is
```python
['/home/usr/tmp.txt']
```
I am using numpy version: numpy-1.12.1 on both a windows 10 and a ubuntu 14.04 machine.",2017-06-18 03:01:31,,"Numpy removes ""/"" at first position from dictionary string keys when saving",['unlabeled']
9252,open,jbrockmendel,"The top results for Google searches like ""numpy subclass ndarray"" point toward the [1.12.0](https://docs.scipy.org/doc/numpy-1.12.0/reference/arrays.classes.html) docs.  Three requests/suggestions.

1) A prominent ""There is a newer version"" link to the 1.13.0 docs.

2) The 1.12.0 page linked above directs the user to use `__numpy_ufunc__` for subclassing.  I'm not entirely clear on the timeline in #8247, but I couldn't make numpy_ufunc work with 1.12.0, so it isn't obvious whether the doc is accurate or (much more likely) I'm screwing up.

3) A non-toy example for numpy_ufunc and/or array_ufunc, ideally demonstrating the tie-ins or use cases for the array_prepare/array_finalize/...",2017-06-14 17:00:37,,__numpy_ufunc__ confusion in docs,['unlabeled']
9244,open,charris,"Currently
```
In [1]: a = np.ones(10)[::2]

In [2]: a.flags
Out[2]: 
  C_CONTIGUOUS : False
  F_CONTIGUOUS : False
  OWNDATA : False
  WRITEABLE : True
  ALIGNED : True
  UPDATEIFCOPY : False

In [3]: b = np.array(a.flat, copy=False)

In [4]: a.flags
Out[4]: 
  C_CONTIGUOUS : False
  F_CONTIGUOUS : False
  OWNDATA : False
  WRITEABLE : False
  ALIGNED : True
  UPDATEIFCOPY : False
```
The reason is that `a.flat.__array__` is called which returns a new array with UPDATEIFCOPY and locks `a` by clearing the WRITEABLE flag. Because b hangs onto a reference of the uncopied array, it does not get deallocated, hence does not re-enable the WRITEABLE flag.

Whether this is a bug or a feature is debatable, but given the usual use of `asarray` (`copy=False`), I think that a copy of the array should be made when UPDATEIFCOPY is set.",2017-06-11 20:52:14,,"BUG?: np.array(a, copy=False) should check UPDATEIFCOPY flag.","['00 - Bug', 'component: numpy._core']"
9225,open,stefdoerr,"Numpy version: 1.12.1

I want to have an array of arrays in numpy and be able to modify them (i.e. delete rows from the inner arrays). When creating an object array of same-sized arrays, numpy automatically concatenates them. When they are different sized it does not. Hence when they are same sized, deleting a row throws an error, while if they are different sized it does not. This is highly confusing.
Here is an example:

```python
In []: import numpy as np

In []: x = np.array([np.zeros((3, 2)), np.zeros((5, 8))], dtype=object)

In []: x.shape  # It made an array of arrays
Out[]: (2,)

In []: x[0] = np.delete(x[0], 1, axis=0)  # This works!

In []: x = np.array([np.zeros((3, 2)), np.zeros((3, 2))], dtype=object)

In []: x.shape  # It converted it to a 3D array!
Out[]: (2, 3, 2)

In []: x[0] = np.delete(x[0], 1, axis=0)  # This fails
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-6-1687d284d03c> in <module>()
----> 1 x[0] = np.delete(x[0], 1, axis=0)

ValueError: could not broadcast input array from shape (2,2) into shape (3,2)
```

Therefore in the odd case that I end up with an array of same sized arrays my code crashes. The only workaround I found is converting the object array to a list, modifying the element and converting back which is ugly.

```python
In [37]: x = np.array([np.zeros((3, 2)), np.zeros((3, 2))], dtype=object)

In [38]: xx = [z for z in x]

In [39]: xx[0] = np.delete(xx[0], 1, axis=0)

In [40]: x = np.array(xx, dtype=object)

In [41]: x
Out[41]: 
array([array([[0.0, 0.0],
       [0.0, 0.0]], dtype=object),
       array([[0.0, 0.0],
       [0.0, 0.0],
       [0.0, 0.0]], dtype=object)], dtype=object)
```",2017-06-07 14:31:58,,Inconsistent behaviour of numpy object arrays,['unlabeled']
9206,open,eric-wieser,"```
>>> m = np.ma.array([""hello""])
>>> np.ma.take(m, 0)
  File ""\numpy\ma\core.py"", line 5670, in take
    out = _data.take(indices, axis=axis, mode=mode)[...].view(cls)
TypeError: string indices must be integers
```

Since `np.string_[...]` is not supported

Introduced by #7586",2017-06-02 11:18:01,,BUG: np.ma.take does not work on string arrays,"['00 - Bug', 'component: numpy.ma']"
9203,open,mhvk,"Inspired by the [trouble](https://github.com/numpy/numpy/pull/9202/files#r119664504) in #9202 to get things to built after moving a couple of functions to a new source file in `multiarray`: it would be lovely if there was some overview of how the core source is organised, what flags like `NUMP_API` mean, and perhaps a work-flow example for editing it, ensuring corresponding entries in, e.g., `_core/code_generators/genapi.py` are updated, etc.

EDIT: 2024-JAN-12: updated following file changes. Still seems worthwhile documenting better.",2017-06-01 17:10:33,,"Document API generation with genapi.py, etc.","['23 - Wish List', '16 - Development', '04 - Documentation']"
9193,open,indiajoe,"I would like to add an example in the documentation of `np.ma.polyfit` to demonstrate that the 2D mask is collapsed on 1D mask before doing the polyfit of a masked array. The current documentation is not very clear on how `np.ma.polyfit` deals with 2D mask while fitting a 2D y array.

For example.
```python
A = np.ma.array([[0, 0, 0],
                 [1, 1, 1],
                 [2, 2, 2],
                 [10, 14, 99]])

x = np.arange(A.shape[0])

# only the mA[3,2] entry is masked in mA below
mA = np.ma.masked_greater(A,90)

print(mA)
# polynomial fit of all three columns of A
np.ma.polyfit(x,A,1)
# Outputs: array([[  3.1,   4.3,  29.8],
                 [ -1.4,  -2.2, -19.2]])

# polynomial fit of all three columns of mA
np.ma.polyfit(x,mA,1)
# Outputs: array([[  1.00000000e+00,   1.00000000e+00,   1.00000000e+00],
                       [  4.10073934e-17,   4.10073934e-17,   4.10073934e-17]])

# Instead of expected:  array([[  3.1,   4.3,  1.00000000e+00],
                     [ -1.4,  -2.2, 4.10073934e-17]])


```
Important Note: Masking one element in the last column affected the polynomial fit of all columns.",2017-05-31 04:08:44,,Clarifying the mask propogation in np.ma.polyfit in documentation,"['00 - Bug', 'component: numpy.lib', 'component: numpy.ma']"
9182,open,HerrMuellerluedenscheid,"Hello together,

I use argmax along axis 0 on large 2D arrays (let's say larger than 1000x1000 elements). When executing the concerning argmax line in my code I saw a doubling of memory usage (using top) and while trying to figure out why the required memory was increasing by a factor of two I also discovered, that it is slightly faster to iterate through the array with a plain for loop and apply argmax slice by slice, as in the example below. Also the memory doubling did not occur this way.


    import numpy as num

    nx = 10000
    ny = 10000
    frames = num.random.random((nx, ny))

    def my_argmax(a):
        i0, i1 = a.shape
        idx = num.zeros(i1, dtype=num.int)
        for iax, ax in enumerate(a.T):
            idx[iax] = num.argmax(ax)

        return idx

    # measuring performance of both 
    import timeit
    setup = ""from __main__ import frames, my_argmax\nimport numpy as num""
    n = 10
    print timeit.timeit(stmt=""my_argmax(frames)"", setup=setup, number=n)         
    print timeit.timeit(stmt=""num.argmax(frames, axis=0)"", setup=setup, number=n)

    # assure equal results:
    imaxs1 = my_argmax(frames)
    imaxs2 = num.argmax(frames, axis=0)
    print all(imaxs1 == imaxs2)

Which prints the following results:

    21.6250059605
    22.8837270737
    True

As stated above I used top to track the memory load. 
As you can see my_argmax is (in this constructed example) a tiny little bit faster. This, of course, strongly depends on how many iterations have to be performed in the for loop compared to elements in each slice to be searched for to find the maxima. However, what was bothering me much more was the memory issue. I felt that this is a rather unexpected behavior since I don't see why there should that much of additional memory be required.

Best regards
Marius

(Using python 2.7.9, numpy 1.14.0.dev0+c556b42)

",2017-05-28 12:49:03,,ENH: argmax can be made faster for non-contiguous axes.,"['01 - Enhancement', 'component: numpy._core']"
9158,open,vlasenkov,"From [here](https://docs.scipy.org/doc/numpy/reference/c-api.array.html#c.PyArray_DescrConverter):

> PyObject* PyArray_Scalar(void* data, PyArray_Descr* dtype, PyObject* itemsize)
> Return an array scalar object of the given enumerated typenum and itemsize by **copying** from memory pointed to by data . If swap is nonzero then this function will byteswap the data if appropriate to the data-type because array scalars are always in correct machine-byte order.

But this C code:

```
static int64_t buffer[] = {7, 14};

static PyObject *
test(PyObject *self, PyObject *args) {
      PyObject *dtype;
      PyObject *is;
      PyArg_ParseTuple(args, ""O!O!"", &PyArrayDescr_Type, &dtype, &PyLong_Type, &is);
      PyObject *ret = PyArray_Scalar(buffer, (PyArray_Descr *)dtype, is);
      return ret;
}

static PyObject *
test2(PyObject *self, PyObject *args) {
      buffer[0] = 100;
      Py_RETURN_NONE;
}
```

Produces this (Numpy version: 1.12.0):

```
>>> dt = np.dtype([('a', np.int64),('b',np.int64)])
>>> a = test(dt, dt.itemsize)
>>> a
(7, 14)
>>> test2()
>>> a
(100, 14)
```

So, `PyArray_Scalar` does not copy memory. Is it a mistake in doc or a bug?",2017-05-22 11:42:02,,PyArray_Scalar behaves like it is not supposed to in docs,['04 - Documentation']
9147,open,eric-wieser,"```python
>>> x = np.array([(0, 1, 0)] * 10, dtype=[('f0', '<i4'), ('f1', '<i4'), ('f2', '<i4')])
>>> x == x
array([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
>>> np.equal(x, x)
FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
NotImplemented
```

Presumably this has ramifications for users overloading `__array_ufunc__` with @shoyer's mixin class, causing operations to fail on structured dtypes - as these do not pass through `ndarray.__eq__`, which seems to special case things to make them work",2017-05-20 12:53:07,,BUG: Operator equal supports `void` and np.equal does not,"['00 - Bug', 'component: numpy.dtype']"
9128,open,djkirkham,"```
>>> import operator
>>> import numpy as np
>>> print np.__version__
1.11.1
>>> a = np.int16(1)
>>> b = np.int16(2)
>>> print np.true_divide(a, b).dtype
float64
>>> print operator.truediv(a, b).dtype
float32
```
Arrays, including scalar arrays, give float64 in both cases.",2017-05-17 13:50:37,,numpy.true_divide results in different dtype to operator.truediv for int16 scalars,['unlabeled']
9126,open,jakirkham,"Would be nice to be able to specify a `dtype` for `fftfreq`/`rfftfreq` other than `float64` (e.g. `float32`), to which it appears to default.",2017-05-17 02:50:14,,ENH: Optional dtype for fftfreq/rfftfreq,"['01 - Enhancement', 'component: numpy.fft']"
9125,open,BowenFu,"As in scipy, we have
`scipy.linalg.eig(a, b=None, left=False, right=True, overwrite_a=False, overwrite_b=False, check_finite=True, homogeneous_eigvals=False)`,
where b is a right-hand side matrix, which can be very useful for generalized eigenvalue problems.

Now in numpy the code has to be
```
invb_a = numpy.linalg.inv(b) * a
w, v = numpy.linalg.eig(invb_a)
```
.
With numpy.linalg.eig(a, b) the code will be like
```
w, v = numpy.linalg.eig(a, b)
```
.
",2017-05-16 15:39:55,,Add a right-hand side matrix parameter in numpy.linalg.eig(h).,"['01 - Enhancement', 'component: numpy.linalg']"
9103,open,LevN0,"MaskedArray's min/max methods will raise an exception if its dtype is object. However ndarray min/max methods will work just fine. For example,

```
>>> test = np.asarray([1,2,3], dtype='object')
>>> test.min()
1

>>> test = np.ma.masked_array([1,2,3], dtype='object', mask=[True, False, False])
>>> test.min()
AttributeError: 'int' object has no attribute 'view'
```

These methods are useful for cases when having to use Python objects as dtype because your data contains BigInteger and BigDouble (i.e. int and Decimal respectively). 

The issue seems to lie in this line in ``np.ma.core.min``,

`result = self.filled(fill_value).min(axis=axis, out=out, **kwargs).view(type(self))`

The view call will fail when dtype is object, because min(...) will return e.g. int or Decimal, rather than some type of ndarray.
",2017-05-11 21:47:00,,MaskedArray min/max methods do not work for dtype object,"['00 - Bug', 'component: numpy._core', 'component: numpy.ma']"
9098,open,cvanwynsberghe,"**What's the need?**
random.multivariate_normal works great for but is limited to sampling real vectors. Today, if ones would like to generate from complex multivariate distribution from (mu, Omega), sampling would need these steps:

1. get Omega^1/2 by cholesky decomposition (or svd)
2. generate a serie z of random complex gaussian numbers whose elements are in Nc(0, 1)
3. get the sample mu + Omega^1/2 * z

**Featuring it? How?**
Would it be possible to extend today's function to complex numbers? I imagine two possibilites 
for implementing that:
1.  in the case where the given input mean array and covariance matrix are complex type, get the dtype of mu and Omega, and infer the right distribution to sample
2. add a kwarg or dtype in function to enforce if resulting sampling is real or complex",2017-05-11 13:58:32,,generalize multivarate_normal generator for complex numbers?,"['01 - Enhancement', 'component: numpy.random']"
9094,open,jakirkham,"It would be really great if [`fftfreq`]( https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fftfreq.html ) and [`rfftfreq`]( https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.rfftfreq.html ) could handle multiple dimensions. In particular, it would be great if it generated something like what [`indices`]( https://docs.scipy.org/doc/numpy/reference/generated/numpy.indices.html ) does except with frequencies over multiple dimensions. This would vastly simplify work done in multidimensional frequency space.",2017-05-10 22:21:16,,Multidimensional fftfreq/rfftfreq,"['23 - Wish List', 'component: numpy.fft']"
9068,open,eric-wieser,"In Python 3, `floor` and `ceil` changed to return integers, rather than floats:

```python
>>> math.floor(1.5), math.ceil(1.5)
(1, 2)
>>> np.floor(1.5), np.ceil(1.5)
(1.0, 2.0)   # also the output for the first case on 2.7
```

Should we update these function in numpy to also return integer types?

I think the deprecation path would be

* add `f->i` loops to the ufunc, so that `np.floor(1.5, dtype=int)` is possible
* on python 3, start `FutureWarning` on `np.floor(1.5)` with no dtype
* on python 3, switch the default dtype to `int`",2017-05-07 15:55:12,,np.ceil and np.floor are inconsistent with math.ceil and math.floor,['component: numpy._core']
9059,open,eric-wieser,"This causes #8518:

```python
>>> np.arange(10, None, 2))
array([0, 2, 4, 6, 8])
>>> range(10, None, 2)
TypeError: 'NoneType' object cannot be interpreted as an integer
```",2017-05-05 21:43:30,,"BUG: np.arange treats start as stop, even when step is specified","['00 - Bug', 'component: numpy._core']"
9052,open,mspacek,"I often need to find where the values in array `v` can be found in the reference array `a`. `np.searchsorted(a, v)` is very useful for exactly this reason, but only if `a` is sorted. If `a` is not sorted, there's a code pattern @joferkington posted [here](http://stackoverflow.com/a/8251668) that does the trick without resorting to something slowish like `np.array([ int(np.where(a == val)[0]) for val in v ])`. For my own use, I've named this pattern `argval`:
```python
def argval(a, v):
    """"""Find indices into input array `a` where values in array `v` match those in `a`.
       a : input array
       v : array of values to find in a

       This should return exactly the same as:

       `np.array([ int(np.where(a == val)[0]) for val in v ])`

       but faster. Adapted from http://stackoverflow.com/a/8251668""""""
    a, v = np.asarray(a), np.asarray(v)
    if not set(v).issubset(a):
        raise ValueError(""values array %r is not a subset of input array %r"" % (v, a ))
    asortis = a.argsort()
    return asortis[a[asortis].searchsorted(v)]

```
I'm currently only using this for arrays of integers, but it works with strings too. Floats might cause problems in the subset test.

If I submit this as a pull request, might it be considered, or am I reinventing something that already exists in numpy?",2017-05-04 23:36:42,,Add np.argmatch() to complement np.argsort()?,['unlabeled']
9051,open,mhvk,"Something we've been working around for a while in astropy: once a fill value has been set for a string array, it cannot be reset to a larger string, even if that string would fit inside the array's dtype:
```
m = np.ma.MaskedArray(['abcde'])
m.fill_value
# 'N/A'
m.fill_value = '-----'
m.fill_value
# '---'

# it works if we reset the internal state
m._fill_value = None
m.fill_value = '-----'
m.fill_value
# '-----'
```

p.s. Strings and fill value do not intereact well generally:

- Filled 'N/A' string being truncated for S1 and S2 arrays: #3295.

- Code 'c' not recognized as string: #5974.",2017-05-04 20:07:39,,Masked array fill value cannot be set to larger string than initial one,"['00 - Bug', 'component: numpy.ma']"
9049,open,bmerry,"In numpy 1.12.1, `_dtype_from_pep3118` requires that modifiers are specified in the order shape then byte order then count, with at most one of each. The PEP is a bit vague, but it seems like modifiers ought to be interpreted recursively i.e. a modifier can be followed by any valid [PEP 3118](https://www.python.org/dev/peps/pep-3118/) specifier.

In practice this can lead to certain types failing to round-trip through a memoryview e.g.:
```python
>>> dt = np.dtype({'formats': [np.dtype((np.dtype((np.int32, (3,))), (2,)))], 'names': ['foo']})
>>> a = np.empty(0, dt)
>>> m = memoryview(a)
>>> np.array(m)
NotImplementedError: memoryview: unsupported format T{(2)(3)i:foo:}
```

It also causes problems if one puts an endianness specifier at the start of the string (which should always be permitted, since the original struct module allows it). I haven't managed to get numpy to generate such a format string when wrapping an array into a memoryview, but it's causing me some problems while developing new features for pybind11:

```python
>>> np.core._internal._dtype_from_pep3118('<(3)i')
ValueError: Unknown PEP 3118 data type specifier '(3)i'
```",2017-05-04 15:53:20,,_dtype_from_pep3118 is overly strict on prefixes,"['00 - Bug', 'component: numpy._core']"
9044,open,hx2A,"I have observed some unusual behavior in the numpy percentile function when dtype is object.

```
In [1]: import numpy as np
   ...: np.__version__
   ...: 
Out[1]: '1.12.1'
```
This works as expected, sorting the values and giving me the percentiles:

```
In [2]: data = np.array([105, 100, 44, 10, 14, 120])
   ...: 
   ...: np.percentile(data, np.arange(0, 101, 10))
   ...: 
Out[2]: array([  10. ,   12. ,   14. ,   29. ,   44. ,   72. ,  100. ,  102.5,  105. ,  112.5,  120. ])
```

If there are NaNs in there I get a warning, as I should:

```
In [3]: data = np.array([105, 100, np.nan, 10, 14, np.nan])
   ...: 
   ...: np.percentile(data, np.arange(0, 101, 10))
   ...: 
/home/jim/INSTALL/anaconda3/envs/numpy_bug/lib/python3.6/site-packages/numpy/lib/function_base.py:4116: RuntimeWarning: Invalid value encountered in percentile
  interpolation=interpolation)
Out[3]: array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan])

```
Now I restart Python and change the dtype to object:

```
In [1]: data = np.array([105, 100, np.nan, 10, 14, np.nan]).astype('object')
   ...: 
   ...: np.percentile(data, np.arange(0, 101, 10))
   ...: 
Out[1]: array([100.0, 102.5, nan, nan, nan, nan, 10.0, 12.0, nan, nan, nan], dtype=object)

```
No warning and bad output.

I know there is the `nanpercentile` function but it rejects input of dtype object.

This works as expected:
```
In [2]: data = np.array([105, 100, np.nan, 10, 14, np.nan])
   ...: 
   ...: np.nanpercentile(data, np.arange(0, 101, 10))
   ...: 
   ...: 
Out[2]: array([  10. ,   11.2,   12.4,   13.6,   31.2,   57. ,   82.8,  100.5,  102. ,  103.5,  105. ])
```

This is rejected:

```
In [3]: data = np.array([105, 100, np.nan, 10, 14, np.nan]).astype('object')
   ...: 
   ...: np.nanpercentile(data, np.arange(0, 101, 10))

TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
```

I found this while using `np.percentile` on a Pandas DataFrame where the underlying numpy array had an object dtype. Interestingly the Pandas `quantile` method (which I probably should have been using instead) handles NaNs correctly. Since that works it's possible somebody has thought about this and I will learn something new. In any case, the no warning and bad output in a large DataFrame with only couple of NaNs results in an easily overlooked mistake.",2017-05-03 02:58:53,,Percentile function gets tripped up by NaNs when dtype is object,['unlabeled']
9032,open,dmopalmer,"I suggest an equivalent to histogram that, rather than returning the count in each bin, returns an array of indices for each bin.

Example use case; average BMI vs height: 
```
x = getheightsandweights()
binargs,binedges = arghistogram(x['height'])
bmibyheight =[(x[args]['weight']/x[args]['height']**2).mean() for args in binargs]
```

This could be either an `arghistogram()` function, or an `args=True` parameter to the `histogram()`.  The former is like `argsort()`, the latter follows the precedent of the `normed=` and `density=` keywords.
",2017-04-30 18:35:06,,`arghistogram()` feature request,['unlabeled']
9030,open,eric-wieser,"For instance, the source link on [the docs for `np.blackman`](https://numpy.org/devdocs/reference/generated/numpy.blackman.html) uses the line numbers from the commit it was built with, but applied to the file contents from master.",2017-04-30 13:04:15,,"BUG/DOC: [source] links on the dev docs point to master, not the sha1 the docs were built on","['00 - Bug', 'component: documentation']"
9029,open,vspinu,"It's rather inconvenient that such a basic property as shape is not printed along the array description. Would it be possible to add an extra option to `set_printoptions` for this? 

Or maybe a more general solution would be to add a `post_formater` function that would take two arguments - original array and string representation as processed by pretty printer. Then users can further adopt printing representation to their liking.",2017-04-30 07:53:17,,"ENH: Add ""print_shape"" to `set_printoptions`",['unlabeled']
9023,open,eric-wieser,"The following fails:

```python
import numpy as np
from numpy.testing import assert_equal 
a = np.array([1, 2, np.nan], object)
assert_equal(a, a)
```
```
AssertionError:
Arrays are not equal

(mismatch 33.33333333333333%)
 x: array([1, 2, nan], dtype=object)
 y: array([1, 2, nan], dtype=object)
```",2017-04-29 02:15:05,,BUG: assert_equal fails on object arrays of nan,"['00 - Bug', 'component: numpy.testing']"
9009,open,eric-wieser,"This would fix help fix #9008.

From most to least-well defined
 1. Object arrays of `float`,`np.float32`, `np.datetime64`... &rarr; `return np.isnan(item)`
 2. Object arrays of `int`,`np.int32`, ... &rarr; `return False`
 3. Object arrays of any type that implements `__float__` or `__int__` - return `np.isnan(float(x))`
 4. Object arrays of `np.ndarray` &rarr; `return np.isnan(item)` (requires a `O->O` loop, not `O->?`)
 5. Any value &rarr; `return x != x`

I think I'd draw the line after 3, and have 4 onwards throw an error.
 ",2017-04-27 17:43:15,,"ENH: Add a np.isnan loop for the object dtype (and possible `isfinite`,  ...)","['01 - Enhancement', 'component: numpy._core']"
8994,open,eric-wieser,"Should be very geared towards a `bXX->X` loop, for every possible `X`.

This would offer:
 * an `out` argument
 * support for subclasses using `__array_ufunc__`
 * Not really desirable, but comes with the package - a `where` argument (!) such that `np.where(c, y, z, out=x, where=w)` is a more efficient `x = np.where(w, x, np.where(c, y, z))`
 * A fix to #5095 

Problems:
 * Are inner loops for `void` and other flexible types possible?",2017-04-26 16:11:06,,ENH: make np.where a ufunc,"['01 - Enhancement', 'component: numpy._core']"
8987,open,eric-wieser,"Simple demo:

```python
In [1]: np.uint8(0) - np.uint8(1)
RuntimeWarning: overflow encountered in ubyte_scalars
Out[1]: 255

In [2]: np.uint8(0)[...] - np.uint8(1)[...]
Out[2]: 255
```

Should we even warn at all for unsigned integers?",2017-04-25 19:21:10,,BUG: Integer overflow warning applies to scalars but not arrays,"['00 - Bug', 'component: numpy._core']"
8975,open,eric-wieser,"`np.minimum` is consistent, `np.fmin` is not

```python
>>> x = np.array([1, np.nan])

>>> np.minimum.reduce(x)
nan
>>> np.minimum.reduce(x.astype(object))
nan

>>> np.fmin.reduce(x)
1.0
>>> np.fmin.reduce(x.astype(object))
nan  # oops
```",2017-04-21 20:39:08,,BUG: np.fmin behaves differently on object arrays,"['00 - Bug', 'component: numpy._core']"
8972,open,nikhilweee,"Python 2.7.12, Numpy 1.12.1, Ubuntu 16.04
```python
>>> a = np.array([])
>>> np.reshape(a, -1, 0)
array([], dtype=float64)
>>> a.reshape(-1, 0)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ValueError: cannot reshape array of size 0 into shape (0)
```
Is this the intended behaviour? Shouldn't this work irrespective of the way I try to reshape?",2017-04-21 15:14:27,,ValueError reshaping empty arrays,"['00 - Bug', 'component: numpy._core']"
8969,open,eric-wieser,"Example:

```python
>>> dt = np.dtype([('f', float), ('s', str)])
>>> np.array([(1.0, ""hello world"")], dt)
array([(1.0, '')], 
      dtype=[('f', '<f8'), ('s', 'S')])
```

Proposal: This should either resize or raise a ValueError (#8970), as it seems like it would always mask bugs",2017-04-21 10:53:40,,BUG: String fields in compound dtypes don't resize as might be expected,"['00 - Bug', 'component: numpy._core', 'component: numpy.dtype']"
8945,open,galentx,"I believe sign should handle NaN's silently, but it yields a warning:

```
Python 2.7.13 |Anaconda custom (64-bit)| (default, Dec 19 2016, 13:29:36) [MSC v.1500 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
>>> import numpy as np
>>> x = np.float64([-2, -1, 0, 1, 2])
>>> np.sign(x)
array([-1., -1.,  0.,  1.,  1.])
>>> y = x.copy()
>>> y[2] = np.nan
>>> y
array([ -2.,  -1.,  nan,   1.,   2.])
>>> np.sign(y)
__main__:1: RuntimeWarning: invalid value encountered in sign
array([ -1.,  -1.,  nan,   1.,   1.])
>>> np.version.version
'1.12.1'
```

I also see similar warnings with greater_equal, less_equal, and absolute in a call to matplotlib symlog:
```
C:\Anaconda2\lib\site-packages\matplotlib\scale.py:290: RuntimeWarning: invalid value encountered in sign
  sign = np.sign(a)
C:\Anaconda2\lib\site-packages\numpy\ma\core.py:2123: RuntimeWarning: invalid value encountered in greater_equal
  condition = (xf >= v1) & (xf <= v2)
C:\Anaconda2\lib\site-packages\numpy\ma\core.py:2123: RuntimeWarning: invalid value encountered in less_equal
  condition = (xf >= v1) & (xf <= v2)
C:\Anaconda2\lib\site-packages\matplotlib\scale.py:297: RuntimeWarning: invalid value encountered in absolute
  ma.log(np.abs(masked) / self.linthresh) / self._log_base)
```

Windows 7 x64
Anaconda 4.3.1+ (custom to bring in np 1.12)
Python 2.7.13
numpy 1.12.1

This may be related to #8230, which suggests this is unique to Windows and Python 2.7, and #7440. The latter suggests it is a 32-bit problem but my environment is 64-bit. I don't think this was an issue with an earlier version of numpy, but I'm not sure of the prior version I used. It couldn't be more than a couple of years old.",2017-04-14 20:19:12,,"BUG: nan's yield warnings in sign, greater_equal, and elsewhere","['00 - Bug', 'component: numpy.ufunc']"
8927,open,KesterTong,"When passed an object that isn't array-like, numpy.sum returns that object.  This is surprising behavior.  Maybe it is consistent with interpreting the input as a scalar, but it is surprising when the input is iterable.  E.g.
```
class A(object):
  pass

a = A()
np.sum(a)  # returns `a`
```

Note that this occurs even for iterables, e.g.

```
class A(object):
  def __getitem__(self, index):
    if index > 3:
        raise IndexError(""that's enough!"")

    return index

a = A()
np.sum(a)  # returns `a`
```",2017-04-11 21:01:36,,numpy sum returns unrecognized objects unchanged,['unlabeled']
8913,open,jachymb,"Hello,
I was computing some correlations while I noticed that I can easily make an implementation significantly faster than `coffcoef` by replacing `sum()` with `count_nonzero()`. Therefore, I think whenever `sum()` is called on a boolean array, it should use `count_nonzero` or such instead of addition.",2017-04-07 17:59:01,,Use count_nonzero instead of sum for boolean arrays,['unlabeled']
8909,open,smiddy,"When initializing an array with `linspace` and the dtype decimal, I experience the following behavour:
```python
import numpy as np
from decimal import Decimal
```
two possible input are tested

### 1. Float as input

```python
rng = np.linspace(300e-6, 250e-6, 11, dtype=Decimal)
```
leads to the solution
```python
array([0.0003, 0.00029499999999999996, 0.00029, 0.000285, 0.00028,
       0.00027499999999999996, 0.00027, 0.000265, 0.00026, 0.000255,
       0.00025], dtype=object)
```
This could perhaps include a warning that the floating point precision is used. And the dtype is not correct.

### 2.  Decimal as input
```python
rng = np.linspace(Decimal('300e-6'), Decimal('250e-6'), 11, dtype=Decimal)
```
leads to the error
```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-5-abb9f020d3c4> in <module>()
----> 1 arr = np.linspace(Decimal('200e-6'), Decimal('250e-6'), 7, dtype=Decimal)

C:\Users\Student\Anaconda3\lib\site-packages\numpy\core\function_base.py in linspace(start, stop, num, endpoint, retstep, dtype)
     88 
     89     # Convert float/complex array scalars to float, gh-3504
---> 90     start = start * 1.
     91     stop = stop * 1.
     92 

TypeError: unsupported operand type(s) for *: 'decimal.Decimal' and 'float'
```
if Decimal should be supported in `linspace`, I could have a look at a fix for this, otherwise a type check would be nice.",2017-04-07 13:00:05,,behaviour of linspace with dtype Decimal,"['00 - Bug', 'component: numpy._core']"
8906,open,eric-wieser,"See below

```python
>>> m = np.ma.array([None], mask=[False])

>>> m[0] = np.ma.masked; m[0]
masked

>>> m[0] = np.eye(3); m[0]
array([[ 1.,  0.,  0.],
       [ 0.,  1.,  0.],
       [ 0.,  0.,  1.]])

>>> m[0] = np.ma.masked; m[0]
masked_array(data =
 [[-- -- --]
 [-- -- --]
 [-- -- --]],
             mask =
 [[ True  True  True]
 [ True  True  True]
 [ True  True  True]],
       fill_value = 1e+20)   # but that was the same line that we ran above!
```

The culprit is the `# Did we extract a single item?` check in `numpy/ma/core.py:3192`.

I just don't think we have the information needed to actually know whether we got a single item from `self.data`, which could be of any subclass (?).

Maybe the code should become something like:

```python
# Normalize the index. This is wrong, but being right is really hard (#8276)
if not isinstance(idx, tuple): 
    idx = (idx,)

# append an ellipsis if there was not already one, and remember whether we did
could_be_scalar = Ellipsis not in idx
if could_be_scalar:
    idx = idx + (Ellipsis,)

# guaranteed to be a (possibly 0d) array and not a scalar element
dout = data[idx]

# flatten 0d arrays that were not requested
if could_be_scalar and dout.ndim == 0:
    result = result[()]
return result
```

<small>https://github.com/numpy/numpy/pull/8276#issuecomment-260622902 for the lazy</small>",2017-04-06 23:46:44,,BUG: Masked object array of ndarrays attaches mask to masked array value,"['00 - Bug', 'component: numpy.ma']"
8899,open,eric-wieser,"Pulled out from discussions in #8886:

* [x] Preallocate the result array (#11971)
* [ ] Alias `np.b_[...]` to `np.block([...])`- shorthands are useful for nesting calls
* [ ] Implement block destructuring, `np.b_[...] = somearr` (comes cheap after preallocation)
* [ ] Add a `np.block(arr : ndarray[object])` overload, that takes an ndarray of blocks that align in all axes (ie, such that the concatenation order does not matter)
    * add a `grid=True` parameter to the same effect (eric-wieser/numpy#2)
* [ ] Allow broadcasting
    *  Extra leading dimensions: With two stacks of arrays, `A.shape == (3, 1, 2, 2)`, `B.shape = (1, 4, 2, 2)`, should `np.block([A, B]).shape == (3,4,2,4)`? (currently: `ValueError`)
    * All dimensions
* [ ] Deprecate `bmat` in favor of `np.block(...).view(np.matrix)`",2017-04-06 01:28:41,,ENH: Possible improvements to `np.block`,"['01 - Enhancement', 'component: numpy._core']"
8881,open,lukelbd,"It seems to me this should be resolved so that the `MaskedArray` subclass is preserved and the `mask` attribute is likewise padded -- `np.repeat` and `np.tile` do this, for example. I suppose I see the complication -- for masking modes like `constant`, whether to mask these new values is ambiguous; perhaps in this case an explicit warning should be issued. But for methods that **sample the edges** of the existing `MaskedArray`, it seems to me it would make more sense to preserve the mask; perhaps this all requires a new `np.ma.pad` module. 

Here is a simple example

```
In [1]: import numpy as np

In [2]: a = np.ma.MaskedArray([-999,-999,0,1,2],mask=[True,True,False,False,False],fill_value=999)

In [3]: a
Out[3]:
masked_array(data = [-- -- 0 1 2],
             mask = [ True  True False False False],
       fill_value = 999)

In [4]: np.pad(a,0,'wrap') # note a.filled() is NOT invoked; instead, object is simply unmasked
Out[4]: array([-999, -999,    0,    1,    2])

In [6]: np.pad(a,0,'edge') # same results as above; same is true for every padding 'mode'
Out[6]: array([-999, -999,    0,    1,    2])

In [8]: np.repeat(a,1) # expected behavior
Out[8]:
masked_array(data = [-- -- 0 1 2],
             mask = [ True  True False False False],
       fill_value = 999)

In [9]: np.tile(a,1) # expected behavior
Out[9]:
masked_array(data = [-- -- 0 1 2],
             mask = [ True  True False False False],
       fill_value = 999)
```


I noticed there are several other inconsistencies here -- for example, there are `np.repeat` and `np.ma.repeat` modules that do the same thing; there is only an `np.tile` method and **no** `np.ma.tile` method (but `np.tile` works as expected); and there is an `np.concatenate` method and `np.ma.concatenate` method where the former has unexpected behavior (sets `mask=False`, unmasks the data, and **changes `fill_value`**). 

Perhaps I should start a more general thread on these inconsistency issues elsewhere?",2017-03-31 17:58:22,,"np.pad applied to the MaskedArray subclass silently unmasks the array, and returns output as ndarray",['component: numpy.ma']
8875,open,jakirkham,"Noticed there is some strange behavior with `numpy.roll` when using multiple shifts and/or axes. Descriptions of the odd cases are given below and a Python snippet is provided to show this in more detail.

First it is possible to use multiple shifts when no `axis` is specified. The no `axis` case flattens the array, which makes specifying multiple shifts weird. It seems `numpy.roll` permits this without error and just sums the shifts. Especially as there is already a way to accomplish this, whose intent is completely clear unlike this. Seems like erroring out would be better behavior IMHO.

Second if a number of shifts and a single axis is specified, the shifts can be summed together and applied along that axis. Much like the previous case, this feels like an indirect/unclear way to accomplish something that is much simpler and clearer otherwise. Note if more than one axis is specified, but doesn't match the number of shifts, it actually errors out. Again IMHO an error seems appropriate for all of these cases.

Examples:

<details>

```python
In [1]: import numpy as np

In [2]: a = np.arange(24.0).reshape(1, 2, 3, 4)

In [3]: a
Out[3]: 
array([[[[  0.,   1.,   2.,   3.],
         [  4.,   5.,   6.,   7.],
         [  8.,   9.,  10.,  11.]],

        [[ 12.,  13.,  14.,  15.],
         [ 16.,  17.,  18.,  19.],
         [ 20.,  21.,  22.,  23.]]]])

In [4]: np.roll(a, (2, 3))
Out[4]: 
array([[[[ 19.,  20.,  21.,  22.],
         [ 23.,   0.,   1.,   2.],
         [  3.,   4.,   5.,   6.]],

        [[  7.,   8.,   9.,  10.],
         [ 11.,  12.,  13.,  14.],
         [ 15.,  16.,  17.,  18.]]]])

In [5]: np.roll(a, 5)
Out[5]: 
array([[[[ 19.,  20.,  21.,  22.],
         [ 23.,   0.,   1.,   2.],
         [  3.,   4.,   5.,   6.]],

        [[  7.,   8.,   9.,  10.],
         [ 11.,  12.,  13.,  14.],
         [ 15.,  16.,  17.,  18.]]]])

In [6]: np.roll(a, (2, 3), (1,))
Out[6]: 
array([[[[ 12.,  13.,  14.,  15.],
         [ 16.,  17.,  18.,  19.],
         [ 20.,  21.,  22.,  23.]],

        [[  0.,   1.,   2.,   3.],
         [  4.,   5.,   6.,   7.],
         [  8.,   9.,  10.,  11.]]]])

In [7]: np.roll(a, 5, 1)
Out[7]: 
array([[[[ 12.,  13.,  14.,  15.],
         [ 16.,  17.,  18.,  19.],
         [ 20.,  21.,  22.,  23.]],

        [[  0.,   1.,   2.,   3.],
         [  4.,   5.,   6.,   7.],
         [  8.,   9.,  10.,  11.]]]])

In [8]: np.roll(a, (1, 2, 3), (0, 1))
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-8-57b473084941> in <module>()
----> 1 np.roll(a, (1, 2, 3), (0, 1))

/zopt/conda2/envs/npenv/lib/python2.7/site-packages/numpy/core/numeric.pyc in roll(a, shift, axis)
   1461 
   1462     else:
-> 1463         broadcasted = broadcast(shift, axis)
   1464         if len(broadcasted.shape) > 1:
   1465             raise ValueError(

ValueError: shape mismatch: objects cannot be broadcast to a single shape
```

</details>

<br>
Environment details:
<br>
<br>

<details>

```yaml
name: npenv
channels: !!python/tuple
- !!python/unicode
  'conda-forge'
- !!python/unicode
  'defaults'
dependencies:
- conda-forge::appnope=0.1.0=py27_0
- conda-forge::backports.shutil_get_terminal_size=1.0.0=py27_1
- conda-forge::blas=1.1=openblas
- conda-forge::ca-certificates=2017.1.23=0
- conda-forge::certifi=2017.1.23=py27_0
- conda-forge::decorator=4.0.11=py27_0
- conda-forge::enum34=1.1.6=py27_1
- conda-forge::ipython=5.3.0=py27_0
- conda-forge::ipython_genutils=0.2.0=py27_0
- conda-forge::libgfortran=3.0.0=0
- conda-forge::ncurses=5.9=10
- conda-forge::numpy=1.12.1=py27_blas_openblas_200
- conda-forge::openblas=0.2.19=1
- conda-forge::openssl=1.0.2h=3
- conda-forge::pathlib2=2.2.1=py27_0
- conda-forge::pexpect=4.2.1=py27_0
- conda-forge::pickleshare=0.7.3=py27_0
- conda-forge::prompt_toolkit=1.0.14=py27_0
- conda-forge::ptyprocess=0.5.1=py27_0
- conda-forge::pygments=2.2.0=py27_0
- conda-forge::python=2.7.13=0
- conda-forge::readline=6.2=0
- conda-forge::scandir=1.5=py27_1
- conda-forge::setuptools=33.1.1=py27_0
- conda-forge::simplegeneric=0.8.1=py27_0
- conda-forge::six=1.10.0=py27_1
- conda-forge::sqlite=3.13.0=1
- conda-forge::tk=8.5.19=1
- conda-forge::traitlets=4.3.2=py27_0
- conda-forge::wcwidth=0.1.7=py27_0
- conda-forge::zlib=1.2.11=0
prefix: /zopt/conda2/envs/npenv
```

</details>",2017-03-30 18:40:12,,Strange roll multiple shift behavior,['component: numpy._core']
8869,open,TobiasLe,"A mean over an array containing only ones should obviously return only ones. However,
```python
>>> import numpy as np
>>> test_array = np.ones((10000000, 4, 15), dtype = np.float32)
>>> print(test_array.mean(axis=(0,1)))
[ 0.4194304  0.4194304  0.4194304  0.4194304  0.4194304  0.4194304
  0.4194304  0.4194304  0.4194304  0.4194304  0.4194304  0.4194304
  0.4194304  0.4194304  0.4194304]
```

This returns the correct result:
```python
>>> print(test_array.mean(axis=0).mean(axis=0))
[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
```
I guess the reason for this problem is some overflow, because it does not appear when I use a test array with dtype = np.float64. I would expect numpy to either give the correct result or to at least give a warning whenever such an overflow happens.

surprisingly mean along all axis gives the correct result again:
```python
>>> print(test_array.mean(axis=(0,1,2)))
1.0
```
(I used numpy-1.12.1 with python 3.5 on Ubuntu 16)",2017-03-30 12:06:10,,numpy.mean along multiple axis gives wrong result for large arrays,"['00 - Bug', 'component: numpy._core']"
8867,open,eric-wieser,"There's a bunch of fields in `PyUFuncObject` that don't make sense for normal `ufunc`s (are nulled), and most methods in `ufunc_methods` don't make sense for gufuncs.

What would seem like a better model for me is the following type heirarchy

 * `base_ufunc`
    * `gufunc`- actually knows about `core_enabled`
    * `ufunc` - has the `reduce`, `accumulate`, and `reduce_at`  methods, and `identity` property

I'm guessing that both binary and API compatibility in C are important - so maybe this could just change from the python side?

It looks to me like it's far too late in the day to cut up `PyUFuncObject` into multiple pieces, but I think just letting it be viewed under different python types would be useful for hiding invalid methods and properties",2017-03-30 11:35:33,,Discussion: Could there be a ufunc type hierarchy?,['component: numpy._core']
8855,open,jakirkham,"Normally NumPy's `rfft2`/`rfftn` returns an array whose shape is largely the same as that of the input (as long as `s` is unspecified) with one exception. Namely the last axis in `axes` will specify the dimension whose length will be reduced by `l // 2 + 1`, where `l` was the original length of that dimension. This is even noted in the docs. However, it seems if the last axis is repeated this no longer holds. Now I could be wrong, but this is not what I would naively expect. If this does make sense for some reason, then the docs probably should be updated.

<details>

```python
In [1]: import numpy

In [2]: import numpy.fft

In [3]: a = numpy.random.random((6, 7, 8, 9))

In [4]: numpy.fft.rfft2(a, axes=(0,)).shape
Out[4]: (4, 7, 8, 9)

In [5]: numpy.fft.rfft2(a, axes=(1, 0)).shape
Out[5]: (4, 7, 8, 9)

In [6]: numpy.fft.rfft2(a, axes=(0, 0)).shape
Out[6]: (6, 7, 8, 9)

In [7]: numpy.fft.rfftn(a, axes=(0,)).shape
Out[7]: (4, 7, 8, 9)

In [8]: numpy.fft.rfftn(a, axes=(1, 0)).shape
Out[8]: (4, 7, 8, 9)

In [9]: numpy.fft.rfftn(a, axes=(0, 0)).shape
Out[9]: (6, 7, 8, 9)
```

</details>
<br>

Exported environment details from `conda`:

<details>

```yaml
name: npenv
channels: !!python/tuple
- !!python/unicode
  'conda-forge'
- !!python/unicode
  'defaults'
dependencies:
- conda-forge::appnope=0.1.0=py27_0
- conda-forge::backports.shutil_get_terminal_size=1.0.0=py27_1
- conda-forge::blas=1.1=openblas
- conda-forge::ca-certificates=2017.1.23=0
- conda-forge::certifi=2017.1.23=py27_0
- conda-forge::decorator=4.0.11=py27_0
- conda-forge::enum34=1.1.6=py27_1
- conda-forge::ipython=5.3.0=py27_0
- conda-forge::ipython_genutils=0.2.0=py27_0
- conda-forge::libgfortran=3.0.0=0
- conda-forge::ncurses=5.9=10
- conda-forge::numpy=1.12.1=py27_blas_openblas_200
- conda-forge::openblas=0.2.19=1
- conda-forge::openssl=1.0.2h=3
- conda-forge::pathlib2=2.2.1=py27_0
- conda-forge::pexpect=4.2.1=py27_0
- conda-forge::pickleshare=0.7.3=py27_0
- conda-forge::prompt_toolkit=1.0.13=py27_0
- conda-forge::ptyprocess=0.5.1=py27_0
- conda-forge::pygments=2.2.0=py27_0
- conda-forge::python=2.7.13=0
- conda-forge::readline=6.2=0
- conda-forge::scandir=1.5=py27_1
- conda-forge::setuptools=33.1.1=py27_0
- conda-forge::simplegeneric=0.8.1=py27_0
- conda-forge::six=1.10.0=py27_1
- conda-forge::sqlite=3.13.0=1
- conda-forge::tk=8.5.19=1
- conda-forge::traitlets=4.3.2=py27_0
- conda-forge::wcwidth=0.1.7=py27_0
- conda-forge::zlib=1.2.11=0
```

</details>
<br>

ref: https://docs.scipy.org/doc/numpy-1.12.0/reference/generated/numpy.fft.rfftn.html",2017-03-27 16:44:34,,NumPy rfft2/rfftn return original shape with repeated axes,['unlabeled']
8849,open,eric-wieser,"Does this make sense, and/or is it desirable?

```python
b = np.iinfo(np.bool_)
assert(b.min == 0)
assert(b.max == 1)
assert(b.bits == ???)
```",2017-03-27 01:42:58,,ENH: Implement np.iinfo(np.bool_),"['01 - Enhancement', '15 - Discussion', 'component: numpy._core']"
8833,open,eric-wieser,"This would:
 * Add broadcasting (#6938)
 * Add an `out` argument
 * Add a `where` argument
 * Make the output dtype derive from the inputs (eg randint)

Issues:
* right now we implement most of this in cython, and there is no cython header for ufuncs.
   * Are we gaining much from the cython, or is it mostly there to handle the iteration that ufuncs would deal with?
* random functions are methods of `RandomState` objects, not functions. How do we pass `self` into the loop?
  * a signature starting with `O`?
  * Creating a new `ufunc` for every instance, and modifying the `data` (bad for `__array_ufunc__`overrides)",2017-03-25 12:36:39,,ENH: Reimplement most of `random` using the ufunc machinery,"['01 - Enhancement', 'component: numpy.random']"
8817,open,jhamman,"It would be useful to allow [`numpy.interp`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.interp.html) to take a (optional) function for the left/right arguments. This would allow user specified handling of the tails in interpolations operations.

(sorry I somehow submitted this issue prematurely, I have edited the summary now.)",2017-03-23 00:15:07,,ENH: Allow left and right arguments to numpy.interp to be functions,"['01 - Enhancement', 'component: numpy.lib']"
8812,open,ax3l,"I am trying to perform the following read via `fromregex`:

```python
import numpy as np
from io import BytesIO

data = b""AA 2.3 2.6 2.8\nBB 3.3 3.6 1.8e2\nCC 33.e2 12.6e-1 4.8""

rg_flt = r""[-\+[0-9]+\.[0-9]*[Ee]*[\+-]*[0-9]*""

d = np.fromregex(
    BytesIO(data),
    ""(..)\s+("" + rg_flt + "")\s+("" + rg_flt + "")\s+("" + rg_flt + "")\s*\n"",
    dtype=[ ('key', '|S2'), ('v1', np.float), ('v2', np.float), ('v3', np.float) ]
)
```

That works, but the resulting structured array is of type
```
array([...], dtype=[('key', 'S2'), ('v1', '<f8'), ('v2', '<f8'), ('v3', '<f8')])
```

I would rather like a `dtype=[('key', 'S2'), ('v', '<f8', (3))])` but the syntax
```python
d = np.fromregex(
    BytesIO(data),
    ""(..)\s+("" + rg_flt + ""\s+"" + rg_flt + ""\s+"" + rg_flt + "")\s*\n"",
    dtype=[ ('key', '|S2'), ('v', '3float32') ]
)
```

fails with
```
----> 5     dtype=[ ('key', '|S2'), ('v', '3float32') ]
      6 )

.../numpy/lib/npyio.pyc in fromregex(file, regexp, dtype)
   1301             output.dtype = dtype
   1302         else:
-> 1303             output = np.array(seq, dtype=dtype)
   1304 
   1305         return output

ValueError: invalid literal for float(): 2.3 2.6 2.8
```

I also tried variations with `('v', np.float, (3,))` and `('v', [('',np.float), ('',np.float), ('',np.float)])` all without success.

Do you have any ideas how to address the dtype and regex properly? My numpy version is `1.12.0`.",2017-03-22 15:54:51,,BUG: fromregex cannot load into sub-array of structured type,"['00 - Bug', 'component: numpy.lib']"
8811,open,mhvk,"EDITS:
- (https://github.com/numpy/numpy/issues/8811#issuecomment-288432763): see #5015 for allowing constants in the dimension, and worries about extending the C `ufunc` API.
- (https://github.com/numpy/numpy/issues/8811#issuecomment-288454518): multiple signatures rather than creating a mini-language?
- (https://github.com/numpy/numpy/issues/8811#issuecomment-288478008): need wishlist of things code should do before thinking of implementation.

Rationale
------------
Before numpy 1.10, there was automatic broadcasting over core dimensions in `gufunc`. While this is not necessarily good for many things (e.g., `np.inner1d` in the example in the documentation), it is for others, such as the `all_equal` implementation in #8528. It would be nice if the signature allowed one to make this distinction.

Wishlist beyond broadcasting
------------------------------------
- Multiple related signatures, such as for matmul: `(i,k),(k,j)->(i,j)`, `(i,j),(i)=(i,j),(i,1)->(i,j)`, `(i),(i,j)=(1,i),(i,j)->(i,j)`
- Possibly having the output signature be calculated from the input dimensions (`min(i,j)`)

Possible implementations
-------------------------------
Noting that `signature` has to be a single `char` to remain compatible with the API:
- Adjust the interpretation of dimensions, e.g., `(i|1), (i|1)->()` or `(i?), (i?)->()` (#5197) might convey what is needed for `all_equal` (easy to do with setting relevant strides to 0)
- Allow multiple signatures; e.g., `(i),(i)->(); (i),()->(); (),(i)->(); (),()->()`. This would likely make the most sense for things like `matmul`, but would seem to imply different functions for each signature.

",2017-03-22 15:15:06,,Feature request: signal broadcasting is OK over core dimension,"['23 - Wish List', 'component: numpy._core', 'component: numpy.ufunc']"
8804,open,piquan,"It would be useful if NumPy had a modular exponentiation function.  Besides performance improvements over `a**b%m`, it also allows computations where `a**b` would not fit in the datatype.

NumPy also seems to ignore Python's third argument to `pow` (the modulus), rather than raising an error if it's set.

    In [2]: pow(np.arange(10), 10, 3)
    Out[2]: 
    array([         0,          1,       1024,      59049,    1048576,
              9765625,   60466176,  282475249, 1073741824, 3486784401])",2017-03-21 08:05:20,,No modular exponentiation?,"['01 - Enhancement', 'component: numpy._core']"
8802,open,glenfletcher,"This isn't exactly a bug in numpy, rather a source of hard to trace bugs in code written in numpy

In code such as:
```.py
data = np.empty((numlines,) dtype=structure)
for i in range(numlines):
    line = # Read line from input source
    ...
    data[field][i,:] = line[start:end]
    ...
```
because of numpys broadcasting rules
```.py
data = np.empty((numlines,) dtype=structure)
for i in range(numlines):
    line = # Read line from input source
    ...
    data[field][:] = line[start:end]
    ...
```
is valid code, and a easy typo, the result is that the field is overwritten on each iteration of the loop rather than simply adding the value at the index, and all values appear to be the same.

Obviously the broadcasting rules can't be change but, is it possible to cause such code to print a warning to stderr the first time its evaluated as this would make tracing such errors easier, this would of course be best if we could make it context aware, however I'm not sure if that is possible.

This exact typo has, recently caused me a loot of grief in tracking down a bug in some code I was working with, as the code looked right, when quickly skimming over it I end up first looking at the files I was reading believe the error to be there and only found this error after adding some debug print statement. On the other hand if my loading function produced a message such as ""WARNING: Broadcasting assignment to array"" I would have taken a closer look at the assignment and found this bug in a matter of minutes instead of wasting a couple of hours trying to debug the wrong part of the code base.",2017-03-21 02:57:50,,Broadcasting Assigments In a Loop,"['01 - Enhancement', '15 - Discussion', 'component: numpy._core']"
8786,open,nschloe,"#### Background

This came up developing [quadpy](https://github.com/nschloe/quadpy) for numerical quadrature. To approximate an integral over the reference interval `[-1, 1]`, one arranges points `x_i` and weights `w_i` such that the number
```
sum(f(x_i) * w_i)
```
approximates the integral of some functions `f` (typically a set of polynomials) reasonably well. Sophisticated schemes have many points and weights, and naturally the more weights there are, the smaller each individual weight must be.

*Bottom line: You'll have to sum up many small values.*

#### When numpy.sum doesn't cut it

I noticed that `numpy.sum` produces large enough round-off errors for the tests to fail with as few as 7 summands (being `-44.34121805, -15.72356145, -0.04563372, 0., 0.04563372, 15.72356145, 44.34121805`, not shown in full precision). `math.fsum` always worked fine, but cannot be used anymore since I'd like to compute the sum for many intervals at once.

#### Work so far

[Kahan's summation](https://en.wikipedia.org/wiki/Kahan_summation_algorithm)
```python
def kahan_sum(a, axis=0):
    s = numpy.zeros(a.shape[:axis] + a.shape[axis+1:])
    c = numpy.zeros(s.shape)
    for i in range(a.shape[axis]):
        # http://stackoverflow.com/a/42817610/353337
        y = a[(slice(None),) * axis + (i,)] - c
        t = s + y
        c = (t - s) - y
        s = t.copy()
    return s
```
does the trick. Note, however, the use of a Python loop here, which leads to a performance degradation if the number of points and weights is large.

`math.fsum` [uses a different algorithm](https://github.com/python/cpython/blob/d267006f18592165ed97e0a9c2494d3bce25fc2b/Modules/mathmodule.c#L1087) which may or may not be better suited than Kahan.

See [here](https://github.com/numpy/numpy/issues/2448) for a previous discussion of the topic.",2017-03-15 19:21:04,,"numpy.sum not stable enough sometimes (Kahan, math.fsum)",['unlabeled']
8784,open,SylvainCorlay,"I am working on the support for the npy format in the [xtensor](https://github.com/QuantStack/xtensor) project.

The specification for the npy format (version 1 or 2) says that after the prefix string and the version numbers, the format contains

 - a little-endian unsigned short specifying the length of the header
 - the header itself.

The header is a pretty-printed Python dict literal. This is where it becomes tricky since it is much broader than things such as json, especially since the first item, `descr` can be *any python object* that can be used to initialize a numpy dtype. If could have nested dicts, list comprehensions etc...

This makes it very hard to parse in general. Parsing that without the actual python interpreter is not in scope. So what I can do since the two other keys of the dict is extract `fortran_order` and `shape` from the end, then isolate the description, and only support the few hard-coded dtypes that are initialized with strings.

However I wanted to open this since I don't think that a Python dictionary literal is appropriate to communicate `shape` and `fortran_order` in a cross-language fashion. Having some Python string is fine, but the shape type and item size should be preferably accessible without parsing Python syntax.",2017-03-14 16:05:53,,Parsing npy format header with in another language,['unlabeled']
8780,open,gerritholl,"When an array has dtype `datetime64`, setting to `NaN` should convert to `NaT`.  Currently, it fails with ValueError:

```
In [49]: x = numpy.zeros(dtype=""M8[s]"", shape=5)
In [51]: x[2] = numpy.nan
…
ValueError: Could not convert object to NumPy datetime
```

The instruction `= numpy.nan` is not ambiguous in any way.  When setting all the values of a structured dtype with floats and datetime types to nan it is much easier if we don't need to separately handle the case for nat.",2017-03-13 14:41:33,,NaN should become NaT when assigned to datetime64,"['54 - Needs decision', 'component: numpy.datetime64']"
8775,open,eric-wieser,"Raised [on stackoverflow](http://stackoverflow.com/q/42746248/102441). Right now:
```python
>>> a = np.float16([600, 800])
>>> np.linalg.norm(a)
inf  # hoping for 1000
>>> np.hypot.reduce(a)
1000  # works as intended
```

Should we be doing some `hypot`-style overflow avoidance here, implementing `norm` as `oldnorm(x / max(x)) * max(x)`? Or perhaps switch to this behaviour after trying the existing method and getting `inf`?",2017-03-12 12:33:30,,BUG? np.linalg.norm overflows for float16,"['00 - Bug', 'component: numpy.linalg']"
8773,open,eric-wieser,"Right now, `reduce` can be roughly described as:

```python
it = iter_along_axis(arr)  #pseudocode
result = func.identity or next(it)
for i in it:
    result = func(result, i)
return result
```

Or diagramatically:
```
       x2     x3     x3      xn
       |      |      |       |
       V      V      V       V
x1 -> (f) -> (f) -> (f) ... (f) --> result
```

What I'm proposing is a more powerful variant that allows extra state to be preserved between each function call:

```python

it = iter_along_axis(arr)  #pseudocode
result =  func.identity or next(it)
state = None
for i in it:
    result, state = func(result, i, reduce_state=state)
return result
```
```
         x2     x3     x3      xn
         |      |      |       |
         V      V      V       V
x1   -> ( ) -> ( ) -> ( ) ... ( ) --> result
        (f)    (f)    (f)     (f)
None -> ( ) -> ( ) -> ( ) ... ( )
```

This makes ufuncs capable of describing:
* `argmin`
* `argmax`
<s>* `gcd` (#8772)</s>",2017-03-11 23:56:37,,ENH: Augment ufunc.reduce with state,['unlabeled']
8765,open,eric-wieser,"With the invariant that:

```python
a = np.random.random(shape)
inds = ...
axis = ...

# copy some values from a to b
b = np.ones(a.shape) * np.nan
vals = np.take(a, inds, axis=axis)
np.put(b, inds, vals, axis=axis)

# check that the copy worked
assert_equal(vals, np.take(b, inds, axis=axis))
```",2017-03-09 17:37:29,,ENH: Add `axis` argument to `np.put` to match `np.take`,['unlabeled']
8758,open,danjump,"I have a list of dictionaries. I'm trying to use np.vectorize to apply a function that modifies dictionary elements for each dictionary in the list. The results seem to show that vectorize is acting twice on the first element.  Is this a bug that can be fixed?(perhaps related to the fact that vectorize checks type on the first element?) Below are some example cases and output:

A simple test case with no dictionary modifications:

``` python
def fcn1(x):
    return x['b']
a = [{'b': 1} for _ in range(3) ]
print(a)
print(np.vectorize(fcn1)(a))
print(a, '\n\n')
```

output:

```
[{'b': 1}, {'b': 1}, {'b': 1}]
[1 1 1]
[{'b': 1}, {'b': 1}, {'b': 1}]
```

Now modify the dictionary and see that the function is applied twice to the first element:

``` python
def fcn2(x):
    x['b'] += 1
    return x['b']
a = [{'b': 1} for _ in range(3) ]
print(a)
print(np.vectorize(fcn2)(a))
print(a, '\n\n')
```

output:

```
[{'b': 1}, {'b': 1}, {'b': 1}]
[3 2 2]
[{'b': 3}, {'b': 2}, {'b': 2}]  
```  

Try a different modification to check consistency of the bug:

``` python
def fcn3(x):
    x['b'] *= 2
    return x['b']
a = [{'b': 1} for _ in range(3) ]
print(a)
print(np.vectorize(fcn3)(a))
print(a, '\n\n')
```

output:

```
[{'b': 1}, {'b': 1}, {'b': 1}]
[4 2 2]
[{'b': 4}, {'b': 2}, {'b': 2}]    
```

You can do the same thing without actually providing a return value (which is how i'm trying to use it in my use case):

``` python
def fcn4(x):
    x['b'] += 1
a = [{'b': 1} for _ in range(3) ]
print(a)
np.vectorize(fcn4)(a)
print(a, '\n\n')
```

output:
    
```
[{'b': 1}, {'b': 1}, {'b': 1}]
[{'b': 3}, {'b': 2}, {'b': 2}]
```

And by the way, there is nothing special about a list of length 3, you can change that and see the same behavior of only the first element being double-modified. 

I'm confirmed the behavior using both Numpy version 1.11.3 and 1.12.0

**EDIT:**
I found a work-around that also confirms its a ""testing the type on the first element"" issue. If you specify the `otypes` argument, the first element doesn't get hit twice:

``` python
def fcn(x):
    x['b'] += 1
    return x['b']
a = [{'b': 1} for _ in range(3)]
print a
print np.vectorize(fcn, otypes=[dict])(a)
print a, '\n\n'
```

output:

```
[{'b': 1}, {'b': 1}, {'b': 1}]
[2 2 2]
[{'b': 2}, {'b': 2}, {'b': 2}]
```",2017-03-08 21:52:21,,BUG: np.vectorize() functions operate twice on the first element (as seen when the function modifies a mutable object),['00 - Bug']
8735,open,eric-wieser,"Does this seem like a good idea? Would be consistent with `np.expand_dims`, and a more obvious inverse.

`np.collapse_dims` would not accept `axis=None`.",2017-03-03 15:27:02,,Add np.collapse_dims as alias for np.squeeze?,['15 - Discussion']
8734,open,eric-wieser,Which involves including `--\n\n` in the docstrings. See http://stackoverflow.com/a/41245451/102441,2017-03-03 00:23:10,,ENH: Make inspect.signature work for numpy C functions,"['01 - Enhancement', 'component: numpy.lib']"
8726,open,eric-wieser,"Raised by `test_umath.test_nextafter_0`, on the [Windows Subsystem for Linux](https://github.com/Microsoft/BashOnWindows)

Simplest test case:
```
>>> tiny = np.nextafter(np.float128(0), np.float128(1))
3.6451995318824746025e-4951
>>> tiny * 1
0.0
>>> tiny * 100   # not the cause, but related
0.0
```",2017-03-02 01:32:01,,BUG: float128 loses precision on windows subsystem for linux,['00 - Bug']
8720,open,nschloe,"Many `linalg` functions are already broadcastable, `lstsq` isn't.

A workaround is via `svd` which is already broadcasted:
```python
u, s, v = numpy.linalg.svd(A, full_matrices=False)
uTb = numpy.einsum('ijk,ij->ik', u, b)
xx = numpy.einsum('ijk, ij->ik', v, uTb / s)
```",2017-03-01 16:52:58,,ENH: broadcast lstsq,"['01 - Enhancement', 'component: numpy.linalg', 'triaged']"
8719,open,eric-wieser,"#4977 added np.linalg.multi_dot, before np.matmul existed. However, this only works for single 2D matrices.

It ought to be relatively straightforward to add a similar function that works on stacks of matrices, using `np.matmul`.

What should such a function be called? Or can `multi_dot` be internally changed to actually call `matmul`? Because for the inputs it currently accepts, there would be no visible difference in behaviour ",2017-03-01 12:43:57,,ENH: Add matmul equivalent of multi_dot,"['01 - Enhancement', 'component: numpy.linalg']"
8712,open,fschaffalitzky,"
[bmatbug.py.txt](https://github.com/numpy/numpy/files/805744/bmatbug.py.txt)

Using numpy.bmat(...) to generate a 2-D array produces a result that, when multiplied with a 1-D vector using the @ operator, does not behave the same way as a 2-D array of the same size produced by calling numpy.copy(...) on the original one.  Illustrative pseudo-code:

```
x = numpy.random.random((3,))        # x.shape is (3,)
a = numpy.bmat([...])                # a.shape is (5,3)
y = a @ x                            # y.shape is (1,5)   *** wrong? ***
y = numpy.copy(a) @ x                # y.shape is (5,)
```
What is also curious is that both 'a' and numpy.copy(a) have the exact same .shape and .strides.

The attached script reproduces the behavior, generating the following output:

```
$ python3 -B bmatbug.py 
x (3,) (8,)
---
a (5, 3) (24, 8)
y (1, 5) (40, 8)
---
a (5, 3) (24, 8)
y (5,) (8,)
```

--- system parameters ---

Python version: 3.5.2
numpy version: 1.11.0 and 1.11.2
OS: Ubuntu and macOS
Output of 'uname -a':
  ""Linux xxx 4.4.0-64-generic #85-Ubuntu SMP Mon Feb 20 11:50:30 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux"" (Ubuntu 16.10)
  ""Darwin xxx 16.4.0 Darwin Kernel Version 16.4.0: Thu Dec 22 22:53:21 PST 2016; root:xnu-3789.41.3~3/RELEASE_X86_64 x86_64"" (macOS Sierra 10.12.3)
",2017-02-28 04:26:04,,matrix returned by numpy.bmat() does not work as expected with @ operator,"['00 - Bug', 'component: numpy.matrixlib']"
8701,open,eric-wieser,"Right now, the default arguments for sorting-related methods are:

Defaults to `axis=-1`:
 * `np.sort`
 * `np.ndarray.sort`
 * `np.argsort`
 * `np.ndarray.argsort`
 * `np.ma.sort`
 * `np.masked_array.sort`
 * The _documentation_ of `np.ma.argsort`
 * The _documentation_ of `np.masked_array.argsort`

Defaults to `axis=None`:
 * The _implementation_ of `np.ma.argsort`
 * The _implementation_ of `np.masked_array.argsort`

For 1D arrays, there is no difference, as `None` just means flatten and use the only axis.

For 2D arrays, however, code written for `ndarrays` can fail in unexpected ways when passed a `masked_array`, because `argsort` breaks liskov subsitution:

```
def foo(x):
    i = x.argsort()
    assert(i.shape == x.shape)

>>> foo(som_arr)  # ok
>>> foo(som_arr.view(MaskedArray)) # AssertionError
```",2017-02-26 13:58:18,,BUG: np.ma.argsort has different default for axis than ndarray,"['00 - Bug', 'component: numpy.ma']"
8696,open,NateyB,"I'm running macOS 10.12.3, Python 3.6.0, and NumPy 1.12.0.

When using the decimal package with NumPy, the arrays are of type 'object'. While most operations are performed flawlessly, calling np.average() with one of these arrays throws an error if weights are not provided (`AttributeError: 'decimal.Decimal' object has no attribute 'dtype'`), and a different error if weights are provided (`AttributeError: 'bool' object has no attribute 'any'`). Contrarily, calling np.mean() executes without error.

The source of the problem in both cases appears to be the assumption that the result of folding the input array results in a standard supported dtype, rather than being based on the operations available to the object type.

Therefore, I suspect that this issue will extend to any types which support the requisite numeric operations but are not natively supported by NumPy.

I resolved the weighted problem for my use case by changing the line `if (scl == 0.0).any()` (line 1138 in `lib/function_base.py`) to check if scl were an array first, and if not then removing the `any()` call, though I don't know if that solution is desirable or acceptable for the purposes of NumPy.

Sample input:
```
import numpy as np                                                              
import decimal as dc                                                            
                                                                                
values = np.array([dc.Decimal(x) for x in range(10)])                           
weights = np.array([dc.Decimal(x) for x in range(10)])                          
weights /= weights.sum()                                                        
                                                                                
print(np.mean(values))                                                          
print(np.average(values, weights=weights))                                      
```

Corresponding output:
```
4.5
Traceback (most recent call last):
  File ""bugreport.py"", line 9, in <module>
    print(np.average(values, weights=weights))
  File ""/usr/local/lib/python3.6/site-packages/numpy/lib/function_base.py"", line 1138, in average
    if (scl == 0.0).any():
AttributeError: 'bool' object has no attribute 'any'
```",2017-02-25 22:51:15,,np.average crashes for 1D decimal object array (now only without weights),"['00 - Bug', 'component: numpy.lib']"
8679,open,eric-wieser,"Unlike `np.array(x, copy=False)`, which does. Follow up from #8666.

Fixes to this bug should probably remove the special casing from `np.ma.asanyarray`",2017-02-23 16:05:07,,"np.ma.masked_array(x, copy=False) never returns x by identity","['00 - Bug', 'component: numpy.ma']"
8657,open,paul-the-noob,"I think a pair of functions `mergesorted` and `argmergesorted` which take as their argument(s) a pair or list  of _sorted_ vectors (not necessarily same length). and return the sorted concatenation in linear (x log #arguments) time would be quite useful.

nd generalisation (with axis argument) and meaning of arg* version should be obvious.

An in-place variant operating on already concatenated vectors (the indices delineating sorted chunks would be the second argument) might also be interesting.

I believe currently, the best option for merging two sorted vectors is concatenating them and then using `np.sort` with `kind='mergesort'`. However, this is still quite wasteful.

Use cases:

   - merging sorted data, obviously
   - linear time finding shared elements between two arrays that happen to be sorted
   - reusing a lexsort with different key order (for example when converting sparse csc to csr)

I guess this could be implemented reusing mergesort code already in numpy. Perhaps special casing inputs of vastly different lengths.
",2017-02-21 22:15:18,,Feature Request: add mergesorted and argmergesorted functions,"['01 - Enhancement', 'component: numpy._core', '54 - Needs decision']"
8627,open,astrofrog,"The following call to ``histogram`` fails:

```python
In [5]: np.histogram([2**53])
/Users/tom/miniconda3/envs/dev/lib/python3.6/site-packages/numpy/lib/function_base.py:566: RuntimeWarning: divide by zero encountered in double_scalars
  norm = bins / (mx - mn)
/Users/tom/miniconda3/envs/dev/lib/python3.6/site-packages/numpy/lib/function_base.py:591: RuntimeWarning: invalid value encountered in multiply
  tmp_a *= norm
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-5-41f77d7d86af> in <module>()
----> 1 np.histogram([2**53])

/Users/tom/miniconda3/envs/dev/lib/python3.6/site-packages/numpy/lib/function_base.py in histogram(a, bins, range, normed, weights, density)
    598             # The index computation is not guaranteed to give exactly
    599             # consistent results within ~1 ULP of the bin edges.
--> 600             decrement = tmp_a_data < bin_edges[indices]
    601             indices[decrement] -= 1
    602             # The last bin includes the right edge. The other bins do not.

IndexError: index -9223372036854775808 is out of bounds for axis 1 with size 11
```

whereas this succeeds:

```python
In [6]: np.histogram([2**53-1])
Out[6]: 
(array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),
 array([  9.00719925e+15,   9.00719925e+15,   9.00719925e+15,
          9.00719925e+15,   9.00719925e+15,   9.00719925e+15,
          9.00719925e+15,   9.00719925e+15,   9.00719925e+15,
          9.00719925e+15,   9.00719925e+15]))
```

The threshold for triggering the bug appears to be 2**53, but this only happens when the min of the array is the same as the max:

```python
In [7]: np.histogram([2**53, 2**53])
/Users/tom/miniconda3/envs/dev/lib/python3.6/site-packages/numpy/lib/function_base.py:566: RuntimeWarning: divide by zero encountered in double_scalars
  norm = bins / (mx - mn)
/Users/tom/miniconda3/envs/dev/lib/python3.6/site-packages/numpy/lib/function_base.py:591: RuntimeWarning: invalid value encountered in multiply
  tmp_a *= norm
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-7-f74f7a6cf401> in <module>()
----> 1 np.histogram([2**53, 2**53])

/Users/tom/miniconda3/envs/dev/lib/python3.6/site-packages/numpy/lib/function_base.py in histogram(a, bins, range, normed, weights, density)
    598             # The index computation is not guaranteed to give exactly
    599             # consistent results within ~1 ULP of the bin edges.
--> 600             decrement = tmp_a_data < bin_edges[indices]
    601             indices[decrement] -= 1
    602             # The last bin includes the right edge. The other bins do not.

IndexError: index -9223372036854775808 is out of bounds for axis 1 with size 11
```

When the min and the max are different, things are fine:

```python
In [8]: np.histogram([2**53, 2**54])
Out[8]: 
(array([1, 0, 0, 0, 0, 0, 0, 0, 0, 1]),
 array([  9.00719925e+15,   9.90791918e+15,   1.08086391e+16,
          1.17093590e+16,   1.26100790e+16,   1.35107989e+16,
          1.44115188e+16,   1.53122387e+16,   1.62129587e+16,
          1.71136786e+16,   1.80143985e+16]))
```



",2017-02-16 23:21:11,,Bug in np.histogram when min == max and values are >= 2**53,['unlabeled']
8624,open,taldcroft,"The following behavior is not what I would expect and seems like a bug:
```
In [1]: x = np.ma.MaskedArray([1,2,3])
In [2]: x[:2] = [100, np.ma.masked]
In [3]: x
Out[3]: 
masked_array(data = [100   0   3],
             mask = False,
       fill_value = 999999)
```
One can correctly set a single item (e.g. `x[2] = np.ma.masked`) or set a slice to a scalar (`x[:2] = np.ma.masked`).

I'm using numpy 1.11.3.",2017-02-16 18:26:19,,Masked array set slice fails with np.ma.masked on right side,['component: numpy.ma']
8621,open,mp-v2,"This would integrate the work-around solutions of a number of `stackoverflow` questions, such as:

http://stackoverflow.com/questions/15637336/numpy-unique-with-order-preserved

> `['b','b','b','a','a','c','c']`
> 
> numpy.unique gives
> `['a','b','c']`
> 
> How can I get the original order preserved
> `['b','a','c']`

The issue with these workarounds is that you _can_ get the unique list unsorted but it is much more complicated to get a matching list with counts (from the `return_counts` boolian).

Having an integrated boolian (i.e. `is_sorted=True`) for sorting of results would clear up this issue and increase the utility of the function.

",2017-02-15 19:35:48,,np.unique needs an option to retain the original order,"['01 - Enhancement', 'component: numpy.lib']"
8581,open,solarjoe,"np.cov and np.corrcoeff are closely related functions.
But the latter is lacking the arguments ""fweights"" and ""aweights"".",2017-02-08 13:45:37,,ENH: Add weights in `np.corrcoeff` and `np.var`,['unlabeled']
8579,open,mwtoews,"Inspecting `numpy.core.defchararray` and [the online docs for string operations](https://docs.scipy.org/doc/numpy/reference/routines.char.html), it seems there is a missing feature to slice a string array in a vectorized way.

It would be nice to have `numpy.core.defchararray.slice(a, start, stop[, step])`

[This Q/A](http://stackoverflow.com/q/39042214/327026) seems to reflect this missing feature, with a few example implementations.",2017-02-08 04:56:43,,Feature request: vectorized character slice,['unlabeled']
8545,open,charris,"Should not be writeable.
```
In [32]: a = np.int_(1)

In [33]: a[...].flags
Out[33]: 
  C_CONTIGUOUS : True
  F_CONTIGUOUS : True
  OWNDATA : False
  WRITEABLE : True
  ALIGNED : True
  UPDATEIFCOPY : False

In [34]: a[...] = 0
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-34-2c54de2a2dee> in <module>()
----> 1 a[...] = 0

TypeError: 'numpy.int64' object does not support item assignment

```",2017-01-29 20:56:17,,Incorrect flags on result of elipsis indexed scalar,"['00 - Bug', 'component: numpy._core']"
8538,open,sashkab,"`isscalar` doesn't support memoryview object:

```python
>>> import numpy
>>> numpy.__version__
'1.13.0.dev0+c5e1773'
>>> numpy.isscalar(memoryview(numpy.array([])))
True
```
",2017-01-27 19:20:19,,BUG: numpy.isscalar thinks that memoryview is always a scalar,"['00 - Bug', '57 - Close?']"
8521,open,anntzer,"See http://stackoverflow.com/questions/30112420/histogram-for-discrete-values-with-matplotlib

When histogramming integer values (or, more generally, discrete values with a constant step -- but the latter is harder to detect), the choice of the binning can be crucial: if you histogram [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] using 10 bins, one of the bins will have twice as many counts as the others. In other words, the binsize should normally be a multiple of the discretization size.

Now that numpy already provides automatic bin selection (Freedman-Diaconis, Sturges, etc.), would it be reasonable to also make these selectors try to ensure that the binning of integer values satisfies the criterion mentioned above?
",2017-01-23 02:11:06,,Improve histogramming of integer values,['component: numpy.lib']
8519,open,godaygo,"When record array is indexed with incorrect field name it raises `ValueError` as expected:
```python
> arr = np.zeros(10, dtype=[('a', 'i8')])
> arr['b']

ValueError: no field of name b
``` 
When a *simple* array is indexed, it raises `IndexError` as unexpected:
```python
> arr = np.arange(10)
> arr['a']

IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or 
boolean arrays are valid indices
```

According to this [question on SO](http://stackoverflow.com/questions/13158452/2d-numpy-array-does-not-give-an-error-when-indexing-with-strings-containing-digi) previously it was `ValueError` in both cases. Because you can not *typecheck*, they are both `ndarrays`, it is odd to have different types of exceptions. 

Was this an intended change?

Also according to all this, the message for `IndexError` is not full, it does not say about fields.  ",2017-01-22 16:39:06,,Incorrect Exception when indexing array with field.,['03 - Maintenance']
8518,open,eric-wieser,"`np.r_[:3] == [0, 1, 2]` is fine, but `np.r_[3:] == [0, 1, 2]` makes no sense at all
```python
>>> np.r_[3:]
array([0, 1, 2])
>>> np.ogrid[3:]
array([0, 1, 2])
>>> np.mgrid[3:]
array([0, 1, 2])
```",2017-01-22 11:26:21,,"BUG: indexing helpers np.r_, np.mgrid, and np.ogrid give nonsense results for stop-less slices","['00 - Bug', 'defunct — difficulty: Intermediate']"
8516,open,empet,"    H=np.array([[-1.0+0.5j, 2+3j],
                    [0.3-4j, 5+27j]])

    H=ma.masked_where(np.abs(H)>10, H)
    print H
    [[(-1+0.5j) (2+3j)]
     [(0.3-4j) --]]
     print np.log(H)
    [[-- (1.2824746787307684+0.982793723247329j)]
    [(1.3890989805021459-1.4959364790841299j) --]]
 
I identified this bug working with a bigger masked array.  log inserts invalid values in positions where the entries are in fact valid.",2017-01-21 22:06:05,,numpy.log does not return the right result for a complex type masked array,"['00 - Bug', 'component: numpy.ma']"
8513,open,mattharrigan,"This is a follow up to this [discussion](https://mail.scipy.org/pipermail/numpy-discussion/2017-January/076388.html).  In certain use cases very large performance gains can be obtained with a gufunc of signature (i),(i)-.() to check if all elements along that dimension are equal.  For large arrays which have any early non equal element, its dramatically faster (1000x) than the current alternative.  For large arrays which are all equal, its ~10% faster due to eliminating the intermediate boolean array.  For tiny arrays its much faster due to a single function call instead of at least two, but its debatable how relevant speed is for tiny problems.

Initial code is [here](https://github.com/mattharrigan/numpy_logical_gufuncs).  It includes other functions in this same family, such as all or any, and eq, ne, lt, le, gt, ge, for multiple dtypes.

On this discussion thread, there we no +1 or negative responses.  I'm not sure what that means.

Assuming the functionality is desired for inclusion in numpy, then I would like advice on where this code should go.",2017-01-21 12:29:00,,gufunc to test for all elements equal along axis,['unlabeled']
8509,open,essandess,"The page [Subclassing ndarray](https://docs.scipy.org/doc/numpy/user/basics.subclassing.html) has an instructive example that unfortunately yields an infinite loop for array dimensions > 1.

Code:

```python
import numpy as np

class MySubClass(np.ndarray):

    def __new__(cls, input_array, info=None):
        obj = np.asarray(input_array).view(cls)
        obj.info = info
        return obj

    def __array_finalize__(self, obj):
        print('In __array_finalize__:')
        print('   self is %s' % repr(self))
        print('   obj is %s' % repr(obj))
        if obj is None: return
        self.info = getattr(obj, 'info', None)

# this works
bar = np.random.normal(size=(5,))
foo = MySubClass(bar,info='1-D') 

# this hits maximum recursion depth
bar = np.random.normal(size=(5,2))
foo = MySubClass(bar,info='2-D') 
```
",2017-01-20 23:32:08,,TST: add a test for Subclassing ndarray to prove recursive looping calls are gone,"['05 - Testing', '59 - Needs tests']"
8502,open,diego898,"I think there should be a standard implementation of a `settdiffXd` function in numpy. Right now, various answers online are ""hackish"" in that they all basically convert them to 1D to end up running setdiff1d. The same goes for `intersect1d`. 

Is there some reason these aren't standard functions in numpy?",2017-01-19 20:25:31,,Feature Request: Adding standard `settdiffXd` and `intersectXd` functions,['unlabeled']
8480,open,gongbudaizhe,"Talk is cheap, below is the code : )
```python
import numpy as np
img = np.random.rand(1000, 1000, 3)

np.mean(img, axis=(0, 1))
# 100 loops, best of 3: 12.5 ms per loop

[np.mean(img[:,:,0]), np.mean(img[:,:,1]), np.mean(img[:,:,2])]
# 100 loops, best of 3: 4.42 ms per loop
```

Environments
- python 2.7.13
- numpy 1.11.3",2017-01-16 11:02:13,,np.mean with axis parameter is slower than naive implementation,['component: numpy._core']
8463,open,gerritholl,"The current behaviour of `sign` on `timedelta64` objects is somewhat surprising:    

```
In [64]: sign(np.timedelta64(3, 'm')) == sign(np.timedelta64(3, 's'))
Out[64]: False
```

I'm not sure if the return type for the sign of a `timedelta64` should even be a `timedelta64` or or simply numerical.  Of course, normally `sign(x)` returns a result of the same `dtype` as `x`, but for any numerical data we have `uint8(1) == int8(1) == float32(1) == ....` etc.  I'm not sure what the best return value would be for the sign of a `timedelta64`, but I think it would be desirable if their signs compared equal.",2017-01-10 19:40:34,,sign for any positive timedelta should be/compare equal,['component: numpy.datetime64']
8460,open,gerritholl,"Closely related to #4592, when a slice containing masked values is assigned to an unmasked float array, the underlying data are copied, but masked elements are not converted to nan:

```
In [289]: A = arange(5, dtype=""f2"")

In [290]: fm = ma.masked_array([1.0, 2.0, 3.0, 4.0], [True, False, True, False])

In [291]: A[0] = fm[0]
/dev/shm/gerrit/venv/stable-3.5/bin/ipython3:1: UserWarning: Warning: converting a masked element to nan.
  #!/dev/shm/gerrit/venv/stable-3.5/bin/python3.5

In [292]: print(A)
[ nan   1.   2.   3.   4.]

In [293]: A[:4] = fm[:4]

In [294]: print(A)
[ 1.  2.  3.  4.  4.]
```",2017-01-10 15:53:38,,masked values not converted to nan when slicing,['component: numpy.ma']
8458,open,jotasi,"`np.fromfile` silently truncates integers that are larger than the maximum signed 64 bit integer (`>2**63-1`)  to `2**63-1` (9223372036854775807) without raising an error or a warning. `np.loadtxt` for example raises an error:

```
In [1]: import numpy as np

In [2]: with open(""/tmp/testfile.txt"", ""w"") as f:
   ...:     f.write(str(2**63))
   ...: 

In [3]: with open(""/tmp/testfile.txt"") as f:
   ...:     print(f.read())
   ...: 
9223372036854775808

In [4]: print(np.fromfile('/tmp/testfile.txt', dtype=int, sep=' '))                                                    
[9223372036854775807]

In [5]: np.loadtxt(""/tmp/testfile.txt"", dtype=int)
---------------------------------------------------------------------------
OverflowError                             Traceback (most recent call last)
<ipython-input-5-a101a776f718> in <module>()
----> 1 np.loadtxt(""/tmp/testfile.txt"", dtype=int)

XXX/numpy/lib/npyio.pyc in loadtxt(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin)
    928
    929             # Convert each value according to its column and store
--> 930             items = [conv(val) for (conv, val) in zip(converters, vals)]
    931             # Then pack it according to the dtype's nesting
    932             items = pack_items(items, packing)

OverflowError: Python int too large to convert to C long
```

Is this the desired behavior? If so, this might be added to the documentation.",2017-01-09 09:35:49,,`np.fromfile` silently truncates integers > signed 64 bit when reading a text file,"['00 - Bug', 'component: numpy._core']"
8449,open,will133,"I'm using numpy 1.11.2 on Linux and here is what I get:

```
In [1]: import numpy

In [2]: x = numpy.array([3, None], 'm8[D]')

In [3]: x.astype('float64')
array([  3.00000000e+00,  -9.22337204e+18])
```
This behavior seems to be the same for datetime64 as well.
 
IMHO, this causes a potentially dangerous problem where the wrong calculation is returned silently and the negative most int64 is used instead of nan.  For example, if you want to do a calculation in dates and then convert this to float to calculate the number of years (approximately), you can't just do an arr.astype('float64') / 365 since the NaT to nan is not handled correctly.
 ",2017-01-05 23:23:46,,Casting a NaT with a timedelta64or datetime64 to float64 return big negative number ,"['00 - Bug', 'component: numpy.datetime64']"
8406,open,godaygo,"The [documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.flatiter.html) about flatiter object says that:
1. It allows iterating over the array as if it were a 1-D array, either in a for-loop or by calling its next method.
2. The iterator can also be indexed using basic slicing or advanced indexing.  

And there is nothing said about some restrictions. In [manual](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#flat-iterator-indexing) I found: 

> This iterator object can also be indexed using basic slicing or advanced indexing as long as the selection object is not a tuple. This should be clear from the fact that x.flat is a 1-dimensional view.

So it seems to me that there is some inconsistency in behavior:
```python
 In[1]: arr = np.arange(10)
 In[2]: arr[1, 2]
Out[2]: IndexError: too many indices for array      
```
but for flatiter, I get an Exception that makes no sense for me:
```python
 In[3]: arr.flat[1, 2]
Out[3]: IndexError: unsupported iterator index
```
Also for 1D array this code is valid:
```python
 In[4]: arr[(1, 2),]
Out[4]: array([1, 2])
```
but for flatiter I again get this incomprehensible error:
```python
 In[4]: arr.flat[(1, 2),]
Out[4]: IndexError: unsupported iterator index
```
Is it the implementation choice?

-----
And for some other stupid attempts array raises some meaningful Exception:
```python
 In[5]: arr[{1,  2}]
Out[5]: IndexError: only integers, slices (`:`), ellipsis (`...`), 
        numpy.newaxis (`None`) and integer or boolean arrays are valid indices
```
but flatiter says something like *""I'm Groot""*
```python
 In[6]: arr.flat[{1, 2}]
Out[6]: IndexError: unsupported iterator index
```",2016-12-20 21:05:42,,flatiter object behavior,"['00 - Bug', 'component: numpy._core']"
8378,open,guziy,"Hi:

I am expecting to get a masked array from a file with the 255 values masked. But that is not happening, please see the example below.

```python
import numpy as np

def main():
    s = [""255 255"", ""255 1""]
    bs = [bl.encode() for bl in s]

    m = np.genfromtxt(bs, usemask=True, missing_values=[b""255"", 255], dtype=np.uint8)
    print(m)
    print(m.mask.sum())


if __name__ == ""__main__"":
    main()
```

And the output I get is 

```
$ python test.py
[[255 255]
 [255 1]]
0
```

I tried different things for missing_values, i.e. 255, ""255"",  b""255"". Am I using it correctly?

Cheers
",2016-12-13 18:31:20,,DOC: genfromtxt missing_values documentation not clear,['04 - Documentation']
8361,open,eric-wieser,"These all work:

    >>> np.choose(np.ones(2, dtype=np.intp), np.zeros((5, 2)))
    array([ 0.,  0.])
    >>> np.choose(np.ones(1, dtype=np.intp), np.zeros((5, 1)))
    array([ 0.])
    >>> np.choose(np.ones(0, dtype=np.intp), np.zeros((5, 0)))
    array([])

So why does this not? (I would expect the output in both cases to be `array([])`)

    >>> np.choose(np.ones(0, dtype=np.intp), np.zeros((0, 5)))
    ValueError: 0-length sequence.
    >>> np.choose(np.ones(0, dtype=np.intp), np.zeros((0, 0)))
    ValueError: 0-length sequence.

Is there anything semantically wrong in saying _""from a set of no sequences, each containing 5 elements, pick the corresponding item from this set of no indices""_?
",2016-12-09 16:21:45,,np.choose does not work for a zero-length sequence,['unlabeled']
8352,open,lukovnikov,"I'm trying to concatenate all elements of a row into a string as follows:
    
    np.apply_along_axis(lambda x: "" "".join(map(str, x)), 1, b)

*b* is

    [[111,111,0,0,0], [111,111,111,111,111]]

However, the result of the line is:

    ['111 111 0 0 0', '111 111 111 1']

It looks like np.apply_along_axis is cutting the second string to be of the same length as the first one. If I put a longer sequence first, the result is correct:

    ['111 111 111 111 111', '111 111 0 0 0']

 So I'm guessing this is a bug?

<hr>

Summary 2019-04-30 by @seberg

`np.apply_along_axis` infers the output dtype from the first pass. Which can be worked around for example but the function returning an array of a correct type.

Actions:
  * `np.apply_along_axis` could/should get a `dtype` kwarg (or similar, compare also `np.vectorize`).",2016-12-07 17:02:53,,apply_along_axis cuts strings,"['01 - Enhancement', 'component: numpy.lib', 'defunct — difficulty: Intermediate']"
8306,open,J-Sand,"It's possible to create a recursive object array:

    >>> a = np.empty(1, dtype=object)
    >>> a[0] = a

Many functions that support object arrays fail to check for such situations, resulting in a `RecursionError` or in some cases a stack overflow:

    >>> print(a)
    <truncated output>
    RecursionError: maximum recursion depth exceeded while calling a Python object
    >>> a in a
    Fatal Python error: Cannot recover from stack overflow.

    Current thread 0x00007fa98491c700 (most recent call first):
      File ""<stdin>"", line 1 in <module>
    Aborted (core dumped)

Python's built-in containers handle recursion gracefully:

    >>> a = []
    >>> a.append(a)
    >>> print(a)
    [[...]]
    >>> a in a
    True

My guess is that this isn't really worth doing for `ndarray`, but it would be nice to at least prevent stack overflows?",2016-11-24 00:55:37,,Various operations fail on recursive object arrays,"['00 - Bug', 'Priority: low']"
8300,open,charris,"* There needs to be a clear distinction between testing the ufunc object (test_ufunc) and the  ufuncs instances (test_umath). Currently the two types of tests are mixed between the files.
* The testing of the various ufuncs is far from complete.

We might want to rename the files also. I'd suggest renaming test_ufunc to test_ufunc_object and renaming test_umath to test_ufuncs.

Note that some of the ufunc tests are in test_numeric. We should probably distinguish tests specific to ufuncs and not have them in the more general array testing.",2016-11-23 15:00:36,,Cleanup test_umath and test_ufunc,"['17 - Task', 'Priority: low']"
8294,open,davydden,"with site.cfg
```
[ALL]
rpath=/home/davydden/spack/opt/spack/linux-ubuntu16-x86_64/gcc-5.4.0/atlas-3.10.2-5jtibobvt7nlwd3zyviionsn7v4z2npd/lib
[atlas]
atlas_libs=satlas
library_dirs=/home/davydden/spack/opt/spack/linux-ubuntu16-x86_64/gcc-5.4.0/atlas-3.10.2-5jtibobvt7nlwd3zyviionsn7v4z2npd/lib
```
numpy decides for itself to use multithreaded version despite specifically being told to use serial one:
```
atlas_3_10_blas_threads_info:
    libraries = ['satlas', 'tatlas']
    library_dirs = ['/home/davydden/spack/opt/spack/linux-ubuntu16-x86_64/gcc-5.4.0/atlas-3.10.2-5jtibobvt7nlwd3zyviionsn7v4z2npd/lib']
    define_macros = [('HAVE_CBLAS', None), ('ATLAS_INFO', '""\\""3.10.2\\""""')]
    language = c
lapack_opt_info:
    libraries = ['tatlas', 'satlas', 'tatlas']
    library_dirs = ['/home/davydden/spack/opt/spack/linux-ubuntu16-x86_64/gcc-5.4.0/atlas-3.10.2-5jtibobvt7nlwd3zyviionsn7v4z2npd/lib']
    define_macros = [('ATLAS_INFO', '""\\""3.10.2\\""""')]
    language = f77
blas_opt_info:
    libraries = ['satlas', 'tatlas']
    library_dirs = ['/home/davydden/spack/opt/spack/linux-ubuntu16-x86_64/gcc-5.4.0/atlas-3.10.2-5jtibobvt7nlwd3zyviionsn7v4z2npd/lib']
    define_macros = [('HAVE_CBLAS', None), ('ATLAS_INFO', '""\\""3.10.2\\""""')]
    language = c
openblas_info:
  NOT AVAILABLE
openblas_lapack_info:
  NOT AVAILABLE
atlas_3_10_threads_info:
    libraries = ['tatlas', 'satlas', 'tatlas']
    library_dirs = ['/home/davydden/spack/opt/spack/linux-ubuntu16-x86_64/gcc-5.4.0/atlas-3.10.2-5jtibobvt7nlwd3zyviionsn7v4z2npd/lib']
    define_macros = [('ATLAS_INFO', '""\\""3.10.2\\""""')]
    language = f77
lapack_mkl_info:
  NOT AVAILABLE
blas_mkl_info:
  NOT AVAILABLE
```",2016-11-21 07:58:24,,atlas: multithreaded BLAS is used despite setting site.cfg to use serial one,"['00 - Bug', 'component: numpy.distutils']"
8288,open,MarkWieczorek,"I have written a very simple python program that calls a fortran routine, and then this fortran routine calls a python callcack function: https://github.com/MarkWieczorek/callback_test 

Everything works as expected. However, once I add a second fortran routine that makes use of the same python callback function, f2py can not even compile the code, complaining that the callback function is being redefined. I am certain that there must be a way to do this, but I can not figure it out.

Here are the two fortran routines, which call the python function `pyfunc`:
```
subroutine routine()
    integer i
    external pyfunc
    i=1
    print*, ""In ROUTINE""
    call pyfunc(i) ! This will be defined to raise a Python exception
    print*, ""Called PYFUNC. Shouldn't be here!""
    return
end subroutine routine

subroutine routine2()
    integer i
    external pyfunc
    i=2
    print*, ""In ROUTINE2""
    call pyfunc(i) ! This will be defined to raise a Python exception
    print*, ""Called PYFUNC. Shouldn't be here!""
    return
end subroutine routine2
```
and here is the python signature file
```
python module callback_test
    interface
        subroutine routine()
            intent(callback,hide) pyfunc
            external pyfunc
            integer i
            call pyfunc(i)
        end subroutine routine

!        subroutine routine2()
!            intent(callback,hide) pyfunc
!            external pyfunc
!            integer i
!            call pyfunc(i)
!        end subroutine routine2
    end interface
end python module callback_test
```
Compiling
```
python3 -m numpy.f2py -c signature.pyf subroutine.f95
```
and then running
```
import callback_test
def f(status): raise Exception(status)
callback_test.pyfunc = f
callback_test.routine()
```
gives the output
```
 In ROUTINE
capi_return is NULL
Call-back cb_pyfunc_in_routine__user__routines failed.
Traceback (most recent call last):
  File ""script.py"", line 4, in <module>
    callback_test.routine()
  File ""script.py"", line 2, in f
    def f(status): raise Exception(status)
Exception: 1
```
However, if I remove the comments from the above signature file, I get the error
```
/usr/local/lib/python3.5/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: ""Using deprecated NumPy API, disable it by ""          ""#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION"" [-W#warnings]
#warning ""Using deprecated NumPy API, disable it by "" \
 ^
/var/folders/9v/k2l3cpqs1wn56ktkr4l5zncr0000gn/T/tmp886anvls/src.macosx-10.11-x86_64-3.5/callback_testmodule.c:327:13: error: redefinition of 'pyfunc_'
extern void F_FUNC(pyfunc,PYFUNC) (int *i_cb_capi) {
            ^
/var/folders/9v/k2l3cpqs1wn56ktkr4l5zncr0000gn/T/tmp886anvls/src.macosx-10.11-x86_64-3.5/callback_testmodule.c:87:21: note: expanded from macro 'F_FUNC'
#define F_FUNC(f,F) f##_
                    ^
<scratch space>:122:1: note: expanded from here
pyfunc_
^
/var/folders/9v/k2l3cpqs1wn56ktkr4l5zncr0000gn/T/tmp886anvls/src.macosx-10.11-x86_64-3.5/callback_testmodule.c:229:13: note: previous definition is here
extern void F_FUNC(pyfunc,PYFUNC) (int *i_cb_capi) {
            ^
/var/folders/9v/k2l3cpqs1wn56ktkr4l5zncr0000gn/T/tmp886anvls/src.macosx-10.11-x86_64-3.5/callback_testmodule.c:87:21: note: expanded from macro 'F_FUNC'
#define F_FUNC(f,F) f##_
                    ^
<scratch space>:121:1: note: expanded from here
pyfunc_
^
1 warning and 1 error generated.
```
Any advice would be appreciated. I tried defining a python module `__user__routines`, but was not able to get this to work at all.",2016-11-17 12:18:20,,[f2py] Impossible for two fortran functions to use the same python callback function,"['00 - Bug', 'component: numpy.f2py']"
8275,open,eric-wieser,This would make overloading/wrapping indexing behaviour far easier that it currently is - duplicating the logic of a 600-line C function perfectly in python is not reasonable to expect.,2016-11-14 22:42:00,,ENH: Expose the workings of prepare_index to user python code,['unlabeled']
8237,open,nouiz,"https://github.com/numpy/numpy/pull/8222 raised the numerical precision in float16 of numpy.mean.

As per comment https://github.com/numpy/numpy/pull/8222#issuecomment-257406456, this can also be done for numpy.std and numpy.var

<hr>
Summary 2019-04-26 by @stefanv:

- Float16 operations are implemented in software; as such intermediate computations can be done in float32 without loss of speed
- For `numpy.mean`, the intermediary type is modified [here](https://github.com/numpy/numpy/blob/maintenance/1.16.x/numpy/core/_methods.py#L71).
- A similar change can be made in the same file for `_var` and `_std` ",2016-11-04 20:53:40,,Raise precision of numpy.std and numpy.var in float16,"['01 - Enhancement', 'component: numpy._core']"
8224,open,butterw,"From the docs, currently available type formatters in np.set_printoptions are:
- 'numpy_str' : types `numpy.string_` and `numpy.unicode_`
- 'str' : all other strings
- 'str_kind' : sets 'str' and 'numpystr'

There is a typo in the doc: it's 'numpystr' not  'numpy_str' 
and it also applies to numpy.bytes_ on PY3

IMO custom formatter for string types need improving. Beyond the current documentation bug, it should at least be possible to set an individual formatter for each of np.string_, np.bytes_ and np.unicode_ so as to be able to be able to handle encoding.
on PY3:
b=np.array([b'a', b'b'])
u=np.array([u'c', u'd'])
np.set_printoptions(formatter={'str_kind': lambda x: str(x.decode()).upper()})

> > >  print(u)
> > > AttributeError: 'numpy.str_' object has no attribute 'decode'
> > >  print(b)
> > > [A B]

'numpy_str' formatter doesn't work:
np.set_printoptions(formatter={''numpy_str': lambda x: str(x).upper()})
On PY3:

> > >  print(b) 
> > > [b'a' b'b']
> > >  print(u)
> > > ['c' 'd']

On PY2:

> > >  print(b)
> > > ['a' 'b']
> > >  print(u)
> > > [u'c' u'd']
",2016-10-29 13:12:38,,set_printoptions: custom formatter issues with string types,['component: numpy._core']
8207,open,ChristophWWagner,"Hello,

as this is my first issue report, please excuse any imperfections. I am keen to learn and willing to improve.

I happen to notice a problem with numpy.min_scalar_type() which I consider a bug as it contradicts the expected function behaviour. min_scalar_type() does not recognize the correct data types for negative exponential floating point values and for vlaues exceeding numpy.float64.

I am running python 2.7 on a x64 machine running debian8 and tested both numpy 1.8.2 and 1.12.0.dev0. There seems to be a regression from 1.8.2 to 1.12.0 as well.

Following a shell log for numpy 1.8.2:
`Python 2.7.9 (default, Mar  1 2015, 12:57:24)  
[GCC 4.9.2] on linux2  
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.                                                                                                                                                                       

> > > import numpy as np
> > > np.version.version
> > > '1.8.2'
> > > import sys
> > > sys.version_info
> > > sys.version_info(major=2, minor=7, micro=9, releaselevel='final', serial=0)
> > > np.finfo(np.float16)
> > > finfo(resolution=0.0010004, min=-6.55040e+04, max=6.55040e+04, dtype=float16)
> > > np.finfo(np.float32)
> > > finfo(resolution=1e-06, min=-3.4028235e+38, max=3.4028235e+38, dtype=float32)
> > > np.finfo(np.float64)
> > > finfo(resolution=1e-15, min=-1.7976931348623157e+308, max=1.7976931348623157e+308, dtype=float64)
> > > np.finfo(np.float128)
> > > finfo(resolution=1e-18, min=-1.18973149536e+4932, max=1.18973149536e+4932, dtype=float128)
> > > np.min_scalar_type(3.1)  
> > > dtype('float16')
> > > np.min_scalar_type(3.1e20)
> > > dtype('float32')
> > > np.min_scalar_type(3.1e50)
> > > dtype('float64')
> > > np.min_scalar_type(3.1e3000)
> > > dtype('float64')
> > > np.min_scalar_type(3.1e-2)  
> > > dtype('float16')
> > > np.min_scalar_type(3.1e-20)
> > > dtype('float16')
> > > np.min_scalar_type(3.1e-50)
> > > dtype('float16')
> > > np.min_scalar_type(3.1e-3000)
> > > dtype('float16')`

I expect the following results:
3.1 -> float16 (OK)
3.1e20 -> float32 (OK)
3.1e50 -> float64 (OK)
3.1e3000 -> float128 (wrong, got float64)
3.1e-2 ->float16(OK)
3.1e-20 -> float32 (wrong, got float16)
3.1e-50 -> float64 (wrong, got float16)
3.1e-3000 -> float128 (wrong, got float16)

Similar things happen with numpy 1.12.0.dev0 (same system and python version):
`Python 2.7.9 (default, Mar  1 2015, 12:57:24) 
[GCC 4.9.2] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

> > > import numpy as np
> > > np.version.version
> > > '1.12.0.dev0+e7d6f36'
> > > np.finfo(np.float16)
> > > finfo(resolution=0.0010004, min=-6.55040e+04, max=6.55040e+04, dtype=float16)
> > > np.finfo(np.float32)
> > > finfo(resolution=1e-06, min=-3.4028235e+38, max=3.4028235e+38, dtype=float32)
> > > np.finfo(np.float64)
> > > finfo(resolution=1e-15, min=-1.7976931348623157e+308, max=1.7976931348623157e+308, dtype=float64)
> > > np.finfo(np.float128)
> > > finfo(resolution=1e-18, min=-1.18973149536e+4932, max=1.18973149536e+4932, dtype=float128)
> > > np.min_scalar_type(1e1)
> > > dtype('float16')
> > > np.min_scalar_type(1e4)
> > > dtype('float16')
> > > np.min_scalar_type(1e5)
> > > dtype('float32')
> > > np.min_scalar_type(1e50)
> > > dtype('float64')
> > > np.min_scalar_type(1e500)
> > > dtype('float16')
> > > np.min_scalar_type(1e-4)
> > > dtype('float16')
> > > np.min_scalar_type(1e-5)
> > > dtype('float16')
> > > np.min_scalar_type(1e-50)
> > > dtype('float16')
> > > np.min_scalar_type(1e-500)
> > > dtype('float16')`

Again: the following behaviour is noticed:
1e1 -> float16 (OK)
1e4 -> float16 (OK)
1e5 -> float32 (OK)
1e50 -> float64 (OK)
1e500 -> float128 (wrong, got float16, worse output than with 1.8.2 (float64))
1e-4 -> float16 (OK)
1e-5 -> float32 (wrong, got float16)
1e-50 -> float64 (wrong, got float16)
1e-500 -> float128 (wrong, got float16)

I hope my description is helpful. I am willing to assist in resolving this issue but do not know how I might contribute best, so please let me know.

Best regards
Christoph
",2016-10-24 13:06:00,,BUG: numpy.min_scalar_type,['unlabeled']
8203,open,asanakoy,"`np.histogram` crashes with `MemoryError` when the input contains very small numbers.

```
In [13]:  print(sys.version)
2.7.6 (default, Jun 22 2015, 17:58:13) 
[GCC 4.8.2]

In [14]: print(np.__version__)
1.11.2

In [15]: x = np.array([ 2, 2, 2 - 1e-15, 2 - 1e-15, 1], dtype=np.float64)

In [16]: np.histogram(x, bins='fd')
---------------------------------------------------------------------------
MemoryError                               Traceback (most recent call last)
<ipython-input-16-4ee7362b8a29> in <module>()
----> 1 np.histogram(x, bins='fd')

/home/artem/.local/lib/python2.7/site-packages/numpy/lib/function_base.pyc in histogram(a, bins, range, normed, weights, density)
    562 
    563         # Initialize empty histogram
--> 564         n = np.zeros(bins, ntype)
    565         # Pre-compute histogram scaling factor
    566         norm = bins / (mx - mn)

MemoryError: 

```

I did some debugging and figured out that in `numpy.lib.function_base._hist_bin_fd`  returns bin width 1.29852472695e-15

```
def _hist_bin_fd(x):  # x: array([ 2, 2, 2 - 1e-15, 2 - 1e-15, 1])
    iqr = np.subtract(*np.percentile(x, [75, 25]))  # iqr: 1.11022302463e-15
    return 2.0 * iqr * x.size ** (-1.0 / 3.0)  # returns 1.29852472695e-15
```

I think that in [numpy/lib/function_base.py:533](https://github.com/numpy/numpy/blob/v1.11.0/numpy/lib/function_base.py#L533) it would be good to check that we get a reasonable number of bins (may be max size of the array) or at least show a warning.
",2016-10-23 00:03:52,,histogram with Freedman Diaconis estimator crashes on small numbers,"['00 - Bug', 'component: numpy.lib']"
8161,open,chris-b1,"``` python
In [9]: np.datetime64(np.iinfo(np.int64).min + 80000000000000, 'ns')
Out[9]: numpy.datetime64('2262-04-11T22:26:03.145224192')

In [11]: np.__version__
Out[11]: '1.11.1'
```

This is same issue as https://github.com/pandas-dev/pandas/issues/14415 -  pandas is using a modified vendored version of [`datetime.c`](https://github.com/numpy/numpy/blob/c90d7c94fd2077d0beca48fa89a423da2b0bb663/numpy/core/src/multiarray/datetime.c)

Potential fix here:
https://github.com/pandas-dev/pandas/pull/14433

Assuming that's the right approach I can submit the same here.
",2016-10-16 12:09:48,,BUG: datetime64 construction can underflow,"['component: numpy._core', '54 - Needs decision', 'component: numpy.datetime64']"
8158,open,anntzer,"The doc for the `np.dtype` ctor documents its second arg as

```
align : bool, optional
Add padding to the fields to match what a C compiler would output for a similar C-struct.
Can be True only if obj is a dictionary or a comma-separated string. If a struct dtype is
being created, this also sets a sticky alignment flag isalignedstruct.
```

but in fact:

```
In [1]: np.dtype(float, 3)
Out[1]: dtype('float64')
```

i.e. `align` is ignored when not applicable.  Error'ing in such a case would help catching typos, as the user more likely intended to write

```
In [2]: np.dtype((float, 3))
Out[2]: dtype(('<f8', (3,)))
```
",2016-10-15 06:24:35,,"dtype ctor allows ""align"" kwargs even when unapplicable",['unlabeled']
8151,open,jakirkham,"If I take the minimum or maximum of complex numbers, I would expect that the magnitude is used. However, I find the real component is used instead. Here is a simple example. My expectation would be max and min are reversed in this case.

``` python
In [1]: import numpy

In [2]: a = numpy.array([1+1j, 1.2])

In [3]: numpy.abs(a)
Out[3]: array([ 1.41421356,  1.2       ])

In [4]: numpy.real(a)
Out[4]: array([ 1. ,  1.2])

In [5]: numpy.min(a)
Out[5]: (1+1j)

In [6]: numpy.max(a)
Out[6]: (1.2+0j)
```
",2016-10-12 20:03:22,,ENH: need matlab compatible min and max for complex numbers,['01 - Enhancement']
8149,open,jakirkham,"It would be pretty neat if `numpy.take` and other `take` methods could take a `slice` in the `indices` argument.
",2016-10-12 19:26:04,,ENH: `take` working with `slice`s,['unlabeled']
8139,open,mattharrigan,"This is a follow up to my stack overflow [question](http://stackoverflow.com/questions/39976383/gemm-using-numpy-einsum).  I propose adding elementwise addition and subtraction functionality to einsum.  The API would remain pretty mimimal, with the just addition of the + and - characters.  My gemm question would become `D = np.einsum(',ij,jk+,ij', alpha, A, B, beta, C)`.

The parsing/API would be that on each side of a + or -, the operations must result in an array of equal shape which are then added or subtracted elementwise.  A  very simple un-optimized implementation could parse the above as `D = np.einsum(',ij,jk', alpha, A, B) + np.einsum(',ij', beta, C)`

Many optimization opportunities exist, including most of the one mentioned in this [NEP](http://docs.scipy.org/doc/numpy/neps/deferred-ufunc-evaluation.html).  The example problem in the beginning could be `A = np.einsum('...+...+...', B, C, D)`.  It could be optimized to a single pass without intermediate arrays.  It might be easier to implement those optimizations inside einsum since it is somewhat more isolated, but I don't understand the internals nearly well enough to comment intelligently.  Maybe `D = np.einsum('i,ij->ij+ij', A[0,:], B, C)` could use fused multiply add

Even without clever optimizations, I think the additional functionality would have significant benefits, making the most powerful linear algebra tool I know of even more comprehensive.

I am not sure if there are any issues with the API I naively suggested above, but thought it was worth suggesting.  Thank you
",2016-10-11 16:05:57,,einsum elementwise addition feature request,['unlabeled']
8102,open,petered,"Theano's [tensor.flatten](http://deeplearning.net/software/theano/library/tensor/basic.html#theano.tensor._tensor_py_operators.flatten) function takes an optional `ndim` argument, specifying the number of dimensions you'd like to flatten to.

Eg, if arr has shape (3, 4, 2)
`barr = arr.flatten(2)`
Will result in barr having shape (3, 8)

It would be very nice for numpy's flatten to have the same syntax, as it would allow for some nice one liners that currently have to be split up.  For example:

```
images = get_images()  # (n_images, size_y, size_x, 3)
flattened_images = images.reshape(images.shape[0], -1)  # (n_images, size_y*size_x*3)
```

Could be simplified to 

```
flattened_images = get_images().flatten(2)  # (n_images, size_y*size_x*3)
```
",2016-09-30 12:48:39,,"""axes"" argument for reshape/flatten/ravel","['01 - Enhancement', '15 - Discussion', 'component: numpy._core']"
8099,open,sque,"## Problem

I tried to pickle and unpickle an object that contains attributes of vectorized functions that were created with `numpy.frompyfunc`. The `pickle.loads` crashed with `AttributeError: 'module' object has no attribute 'test (vectorized)'`
## How to reproduce

I tried to create the minimum example to reproduce the problem and here it is:

``` python
import numpy as np

def func(a):
    return a*2

vec_func = np.frompyfunc(func, 1, 1)
pickle.loads(pickle.dumps(vec_func))
```

This crashes with the following error:

```
AttributeError                            Traceback (most recent call last)
<ipython-input-33-b5f3d96132c6> in <module>()
      4 
      5 vec_func = np.frompyfunc(func, 1, 1)
----> 6 pickle.loads(pickle.dumps(vec_func))

/usr/lib/python2.7/pickle.pyc in loads(str)
   1386 def loads(str):
   1387     file = StringIO(str)
-> 1388     return Unpickler(file).load()
   1389 
   1390 # Doctest

/usr/lib/python2.7/pickle.pyc in load(self)
    862             while 1:
    863                 key = read(1)
--> 864                 dispatch[key](self)
    865         except _Stop, stopinst:
    866             return stopinst.value

/usr/lib/python2.7/pickle.pyc in load_reduce(self)
   1137         args = stack.pop()
   1138         func = stack[-1]
-> 1139         value = func(*args)
   1140         stack[-1] = value
   1141     dispatch[REDUCE] = load_reduce

/home/sque/workspace/project/venv/local/lib/python2.7/site-packages/numpy/core/__init__.pyc in _ufunc_reconstruct(module, name)
     69     # scipy.special.expit for instance.
     70     mod = __import__(module, fromlist=[name])
---> 71     return getattr(mod, name)
     72 
     73 def _ufunc_reduce(func):

AttributeError: 'module' object has no attribute 'func (vectorized)'
```
## Possible fix

Probably it has to do with the `__name__` that `frompyfunc` sets in the new wrapper function.
",2016-09-29 16:52:57,,Cannot pickle and unpickle vectorized functions.,"['00 - Bug', 'component: numpy.lib']"
8097,open,phelippeneveu,"Hi,

We are getting a crash in the numpy code when importing numpy after Py_Finalize() is called. If we comment out the call to Py_Finalize() the second import will be successful.

```
#include ""python2.7/Python.h""

int RunPythonScript(const char * pszScript)
{
    Py_Initialize();

    int nResult = PyRun_SimpleStringFlags(pszScript, NULL);

    Py_Finalize();

    return nResult;
}

int main(void)
{
    RunPythonScript(""import numpy"");

    printf(""First import of numpy ran successfully!\n"");

    RunPythonScript(""import numpy"");

    printf(""Second import of numpy ran successfully!\n"");

    return 0;
}
```

I compiled this example using GCC: gcc -o numpy_crash numpy_crash.cpp -lpython2.7

This simple example should not crash. My guess is that Python will destroy some global objects when Py_Finalize() is called. But these global objects are still referenced in the numpy code. You would need to register an atexit function using Py_AtExit() to reinitialize any global objects to NULL so that they are re-created on subsequent import of numpy.

This bug is most likely the same bug as #656 and #3961. But we are only running a single Python interpreter. We call Py_Finalize() every time a user runs a new Python script so that the variables they defined on the previous runs are no longer available in the Python environment.

Regards,

Phelippe Neveu
",2016-09-28 18:15:46,,Crash when importing numpy from the Python C-API after calling Py_Finilize(),"['component: numpy._core', 'Embedded']"
8089,open,2sn,"in numpy array elements of type numpy.bytes_ are (incorrectly) returned w/o trailing zero bytes (b'\00')

``` python
>>> x = b'a\00'
>>> x
b'a\x00'
>>> y = np.array(x)
>>> y
array(b'a', 
      dtype='|S2')
>>> y[()] == x
False
```

It would be more consistent to return the object that was put in, similar to python.  

The following I find even more ""unfortunate"":

``` python
>>> y == x
array(True, dtype=bool)
>>> y == b'a'
array(True, dtype=bool)
>>> y == b'a\00'
array(True, dtype=bool)
>>> y == b'a\00\00'
array(True, dtype=bool)

# whereas
>>> b'a' == b'a\00'
False
```

I would appreciate if that could be fixed such that

``` python
>>> y[()]
b'a\00'
```
",2016-09-25 01:19:26,,python incompatibility: bytes_ behaviour inconsistent with python,"['component: numpy._core', '54 - Needs decision']"
8039,open,anntzer,"Initially noticed in #8034: ""same_kind"" casting refuses to drop a minus sign when casting signed int -> unsigned int, but happily wraps around large values to negative values when casting unsigned int -> signed int:

```
In [17]: np.array(-1, np.byte).astype(np.ubyte, casting=""same_kind"")
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-17-100f7edb5d2f> in <module>()
----> 1 np.array(-1, np.byte).astype(np.ubyte, casting=""same_kind"")

TypeError: Cannot cast array from dtype('int8') to dtype('uint8') according to the rule 'same_kind'

In [18]: np.array(129, np.ubyte).astype(np.byte, casting=""same_kind"")
Out[18]: array(-127, dtype=int8)
```

(This is also affects casting of arrays: their values are specifically checked.)
I personally think the latter behavior (wraparound allowed) should be used in both cases... but at least it should be consistent :-)
",2016-09-11 05:29:30,,"Inconsistent behavior of ""same_kind"" casting wrt. signed/unsigned ints",['unlabeled']
8034,open,anntzer,"`np.int8` and friends currently ignore any kwarg passed in:

```
In [29]: np.uint8(1, foo=""bar"")
Out[29]: 1
```

which doesn't even really qualify as an annoyance, I guess.

I noticed this while hoping that something like

```
In [30]: np.int16(np.int32(-123), casting=""safe"")
Out[30]: -123
```

would be available.  (I think right now the only way to do it is an explicit call to `can_cast`?)
",2016-09-08 21:36:08,,kwargs support for dtype constructors,"['00 - Bug', 'defunct — difficulty: Intermediate', 'component: numpy.dtype']"
8003,open,tpgillam,"Consider the following example:

``` python
import numpy

x = numpy.ma.masked_array([1, 2, numpy.inf], mask=[1, 0, 0])
print(x)
print(2 * x)
print(x * 2)
print(x + x)
```

With numpy 1.11.1 (tested on windows & linux 64 bit), this gives the following output. Note that these last two ""nan""s should both be ""inf"".

```
[-- 2.0 inf]
[-- 4.0 inf]
[-- 4.0 nan]
[-- 4.0 nan]
```

Note that the same operations with a raw array (or indeed when the mask above is set to [0, 0, 0]) gives the answers I would expect:

``` python
y = x.data
print(y)
print(2 * y)
print(y * 2)
print(y)
```

```
[  1.   2.  inf]
[  2.   4.  inf]
[  2.   4.  inf]
[  1.   2.  inf]
```

If it helps, I also tested in numpy version 1.7.2, in which case the first code block also gives the wrong answer for the ""2 \* x"" case:

```
[-- 2.0 inf]
[-- 4.0 nan]
[-- 4.0 nan]
[-- 4.0 nan]
```
",2016-09-01 13:23:48,,Incorrect nans when operating on masked arrays containing inf,['component: numpy.ma']
8000,open,seberg,"When the input array is a real array, but the dtype argument is given to be complex, the nanstd and nanvar functions will give a ComplexWarning, even though the complex part is always zero. This is due to the line:

```
np.subtract(arr, avg, out=arr, casting='unsafe')
```

where `arr` is the original type, but `avg` the new (complex) type, which gives the warning.
",2016-08-31 19:20:23,,BUG: nanstd/nanvar gives spurious ComplexWarning when input is real but dtype is complex,['unlabeled']
7993,open,anntzer,"Currently, cumsum() operates on flattened arrays.  This behavior is not that useful, as even in its absence it could trivially be obtained by calling ravel() yourself first.
Perhaps a more useful approach would be to return a summed area table (https://en.wikipedia.org/wiki/Summed_area_table), i.e. compute for each position the sum of entries with no indices greater than the current entry, which is AFAICT trickier to compute in a ""vectorized"" manner.
I guess a reasonable transition path would be to start by emitting a warning whenever cumsum() is called on a >=2d array with no axis kwarg.
",2016-08-30 07:20:40,,cumsum() could (should?) return a summed area table for higher dimensional inputs,['unlabeled']
7991,open,jjhelmus,"In-place addition and other in-place operations on masked arrays where the mask is not nomask results in an ufunc casting rule check which is not performed when the operation is performed normally (not in-place) or on a normal ndarray.

Example

``` Python
>>> import numpy as np
>>> x = np.ma.array([0, 1, 2], dtype='uint8', mask=False)
>>> x += 4
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/jhelmus/anaconda/lib/python3.5/site-packages/numpy/ma/core.py"", line 4025, in __iadd__
    getdata(other)))
TypeError: Cannot cast ufunc add output from dtype('int64') to dtype('uint8') with casting rule 'same_kind'
>>> # Not in place
>>> x = x + 4
>>> # on a normal ndarray
>>> x = np.array([0, 1, 2], dtype='uint8')
>>> x += 4
```
",2016-08-29 17:59:19,,In-place addition (and other operations) of masked array invoke extra casting check,['component: numpy.ma']
7989,open,dietmar,"I would like to `numpy.load` an ndarray from a file within a tar archive, without extracting to disk, in Python3.

``` python
import os
import numpy
import tarfile

# create a test tar archive
a = numpy.random.rand(10, 4)
numpy.save('a.npy', a)
with open('foo.txt', 'w') as f:
    f.write('hey\nho\n')
t = tarfile.open('abc.tar', 'w')
t.add('a.npy')
t.add('foo.txt')
t.close()
del t, a
os.remove('a.npy')
os.remove('foo.txt')

# -----

t = tarfile.open('abc.tar', 'r')
a = numpy.load(t.extractfile('a.npy'))
```

The above code snippet works fine in Python 2.7.12 with numpy 1.11.1, but fails in Python 3.5.2 with numpy 1.11.1 (Debian Linux):

```
Traceback (most recent call last):
  File ""loadtest.py"", line 21, in <module>
    a = numpy.load(t.extractfile('a.npy'))
  File ""[...]/lib/python3.5/site-packages/numpy/lib/npyio.py"", line 406, in load
    pickle_kwargs=pickle_kwargs)
  File ""[...]/lib/python3.5/site-packages/numpy/lib/format.py"", line 648, in read_array
    array = numpy.fromfile(fp, dtype=dtype, count=count)
AttributeError: '_FileInFile' object has no attribute 'fileno'
```

Am I doing something wrong? [The numpy docs](http://docs.scipy.org/doc/numpy/reference/generated/numpy.load.html) say that I can pass a file-like object to `numpy.load`, which must support the `seek()` and `read()` methods, which the `io.BufferedReader` I receive from [TarFile.extractfile()](https://docs.python.org/3/library/tarfile.html#tarfile.TarFile.extractfile) does.
",2016-08-29 16:15:39,,numpy.load cannot read from tar archive in python3 ('_FileInFile' object has no attribute 'fileno'),['unlabeled']
7982,open,2sn,"``` python3
In [1]: np.__version__
Out[1]: '1.11.1'

In [2]: x = np.ndarray((2,3))

In [3]: x.flat[()] = 1
Segmentation fault (core dumped)
```
",2016-08-28 03:33:55,,BUG: indexing np.flat with empty tuple causes segfault,"['00 - Bug', 'component: numpy._core', 'Patch', 'defunct — difficulty: Intermediate']"
7969,open,isacarnekvist,"Originally posted [here](https://github.com/pydata/pandas/issues/14042), but looks like a numpy issue?
#### Code Sample, a copy-pastable example if possible

```
import pandas as pd
import numpy as np
indices = pd.date_range('2016-01-01 00:00:00+0200', freq='S', periods=10)
np.split(indices, indices_or_sections=[])
Out:
[DatetimeIndex(['2015-12-31 20:00:00+02:00', '2015-12-31 20:00:01+02:00',
               '2015-12-31 20:00:02+02:00', '2015-12-31 20:00:03+02:00',
               '2015-12-31 20:00:04+02:00', '2015-12-31 20:00:05+02:00',
               '2015-12-31 20:00:06+02:00', '2015-12-31 20:00:07+02:00',
               '2015-12-31 20:00:08+02:00', '2015-12-31 20:00:09+02:00'],
              dtype='datetime64[ns, pytz.FixedOffset(120)]', freq='S')]
```
#### Expected Output

```
[DatetimeIndex(['2016-01-01 00:00:00+02:00', '2016-01-01 00:00:01+02:00',
               '2016-01-01 00:00:02+02:00', '2016-01-01 00:00:03+02:00',
               '2016-01-01 00:00:04+02:00', '2016-01-01 00:00:05+02:00',
               '2016-01-01 00:00:06+02:00', '2016-01-01 00:00:07+02:00',
               '2016-01-01 00:00:08+02:00', '2016-01-01 00:00:09+02:00'],
              dtype='datetime64[ns, pytz.FixedOffset(120)]', freq='S')]
```
",2016-08-24 13:53:35,,numpy.split on pandas dataframe with non-utc timezone,['unlabeled']
7967,open,SylvainCorlay,"Here is a list of surprising behaviors of numpy. There are certainly some deep reasons for most of them.
* [x] 1. Initialize a 0-D array with None
   
   ``` python
   >>> np.array(1) ==  1
   True
   >>> np.array(None) ==  None
   False  # wat?
   ```
   
   Regardless on whether comparing with None is good or not, this is surprising to the user that the behaviors differ.
   Besides, the warning on comparison with `None` should probably not be displayed on 0-D array.
   
   I came across this where the value passed were in variables...

* [ ] 2. Ufuncs return types for 0-D arrays
   
   ``` python
   >>> np.exp(1).__class__
   numpy.float64  # ok: scalar input scalar output
   
   >>> np.exp(np.array([1])).__class__
   numpy.ndarray  # ok: array input, array output
   
   >>> np.exp(np.array(1)).__class__
   numpy.float64  # wat? 0-D array input, scalar output
   ```
   
   I think that the return type of ufuncs should be arrays in case of array-like objects.

* [ ] 3. repr of empty arrays
   
   ``` python
   >>> np.ndarray(shape=(0, 0))
   array([], shape=(0, 0), dtype=float64)  # ok
   
   >>> np.ndarray(shape=(1, 1))
   array([[0.]])  # ok 
   
   >>> np.ndarray(shape=(1, 0))
   array([], shape=(1, 0), dtype=float64)  # wat? 
   ```
   
   why is the repr no equal to `array([[]], shape=(1, 0), dtype=float64)` in this case?
   
* [x] 3b. (duplicate of #9583) Similarly
   
   ``` python
   >>> bool(np.ndarray(shape=(0, 0)))
   False  # ok
   
   >>> np.ndarray(shape=(1, 0))
   False  # wat?
   ```
   
   why is the latter a falsy value? it has a shape of `(1, 0)`. Note: I do expect an array of shape `(0, 1)` to be falsy because an empty list _is_.

* [ ] 4. ndarray default constructor
   
   This one is more of a general question: I generally expect value-semantic objects to have an argument-less default constructor returning a falsy value that is semantically zero.
   
   Could `np.ndarray()` return a 0-D array holding the `0.0` value instead of always requiring a shape?
* [ ] 5. `__eq__` / `equal` / `array_equal`
   
   I know that this would be a major backward compatibility breakage, but it seems that `==` corresponds to `equal` (except for `None`, which generates a warning, and will soon correspond to `equal`).
   
   It is a very unusual thing for a python object's `__eq__` to not return a boolean and downstream libraries special case for numpy array everywhere because it does not follow the expected semantic and is a very common type. Having `__eq__` correspond to `array_equal` would be much cleaner in my opinion.
* [ ] 6. reshape (duplicate of #7964)
   
   ``` python
   >>>np.reshape(int(0), ())
   array(0)
   
   >>> np.reshape(np.int64(0), ())
   0
   ```
   
   The document specifies that the return value should be an array.
",2016-08-24 10:27:49,,Surprising behaviors of NumPy,['unlabeled']
7964,open,goretkin,"```python
In [2]: np.reshape(int(0), ())
Out[2]: array(0)

In [3]: np.reshape(np.int64(0), ())
Out[3]: 0
```

I would prefer the first behavior for both. In fact, I don't know how to get an array(0) from an np.int64(0) right now.

`np.__version__` is `""1.11.0""`
",2016-08-23 04:10:56,,BUG: np.reshape sometimes returns scalars,"['00 - Bug', 'component: numpy._core']"
7950,open,2sn,"consider

``` python3
>>> x = [b'abc']*12
>>> x[2] = b'1234'
>>> a = np.array(x, dtype=np.bytes_)
>>> a[0] += b'F'
>>> a[0]
b'abcF'
```

but if I try to do array operations of any kind I get an ufuc error, e.g.,

``` python3
>>> a += b'F'
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-133-280710bc9d48> in <module>()
----> 1 a += b'F'

TypeError: ufunc 'add' did not contain a loop with signature matching types dtype('S4') dtype('S4') dtype('S4')
```

I would have expected same behavior as for single element.  Can such ufunc matching for dynamic types like `S4` be added?
",2016-08-19 21:26:43,,"feature request: adding (concatenating) numpy ""strings"" (unicode_, bytes_)","['01 - Enhancement', 'component: numpy.ufunc']"
7945,open,gerritholl,"[The largest year number allowed in a date or datetime object. MAXYEAR is 9999.](https://docs.python.org/3.5/library/datetime.html#datetime.MAXYEAR)

The range for `datetime64` may be larger, depending on the type.

When converting a `datetime64` that is out of range for `datetime.datetime`, we get an integer back:

```
In [21]: future = datetime64(10**12, 's')                                                 

In [22]: print(future)
33658-09-27T01:46:40

In [23]: print(future.astype(datetime64))
33658-09-27T01:46:40

In [24]: print(future.astype(datetime.datetime))                                          
1000000000000
```

Converting `datetime64(NaT)` returns None:

```
In [40]: print(datetime64('NaT').astype(datetime.datetime))
None
```

In both cases, this should probably raise some kind of warning or error, perhaps in agreement with numpys `errstate` (`geterr`/`seterr`) for `invalid`.  
",2016-08-18 11:31:08,,datetime64.astype(datetime.datetime) returns int for dates in the far future,"['54 - Needs decision', 'component: numpy.datetime64']"
7906,open,stuartarchibald,"I'm unsure of whether this is the desired behaviour (in which case perhaps it ought be documented), or if this is just a bug. To me, I think it is a bug.
Running the following numerical rank computations (I used numpy 1.11):

```python
import numpy as np
from numpy.linalg import matrix_rank as rank

# rank 0 vector
print(rank(np.array([0., 0., 0.])))

# rank 1 vector
print(rank(np.array([0., 0., 1.e-14])))

# rank 0 vector at specified tolerance
a = np.array([0., 0., 1.e-14])
print(rank(a, 1e-13))
# singular value is
print(np.linalg.svd(a[np.newaxis])[1])

# rank 2 matrix
a = np.array([[2., 0.],[0., 1e-14]])
print(rank(a))

# rank 1 matrix at specified tolerance
print(rank(a, 1e-13))
# singular values are
print(np.linalg.svd(a)[1])
```

produces:

```
0
1
1
[  1.00000000e-14]
2
1
[  2.00000000e+00   1.00000000e-14]
```

Result 1 is correct, the zero vector has rank 0.
Result 2 is correct, a non-zero vector has rank 1 (the default threshold would be `1e-14*~2.22e-16*3 ~= 6e-30`). 
Result 3 I think is incorrect, it is 1 when it ought perhaps be a 0 given result 4 is the singular value of the vector which is clearly below the tolerance `1e-13`.
The last 3 results demonstrate that this does not impact matrices.

I'd be happy to document or fix. I believe this is the line that causes it:
https://github.com/numpy/numpy/blob/master/numpy/linalg/linalg.py#L1541-L1542
",2016-08-05 13:19:32,,`np.linalg.matrix_rank` ignores `tol` arg in vector context,"['00 - Bug', 'component: numpy.linalg']"
7874,open,anntzer,"Currently, numpy relies on the naive binary-search-over-the-cumulative-sum approach to generate discrete samples with unequal specified probabilites, with a complexity of O(log n)-per-sample (n=number of possible outputs).
There's a O(1)-per-sample algorithm available (with O(n) initial work, which is needed in both cases), known as the alias or Walker method (https://en.wikipedia.org/wiki/Alias_method).

But any progress will require sorting out the mess that's RNG back-compatibility first...
",2016-07-27 08:47:10,,Alias/Walker method for faster generation of discrete samples with unequal probabilities,['component: numpy.random']
7852,open,JJL-ITU,"I just mentioned this issue on the Numpy mailing list. So far, there is agreement that what I am reporting here is unwanted behavior, a bug and not a feature.

The following minimal code has been tried on Python 3.4 and 3.5, with Numpy 1.8 and Numpy 1.11, respectively.  I want to temporarily change the way that a Numpy array is printed, then change it back to the defaults.

> import numpy as np
> 
> a = np.random.random((4,3))
> print(a, ""\n"")
> opt = np.get_printoptions()
> np.set_printoptions(precision = 3, suppress = True)
> print(a, ""\n"")
> np.set_printoptions(opt)
> print(a, ""\n\nDone.\n"")

Here is the traceback:

> Traceback (most recent call last):
>   File ""set_printoptions test.py"", line 11, in <module>
>     print(a, ""\n\nDone.\n"")
>   File ""/usr/lib/python3/dist-packages/numpy/core/numeric.py"", line 1615, in array_str
>     return array2string(a, max_line_width, precision, suppress_small, ' ', """", str)
>   File ""/usr/lib/python3/dist-packages/numpy/core/arrayprint.py"", line 454, in array2string
>     separator, prefix, formatter=formatter)
>   File ""/usr/lib/python3/dist-packages/numpy/core/arrayprint.py"", line 328, in _array2string
>     _summaryEdgeItems, summary_insert)[:-1]
>   File ""/usr/lib/python3/dist-packages/numpy/core/arrayprint.py"", line 523, in _formatArray
>     summary_insert)
>   File ""/usr/lib/python3/dist-packages/numpy/core/arrayprint.py"", line 497, in _formatArray
>     word = format_function(a[-i]) + separator
>   File ""/usr/lib/python3/dist-packages/numpy/core/arrayprint.py"", line 616, in __call__
>     s = self.format % x
> AttributeError: 'FloatFormat' object has no attribute 'format'

My bug is on the second to last line, which should read:

`np.set_printoptions(**opt)`

This would unpack the dictionary, opt, which was retrieved earlier in the program with the call to get_printoptions().

However: no one has identified a legitimate reason that set_printoptions() would ever make use of a packed dictionary as a single argument. I propose that a TypeError be raised in this case.  Currently, the Numpy formatter is silently destroyed by supplying a bad argument to set_printoptions().  You don't discover this fact until the next time you attempt to print an array, when an AttributeError is raised.
",2016-07-19 22:32:13,,"numpy.set_printoptions accepts a dict as argument, then silently fails",['unlabeled']
7845,open,mnmelo,"When no normalization is performed the `histogram*d` functions should return an `int` array, just as `histogram` does. A `float` array is returned instead.

The docs of `histogram2d` don't mention anything about return type, but also don't follow the style of the docs of `histogram`, in which it is stated that semantics depend on normalization (I guess this could be written more explicitly). `histogramdd` does follow the doc style of `histogram`, but also returns a `float` array, like `histogram2d` does.

Running NumPy 1.11.0 on Ubuntu 14.04
Sample code:

``` python
arr = np.random.random((20))

## 1D
np.histogram(arr)[0]
Out: array([5, 1, 1, 1, 2, 1, 2, 1, 2, 4])

## 2D
np.histogram2d(arr,arr)[0]
Out: 
array([[ 5.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.]])

## N-D
np.histogramdd(arr[:,None])[0]
Out: array([ 5.,  1.,  1.,  1.,  2.,  1.,  2.,  1.,  2.,  4.])
#
np.histogramdd(np.column_stack((arr, arr)))[0]
Out: 
array([[ 5.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.]])
```
",2016-07-18 17:17:32,,histogram2d and histogramdd always return a float array,"['00 - Bug', 'component: numpy.lib', '54 - Needs decision']"
7831,open,maxnoe,"As for as I know, `fp.flush()` is only ever needed for write operations.

So why does `numpy.fromfile` need a `fp.flush`?

``` python
import numpy as np


class MyFile:
    def read(self):
        return b'\x00\x01\x02\x03'

fp = MyFile()
print(np.fromfile(fp, 'u2'))
```

results in:

```
AttributeError: 'MyFile' object has no attribute 'flush'
```

Adding a  method like this:

```
def flush(self):
    pass
```

Results in 

```
TypeError: argument must be an int, or have a fileno() method.
```

What do I have to provide to make my own file-like object work? Why is a simple `read()` not enough?
",2016-07-12 17:08:41,,Why does numpy fromfile need flush and fileno?,['unlabeled']
7829,open,Cerno-b,"It seems to me that the result of vstack does not inherit Endianness from the function parameters correctly.

Example:

```
a = np.zeros((3,3), dtype='>i2')
print a.dtype
b = np.vstack((a,a))
print b.dtype
```

The first print gives the correct '>i2' but the second one gives 'uint16' which should mean Little Endinness, since I'm running a 64bit Windows 7 machine.

The problem arose when trying to load and concatenate binary 16 bit pgm images. Pgm stores images in Big Endian, and my routine can read and store the images correctly. It is only when I try to concatenate two images with vstack that the resulting image is corrupted since the byte order gets switched.
",2016-07-12 15:05:06,,np.concatenate loses endianness / byte order,['component: numpy._core']
7811,open,dmopalmer,"lib.recfunctions.append_fields is too slow to be usable for any reasonably-sized (~million row) record array.

This is discussed in http://stackoverflow.com/questions/5355744/numpy-joining-structured-arrays which gives solutions two orders of magnitude faster.

Including this speed-up in the library would be useful.

How slow you might ask?

For joining a couple of two-field records, 1M rows takes more than 20 seconds.

```
import numpy as np
from numpy.lib.recfunctions import append_fields

nrows = 1000000

a1 = np.array(np.arange(nrows), dtype=[('x', int), ('y', int)])
a2 = np.array(np.arange(nrows), dtype=[('w', int), ('z', int)])

%timeit append_fields(a1, names=a2.dtype.names, data=[a2[n] for n in a2.dtype.names])
a3 = append_fields(a1, names=a2.dtype.names, data=[a2[n] for n in a2.dtype.names])
## -- End pasted text --

1 loop, best of 3: 22.4 s per loop
```
",2016-07-08 00:11:35,,append_fields is too slow to be usable,['unlabeled']
7797,open,aldanor,"Here's converting an array to a memoryview and back (Python 3) where itemsize equals the offsets of the last item plus its size:

``` python
>>> d1 = np.dtype({'formats': ['u1'], 'offsets': [0], 'names': ['x']})
>>> a1 = np.empty(0, d1)
>>> memoryview(a1).format
'T{B:x:}'
>>> memoryview(a1).itemsize
1
>>> np.array(memoryview(a1))
array([], 
      dtype=[('x', 'u1')])
```

If we try to do the same where itemsize is bigger, it fails:

``` python
>>> d2 = np.dtype({'formats': ['u1'], 'offsets': [0], 'names': ['x'], 'itemsize': 4})
>>> d2.descr
[('x', '|u1'), ('', '|V3')]  # the trailing padding is there
>>> a2 = np.empty(0, d2)
>>> memoryview(a2).format
'T{B:x:}'  # shouldn't this be 'T{B:x:3x}'?
>>> memoryview(a2).itemsize
4
# so far so good...
# however:
>>> np.array(memoryview(a2))
RuntimeWarning: Item size computed from the PEP 3118 buffer format string does not match the actual item size.
NotImplementedError: memoryview: unsupported format T{B:x:}
```

This seems quite wrong.

Looking at the code where it fails, `_dtype_from_pep3118` only accepts a format string and not the itemsize, so the generated format string is probably wrong and should explicitly contain the trailing bytes?

May be somewhat related: #6361
",2016-07-02 16:31:58,,Array from memoryview fails if there's trailing padding,"['00 - Bug', 'Project', '64 - Good Idea']"
7786,open,jnturton,"My tests follow.  Is this ultimately caused by my BLAS installation (OpenBLAS 0.2.12-1 amd64 on Debian)?

``` python
import numba
import numpy
```

``` python
numpy.version.full_version
```

'1.11.1'

``` python
x = numpy.random.rand(100000000)
%timeit numpy.linalg.norm(x,3)
```

1 loop, best of 3: 8.29 s per loop

``` python
@numba.jit(nopython=True)
def L3Norm(x):
    z = 0.0
    for i in range(len(x)):
        z += numpy.abs(x[i]) ** 3
    return z ** (1.0/3)
%timeit L3Norm(x)
```

10 loops, best of 3: 107 ms per loop

``` python
%load_ext fortranmagic
```

``` python
%%fortran
subroutine fortran_L3Norm(x, z)
    real*8, intent(in) :: x(:)
    real*8, intent(out) :: z

    z = sum(abs(x ** 3)) ** (1.0/3)
end subroutine
```

``` python
%timeit fortran_l3norm(x)
```

10 loops, best of 3: 112 ms per loop

``` python
print(""numpy = {}, numba'd function = {}, fortran via f2py = {}"".format(numpy.linalg.norm(x,3),L3Norm(x), fortran_l3norm(x)))
```

numpy = 292.41893219483336, numba'd function = 292.4189321947911, fortran via f2py = 292.4189816788551
",2016-06-29 16:06:26,,"numpy.linalg.norm(x,3) seems unduly slow",['component: numpy.linalg']
7784,open,saullocastro,"I would like to add a tolerance parameter to `np.in1d` to support robust float comparisons.

Original function:
https://github.com/numpy/numpy/blob/master/numpy/lib/arraysetops.py#L305

Here is one example that would require an error-tolerant comparison:

```
a = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7 ], dtype=np.float32)
b = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7 ], dtype=np.float64)
print(np.in1d(a.astype(np.float64), b))
#array([False, False, False, False,  True, False, False], dtype=bool)
```

What do you guys think of this suggestion?
",2016-06-28 23:16:00,,np.in1d support tolerance parameter for float comparisons,['unlabeled']
7781,open,MorBilly,"Here is the issue I run into during my work
The problem with the mask when it is not set

```python
>>> a =np.arange(6,dtype=np.float64).reshape((2,3))
>>> a
array([[ 0.,  1.,  2.],
       [ 3.,  4.,  5.]])
>>> am = np.ma.masked_array(a)
>>> am
masked_array(data =
 [[ 0.  1.  2.]
 [ 3.  4.  5.]],
             mask =
 False,
       fill_value = 1e+20)

>>> am[0][1]=np.ma.masked
>>> am
masked_array(data =
 [[ 0.  1.  2.]
 [ 3.  4.  5.]],
             mask =
 False,
       fill_value = 1e+20)

###--------- doesn't work----------------

>>> am[0,1]=np.ma.masked
>>> am
masked_array(data =
 [[0.0 -- 2.0]
 [3.0 4.0 5.0]],
             mask =
 [[False  True False]
 [False False False]],
       fill_value = 1e+20)

###-------this way it works---------

>>> am[1][1]=np.ma.masked
>>> am
masked_array(data =
 [[0.0 -- 2.0]
 [3.0 -- 5.0]],
             mask =
 [[False  True False]
 [False  True False]],
       fill_value = 1e+20)

###--------now it surprisingly works again--

Linux  3.19.8-100.fc20.x86_64 
Python 2.7.5
>>> np.__version__
'1.8.2'
```

I know my system is not up to date but I asked a friend who has 
and he confirms that issue exists
",2016-06-27 13:25:42,,BUG: Setting the mask on a view with mask=nomask does not propagate to the owner,"['00 - Bug', 'component: numpy.ma']"
7780,open,florianjacob,"I was fitting some polynomials using [numpy.polyfit](http://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html) with `cov=True` to get the covariance of the fitted parameters.

I wanted to switch to the `numpy.polynomial` module to be able to use other polynom parameterizations and the nice convenience classes. The fit functions in that module provide no option to get the covariance matrix, though:
- [numpy.polynomial.legendre.Legendre.fit](http://docs.scipy.org/doc/numpy/reference/generated/numpy.polynomial.legendre.Legendre.fit.html#numpy.polynomial.legendre.Legendre.fit)
- [numpy.polynomial.polynomial.Polynomial.fit](http://docs.scipy.org/doc/numpy/reference/generated/numpy.polynomial.polynomial.Polynomial.fit.html#numpy.polynomial.polynomial.Polynomial.fit)

Is there any option to get the covariance matrix from those analytical fits?

I ended up using [numpy.polynomial.polynomial.polyval](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polynomial.polynomial.polyval.html#numpy.polynomial.polynomial.polyval) together with the numerical fit provided by [scipy.optimize.curve_fit](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html#scipy.optimize.curve_fit).
",2016-06-27 09:43:42,,Determine covariance matrix of fitted parameters with the numpy.polynomial module,['component: numpy.polynomial']
7730,open,markrichardson,"Ubuntu 14.04
Python 2.7.11

``` python
In [1]: import numpy as np

In [2]: np.__version__
Out[2]: '1.10.4'

In [3]: v = 4 - 0.1

In [4]: np.array([v])
Out[4]: array([ 3.9])

In [5]: a = np.array([1]); a[0] = v; a
Out[5]: array([3])
```
",2016-06-11 13:59:52,,Deprecate/remove silent truncation on assignment of floats into an integer array,['unlabeled']
7713,open,gerritholl,"An [ancient ticket from 2007](https://github.com/numpy/numpy/issues/1103) notes that `fromfile` does not work with files opened with gzip.  Instead, it will interpret the literal gzip data.  This is due to the fundamental way `fromfile` works and the resolution of the bug was essentially wontfix.

Secondly, it should be made clear in the documentation that you cannot pass a `GzipFile`.  Some file-like objects trigger `io.UnsupportedOperation`.

It may save bugs and headaches if `fromfile` were to raise an Exception if it is passed a file-object that is not a low-level file.
",2016-06-07 14:19:57,,`fromfile` should raise an exception if not passed a “low-level file”,['unlabeled']
7700,open,jseabold,"Python 3.5.1 NumPy 1.11.0? I would have expected an error.

```
>>> np.searchsorted(np.arange(50), 'b')
50

>>> np.searchsorted(np.arange(50), '!')
0
```
",2016-06-01 19:48:16,,searchsorted unexpected type promotion,"['00 - Bug', 'component: numpy.dtype']"
7647,open,NeilGirdhar,"Boost does it; could we just depend on Boost's implementation?  http://www.boost.org/doc/libs/1_58_0/libs/multiprecision/doc/html/boost_multiprecision/tut/floats/float128.html
",2016-05-18 02:49:11,,Implement the 128-bit floating point number type,['unlabeled']
7644,open,mrclary,"With numpy version <= 1.9.2 polyfit behaved correctly, as follows:

```
In [1]: np.polyfit([1, 10], [np.nan, np.nan], 1)
Out[1]: array([ nan,  nan])
```

But for version >=1.9.3, polyfit raises the following error:

```
In [1]: np.polyfit([1, 10], [np.nan, np.nan], 1)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-3-bf0d5d071f08> in <module>()
----> 1 np.polyfit([1, 10], [np.nan, np.nan], 1)

/Users/rclary/anaconda/lib/python2.7/site-packages/numpy/lib/polynomial.pyc in polyfit(x, y, deg, rcond, full, w, cov)
    580     scale = NX.sqrt((lhs*lhs).sum(axis=0))
    581     lhs /= scale
--> 582     c, resids, rank, s = lstsq(lhs, rhs, rcond)
    583     c = (c.T/scale).T  # broadcast scale coefficients
    584 

/Users/rclary/anaconda/lib/python2.7/site-packages/numpy/linalg/linalg.pyc in lstsq(a, b, rcond)
   1865         work = zeros((lwork,), t)
   1866         results = lapack_routine(m, n, n_rhs, a, m, bstar, ldb, s, rcond,
-> 1867                                  0, work, lwork, iwork, 0)
   1868     if results['info'] > 0:
   1869         raise LinAlgError('SVD did not converge in Linear Least Squares')

ValueError: On entry to DGELSD parameter number 6 had an illegal value
```
",2016-05-16 17:42:13,,polyfit should return nan values on nan input values,['unlabeled']
7623,open,abalkin,"Consider two masked arrays:

```
In [40]: a = np.ma.array([True] * 3, mask=[False] * 3)

In [41]: b = np.ma.array([True] * 3)
```

The only difference is that one has an all-false mask and the other has `mask=nomask`.  However, `ma.add.reduce` returns different values:

```
In [42]: np.ma.add.reduce(a)
Out[42]: True

In [43]: np.ma.add.reduce(b)
Out[43]: 3
```
",2016-05-11 17:55:29,,Odd behavior of ma.add.reduce on boolean arrays ,"['00 - Bug', 'component: numpy._core', 'component: numpy.ma']"
7619,open,shoyer,"This is obviously not useful:

```
In [4]: np.array(np.datetime64('2000-01-01', 'us')).item()
Out[4]: datetime.datetime(2000, 1, 1, 0, 0)

In [5]: np.array(np.datetime64('2000-01-01', 's')).item()
Out[5]: datetime.datetime(2000, 1, 1, 0, 0)

In [6]: np.array(np.datetime64('2000-01-01', 'ns')).item()
Out[6]: 946684800000000000
```

The underlying issue is that not all datetime64 types can be represented in `datetime.datetime` objects, which have fixed us precision.

We should either:
1. Raise `TypeError` in all cases when calling `.item()` on a datetime64[ns] array.
2. Convert to `datetime.datetime` if it can be done safely, raise `ValueError` otherwise.

My preference is for 2.
",2016-05-11 05:11:45,,BUG: .item() on a 0-dimensional datetime64[ns] array yields an integer,"['00 - Bug', 'component: numpy._core', 'component: numpy.datetime64']"
7615,open,RomanSteinberg,"I use the following code to widen masks (boolean 1D numpy arrays).
`mask = np.hstack([[False] * start, absent, [False]*rest])`
When start and rest are equal to zero, I've got an error, because mask becomes floating point 1D array. Generally speaking, when someone writes code like this:
`bool_a = np.hstack([bool_b, bool_c])`
he can get an error because semantics of stacking zero-length arrays with not zero-length assumes only floating point arrays in return. Actually, I haven't checked this code with arrays of type differ from bool and float. I think it is a bug.
",2016-05-10 14:27:09,,hstack and zero-length arrays,"['01 - Enhancement', '23 - Wish List']"
7594,open,frejanordsiek,"`numpy.char.encode` and `numpy.char.decode` should effectively be inverses of each other if given the same encoding. However, I have found this not to be the case. It seems that truncating nulls (`'\x00'`) from the end of the individual elements of bytes ndarrays when accessing individual elements is causing the problem.

For reference, I am working with numpy 1.11.0 and Python 3.4.2 and 2.7.9 on a 64-bit (64-bit python environments as well) little endian machine (Intel i5) running Debian. Though, I think that the same result might happen on a big endian machine, but I lack a machine or virtual machine to test it (my attempts at getting such a virtual machine running have not worked).

If I take a unicode ndarray, run it through `numpy.char.encode` with UTF-32BE or UTF-16BE encoding and then run the result through `numpy.char.decode` with the same encoding, I get the original array back. This is the following code in Python 2.7 and Python 3.4. The example here is using UTF-16BE

``` python
import numpy as np

encoding = 'UTF-16BE'

a = np.array([[u'abc', u'012', u'ABC\U00010437'], [u'W', u'Q', u'Z']], dtype='U4')
b = np.char.encode(a, encoding)

np.char.decode(b, encoding)
```

It works, giving the following output

Python 3.4

``` python
array([['abc', '012', 'ABC𐐷'],
       ['W', 'Q', 'Z']], 
      dtype='<U4')
```

Python 2.7

``` python
array([[u'abc', u'012', u'ABC\U00010437'],
       [u'W', u'Q', u'Z']], 
      dtype='<U4')
```

But, if I change the encoding to little endian (`'UTF-32LE'` or`'UTF-16LE'``), I get the following errors instead

Python 3.4

```
UnicodeDecodeError                        Traceback (most recent call last)
<ipython-input-1-b92715410044> in <module>()
      6 b = np.char.encode(a, codec)
      7 
----> 8 np.char.decode(b, codec)

/home/user/.local/lib/python3.4/site-packages/numpy/core/defchararray.py in decode(a, encoding, errors)
    503     """"""
    504     return _to_string_or_unicode_array(
--> 505         _vec_string(a, object_, 'decode', _clean_args(encoding, errors)))
    506 
    507 

/usr/lib/python3.4/encodings/utf_16_le.py in decode(input, errors)
     14 
     15 def decode(input, errors='strict'):
---> 16     return codecs.utf_16_le_decode(input, errors, True)
     17 
     18 class IncrementalEncoder(codecs.IncrementalEncoder):

UnicodeDecodeError: 'utf-16-le' codec can't decode byte 0x63 in position 4: truncated data
```

Python 2.7

```
UnicodeDecodeError                        Traceback (most recent call last)
<ipython-input-1-b92715410044> in <module>()
      6 b = np.char.encode(a, codec)
      7 
----> 8 np.char.decode(b, codec)

/home/user/.local/lib/python2.7/site-packages/numpy/core/defchararray.pyc in decode(a, encoding, errors)
    503     """"""
    504     return _to_string_or_unicode_array(
--> 505         _vec_string(a, object_, 'decode', _clean_args(encoding, errors)))
    506 
    507 

/usr/lib/python2.7/encodings/utf_16_le.pyc in decode(input, errors)
     14 
     15 def decode(input, errors='strict'):
---> 16     return codecs.utf_16_le_decode(input, errors, True)
     17 
     18 class IncrementalEncoder(codecs.IncrementalEncoder):

UnicodeDecodeError: 'utf16' codec can't decode byte 0x63 in position 4: truncated data
```

If I use the little endian encoding (`'UTF-32LE'` or `'UTF-16LE'`) and look at `a` and `b` closer, the reason for the error becomes more apparent. The dtypes of `a` and `b` are `'<U4'` and `'S10'` respectively. But if I look at the dtypes of element `[1, 0]`, I get `'<U1'` and `'S1'` respectively.

The trailing null characters (`'\x00'`) are dropped when accessing the individual elements. When trying decode from `'UTF-16LE'` or `'UTF-16LE'`, `numpy.char.decode` is grabbing data with an odd number of bytes (dropping the required trailing nulls) and trying to decode it, which can't work because the encoding works with quads or pairs of bytes (thus a number divisible by 4 or 2).

This indicates that neither `numpy.char.decode` nor the decode function that `numpy.char.decode` passes off to elementwise is padding the ends of the individual string elements with the required null bytes.

This only shows up in little endian encodings since that results in trailing nulls. Big endian encodings don't have that problem.
",2016-05-01 07:11:09,,Roundtripping error for unicode ndarrays with char.encode and char.decode with UTF-32LE and UTF-16LE encodings but not their big endian versions,"['00 - Bug', 'component: numpy._core']"
7588,open,eric-wieser,"``` python
>>> a = np.ma.masked_array([1, 2], [0, 1])
>>> np.isscalar(a[0])
True
>>> np.isscalar(a[1])
False
```

This looks undesirable
",2016-04-29 17:28:43,,np.ma.masked is not a scalar,['component: numpy.ma']
7577,open,ZedThree,"When trying to call python functions from fortran code, functions with a complex return value cause a segfault. This doesn't happen for subroutines which have a complex `intent(out)` argument, or functions which take a complex argument and return a `real`.

I lay out why this happens, and two workarounds, in [this answer on SO](http://stackoverflow.com/a/36869269/2043465), but basically there's an (undocumented?) preprocessor macro which is not defined, `F2PY_CB_RETURNCOMPLEX`, which results in the wrong function definition for functions with a complex return value. The only references to this macro in the f2py code seems to just be in constructing the Cython interface code, I think.

MVCE below:

``` fortran
! mvce.f90
module mvce

  implicit none

contains

  function foo(y, sub_complex, func_real, func_complex) result(n)
    implicit none
    complex, intent(in) :: y
    complex :: n

    interface
       subroutine sub_complex(f, z)
         complex, intent(out) :: f
         complex, intent(in) :: z
       end subroutine sub_complex

       function func_real(z) result(f)
         real :: f
         complex, intent(in) :: z
       end function func_real

       function func_complex(z) result(f)
         complex :: f
         complex, intent(in) :: z
       end function func_complex
    end interface

    print*, ""calling sub_complex""
    call sub_complex(n, y)
    print*, n
    print*, ""calling func_real""
    n = func_real(y)
    print*, n
    print*, ""calling func_complex""
    ! segfault here
    n = func_complex(y)
    print*, n
  end function foo

end module mvce
```

Running f2py like:

```
f2py -m mvce -h mvce.pyf mvce.f90  # In order to check the interface is actually correct
f2py -c --debug --build-dir build mvce.pyf mvce.f90
```

Then from python:

``` python
In [1]: import mvce
In [2]: def foo(z): return z*z
In [3]: mvce.mvce.foo(1.0j, foo, foo, foo)
 calling sub_complex
 ( -1.00000000    ,  0.00000000    )
 calling func_real
 ( -1.00000000    ,  0.00000000    )
 calling func_complex
Segmentation fault
```

Then in `./build/src.linux-x86_64/mvcemodule.c`, you can see lots of `#ifdef F2PY_CB_RETURNCOMPLEX`, mostly in typedefs/function declarations. If you pass `-DF2PY_CB_RETURNCOMPLEX` manually to `f2py -c`, then it all works fine, but presumably this should happen automatically?
",2016-04-27 15:40:09,,f2py segfaults when calling complex valued python functions from Fortran,"['00 - Bug', 'component: numpy.f2py']"
7556,open,excentrik,"Numpy seems to check if each element in an array is iterable (by using len and iter) because it might receive a multidimensional array.

But this seems to raise an issue for dict-like classes (meaning isinstance(element, dict) == True).
These will not be interpreted as another dimension. 

But when using an object derived from `collections.MutableMapping`, the object is not handled correctly.
Probably there is check for `isinstance(obj, dict)` which obviously fails on a `collections.Mapping`.

This happens as least in python 2.7.10, numpy 1.11.0, and OSX 10.10.5

Here is exemplary code:

```
import collections
import numpy


class Extendeddict(collections.MutableMapping):
    """""" Dictionary based on MutableMapping

    :rtype : Extendeddict
    """"""
    # noinspection PyMissingConstructor
    def __init__(self, *args, **kwargs):
        self._store = dict()
        self.update(dict(*args, **kwargs))

    def __getitem__(self, key):
        return self._store[key]

    def __setitem__(self, key, value):
        self._store[key] = value

    def __delitem__(self, key):
        del self._store[key]

    def __iter__(self):
        return iter(self._store)

    def __len__(self):
        return len(self._store)

    def __repr__(self):
        return self._store.__repr__()


basic_dict = {'Test': 'test'}
extended_dict = Extendeddict(basic_dict)
print 'Normal dictionary in numpy array: {0}'.format(numpy.array([basic_dict], dtype=object))
print 'Extended dictionary in numpy array: {0}'.format(numpy.array([extended_dict], dtype=object))
```
",2016-04-18 07:06:08,,Use collections.MutableMapping instead of dict when checking object type,['unlabeled']
7552,open,eric-wieser,"Adding a `keys` method to ndarray with the following definition:

``` python
def keys(self):
    if self.dtype.names is not None
        return d.dtype.names
    else:
        raise TypeError('Only structured arrays can be used as mappings')
```

Would allow structured arrays to be used inside a kwarg-splat:

``` python
def foo(a, b):
    print(a, b)

data = np.empty(20, dtype=[('a', np.float32), ('b', np.float32)])
foo(**data)
```

If an identical `keys` method is also added to `np.void`, then the following also behaves as expected:

```
foo(**data[0])
```
",2016-04-16 03:13:38,,Make ndarray and np.void splattable,"['01 - Enhancement', 'component: numpy._core']"
7541,open,pganssle,"I noticed that for datetime types, the comparison operators have the opposite of the intuitive meaning:

``` python
>>> np.dtype('datetime64[Y]') > np.dtype('datetime64[W]')
False
>>> np.dtype('datetime64[W]') > np.dtype('datetime64[Y]')
True
```

My understanding is that this behavior is carried over from [this comparison operator](https://github.com/numpy/numpy/blob/7ffa81f04ba046/numpy/core/src/multiarray/descriptor.c#L3507), where a `dtype` is considered ""less than"" another dtype if you can cast from it to the other (presumably without loss of precision). This makes sense for `np.dtype('float64') > np.dtype('float32')`, but is kinda counter-intuitive when talking about units of time.

I think it's probably not a good idea to change the behavior of comparisons on `dtype` objects, but maybe this could be added to the documentation for datetime64/timedelta64 to make it obvious why the behavior is the way it is.
",2016-04-12 15:39:16,,Comparison between datetime/timedelta dtypes has counterintuitive meaning,['component: numpy.datetime64']
7523,open,samverstocken,"Hi,

I am using Numpy version 1.11 and I experienced the following problem:
- When converters are not used, unpack=True gives the expected output
- When I set the converters parameter, unpack=True is ignored: I get one array with an entry for each row..
",2016-04-07 15:07:28,,"BUG: In genfromtxt, unpack=True is ignored when converters are used",['00 - Bug']
7495,open,toddrjen,"When you try to transpose a 1D array, it does nothing.  This is the correct behavior, since it transposing a 1D array is meaningless.  However, this can often lead to unexpected errors since this is rarely what you want.  You can convert the array to 2D, using `np.atleast_2d` or `arr[None]`, but this makes simple linear algebra computations more difficult.

I propose adding an argument to transpose, perhaps called `expand` or `expanddim`, which if `True` (it is `False` by default) will force the array to be at least 2D.  A shortcut property, `ndarray.T2`, would be the same as `ndarray.transpose(True)`.
",2016-04-01 16:22:30,,ENH: ndarray.T2 for 2D transpose.,"['01 - Enhancement', 'component: numpy.linalg']"
7494,open,Vargeel,"So I apologise if this is by design, but it didn't seem evident considering the function description. 
I assumed that unwrap would unwrap angular values between -pi and pi, but instead it seems to be unwrapping them in the [-pi+n_pi, pi+n_pi] segment containing the first value passed as an input. 

As such, we get these results :
**In [19]:** `a = [9.2,9.2,9.3]`
**In [20]:** `np.unwrap(a)`
**Out[20]:** `array([ 9.2,  9.2,  9.3])`

**In [21]:** `a = [9.2,9.2,9.3,0.5]`
**In [22]:** `np.unwrap(a)`
**Out[22]:** `array([ 9.2       ,  9.2       ,  9.3       ,  6.78318531])`

**In [23]:** `a = [0.5,9.2,9.2,9.3,0.5]`
**In [24]:** `np.unwrap(a)`
**Out[24]:** `array([ 0.5       ,  2.91681469,  2.91681469,  3.01681469,  0.5       ])`

This is with numpy v1.11.0

Thank you for your time.
",2016-04-01 16:04:18,,Unwrap function needs first value of the input to be within the wanted -pi to pi segment,['unlabeled']
7490,open,anntzer,"The docs say

```
    tol : float
        Tolerance in machine epsilons for the complex part of the elements
        in the array.
```

but

```
In [8]: np.real_if_close(.5j, 1)
Out[8]: array(0.0) # Oops.
```

Actually, the source of the issue is rather obvious in the source code:

```
    if tol > 1: # <--- Uh?
        from numpy.core import getlimits
        f = getlimits.finfo(a.dtype.type)
        tol = f.eps * tol
```

and the tests (`test_type_check.py`) actually cover a weird case:

```
        a = np.random.rand(10)
        b = real_if_close(a+1e-15j)
        assert_all(isrealobj(b))
        assert_array_equal(a, b)
        b = real_if_close(a+1e-7j)
        assert_all(iscomplexobj(b))
        b = real_if_close(a+1e-7j, tol=1e-6) # <--- Uh?
        assert_all(isrealobj(b))
```

I guess the writer of the test thought it was supplying a test in absolute value...
",2016-03-31 02:21:44,,"Incorrect behavior/testing of real_if_close's ""tol"" argument",['unlabeled']
7488,open,coderforlife,"The `out` argument would be used for the output copy. This means that the array will always be copied, even if it could be reshaped without copying, thus becoming the opposite of doing `a.shape = (...)` which never copies. This would allow one to copy the data where both the array being reshaped and the destination array cannot be reshaped to the other without a copy being made and not make an intermediate. This would also give options for emulating `repeat` and `tile` with an `out` argument by using `as_strided` and `reshape`.
",2016-03-31 01:55:17,,Feature request: support out argument in reshape,['unlabeled']
7487,open,dmbelov,"It seems to me that operations for datetime64 objects are poorly implemented. For example, comparison of two arrays of type 'M8[s]' takes **7 times** longer than comparison of 'i8' arrays of the same shape.  There are similar issues with computing the difference.

Could you please improve the performance of operations on datetime64 arrays?

Here is the example:

```
date_d = numpy.arange(numpy.datetime64('1990-01-02'), numpy.datetime64('2005-12-31'))
time_t = numpy.arange(1440).view('m8[m]')
utc_dt = date_d[:, None] + time_t[None, :]

# This is very slow
In [17]: %timeit utc_dt > utc_dt[:, 0:1]
10 loops, best of 3: 100 ms per loop

# But, if convert it to i8 it works 7 times faster!
In [18]: %timeit utc_dt.view('i8') > utc_dt[:, 0:1].view('i8')
100 loops, best of 3: 14.4 ms per loop

### same thing for minus sign
In [19]: %timeit utc_dt - utc_dt[:, 0:1]
10 loops, best of 3: 119 ms per loop

In [20]: %timeit utc_dt.view('i8') - utc_dt[:, 0:1].view('i8')
10 loops, best of 3: 35.2 ms per loop
```

Thanks,
Dmitry.
",2016-03-30 23:05:24,,Comparison of datetime64 arrays is very slow in NumPy 1.9.3,"['component: benchmarks', 'component: numpy.datetime64']"
7461,open,zegkljan,"If I try to do `np.linalg.pinv()` of a matrix that contains some `np.inf`s, sometimes the function just hangs with 100% CPU usage, other times it returns something:

```
In [1]: import numpy as np
In [2]: x = np.array([[550.0, 1], [1, np.inf]])
In [3]: np.linalg.pinv(x)
Out[3]: 
array([[ 0.,  0.],
       [ 0.,  0.]])

In [4]: x = np.array([[np.inf, np.inf], [np.inf, np.inf]])
In [5]: np.linalg.pinv(x)
Out[5]: 
array([[ nan,  nan],
       [ nan,  nan]])

In [6]: x = np.array([[np.inf, np.inf], [550.0, 1]])
In [7]: np.linalg.pinv(x)
Out[7]: 
array([[ nan,  nan],
       [ nan,  nan]])

In [8]: x = np.array([[550.0, 1], [np.inf, np.inf]])
In [9]: np.linalg.pinv(x) # here it just hangs and I must kill it from outside
Killed
```

I think the correct behaviour should be to raise an exception or return `nan`s or whatever, but definitely not hang.

My configuration:
Ubuntu 15.10

```
$ uname -srvmpio
Linux 4.2.0-34-generic #39-Ubuntu SMP Thu Mar 10 22:13:01 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
```

python version: 3.5.0+
numpy version: 1.10.4
installed via pip in a virtual environment, log of installation: [pip-install-numpy.txt](https://github.com/numpy/numpy/files/189806/pip-install-numpy.txt)
gcc version: gcc (Ubuntu 5.2.1-22ubuntu2) 5.2.1 20151010
",2016-03-25 17:39:31,,np.linalg.pinv() hangs for certain np.inf arrangements in the inversed matrix,['component: numpy.linalg']
7449,open,njsmith,"Apropos #7447 (CC @ewmoore @charris):

I was curious about what was going on here, and it looks much uglier than we realized :-(

Little test script that tries doing `4 ** -4` using different types:

<details> <summary> Old testing script (and results) </summary>

``` python
signed_int_types = [np.int8, np.int16, np.int32, np.int64, int]
all_int_types = signed_int_types + [np.uint8, np.uint16, np.uint32, np.uint64]
for base_type in all_int_types:
    for exponent_type in signed_int_types:
        result = base_type(4) ** exponent_type(-4)
        print(""{} ** {} -> {} ({})"".format(base_type.__name__,
                                           exponent_type.__name__,
                                           result.__class__.__name__,
                                           result))
```

On current master (after #7447 is merged):

```
int8 ** int8 -> float32 (0.00390625)
int8 ** int16 -> int16 (0)
int8 ** int32 -> int32 (0)
int8 ** int64 -> int64 (0)
int8 ** int -> int64 (0)
int16 ** int8 -> float32 (0.00390625)
int16 ** int16 -> float32 (0.00390625)
int16 ** int32 -> int32 (0)
int16 ** int64 -> int64 (0)
int16 ** int -> int64 (0)
int32 ** int8 -> float64 (0.00390625)
int32 ** int16 -> float64 (0.00390625)
int32 ** int32 -> float64 (0.00390625)
int32 ** int64 -> int64 (0)
int32 ** int -> int64 (0)
int64 ** int8 -> float64 (0.00390625)
int64 ** int16 -> float64 (0.00390625)
int64 ** int32 -> float64 (0.00390625)
int64 ** int64 -> float64 (0.00390625)
int64 ** int -> float64 (0.00390625)
int ** int8 -> int64 (0)
int ** int16 -> int64 (0)
int ** int32 -> int64 (0)
int ** int64 -> float64 (0.00390625)
int ** int -> float (0.00390625)
uint8 ** int8 -> int16 (0)
uint8 ** int16 -> int16 (0)
uint8 ** int32 -> int32 (0)
uint8 ** int64 -> int64 (0)
uint8 ** int -> int64 (0)
uint16 ** int8 -> int32 (0)
uint16 ** int16 -> int32 (0)
uint16 ** int32 -> int32 (0)
uint16 ** int64 -> int64 (0)
uint16 ** int -> int64 (0)
uint32 ** int8 -> int64 (0)
uint32 ** int16 -> int64 (0)
uint32 ** int32 -> int64 (0)
uint32 ** int64 -> int64 (0)
uint32 ** int -> int64 (0)
uint64 ** int8 -> float64 (0.00390625)
uint64 ** int16 -> float64 (0.00390625)
uint64 ** int32 -> float64 (0.00390625)
uint64 ** int64 -> float64 (0.00390625)
uint64 ** int -> float64 (0.00390625)
```

This is all extremely extremely weird and inconsistent...

</details>

---

Summary/edit by @seberg 2019-05-23:

On current master (and 1.16) we now error for many of these cases as found using the modified snippet:

```
signed_int_types = [np.int8, np.int16, np.int32, np.int64, int]
all_int_types = signed_int_types + [np.uint8, np.uint16, np.uint32, np.uint64]
for base_type in all_int_types:
    for exponent_type in signed_int_types:
        try:
            result = base_type(4) ** exponent_type(-4)
        except Exception as e:
            result = e
        print(""{} ** {} -> {} ({})"".format(base_type.__name__,
                                           exponent_type.__name__,
                                           result.__class__.__name__,
                                           result))
```

Giving (again 1.16, master shortly before 1.17):
```
int8 ** int8 -> ValueError (Integers to negative integer powers are not allowed.)
int8 ** int16 -> ValueError (Integers to negative integer powers are not allowed.)
int8 ** int32 -> ValueError (Integers to negative integer powers are not allowed.)
int8 ** int64 -> ValueError (Integers to negative integer powers are not allowed.)
int8 ** int -> ValueError (Integers to negative integer powers are not allowed.)
int16 ** int8 -> ValueError (Integers to negative integer powers are not allowed.)
int16 ** int16 -> ValueError (Integers to negative integer powers are not allowed.)
int16 ** int32 -> ValueError (Integers to negative integer powers are not allowed.)
int16 ** int64 -> ValueError (Integers to negative integer powers are not allowed.)
int16 ** int -> ValueError (Integers to negative integer powers are not allowed.)
int32 ** int8 -> ValueError (Integers to negative integer powers are not allowed.)
int32 ** int16 -> ValueError (Integers to negative integer powers are not allowed.)
int32 ** int32 -> ValueError (Integers to negative integer powers are not allowed.)
int32 ** int64 -> ValueError (Integers to negative integer powers are not allowed.)
int32 ** int -> ValueError (Integers to negative integer powers are not allowed.)
int64 ** int8 -> ValueError (Integers to negative integer powers are not allowed.)
int64 ** int16 -> ValueError (Integers to negative integer powers are not allowed.)
int64 ** int32 -> ValueError (Integers to negative integer powers are not allowed.)
int64 ** int64 -> ValueError (Integers to negative integer powers are not allowed.)
int64 ** int -> ValueError (Integers to negative integer powers are not allowed.)
int ** int8 -> ValueError (Integers to negative integer powers are not allowed.)
int ** int16 -> ValueError (Integers to negative integer powers are not allowed.)
int ** int32 -> ValueError (Integers to negative integer powers are not allowed.)
int ** int64 -> ValueError (Integers to negative integer powers are not allowed.)
int ** int -> float (0.00390625)
uint8 ** int8 -> ValueError (Integers to negative integer powers are not allowed.)
uint8 ** int16 -> ValueError (Integers to negative integer powers are not allowed.)
uint8 ** int32 -> ValueError (Integers to negative integer powers are not allowed.)
uint8 ** int64 -> ValueError (Integers to negative integer powers are not allowed.)
uint8 ** int -> ValueError (Integers to negative integer powers are not allowed.)
uint16 ** int8 -> ValueError (Integers to negative integer powers are not allowed.)
uint16 ** int16 -> ValueError (Integers to negative integer powers are not allowed.)
uint16 ** int32 -> ValueError (Integers to negative integer powers are not allowed.)
uint16 ** int64 -> ValueError (Integers to negative integer powers are not allowed.)
uint16 ** int -> ValueError (Integers to negative integer powers are not allowed.)
uint32 ** int8 -> ValueError (Integers to negative integer powers are not allowed.)
uint32 ** int16 -> ValueError (Integers to negative integer powers are not allowed.)
uint32 ** int32 -> ValueError (Integers to negative integer powers are not allowed.)
uint32 ** int64 -> ValueError (Integers to negative integer powers are not allowed.)
uint32 ** int -> ValueError (Integers to negative integer powers are not allowed.)
uint64 ** int8 -> float64 (0.00390625)
uint64 ** int16 -> float64 (0.00390625)
uint64 ** int32 -> float64 (0.00390625)
uint64 ** int64 -> float64 (0.00390625)
uint64 ** int -> float64 (0.00390625)
```

So the integer+integer which use float as a common type are a bit strange in this sense.",2016-03-22 22:23:03,,NumPy's implementation of scalar integer ** integer has very weird casting,['00 - Bug']
7440,open,jreback,"occurs also on 1.10.4

```
[pandas_test] C:\Miniconda3-32\Scripts>python
Python 2.7.11 |Continuum Analytics, Inc.| (default, Jan 29 2016, 15:36:56) [MSC v.1500 32 bit (Intel)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
>>> import numpy as np
>>> np.__version__
'1.9.2'
>>> np.abs(np.array([0,np.nan],dtype='float64'))
__main__:1: RuntimeWarning: invalid value encountered in absolute
array([  0.,  nan])
>>>
```

you are giving a warning, but not sure why are not handling it.
",2016-03-20 20:21:37,,BUG: abs with nan on 64-bit floats on 32-bit platform?,['unlabeled']
7394,open,ahaldane,"The ufuncs defined for `MaskedArrays` don't return `out` if it is supplied as a keyword, unlike normal ufuncs.

``` python
>>> a = np.ones(2)
>>> np.add([1,1], 2, out=a) is a
True
>>> np.ma.add([1,1], 2, out=a) is a
False
```
",2016-03-08 23:16:27,,MaskedArray ufuncs don't return `out` parameter,['component: numpy.ma']
7383,open,anntzer,"Currently, ""in-place"" operations for poly1d objects actually return a new object:

```
In [2]: t = np.poly1d([1, 2])

In [3]: u = t

In [4]: t -= 1

In [5]: t
Out[5]: poly1d([1, 1])

In [6]: u
Out[6]: poly1d([1, 2])
```

I'd argue that this behavior is unexpected, and should be changed to an ndarray-like behavior (actual modification of the underlying array).  Thoughts?
",2016-03-06 09:36:17,,In-place operations for poly1d objects,['unlabeled']
7380,open,ghost,"I am trying to write a numpy array data to a pipe opened using subprocess.Popen in Python 3.4 and numpy 1.10.4. Here is the code:

```
import numpy
import subprocess

myArray = numpy.arange(10000).reshape([100,100])

# use xpaset to communicate with ds9
fullCmd = ""xpaset DS9Test array [xdim=100,bitpix=64,arch=littleendian,ydim=100]""

mp = subprocess.Popen(
    fullCmd, 
    shell = True, 
    stdin = subprocess.PIPE, 
    stdout = subprocess.PIPE, 
    stderr = subprocess.STDOUT,
    bufsize = 0
)

myArray.tofile(mp.stdin)
```

Unfortunately I am getting the following error:

```
  File ""/Users/avigan/Work/HC-HR/FTS/test.py"", line 25, in <module>
    myArray.tofile(mp.stdin)

OSError: first argument must be a string or open file
```

However, if I do:

```
In [64]: print(mp.stdin)
<_io.FileIO name=71 mode='wb'>
```

I interpret this as a sign that the file descriptor is indeed open.

The same code was working fine in Python 2.7, so I guess it's linked to Python 3+ and the current version of numpy.
",2016-03-04 22:20:10,,Writing numpy array data to a pipe opened with Popen,['unlabeled']
7371,open,ssolari,"Decode should never switch from a string type to a float.  The behavior should likely convert empty arrays to the same dimension in unicode as the original string dtype.   Same behavior in python 2.7 or 3.5.

``` python
>>> import numpy as np
>>> np.core.defchararray.decode(np.array([], dtype='|S1'))
array([], dtype=float64)
```

behavior should result in

``` python
>>> np.array([], dtype='|U1')
array([], 
      dtype='<U1')
```
",2016-03-02 01:32:42,,defchararray decodes empty array incorrectly to float,"['00 - Bug', 'component: numpy._core']"
7356,open,gmatty,"Join only seems to return correct result when seq array is not uniform. 
Fail Examples:

> np.core.defchararray.join(""x"",[[""A"",""B"",""C""]])
> array([['A', 'B', 'C']], 
>       dtype='|S1')
> 
> np.core.defchararray.join(""x"",[[""A"",""B"",""C""],[""X"",""Y""]])
> array(['AxBxC', 'XxY'], 
>       dtype='|S5')

Success Examples:

> np.core.defchararray.join(""x"",[[""A"",""B"",""C""],[""X"",""Y""]])
> array(['AxBxC', 'XxY'], 
>       dtype='|S5')
> 
> np.core.defchararray.join(""x"",[[""A"",""B""],[""X"",""Y"",""Z""],[""F"",""G""]])
> array(['AxB', 'XxYxZ', 'FxG'], 
>       dtype='|S5')
",2016-02-27 09:12:22,,BUG: np.core.defchararray.join returns wrong result,['00 - Bug']
7330,open,MSeifert04,"I thought most/all of the numpy ufuncs would consider if masked-arrays are used but for the `np.median` (and `np.average`) this is not the case:

```python
import numpy as np
normal_array = np.arange(10)
masked_array = np.ma.array(normal_array, mask=normal_array>7)
np.median(normal_array) # Returns 4.5
np.median(masked_array) # Returns 4.5
np.ma.median(masked_array)[0] # Returns 3.5 (it returns a masked array instead of a scalar btw)
```

whereas for sums it works as one would expect:

```python
np.sum(normal_array) # Returns 45
np.sum(masked_array) # Returns 28
```

mean/std/var all work like sum, why the exception for median/average?
",2016-02-24 16:14:34,,BUG: `np.median` and `np.average` don't work on masked arrays directly,"['00 - Bug', 'component: numpy.ma']"
7329,open,poes-metop-sem,"Copying a Masked Array does not create a new MA with it's own fill value. Modifying the fill_value attribute on the destination array modifies the fill_value on the original source array.

The following code illustrates the issue:

`>>> import numpy as np, numpy.ma as ma`
`>>> a = ma.masked_values( np.array([9999., 0., 1., 2.]), 9999. )`
`>>> a.fill_value`
**9999.0**
`>>> b = ma.copy( a )`
`>>> ma.set_fill_value( b, -1. )`
`>>> a.fill_value`
**-1.0**
`>>>`
",2016-02-24 15:44:35,,MaskedArray.copy Shares Fill Value,"['00 - Bug', 'component: numpy.ma']"
7277,open,parmegv,"Hi!

I'm using numpy from Anaconda, version 1.10.4.

`prime = 9369319`
`polynomial = [9286959, 1556801, 1900001]`
`point = (1618823, 5704906)`
`np.polyval(polynomial, point[0])`
`anaconda2/lib/python2.7/site-packages/numpy/lib/polynomial.py:680: RuntimeWarning: overflow encountered in long_scalars`

 I can generate examples like this easily, so if you need more samples I can provide them.

 The warning itself is not a problem, but the method returning a result is. 5704906 is the result modulo prime, and it is wrong (the real result modulo prime is 8372224 according to octave).

 The corresponding line in github is https://github.com/numpy/numpy/blob/master/numpy/lib/polynomial.py#L681.
",2016-02-18 17:39:12,,polyval returns a bad result due to overflow encountered in long_scalars,['unlabeled']
7267,open,ahaldane,"As discovered in #7259 and #7260, string- and unicode- numpy scalars do not support indexing with an empty tuple, unlike other scalars:

```
>>> numpy.string_(""ABC"")[()]
TypeError: string indices must be integers, not tuple
>>> numpy.int32(1)[()]
1
```

This may be a problem because indexing with an empty tuple is a way of converting possible 0-d arrays to scalars:

```
>>> a = numpy.int64(1)
>>> b = numpy.array(a)
>>> type(a[()]), type(b[()])
(numpy.int64, numpy.int64)
```

As of writing this it's not 100% clear to me we actually want this. It could be that instead of just writing

```
return possiblearr[()]
```

we should encourage people to write something like

```
if isinstance(possiblearr, ndarray):
    return possiblearr[()]
return possiblearr
```

But, on the other hand it's probably a good idea for all scalar types to behave similarly. I made a rough outline for a fix in a comment in #7260.
",2016-02-17 00:16:43,,can't index string- and unicode- numpy-scalars with an empty tuple,['00 - Bug']
7265,open,ahed87,"Hi,

I think it would be great with some basic groupby functionality in numpy, on int's and float's.
Using it would be something like a one liner below (just thinking of how a call could look like, not considering if it would actually work with current input/output of for example the mean function).

result = np.groupby(x, y).mean() # x.shape = (n, ) y.shape = (n, m)
or maybe,
result = np.groupby(x, y, how='mean') # reasonable options would be mean, min, max, var or std
or even,
result = np.mean(np.groupby(x, y))

result would be result.shape = (np.unique(x), m)

Background:
I recently did some cleaning of measurements signals (~5e6 samples) - all of them floats (also the groupby series).

One of the things to do with the signals was to reduce the length a bit with for example averaging, however the signal was not evenly spaced and with larger gaps in time so a simple rolling average and dropping every second sample would not work, of course the signal had some nan's scattered around as well, which does not matter until one has to use nanmean instead of mean (maybe mean with a keword like bubblenan=True could be an idea for api-simplification instead of nanmean?).

I started out with pandas since it has some great functionality for grouping, however that turned out to be like hitting a brickwall in speed (don't really understand why - but after minutes of waiting it was quite obvious..., and yes I did try some different ways) - so in the end not usable for this case.

Next step was to turn to numpy, and after a bit of searching on the web I did not really find anything that I did not already know, python loops are slow, python has itertools etc, there was some examples of grouping with numpy but they seemed to be based on integers, and it was only grouping, not including aggregating with mean in most of the cases. Quite some time ago I did some stuff with ufuncs and found them pretty slow compared to vectorized operations (I probably did something wrong), so that I never tried for this case.

In the end I wrote a small piece of code (<20 lines) based on np.diff, np.where, np.mean and np.delete in a while statement, breaking out when all elements where unique. The whole aggregation was done only on consecutive pairs, which was ensured by an internal loop over the mask dropping the multiple duplicates, leaving only consecutive pairs.

The numpy version I did was doing it in around 5 seconds per series which was good enough for me. I do not know the actual speed of pandas since I never had patience to wait for it to finish.

Maybe if the thing is really implemented in numpy it would be less than half the time,
and would be a simple oneliner.
",2016-02-16 22:54:58,,Enhancement: groupby function,['unlabeled']
7242,open,NeilGirdhar,"Summary
---
Make DType objects, their corresponding types, and their string names all compare unequal.

Historical issue
---
For some reason, `dtype` objects and the numpy types they are based off of compare equal.  They don't hash equal of course, which goes against Python's docs that say

> The only required property is that objects which compare equal have the same hash value…

Can we make them compare unequal?  Is there any reason for them to compare equal?

```
In [6]: np.dtype(np.float) == np.float
Out[6]: True
```
",2016-02-13 18:46:14,,BUG: dtype comparison coerces generously leading to hash problems,"['00 - Bug', 'component: numpy.dtype', '62 - Python API', 'component: numpy.array_api']"
7236,open,charris,"Test skipping `assert_equal`:

```
----------------------------------------------------------------------
Ran 4 tests in 1.274s
```

Test not skipping `assert_equal`:

```
----------------------------------------------------------------------
Ran 4 tests in 16.090s
```
",2016-02-12 19:42:13,,"assert_equal is really, really slow.","['00 - Bug', '05 - Testing']"
7212,open,jdemaria,"Hi, trying the example of [(http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.ma.masked_equal.html#numpy.ma.masked_equal)] I get:
a = np.arange(4)
np.ma.masked_equal(a, 2)
==> masked_array(data = [0 1 -- 3],
                    mask = [False False  True False],
              fill_value = 2)
but
np.ma.masked_where(a == 2, a)
==> masked_array(data = [0 1 -- 3],
                    mask = [False False  True False],
              fill_value = 999999)
tested with 1.10.4

Is it the normal behaviour? if yes so the documentation is wrong and this is not a shortcut of masked_where?

thanks in advance
",2016-02-08 17:01:29,,ma.masked_equal() change fill_value,['component: numpy.ma']
7210,open,jakirkham,"There seems to be a bug where `MaskedArray`s will still have their `sharedmask` flag set after copying. However, the mask is no longer shared as a copy has been made. A brief code example below demonstrates the problem.

```
import numpy as np

a = np.zeros((2,3))
a = np.ma.masked_array(a, np.ma.getmaskarray(a))

b = a[...]
assert b.mask.base is a.mask
assert b.sharedmask

b = b.copy()
assert b.mask.base is not a.mask
assert not b.sharedmask                # Assertion fails.
```
",2016-02-08 13:06:32,,`MaskedArray`s still share mask on copy,['component: numpy.ma']
7189,open,keceli,"The following is a minimal working example:

``` fortran
module minimal
  integer, dimension(2) :: d
  data d(1) / 1 /
  data d(2) / 2 /
end module
```

On the other hand the same syntax works when it is used in a subroutine. There are other alternatives to use i.e. `data d / 1, 2 /`,  which works either in a module or subroutine, but prior syntax is also commonly used. 

The signature file gives:

``` fortran
module minimal ! in minimal.f95
    real, optional :: d(1)=1
    integer dimension(2) :: d
    real, optional :: d(2)=2
end module minimal
```

Showing that f2py tries to convert d(1) to arguments. If you put `implicit none`, then f2py complains with `vars2fortran: No typespec for argument ""d(1)""`
",2016-02-04 21:17:09,,f2py fails with a specific data syntax in modules,['component: numpy.f2py']
7179,open,jcrist,"Currently, calls to routines in `np.linalg` don't release the interpreter lock, but (as far as I can tell) should be able to. This has caused non-optimal behavior for [dask.array.linalg](https://github.com/blaze/dask/blob/master/dask/array/linalg.py) when trying to use these functions in parallel using threads.
",2016-02-03 17:21:14,,ENH: linalg.qr should be a gufunc (which will release the GIL),"['01 - Enhancement', 'component: numpy._core']"
7051,open,charris,"Just return the array, don't error.
```In [10]: a = ones(())

In [11]: a.ndim
Out[11]: 0

In [12]: a.sort()

ValueError                                Traceback (most recent call last)
<ipython-input-12-e7eb8b51a6fa> in <module>()
----> 1 a.sort()

ValueError: axis(=-1) out of bounds

```
```
",2016-01-18 18:41:14,,Sorting should work for array scalars.,"['01 - Enhancement', 'component: numpy._core']"
7025,open,shoyer,"In the process of making datetime64 time zone naive (#6453), I came across this function that was added as part of datetime support, but that unfortunately @mwiebe [never got around to documenting](https://mail.scipy.org/pipermail/numpy-discussion/2013-April/066084.html). Nonetheless, it can of course be found [in the wild](http://stackoverflow.com/questions/29616292/convertion-of-datetime-to-numpy-datetime-without-timezone-info) -- my github search also found a few snippets, though AFAICT it's not used in any major projects.

The function provides the ability to print datetimes assumed to be specified in UTC with different precision and time zones. See [its unit tests](https://github.com/numpy/numpy/blob/c45f3e7d56686590af9b27829a505de709702563/numpy/core/tests/test_datetime.py#L1317) for examples. As such, it doesn't really fit in with the new time zone naive datetime64. Removing it would let us delete a significant block of code.

I could simply deprecate the timezone handling, but without timezone handling it doesn't do very much. So I'm inclined to remove the entire rarely used function. Given that it's not documented, do we need it to go through a normal deprecation cycle?

cc @PythonCHB
",2016-01-16 00:26:45,,"Document, deprecate or simply remove np.datetime_as_string","['15 - Discussion', '54 - Needs decision', 'component: numpy.datetime64']"
7002,open,charris,"Numpy umath has a file `scalarmath.c.src` that implements scalar arithmetic using special functions that are about 10x faster than the equivalent ufuncs.

```
In [1]: a = np.float64(1)

In [2]: timeit a*a
10000000 loops, best of 3: 69.5 ns per loop

In [3]: timeit np.multiply(a, a)
1000000 loops, best of 3: 722 ns per loop
```

I contend that in large programs this improvement in execution time is not worth the complexity and maintenance overhead; it is unlikely that scalar-scalar arithmetic is a significant part of their execution time.  Therefore I propose to use ufuncs for all of the scalar-scalar arithmetic. This would also bring the benefits of  `__numpy_ufunc__` to scalars with minimal effort.

Thoughts?
",2016-01-13 05:07:35,,Get rid of special scalar arithmetic.,"['15 - Discussion', 'component: numpy._core', 'Proposal']"
6999,open,chem,"I've encountered an issue with underflows in numpy.  Example code as follows:

```
#!/usr/bin/env python
import numpy as np
import numpy.linalg as la

a=np.asarray([[0.70710678,0.],[0.70710678,1.]])
b=np.asarray([0.4, 0.2])
c=5.0e-16

print 'in'
r1, r2, r3, r4 = la.lstsq(a, b, c)
print 'out'
```

A floating point exception is thrown before 'out' is reached, when using intel compilers with the ""-fp-trap-all=underflow"" flag, and numpy 1.10.2 / python 2.7.11.  Compiles using gcc or intel and _not_ using underflow detection will complete successfully.  I eventually tracked the offending line down to L1918 of linalg.py:

```
        results = lapack_routine(m, n, n_rhs, a, m, bstar, ldb, s, rcond,
                                 0, work, lwork, iwork, 0)
```

lapack_routine is making a call to lapack_lite.dgelsd -- so, a C library is being loaded for that call to a LAPACK function, and something during the call underflows.  Underflows are unusual traps; not set by default with gcc or intel's fp-trap-all=common, but I am interested in detecting them elsewhere should they occur.  It would be great if numpy could run with it enabled.

icc --version
icc (ICC) 15.0.3 20150407

System is a Linux machine, SGI ICE X running SLES11 and kernel 3.0.

I compiled numpy without linking intel MKL, since the offending lapack_lite call may be affected by it, and obviously we're interested in the non-MKL version on numpy's bug tracker.  To disable MKL being linked I unset my MKLROOT and disabled MKL detection in numpy/distutils/system_info.py.  ldd confirms that mkt_rt.so was not linked, nor any other blas/lapack library.  Otherwise I generally followed intel's instructions for compilation at: https://software.intel.com/en-us/articles/numpyscipy-with-intel-mkl , while adding -fp-trap-all=underflow to the lines in numpy/distutils/intelccompiler.py with xhost/fpic/etc.

I see that the lapack_lite code is a C translation of Fortran LAPACK, so I'm not sure if this issue is solvable on the numpy end.  The objective would be to have my original code sample run with underflow traps enabled using intel compilers.  Can numpy devs help?  If not, could they bump the issue up to the appropriate place (perhaps LAPACK itself)?

I also noticed that all the lapack code in dlapack_lite.c appears to be based on LAPACK version 3.0 (according to the code comments), which is now 16 years old.  LAPACK 3.6.0 was released in November, 2015.  Would numpy benefit from updating its LAPACK rountines?

Thank you to those who work on numpy; great package.
",2016-01-12 18:28:47,,floating point exception in linalg / lapack with intel compiler,"['00 - Bug', 'component: numpy.linalg']"
6971,open,sbyrnes321,"When you assign a float to an array of int's, it is silently truncated:

```
a = np.array([0,0,1])
a[1] = 2.5

print(a) -----> [0 2 1]
```

It seems to me that this is the wrong behavior: A crazy, frustrating ""gotcha"", and an obvious source of bugs. I feel that there should be an error, or at least a warning---just as there is when you try to shoehorn a complex number into an array of reals. ""Explicit is better than implicit"".

I'm sure that it has been this way forever but if we do it gradually with DeprecationWarnings, I don't see why we can't change it. Let's make it easier for future generations to write bug-free NumPy code!
",2016-01-08 14:13:47,,we should not silently truncate floats to ints,"['00 - Bug', 'component: numpy._core', '54 - Needs decision']"
6951,open,jakirkham,"Related: https://github.com/numpy/numpy/issues/6794
Related: https://github.com/numpy/numpy/pull/6932

In addition to optimizing `a @ a.T` using `syrk`, there should be a way to optimize `a @ a.H` using `hyrk`. This is a bit tricky as `a.H` is not a view. So, it cannot be done the same way that was done for `a @ a.T` ( https://github.com/numpy/numpy/pull/6932 ). There are number of proposals for handling this optimization in some fashion, but one should be picked.
1. Create a ""transposed-complex"" type. ( https://github.com/numpy/numpy/issues/6794#issuecomment-168426443 )
2. Add a function for all `a @ a.T` and `a @ a.H` that takes `a` and an argument for whether the `conj` needs to be taken. ( https://github.com/numpy/numpy/issues/6794#issuecomment-168436451 )
3. Add a keyword argument to `dot` that determines whether to do a conjugated dot product or not. ( https://github.com/numpy/numpy/issues/6794#issuecomment-168475903 )
4. Change `np.dot` to allow it to take a single argument to allow for the same behavior presented in 2.

Raising to the attention of the previously interested parties: @njsmith @charris @argriffing 
",2016-01-06 17:20:07,,Optimization of `a @ a.H` style operations using BLAS herk routines,['unlabeled']
6854,open,aplavin,"Currently it returns array of floats for no reason;

```
> x = np.arange(10)
> w = np.arange(10)
> np.bincount(a, x)
array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])
```
",2015-12-18 06:07:33,,Bincount with integer weights should return integer array,"['component: numpy._core', '54 - Needs decision']"
6846,open,pitrou,"I'm not sure fast half-float computations are supported, so that sounds suboptimal. And in this particular case, it even loses precision:

```
>>> np.sin(np.int8([1,2,3]))
array([ 0.84130859,  0.90917969,  0.14111328], dtype=float16)
>>> np.sin(np.int16([1,2,3]))
array([ 0.84147096,  0.90929741,  0.14112   ], dtype=float32)
>>> np.sin(np.int32([1,2,3]))
array([ 0.84147098,  0.90929743,  0.14112001])
>>> np.__version__
'1.10.2'
```
",2015-12-17 10:57:40,,built-in ufunc(s) unexpectedly returns half-floats,"['01 - Enhancement', 'component: numpy._core', '54 - Needs decision', 'component: numpy.dtype']"
6818,open,embray,"As we found over in astropy/astropy#4275, some implementations of `expm1` incorrectly return `NaN` instead of `INF` on large inputs (it still correctly raises an overflow error, but returns an incorrect result).

The Numpy implementation of this function doesn't have this problem as far as I can tell.  However, per https://sourceware.org/bugzilla/show_bug.cgi?id=6814 some versions of glibc and, it seems, MSVCRT have this problem.

Numpy uses `#HAVE_EXPM1` to determine if this function is provided by the system.  But it doesn't have a way to check if the implementation is buggy or not.

Numpy uses [`config.check_func`](https://github.com/numpy/numpy/blob/v1.10.1/numpy/distutils/command/config.py) to check if a function is available and can be called.  But I couldn't see if there was any existing way to test the return value of a function, so that we can test it for bugs, etc.  That might be nice to have and I will provide an implementation if there's agreement on that.  _Should_ Numpy be overriding buggy platform versions of some functions that are otherwise available?
",2015-12-11 21:16:32,,expm1 buggy on some platforms,"['00 - Bug', 'component: npy_math']"
6811,open,anntzer,"First noted in #6776: the following line segfaults python:

```
np.frombuffer(np.array(None), np.int64)[0] = 1
```

It is not completely clear how to handle this (or whether this should be handled at all).  A suggestion (I have no idea of whether this is enough or too much in practice) is to return a read-only buffer when accessing the `data` attribute of object (or struct-including-object) arrays (as frombuffer respects this flag).
",2015-12-11 14:03:25,,frombuffer segfault,"['00 - Bug', 'component: numpy._core', 'Project']"
6767,open,mfenner1,"In the docs for `np.fill_diagonal` (http://docs.scipy.org/doc/numpy/reference/generated/numpy.fill_diagonal.html), we have:

```
val : scalar
Value to be written on the diagonal, its type must be compatible with that of the array a.
```

Which to my reading says that it is scalars _only_.  Turns out that the main assignment in `np.fill_diagonal` (https://github.com/numpy/numpy/blob/v1.10.1/numpy/lib/index_tricks.py#L776):

``` python
    a.flat[:end:step] = val
```

will take any object that can broadcast-compatible.  So, something like ""val must be shape compatible with the diagonal of a"" would make sense.

If this is worth a fix, I can put a PR together for it.
",2015-12-04 16:35:57,,Documentation:  np.fill_diagonal val : scalar implies scalar only?,['component: documentation']
6765,open,gerritholl,"Currently, the `MaskedArray` attributes `itemsize` and `nbytes` count only the size taken up by the data.

This affects the following attributes:
- `itemsize`
- `nbytes`
- `strides`?
- ... others?

It would be more correct to add up the size by the data and the size by the mask.
",2015-12-04 15:12:34,,MaskedArray attributes itemsize and nbytes should include mask,"['component: numpy.ma', 'Proposal']"
6743,open,andsok75,"The following

```
import numpy
import numpy.lib.recfunctions
a=numpy.zeros(3,numpy.dtype([('x', 'f8'), ('y', 'f8')]))
b=numpy.zeros(3,numpy.dtype([('f0', 'f8')]))
numpy.lib.recfunctions.merge_arrays( (a,b ), usemask=False )
```

yields

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-69-015ef99f22fa> in <module>()
----> 1 numpy.lib.recfunctions.merge_arrays( (a,b ), usemask=False )

/usr/lib/python2.7/dist-packages/numpy/lib/recfunctions.pyc in merge_arrays(seqarrays, fill_value, flatten, usemask, asrecarray)
    411     maxlength = max(sizes)
    412     # Get the dtype of the output (flattening if needed)
--> 413     newdtype = zip_descr(seqarrays, flatten=flatten)
    414     # Initialize the sequences for data and mask
    415     seqdata = []

/usr/lib/python2.7/dist-packages/numpy/lib/recfunctions.pyc in zip_descr(seqarrays, flatten)
    186             else:
    187                 newdtype.extend(current.descr)
--> 188     return np.dtype(newdtype).descr
    189 
    190 

ValueError: two fields with the same name
```

The problem is that merge_arrays essentially flattens an array if it has only one field, which could lead to conflicts in some cases, e.g.
- two single-field arrays with identical field names
- second array has one field 'f0' while first array has more than one field
- first array has one field 'f1' while second array has more than one field
  
  ( numpy 1.8.2 on ubuntu 15.10)
",2015-11-28 06:55:51,,recfunctions.merge_arrays fails to merge some arrays with standard field names,"['00 - Bug', 'component: numpy._core', 'component: numpy.lib']"
6708,open,anntzer,"```
In [1]: np.array(1, np.int64(1))
Out[1]: array(1)

In [2]: np.array(1, np.float64(1))
Out[2]: array(1.0)
```

Same with `empty`, `zeros`, etc.
This is IMO a good place for a bug to hide.  Or perhaps there has always been this behavior?
Python 3.5, numpy 1.10.1.
",2015-11-20 00:44:59,,"array constructor accepts instance of dtype instead of dtype for the ""dtype"" kwarg","['54 - Needs decision', '07 - Deprecation', 'component: numpy.dtype']"
6661,open,andim,"`np.ma.testutils.assert_equal(np.nan, np.nan)` throws an AssertionError. In my opinion it should not consistent with the behavior of `np.testing.assert_equal(np.nan, np.nan)`.
",2015-11-10 14:16:59,,Bug: ma.testutils.assert_equal on nans throws AssertionError,"['00 - Bug', '05 - Testing', 'component: numpy.ma', 'Priority: low']"
6631,open,congma,"Hi,

During my experience with the `numpy.einsum` function, I discovered that I frequently use it in the following pattern:  The zeroth argument to `einsum()`, the string describing the subscripts, is a fixed one, but the array arguments will vary during runtime.  That is to say, I apply the same summation rule to many different array arguments repeatedly.

I don't know if `einsum()` keeps its subscript-parsing results in a cache, but if not, this could be a bit wasteful, because the same subscript description need to be unnecessarily parsed again and again.

So I wonder if it is possible to have a ""curriable"" version of `einsum` that, when called with the subscript description string, returns a callable that has already parsed the string.  This returned callable is then ready to be applied to the array arguments.

On the other hand, I'm not sure about the performance impact of parsing.  Is this kind of optimization worth the effort?

Just my 2c.
",2015-11-05 10:41:36,,Request: currying `numpy.einsum` for repeated calls with the same subscript string?,['component: numpy._core']
6597,open,charris,"These are old functions that check if an array is Fortran contiguous and _not_ C contiguous. Their output depends on whether relaxed stride checking is in effect of not and they seem to have little use in current numpy.
",2015-10-31 19:01:09,,Deprecate isfortran and a.flags.fnc,"['17 - Task', '07 - Deprecation']"
6581,open,wolever,"Memory is leaked when an array contains a circular reference:

``` python
>>> import gc
>>> import sys
>>> import numpy as np
>>> class Circular(object): pass
...
>>> c = Circular()
>>> c.arr = np.array([c])
>>> del c
>>> gc.collect()
0
>>> gc.collect()
0
>>> print [ repr(o) for o in gc.get_objects() if type(o) == Circular ]
['<__main__.Circular object at 0x100e97f10>']
>>> print [ sys.getrefcount(o) for o in gc.get_objects() if type(o) == Circular ]
[2]
```

This is because `PyArray` doesn't implement `tp_traverse` (for fairly reasonable reasons)… but also leads to hard-to-track-down memory leaks.

At first pass it seems reasonable to implement a `tp_traverse` which only traverses if `dtype=object`… but that does have a performance tradeoff.
",2015-10-29 19:30:36,,Memory leak when array contains circular references,"['01 - Enhancement', 'component: numpy._core']"
6555,open,frobnitzem,"Numpy's any() (and all()) functions should work with generator expressions to be consistent with basic python.  I spent several hours trying to debug a case of this, which would work if I used a list, but not if I used a generator expression -- only to discover that it was because numpy's all() function was being used instead of the usual python one that works identically with both generators and lists.

``` python
lst = [0,1,2]

print any(i < 0 for i in lst) # False
from numpy import any
print any([i < 0 for i in lst]) # False
print any(i < 0 for i in lst) # <generator object <genexpr> at 0x1006bd500>
```
",2015-10-24 18:16:58,,any/all functions do not work with generators,['unlabeled']
6550,open,sdementen,"The following code explains the issue

```
>>> import numpy
>>> print numpy.__version__
1.10.1
>>> a = [[""a"", 1], [2, 3]]
>>> b = numpy.array(a)
>>> print repr(a)
[['a', 1], [2, 3]]
>>> print repr(b)
array([['a', '1'],
       ['2', '3']],
      dtype='|S1')
```

I would have expected that not specifying a dtype would have given the result as if I had specify a dtype(object) (to be able to hold both ints and strings)

```
>>> b = numpy.array(a, dtype=numpy.dtype(object))
>>> print repr(b)
array([['a', 1],
       [2, 3]], dtype=object)
```

as the doc http://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html describes 

dtype : data-type, optional
The desired data-type for the array. **If not given, then the type will be determined as the minimum type required to hold the objects in the sequence.** This argument can only be used to ‘upcast’ the array. For downcasting, use the .astype(t) method.

But I may have missed some point in the docs
",2015-10-22 13:47:10,,array(...) casts mix lists of int and string to string automatically (excepted dtype object),"['01 - Enhancement', 'component: numpy.dtype', '62 - Python API']"
6471,open,lgerardSRI,"Probably linked to #4955

Note that `dtype(('S`x`', ()))` with x different from 0 works perfectly
",2015-10-14 03:03:36,,"dtype(('S0', ())) raise unexpected exception",['unlabeled']
6395,open,andsor,"Hi @juliantaylor, it's me again. I encounter a similar issue as before (#6251) with the following code using now Python 3.4.3 (GCC 4.7.2) with numpy 1.9.3:

``` python
from __future__ import division, print_function, absolute_import

import numpy as np
import numpy.ma as ma

from pprint import pprint


a = ma.array(
    np.array(
        [0.25375743, 0.49557181, -0.32551902, -0.37675809, -0.04705212999999997]
#        [0.67696689, 0.91878127, 0.09769044, 0.04645137, 0.37615733]
    ),
    mask=np.array([False, False, False, False, False], dtype=np.bool)
)

pprint(a)

ma_squared = a**2
a = np.asarray(a)
a_squared = a**2

pprint(a_squared - ma_squared)
pprint(a_squared[0])
pprint(ma_squared[0])
```

The output is

```
masked_array(data = [0.25375743 0.49557181 -0.32551902 -0.37675809 -0.04705212999999997],
             mask = [False False False False False],
       fill_value = 1e+20)

masked_array(data = [ -1.38777878e-17   0.00000000e+00  -1.38777878e-17   0.00000000e+00
   0.00000000e+00],
             mask = False,
       fill_value = 1e+20)

0.064392833280204897
0.064392833280204911
```

numpy.show_config():

```
blas_opt_info:
    libraries = ['f77blas', 'cblas', 'atlas']
    language = c
    library_dirs = ['/usr/nld/atlas-3.8.4-skadi-seq/lib']
    include_dirs = ['/usr/nld/atlas-3.8.4-skadi-seq/include']
    define_macros = [('ATLAS_INFO', '""\\""3.8.4\\""""')]
openblas_info:
  NOT AVAILABLE
lapack_opt_info:
    libraries = ['lapack', 'f77blas', 'cblas', 'atlas']
    language = f77
    library_dirs = ['/usr/nld/atlas-3.8.4-skadi-seq/lib']
    include_dirs = ['/usr/nld/atlas-3.8.4-skadi-seq/include']
    define_macros = [('ATLAS_INFO', '""\\""3.8.4\\""""')]
atlas_3_10_blas_threads_info:
  NOT AVAILABLE
mkl_info:
  NOT AVAILABLE
atlas_blas_threads_info:
  NOT AVAILABLE
atlas_3_10_threads_info:
  NOT AVAILABLE
atlas_threads_info:
  NOT AVAILABLE
openblas_lapack_info:
  NOT AVAILABLE
lapack_mkl_info:
  NOT AVAILABLE
atlas_3_10_blas_info:
  NOT AVAILABLE
atlas_blas_info:
    libraries = ['f77blas', 'cblas', 'atlas']
    language = c
    library_dirs = ['/usr/nld/atlas-3.8.4-skadi-seq/lib']
    include_dirs = ['/usr/nld/atlas-3.8.4-skadi-seq/include']
    define_macros = [('ATLAS_INFO', '""\\""3.8.4\\""""')]
atlas_info:
    libraries = ['lapack', 'f77blas', 'cblas', 'atlas']
    language = f77
    library_dirs = ['/usr/nld/atlas-3.8.4-skadi-seq/lib']
    include_dirs = ['/usr/nld/atlas-3.8.4-skadi-seq/include']
    define_macros = [('ATLAS_INFO', '""\\""3.8.4\\""""')]
atlas_3_10_info:
  NOT AVAILABLE
blas_mkl_info:
  NOT AVAILABLE
```

The numpy tests pass, but this lets one scipy test fail, scipy/scipy#5197 .

Do you think it is related. This time I used the newer C compiler, though.
",2015-09-30 19:56:23,,masked_array ** 2 operation differs from array ** 2 operation on cluster architecture,"['00 - Bug', 'component: numpy.ma']"
6393,open,NeilGirdhar,"It would be nice to make `ndindex` an abc.Sequence so that it supports indexing (e.g., `np.ndindex(2,3)[3]`), `__contains__` and `__reversed__` efficiently.
",2015-09-30 10:13:30,,Add indexing to ndindex,['unlabeled']
6364,open,argriffing,"The notes section says ""The condition number of x is defined as the norm of x times the norm of the inverse of x"" but the implementation is more general -- it returns nontrivial values for non-square matrices. The fix would not be as simple as replacing 'inverse' with 'pseudoinverse' in the notes, because the implementation distinguishes between 'structural' rank vs. numerical rank.
",2015-09-25 18:19:13,,linalg.cond documentation is incomplete or misleading for non-square matrices,['unlabeled']
6339,open,reupen,"It would be great if `numpy.percentile` supported Decimal. Most numpy functions I've tried are fine with `numpy.array`s containing `Decimal`s, but not numpy.percentile.

``` python
from decimal import Decimal
import numpy as np
In[81]: x = np.array([[Decimal(1), Decimal(2)], [Decimal(1), Decimal(2)]])
In[82]: x
Out[82]: 
array([[Decimal('1'), Decimal('2')],
       [Decimal('1'), Decimal('2')]], dtype=object)
In[84]: np.min(x, axis=1)
Out[84]: array([Decimal('1'), Decimal('1')], dtype=object)
In[85]: np.median(x, axis=1)
Out[85]: array([Decimal('1.5'), Decimal('1.5')], dtype=object)
In[87]: np.std(x, axis=1)
Out[87]: array([Decimal('0.5'), Decimal('0.5')], dtype=object)
In[88]: np.mean(x, axis=1)
Out[88]: array([Decimal('1.5'), Decimal('1.5')], dtype=object)
In[86]: np.percentile(x, [50], axis=1)  # or np.percentile(x, [Decimal(50)], axis=1)
Traceback (most recent call last):
  File ""C:\Python34\lib\site-packages\IPython\core\interactiveshell.py"", line 3035, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-86-734677e766a8>"", line 1, in <module>
    np.percentile(x, [50], axis=1)
  File ""C:\Python34\lib\site-packages\numpy\lib\function_base.py"", line 3054, in percentile
    interpolation=interpolation)
  File ""C:\Python34\lib\site-packages\numpy\lib\function_base.py"", line 2803, in _ureduce
    r = func(a, **kwargs)
  File ""C:\Python34\lib\site-packages\numpy\lib\function_base.py"", line 3143, in _percentile
    x1 = take(ap, indices_below, axis=axis) * weights_below
TypeError: unsupported operand type(s) for *: 'decimal.Decimal' and 'float'
In[14]: y = np.array([[1, 2], [1, 2]])
In[17]: np.percentile(y, [50], axis=1)  # this is fine, as expected
Out[17]: array([[ 1.5,  1.5]])
```

I'm using Python 3.4.3 and numpy 1.9.3.

Thanks
",2015-09-22 12:26:48,,Support for Decimal in numpy.percentile,['defunct — difficulty: Intermediate']
6318,open,ahaldane,"I was looking through numpy ufuncs, and noticed `np.reciprocal` treats integer zero strangely:

```
>>> np.reciprocal(np.array(0))
-9223372036854775808
```

 Probably should return 0 with a warning, like `np.int32(1)/np.int32(0)`.  Thus for ints, reciprocal would return 1 if x==1, otherwise 0.
",2015-09-15 17:13:58,,np.reciprocal gives strange result for integer zero,['unlabeled']
6316,open,gauteh,"`convolve(a, v, mode)` swaps the input arguments `a` and `v` if `v` has greater length than `a`. 

When using the `mode = 'same'` the output is clipped to the size of `a`. When the arguments have been swapped you can end up with the output being the length of `v`, while I would expect to always get the same behaviour: when using 'same' the output should be the length of the first argument.

Foe example, when using an averaging filter on an input vector: if the input vector length (`a`) decreases to below the filter length (`v`) you will get the unexpected result of `y` having a greater length than the input (`a`).

```
a = np.random.randn(10,)
v = np.ones ((4,)) / 4

y = np.convolve (a, v, 'same')
```
",2015-09-15 07:24:19,,numpy.convolve() swap of input arguments are illogical when using 'same',['unlabeled']
6303,open,embray,"I was just poking over some old code I wrote, trying to understand why it existed.  It turned out to just be doing the equivalent of an `rsrtrip` on a string array.  However, I know I wanted to do it in-place (i.e. just replacing all trailing spaces with nulls), rather than make a copy of the array, which is unfortunately what you get from `np.char.rstrip()`.

I don't think it would be hard to add an optional `out=` argument to all the string functions in `np.char` so that they can output to an existing array (so long as that array has the correct dtype or can is castable in some way).  At the same time this would enable in-place string operations, since looking at the `_vec_string` function I don't think there's any reason the output array can't be the input array.

I'd be happy to work on a patch if this sounds worthwhile.
",2015-09-11 18:31:17,,Add optional out= argument to vectorized string functions; allow inplace string manipulations,['unlabeled']
6296,open,harm-devries,"In the python api there is support for multiple axes (i.e. a tuple of integers) in the max operator. However, the c-api only seems to support a single axis (i.e an integer); see http://docs.scipy.org/doc/numpy/reference/c-api.array.html. 

Is there an efficient implementation for dealing with multiple axes? Of course, I could use a for loop to go over the axes, but I am wondering if there something more efficient. 

Also, the argmax does not support multiple axes (not even in the python api). Is there going to be support in the future? Does anybody know an efficient way to deal with multiple axes?
",2015-09-09 21:56:38,,Support of multiple axes in max operator for c-api,['unlabeled']
6295,open,perimosocordiae,"I encountered the following while working on [scipy#5213](https://github.com/scipy/scipy/pull/5213):

``` python
In [1]: np.int64 is np.typeDict['longlong']
Out[1]: False

In [2]: set((np.int64, np.typeDict['longlong']))
Out[2]: {numpy.int64, numpy.int64}

In [3]: np.__version__
Out[3]: '1.11.0.dev0+60df9b1'
```

This has implications for things like using dtypes as keys in a dictionary, which is fairly common in the scipy unit tests.
",2015-09-09 18:02:56,,BUG: standard dtypes are not singletons,['unlabeled']
6287,open,larsmans,"GCC has a `__builtin_copysign` that it inlines, so call like `copysign(1, x)` get specialized. When using `npy_copysign`, that doesn't happen. It would be nice to conditionally

``` c
#define npy_copysign copysign
```

in `npy_math.h` so that the builtin gets used.
",2015-09-07 08:10:17,,npy_copysign should be a macro,"['01 - Enhancement', 'component: npy_math']"
6286,open,mikecroucher,"On 64 bit Windows, Anaconda Python:

```
np.log1p(1.7976931348622732e+308)
```

gives inf. On Linux and Mac it gives 709.78....
More detail at http://www.walkingrandomly.com/?p=5852 
",2015-09-05 15:07:10,,log1p gives different result in Windows compared to Linux/Mac for large argument,['unlabeled']
6282,open,okomarov,"As described in the [SO question](http://stackoverflow.com/questions/32188039/apply-digitize-to-ndarray-by-row) I wanted to apply `digitize()` by row, where the bins also change by row. 

In my application I have a  5000 by 9000 matrix and I want to digitize each row into the corresponding decile. It takes approx. 4-5 seconds and I can live with that, but maybe the enhancement is simple enough and could provide some significant speedup?

I can give it a stab, if you point me in the right direction, or let it go if you reckon the improvement might be negligible with respect to the solution I adopted in the SO question.
",2015-09-03 12:17:14,,ENH: digitize to accept bins same shape as x,['unlabeled']
6266,open,davidtrem,"`.imag`, `.real`,  `.angle` don't call `__array_wrap__` after their execution
Looks like a bug.
https://github.com/pydata/pandas/issues/10921#issuecomment-135846584
",2015-08-29 04:28:35,,".imag, .real, .angle don't call __array_wrap__ after their execution ","['00 - Bug', 'component: numpy._core', '57 - Close?']"
6248,open,anntzer,"This would allow calling `round(array)`.  While this would certainly be equivalent to `np.round(array)`, a similar feature is already there with `abs(array)` (which calls `__abs__`) and `np.abs(array)`.
",2015-08-26 07:56:16,,ENH: Implement __round__ special method for ndarrays,"['01 - Enhancement', 'component: numpy._core', 'triaged']"
6239,open,davidheff,"Consider the following commonly used pattern. We have a function with this signature:

```
void foo(double *array, int *length)
```

We call it twice. First to find the required array length, then again to populate the array.  The call sequence looks like this:

```
int length;
foo(NULL, &length);
double *array = malloc(length * sizeof *array);
foo(array, &length);
```

When we try to wrap this using numpy and ctypes we wish to use `ctypeslib.ndpointer` to create the `argtypes` specification for the array.  That allows us to pass the numpy object's memory to the external function and avoid unnecessary copying.

However, this fails.  We write:

```
lib.foo(None, length)
```

which produces an error of this nature:

> ctypes.ArgumentError: argument 1: : argument must be an ndarray

This could be solved if the type returned by `ctypeslib.ndpointer` had a `from_param` that handled being passed `None`. If instead of treating `None` as an error, `from_param` returned `None` in that scenario, this calling pattern would work out.

As an illustration, I can subclass the type returned by `ctypeslib.ndpointer` myself to support this calling pattern.

```
_DoubleArrayTypeBase = numpy.ctypeslib.ndpointer(dtype=numpy.float64, ndim=1,
    flags='C_CONTIGUOUS')

def _from_param(cls, obj):
    if obj is None:
        return obj
    return _DoubleArrayTypeBase.from_param(obj)

DoubleArrayType = type(
    'DoubleArrayType',
    (_DoubleArrayTypeBase,),
    {'from_param': classmethod(_from_param)}
)
```

It would be great if this could be handled by numpy direcrtly.

Reference: http://stackoverflow.com/q/32120178/505088
",2015-08-24 12:15:34,,ctypeslib.ndpointer should support passing None as the argument,"['01 - Enhancement', 'component: numpy.ctypes']"
6231,open,argriffing,"[Welford's algorithm](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm) computes the mean and variance in a way that pays attention to stability while using less memory overhead than the current numpy implementation of variance or standard deviation. See http://stackoverflow.com/questions/32135572, https://github.com/numpy/numpy/issues/1696, https://github.com/numpy/numpy/pull/3685.
",2015-08-21 15:19:46,,add Welford's algorithm for stable low-memory calculation of mean and variance,['unlabeled']
6230,open,argriffing,"This is just the standard math library.

``` python
>>> import numpy as np
>>> import math
>>> np.math is math
True
```

Related: https://github.com/numpy/numpy/pull/6103, https://github.com/numpy/numpy/issues/2448#issuecomment-21218184.
",2015-08-21 14:27:58,,deprecate np.math,['Priority: low']
6217,open,rainwoodman,"Looks like there is no easy way to achieve a stable reversed indirect sort with the sorting functions in numpy:

argsort()[::-1] would give a reversed indirect sort.

argsort(kind='mergesort')[::-1] does not give a stable reversed indirect sort. ([::-1] makes it unstable.

Would you consider including a 'reversed' parameter to argsort? 
",2015-08-18 20:56:07,,stable reversed indirect sort?,['01 - Enhancement']
6209,open,jreback,"These non-float `nansum's` are a little suspect
xref https://github.com/pydata/pandas/pull/10815

```
In [1]: np.__version__
Out[1]: '1.11.0.dev0+59bbc06'

In [2]: np.nansum(np.array([np.nan]))
Out[2]: 0.0

In [3]: np.nansum(np.array([np.nan],dtype='object'))
Out[3]: nan

In [4]: np.nansum(np.array([np.timedelta64('nat')]))
Out[4]: numpy.timedelta64('NaT')
```

<hr>

Summary 2019-05-03 by @seberg 

The second example now actually returns 0 as well, since objects use `arr != arr` to guess if NaNs exist. Timedelta NaT are still not considered NaN for `nanfuncs`, which may still be up for discussion.
",2015-08-14 19:21:30,,bug in nansum with non-float64 dtypes,"['00 - Bug', 'component: numpy.lib']"
6207,open,jhamman,"The current `datetime64` API only supports the standard Gregorian calendar.  In the climate science community (and presumably others), a number of other calendars systems are used, for example:
- [Julian Calendar](https://en.wikipedia.org/wiki/Julian_calendar)
- [No Leap Day Calendar](http://cfconventions.org/Data/cf-conventions/cf-conventions-1.6/build/cf-conventions.html#calendar):  Gregorian calendar without leap years, i.e., all years are 365 days long. 
- [All Leap Day Calendar](http://cfconventions.org/Data/cf-conventions/cf-conventions-1.6/build/cf-conventions.html#calendar):  Gregorian calendar with every year being a leap year, i.e., all years are 366 days long.

I'd like to propose adding calendar support to the `datetime64` API with the hope that changes here will trickle down into [pandas](https://github.com/pydata/pandas) and [xray](https://github.com/xray/xray).

I'm not sure exactly how I'd like to see the API and would like to hear from those familiar with `datetime64` on how it could be done.  Of course, the default calendar behavior should remain the Gregorian calendar.

xref: https://github.com/xray/xray/issues/521, https://github.com/xray/xray/pull/523, https://github.com/pydata/pandas/issues/7307
cc: @shoyer @rabernat @ocefpaf
",2015-08-14 17:17:55,,Non-standard Calendar Support,"['01 - Enhancement', '23 - Wish List', 'component: numpy.dtype']"
6205,open,anntzer,"I would like to suggest a few improvements to the functions in `np.lib.recfunctions`.  They are backwards-incompatible, but given that there are some suggestions (e.g. #5008) to move that functionality to the root namespace, perhaps at least the new functions in the root namespace can get the improvements.
- drop the `usemask` kwarg from all functions, and just return a masked array iff. the input array is masked (at the very least, the default should be changed to `False`).
- drop the `asrecarray` kwarg, and just return a recarray iff. the input is a recarray.
- drop the `rec_*` functions, which are AFAICT just duplicates of the functions without the `rec_` prefix (but with `asrecarray` set to True).
",2015-08-13 20:39:28,,recfunctions improvements,"['23 - Wish List', 'component: numpy.lib']"
6197,open,anntzer,"```
In [1]: np.median([])
/usr/lib/python3.4/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.
  warnings.warn(""Mean of empty slice."", RuntimeWarning)
Out[1]: nan
```

Oops.
[numpy 1.9.2]
",2015-08-12 05:07:06,,Confusing warning for median of empty array.,"['00 - Bug', 'component: numpy._core']"
6192,open,Zaharid,"In particular Python 3 scalars will refuse to be compared against a list while numpy will compare elementwise:

``` python
import numpy as np

l = [0.1]

arr = np.array([1,2,4])
n = np.dot(arr,arr)

print(n==21)

print(n > l)
print(21 > l)
```
",2015-08-11 09:27:44,,Pythnon 3 and numpy scalars have different comparison rules,"['15 - Discussion', 'component: numpy.dtype']"
6132,open,argriffing,"~~I think this is due to the `same_kind` casting in numpy 1.10, and~~ I don't know whether this is considered a bug.

``` python
>>> import numpy as np
>>> probs = np.array([0.4, 0.4, 0.2], dtype=np.float128)
>>> np.random.choice([1, 2, 3], p=probs)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""mtrand.pyx"", line 1087, in mtrand.RandomState.choice (numpy/random/mtrand/mtrand.c:11331)
TypeError: Cannot cast array data from dtype('float128') to dtype('float64') according to the rule 'safe'
```
",2015-07-29 22:52:45,,random.choice doesn't allow float128 probs,['component: numpy.random']
6114,open,sinhrks,"Assuming following inputs.

```
import numpy as np
np.__version__
'1.9.2'

i = np.array([1, 2, 3])
i
# array([1, 2, 3])

td = np.array([np.timedelta64(1, 'D'), np.timedelta64(2, 'D'), np.timedelta64(3, 'D')])
td
# array([1, 2, 3], dtype='timedelta64[D]')

dt = np.array([np.datetime64('2011-01-01'), np.datetime64('2012-01-01'), np.datetime64('2013-01-01')])
dt
# array(['2011-01-01', '2012-01-01', '2013-01-01'], dtype='datetime64[D]')
```

`iscomplex` return bool array against all inputs (OK).

```
np.iscomplex(dt)
# array([False, False, False], dtype=bool)

np.iscomplex(i)
# array([False, False, False], dtype=bool)

np.iscomplex(td)
# array([False, False, False], dtype=bool)
```

Otherwise, `isreal` returns bool scalar for `datetime64` array. Even though the [doc](http://docs.scipy.org/doc/numpy/reference/generated/numpy.isreal.html) says it should return ""Boolean array of same shape as x"".

```
np.isreal(i)
# array([ True,  True,  True], dtype=bool)

np.isreal(td)
# array([ True,  True,  True], dtype=bool)

np.isreal(dt)
# False
```
",2015-07-25 22:00:06,,Inconsistency between isreal and iscmplex for datetime64 array,"['00 - Bug', 'component: numpy.datetime64']"
6083,open,pbrod,"Consider the following examples:
The first few terms of taylor-series for arctan is given by

```
>>> import numpy as np
>>> def arctan_3(x):
...     return x-x**3/3+x**5/5
```

Then we compare the taylor-series with numpy.arctan for small complex number:

```
>>> x =  0.01+1je-14
>>> v1 = np.arctan(x); v1
(0.0099996666866652394+9.9920072216263095e-15j)

>>> v2 = arctan_3(x); v2
(0.009999666686666667+9.9990001e-15j)


>>> (v2-v1).imag/v2.imag
0.0006993577661521059
```

Lets compute the complex step derivative of arctan

```
>>> y = 0.01
>>> h = 1e-14
>>> np.arctan(y+1j*h).imag/h
0.99920072216263089
```

The exact value is

```
>>>1./(1+y**2) 
0.9999000099990001:

>>> arctan_3(1j*h).imag/h
0.9999000100000001
```

In both the examples the relative error of the complex part of numpy.arcsin function is around 1e-4.

It should be relatively easy to fix this as the complex part of arctan function is proportional to x when x is small.
",2015-07-15 15:27:01,,BUG: arctan inaccurate for small complex numbers,['unlabeled']
6082,open,pbrod,"Consider the following examples:
The first few terms of taylor-series for arcsin is given by

```
>>> import numpy as np
>>> def arcsin_3(x):
...     return x+x**3/6+3*x**5/40
```

Then we compare the taylor-series with numpy.arcsin for small complex number:

```
>>> x =  0.01+1je-14
>>> v1 = np.arcsin(x); v1
(0.010000166674167112+1.0103029524088975e-14j)

>>> v2 = arcsin_3(x); v2
(0.010000166674166668+1.00005000375e-14j)

>>> (v2-v1).imag/v2.imag
-0.010252435998650904
```

Lets compute the complex step derivative of arcsin

```
>>> y = 0.01
>>> h = 1e-14
>>> np.arcsin(y+1j*h).imag/h
1.0103029524088976
```

The exact value is 1./np.sqrt(1-0.01**2)=1.0000500037503126:

```
>>> arcsin_3(1j*h).imag/h
1.00005000375
```

In both the examples the relative error of the complex part of numpy.arcsin function is around 1e-2.

It should be relatively easy to fix this as the complex part of arcsin function is proportional to x when x is small.
",2015-07-15 15:12:23,,BUG: arcsin inaccurate for small complex numbers,['unlabeled']
6081,open,pbrod,"Consider the following examples:
The first few terms of taylor-series for arccos is given by

```
>>> import numpy as np
>>> def arccos_3(x):
...     return np.pi/2-x-x**3/6-3*x**5/40
```

Then we compare the taylor-series with numpy.arccos for small complex number:

```
>>> x =  1je-14
>>> v1 = np.arccos(x); v1
(1.5707963267948966-9.9920072216263584e-15j)

>>> v2 = arccos_3(x); v2
(1.5707963267948966-1e-14j)

>>> (v2-v1).imag/v2.imag
0.00079927783736416112
```

Lets compute the complex step derivative of arcos

```
>>> y = 0
>>> h = 1e-14
>>> np.arccos(y+1j*h).imag/h
-0.99920072216263589
```

The exact value is -1:

```
>>> arccos_3(1j*h).imag/h
-1.0
```

In both the examples the relative error of the complex part of numpy.arccos function is around 1e-3.

It should be relatively easy to fix this as the complex part of arccos function is proportional to x when x is small.
",2015-07-15 14:40:50,,BUG: arccos inaccurate for small complex numbers,['unlabeled']
6079,open,ocehugo,"Currently np.insert do not follow the masked rules of a np.ma.Maskedarray.

How to Reproduce:

```
import numpy as np
a=np.ma.masked_array([1,0],[0,1])
b=np.insert(a,len(a),np.ma.masked_array([0,1],[1,0]),axis=0)
len(b.mask)

```

outputs:

`object of type 'numpy.bool_' has no len()`

So the insert function doens't take the mask in account even if the input is masked. The same behaviour occurs when input is not masked, so the operation causes a lost of the masking information ( it would be desirable that even in this case the output would be at least not masked and the operation would preserve the mask information).

I believed that the mask information would be passthrough to the new array.
",2015-07-15 05:18:14,,support masked arrays for np.insert,"['01 - Enhancement', 'component: numpy.ma']"
6070,open,ChrisBarker-NOAA,"NOTE: as discussed at SciPy2015 -- just thought I'd get an issue in.

numpy currently will default to making an dtype-object array for anything it can't make other sense of. 

This is hardly ever what someone would want. For example, I just banged my head against a bug due to this:

In [5]: np.array(object)
Out[5]: array(<type 'object'>, dtype=object)

Granted, due to a stupid typo -- but why would anyone want this?

When a user does want the object dtype, numpy almost never can get it right by default anyway, so it's not much burden to have to specify the object dtype if you really want that.
",2015-07-12 20:58:37,,Please Deprecate creation of numpy arrays for arbitrary objects.,['unlabeled']
6061,open,levitsky,"With `numpy 1.9.2` I see quite counter-intuitive behavior of `searchsorted` on integer arrays with a `str` needle:

``` python
In [1]: import numpy as np

In [2]: a = np.arange(1000, dtype=int)

In [3]: a.searchsorted('15')
Out[3]: 150

In [4]: a.searchsorted('150')
Out[4]: 150

In [5]: a.searchsorted('1500')
Out[5]: 151

In [6]: a.searchsorted('foo')
Out[6]: 1000
```

The main reason why I believe this behavior is broken is because when one converts an integer array to strings, it's not sorted anymore.
I believe that the least surprising result would be an exception, especially because this is what happens if the array contains floats and not integers.

P.S. I initially asked about this on [StackOverflow](http://stackoverflow.com/q/31325001/1258041), and @jaimefrio suggested creating a bug report in the comments.
",2015-07-10 17:05:47,,Calling searchsorted on an int haystack with str needle converts haystack to str,"['00 - Bug', 'component: numpy._core']"
6056,open,matthew-brett,"I did not expect this behavior:

```
In [2]: import numpy as np
In [3]: np.__version__
Out[3]: '1.10.0.dev0+b1a7d4c'
In [4]: np.prod((500, 500, 500, 500))
Out[4]: -1924509440
```

I assume this is because:

```
In [8]: np.array((500, 500, 500, 500)).dtype
Out[8]: dtype('int32')
```

Is there a good reason not to default to int64 int casting on 32 bit platforms?  Is the speed difference large enough to justify this risk?  Or is it the memory consumption?
",2015-07-08 22:16:23,,Using C int default leads to overflow on 32-bit platforms,['unlabeled']
6051,open,evertrol,"(This may be a duplicate issue, but so far I've only found few somewhat related issues, e.g. #3161 , #2500 and perhaps #1774 and #3311.)

When creating an array of a single (non-list) element, the result is a 0-dimensional array:

```
>>> a = np.array(1)
>>> type(a), a.shape, a.ndim
(<class 'numpy.ndarray'>, (), 0)
```

When applying a numpy function to `a`, the result is a numpy scalar instead:

```
>>> b = np.cos(a)
>>> type(b), b.shape, b.ndim
(<class 'numpy.float64'>, (), 0)
```

(or one can simply multiply by an int:

```
>>> b = 100 * a
>>> type(b), b.shape, b.ndim
(<class 'numpy.int64'>, (), 0)
```

)
This effectively changes the type, and options available to the result. For example, one can assign to an indexed 0-dimensional array, but not to an indexed numpy scalar:

```
>>> a[True] = 5
>>> b[True] = 5
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: 'numpy.float64' object does not support item assignment
```

Of course, indexing a scalar is awkward, but it is consistent if one doesn't known whether the input is a (Python) float or int (to which (as)array has been applied), a list, a one element array `[1]` or a 1 or higher-dimensional array: code written would not need to distinguish between a scalar and an array.

My workaround for now is to use `asarray`:

```
>>> b = np.asarray(b)
>>> b[True] = 5
```

but I wonder if there is a better way.

Moreover, how consistent should the return type of numpy functions be with the input type?

Apologies if this topic has come up before, and may have been discussed to dead, but there was relatively little I could find that appears to address exactly this issue.
",2015-07-07 04:22:31,,Numpy functions return a scalar when the input is a 0-dimensional array,['unlabeled']
6044,open,njsmith,"I just found myself writing:

```
points_on_path = ...
point_to_point_distances = np.hypot(points_on_path[1:, :], point_ons_path[:-1, :])
arclengths = np.cumsum(point_to_point_distances)
arclengths = np.concatenate(([0], arclengths)
# now arclengths.shape == points_on_path.shape
```

and it occured to me that this is definitely not the first time I've found myself doing the awkward concatenate-a-zero dance in order to get a cumsum array whose shape matches what I want. Maybe we should have a convenience argument to `cumsum` (and to `ufunc.accumulate` in general I guess) to insert the operation's identity as the first argument.

No brain to figure out the details of how this would work right now, but filing a bug to remind me/us...
",2015-07-04 21:54:33,,cumsum (and accumulation operators generally?) should have an option to leave the identity in the first spot,"['01 - Enhancement', '23 - Wish List', 'component: numpy.ufunc']"
6040,open,Kambrian,"When dealing with weights with a large dynamic range, the result is problematic. The problem can be reproduced with this code in numpy 1.9.1 and 1.9.2:

``` python
sample_size=1e5
xmin,xmax=-10.,0.
data=np.random.rand(sample_size)*(xmax-xmin)+xmin #some uniform data
weight=np.exp(-4*data) #weights

y,x=np.histogram(data,50,weights=weight)
print y[-1]
print np.sum(weight[(data>x[-2])&(data<x[-1])])
```

This should print two identical numbers (which is the case if, for example, we use `xmin=-1.`) but instead it produces:

```
0.0
2991.44627547
```

This is not a datatype issue because every variable is `float64`. Replacing the bin number parameter with double precision bins does not help.
",2015-07-03 16:52:47,,numpy histogram precision bug,"['00 - Bug', 'component: numpy.lib']"
6027,open,richardotis,"I am trying to solve several independent systems of equations at the same time using `numpy.linalg.solve`, e.g., `a` has shape (N, M, M). The challenge I'm running into is how to deal with the case when, for some values of N, the last two dimensions comprise a singular matrix.

Consider this example

``` python
import numpy as np
arr = [[[1, 0], [0, 1]], [[1, 0], [0, 1]], [[0, 0], [0, 0]]]
np.linalg.solve(arr, 1)
```

On numpy 1.9.2, this will raise a `LinAlgError`, but I still would like to know the solutions for all the full-rank matrices. (I would be okay with getting back NaNs for the singular cases.) I'm aware I can compute the singular values and use fancy indexing to slice the array, but in my algorithm I have to do several slicing/filtering steps and I would prefer not to lose the alignment of `a` with `b` -- my real-world `a` typically has 6-7 dimensions and all the indexing arrays make the code hard to follow.

My ideal solution seems to involve masked arrays since I end up doing several other filtering steps, but it appears that most (all?) functions in the `linalg` family ignore array masks.

Is there an alternative solution already available, or perhaps a suggestion on how I could contribute a solution that I could package as a PR?
",2015-06-30 00:06:00,,Handling ndarrays with singular matrices in linalg functions,['unlabeled']
6025,open,pitrou,"Looking at http://docs.scipy.org/doc/numpy/reference/c-api.html it doesn't seem to be documented which macros and/or magic values to check for the availability of certain APIs (say, for example, I want to access the datetime C APIs, but also provide some compatibility with Numpy 1.6). It would be nice to document that somewhere.
",2015-06-29 13:36:59,,Document versioning of C API,"['00 - Bug', 'component: documentation']"
6010,open,doutriaux1,"``` python
import gc
gc.enable()
gc.set_debug(gc.DEBUG_LEAK)
import sys  
def runTest():
    import numpy
    import numpy.ma
    a = numpy.ma.array([1,2,34,5,7.])
    b = numpy.ma.masked_equal(a,0.)

result = runTest()
gc.collect()
for i,g in enumerate(gc.garbage):
      print ""Leak %d: %s @0x%x:\n   %s""%(i, type(g), id(g), str(g)[:70])
```

leads to:

```
gc: collectable <tuple 0x7f19e59bd650>
gc: collectable <StgDict 0x7f19e59d43a0>
gc: collectable <_ctypes.PyCArrayType 0x24cb910>
gc: collectable <getset_descriptor 0x7f19e59be5f0>
gc: collectable <getset_descriptor 0x7f19e59be638>
gc: collectable <tuple 0x7f19e59c34c8>
gc: collectable <cell 0x7f19e9585e18>
gc: collectable <tuple 0x7f19e9587210>
gc: collectable <function 0x7f19e5978488>
Leak 0: <type 'tuple'> @0x7f19e59bd650:
   (<type '_ctypes.Array'>,)
Leak 1: <type 'StgDict'> @0x7f19e59d43a0:
   {'__module__': 'numpy.ctypeslib', '__dict__': <attribute '__dict__' of
Leak 2: <type '_ctypes.PyCArrayType'> @0x24cb910:
   <class 'numpy.ctypeslib.c_int_Array_1'>
Leak 3: <type 'getset_descriptor'> @0x7f19e59be5f0:
   <attribute '__dict__' of 'c_int_Array_1' objects>
Leak 4: <type 'getset_descriptor'> @0x7f19e59be638:
   <attribute '__weakref__' of 'c_int_Array_1' objects>
Leak 5: <type 'tuple'> @0x7f19e59c34c8:
   (<class 'numpy.ctypeslib.c_int_Array_1'>, <type '_ctypes.Array'>, <typ
Leak 6: <type 'cell'> @0x7f19e9585e18:
   <cell at 0x7f19e9585e18: function object at 0x7f19e5978488>
Leak 7: <type 'tuple'> @0x7f19e9587210:
   (<cell at 0x7f19e9585e18: function object at 0x7f19e5978488>,)
Leak 8: <type 'function'> @0x7f19e5978488:
   <function _recursive_mask_or at 0x7f19e5978488>
```
",2015-06-24 00:34:37,,numpy.ma.masked_equal seems to leak,"['00 - Bug', 'component: numpy.ma']"
5976,open,dtheodor,"Hi,

`numpy.load` with memory-mapped file support is great, however the argument `mmap_mode` is silenty ignored when the file being loaded is anything else other than a `.npy` file. I need to combine `numpy.savez_compressed` with loading directly to mem-mapped files.

`numpy.load` returns an `NpzFile` when `.npz` files are loaded.  `NpzFile` is hard-coded to load data directly to numpy arrays. Is it a possibility to pass the `mmap_mode` argument to the initialization of the `NpzFile` and have it restore arrays to a memory-mapped file?
",2015-06-17 15:47:26,,NpzFile and memmap arrays,['unlabeled']
5974,open,barronh,"Unable to set fill value for string instantiated with 'c' char, but no problem with string instantiated with 'S1'.

Applies to version '1.9.2' and the current code in the repository.

To reproduce, run the following lines of code. The first passes set_fill_value passes while the second fails.

```
import numpy as np
np.ma.zeros((2,), dtype = 'S1').set_fill_value('N')
np.ma.zeros((2,), dtype = 'c').set_fill_value('N')
```

```
TypeError: Cannot set fill value of string with array of dtype |S1
```

The problem originates in lines 424 to 426 in numpy/ma/core.py copied below

```
        if isinstance(fill_value, basestring) and (ndtype.char not in 'OSVU'):
            err_msg = ""Cannot set fill value of string with array of dtype %s""
            raise TypeError(err_msg % ndtype)
```

When a 1 length string dtype is instantiated with 'c', the char is 'c'. When a 1 length string type is instantiated with 'S1', the char is 'S'.  Otherwise the dtypes are the same.

The char value however is not correctly identified as a string on line 424.

If this makes sense, I recommend adding c to the OSVU or using the kind property of dtype.
",2015-06-17 02:00:28,,Unable to set fill_value for 'c' type,"['00 - Bug', 'component: numpy.ma']"
5972,open,jerasman,"# doesnt work

v= numpy.full(shape, [], dtype='object')
# works

v=numpy.empty(shape, dtype='object')
v.fill([])
",2015-06-16 12:00:36,,BUG: numpy.full does not fill all types of data,['unlabeled']
5934,open,toddrjen,"It is currently relatively complicated to create a structured or record array from a `dict`.  This simplest approach I can find, `numpy.rec.fromrecords`, still requires a fair amount of boilerplate:

```
data={1: [1,2,3], 2:[4,5,6]}
np.rec.fromrecords(list(data.values()), names=list(data.keys()))
```

I suggest allowing `numpy.rec.fromrecords` to allow using a dict directly:

```
data={1: [1,2,3], 2:[4,5,6]}
np.rec.fromrecords(data)
```

Alternatively, there could be separate convenience functions to construct record arrays and structured arrays from dicts or sequences of key/value pairs.
",2015-06-02 10:12:37,,ENH: rec.fromrecords accept dict,"['01 - Enhancement', '23 - Wish List']"
5933,open,toddrjen,"As discussed in issue #5303, currently it is not possible to create arrays of object dtype containing equal-length sequences, since the sequence is automatically read in as array elements.  There is a suggestion to only do this for lists, but this would be a major backwards compatibility break and would require a long deprecation period.

Another approach would be to have a function explicitly for creating arrays with an object dtype.  Perhaps this could be called ""objectarray"".  The default for this function would be to take in a sequence, and consider each element of the sequence as an element in a 1D object array.

The function, however, could have an optional ""ndim"" or ""depth"" argument, that could be used to specify how many levels of the sequence should be considered part of the array.  This would default to 0 (only the outermost level is considered).  This would raise an exception if the dimensions don't match.  

Note that this approach is not mutually exclusive with the alternative, but has the advantage that it wouldn't break backwards-compatibility.

So for example:

```
>>> arr = objectarray([((1, 2, 3), (4, 5, 6)), ((7, 8, 9), (10, 11, 12))])
>>> arr
array([((1, 2, 3), (4, 5, 6)), ((7, 8, 9), (10, 11, 12))], dtype=object)
>>> arr.shape
(2,)

>>> arr = objectarray([((1, 2, 3), (4, 5, 6)), ((7, 8, 9), (10, 11, 12))], depth=1)
>>> arr
array([[(1, 2, 3), (4, 5, 6)],
       [(7, 8, 9), (10, 11, 12)]], dtype=object)
>>> arr.shape
(2, 2)

>>> arr = objectarray([((1, 2, 3), (4, 5, 6)), ((7, 8, 9), (10, 11, 12))], depth=2)
>>> arr
array([[[1, 2, 3],
        [4, 5, 6]],

       [[7, 8, 9],
        [10, 11, 12]]], dtype=object)
>>> arr.shape
(2, 2, 3)
```
",2015-06-02 09:53:56,,Enh: Object array creation function,['unlabeled']
5895,open,pv,"Consider this:

```
>>> from scipy.special import sph_harm
>>> sph_harm.types
['llff->F', 'lldd->D', 'ffff->F', 'dddd->D']
>>> sph_harm(0,[0],0,0).dtype
dtype('complex64')   # <- uh-oh
>>> sph_harm(0,[0],0,0.0).dtype
dtype('complex128')
>>> sph_harm(0,0,0,0).dtype
dtype('complex128')
>>> sph_harm(0,[0],[0],0).dtype
dtype('complex128')
```

Apparently, the ufunc loop selector prefers to cast integers to float32 in this case, which is somewhat surprising. This may have to do with the fact that given an integer array as one of the first two arguments, there are still two possible loop choices, and deciding which to use would need considering appropriate scalar casting rules for the last two arguments. Note that if one of the last arguments is an array instead of a scalar, the dtype decision is as expected, so maybe this is an issue with scalar special cases.
",2015-05-20 08:08:07,,float32 has priority over float64 for scalar integer ufunc inputs with other arrays,['unlabeled']
5894,open,mosco,"As I understand, the numpy.maximum and numpy.minimum functions do not support sparse arrays. However, they fail _quietly_, producing erroneous output when given a scipy.sparse.dok_matrix:

```
>>> import numpy
>>> from scipy.sparse import dok_matrix
>>> numpy.maximum(dok_matrix([[1,0]]), dok_matrix([[0,1]])).toarray()
75: array([[0, 1]])
```

When given a scipy.sparse.csr_matrix, these functions fail with an exception, but the error is not clear:

```
>>> import numpy
>>> from scipy.sparse import csr_matrix
>>> numpy.maximum(csr_matrix([[1,0]]), csr_matrix([[0,1]])).toarray()
Traceback (most recent call last):
  File ""<pyshell#169>"", line 1, in <module>
    numpy.maximum(csr_matrix([[1,0]]), csr_matrix([[0,1]])).toarray()
  File ""/home/amitmo/virtualenv/local/lib/python2.7/site-packages/scipy/sparse/base.py"", line 183, in __bool__
    raise ValueError(""The truth value of an array with more than one ""
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().
>>> 
```

I think an exception should be raised if the data type is not supported.
",2015-05-19 14:44:40,,numpy.maximum / numpy.minimum fail quietly on sparse matrices,['unlabeled']
5867,open,anntzer,"Two quick suggestions:
- [ ] add an `inplace={False|True}` (or perhaps just `out`, like for the ufuncs).
- [x] support shifting along multiple axes simultaneously: either allow `shift` to be an array-like of length equal to the array's ndim, or allow passing two sequences, `shift` and `axis`, of the same length (or allow both).
",2015-05-11 18:56:47,,np.roll improvements,['01 - Enhancement']
5842,open,jaimefrio,"Complete the deprecations started in #5840 and #5841.
",2015-05-06 03:45:50,,MANT: Raise an error for 'O4' and 'O8' dtype descriptors,['03 - Maintenance']
5835,open,jg-you,"cov() uses a bias of 1 by default.
var() uses a bias of 0 by default.

Such that

``` python
import numpy as np

x = np.random.rand(100)

if np.isclose(np.cov([x,x])[0,0], np.var(x)):
    print(""Consistent by default."")

if np.isclose(np.cov([x,x],ddof=0)[0,0], np.var(x,ddof=0))
    print(""Consistent."")
```

will only print the second line.
",2015-05-04 17:46:16,,numpy.cov() and numpy.var() default bias are inconsistent (numpy 1.9.2),"['00 - Bug', 'component: numpy.lib']"
5806,open,ahaldane,"Whenever a MaskedArray method returns a scalar value but that value should be masked, all MaskedArray methods return a reference to a singleton instance `np.ma.masked` of type `MaskedConstant`. I think the motivation was to be able to write code like 'if result is masked:'. However, returing a singleton causes some problems that make it difficult to use np.ma.MaskedArray as a 'drop in' replacement for ndarray.

Here is a summary of the problems I see, in the hope that they can be fixed.
#### 1. Many operations involving `masked` are coerced to float

Probably the worst issue is that `masked` is of type `float`. This means the return value of a method may be of a different type than the original array. This is especially bad for boolean arrays. For example, if arr is a boolean array, but all or any return a masked value, the following line will fail since you cannot do `~` to a float.

```
>>> a = np.ma.array([True, True], mask=[True, True])
>>> ~a.all()
```

It also means that certain series of operations on masked arrays will sometimes get cast to float when they wouldn't be with ndarrays.
#### 2. Overwriting `masked` causes strange results in completely separate code

A less serious problem arises if someone tries to assign to the return value of a MaskedArray method, which would end up assigning to the singleton. That will then affect code anywhere that involves the masked singleton. I came across this when one numpy unit test would modify the singleton, and then another would read it, and I would get an error depending on the order the unit tests were run. The problem arises in case like `marr2[:] = marr1.method()` if the method returns `masked`. This means marr2 will get filled with arbitrary gargage, but maybe that's not a problem since those values will be masked garbage. (Although, it was a somewhat confusing bug to fix).
#### 3. Code acting on a return value of a MaskedArray method can fail (if `masked` was returned)

Consider some code of the form

```
>>> result = arr.sum()
>>> dosomething(result)
```

This might work fine most of the time, but fail in the (possibly rare) case that the sum returns the singleton. It might be that the operation is not allowed on `np.ma.masked`, or it might be that further use of `np.ma.masked` wouldn't work as before.

Most cases of 'dosomething' I checked seem OK, but here are some that cause problems:
##### a) What if someone decides to remove the mask on a return value? Eg

```
>>> result = arr.sum()
>>> result.mask = False
```

if arr.sum happened to return the masked singleton, this would cause havoc. @rgommers suggested making `.mask` readonly which sounds like a good idea to me, although it means the code will generally run fine for most arr but will raise an error in the possibly rare case the sum is fully masked. 
##### b) writing to a scalar

Consider

```
>>> result[()] = 6
```

which would be fine for ndarrays, but raises an error for masked arrays if result is masked (though it's hard to imagine a case where someone would want to index a numpy scalar this way). 
##### c) passing `masked` as the `out` parameter of a ufunc

```
>>> np.ma.log(inputarr, out=result)
```

I think (though there are other bugs involved here) that using a variable which might be `np.ma.masked` as the out parameter to a ufunc will cause problems. 
",2015-04-27 18:07:48,,mp.ma.masked singleton causes difficulties,"['00 - Bug', 'component: numpy.ma', '54 - Needs decision']"
5795,open,mathause,"np.average allows to average over multiple axis at once but np.ma.average does not. Compare:

```
import numpy as np
a = np.random.rand(4, 4, 4)
np.average(a, axis=(1, 2))
np.ma.average(a, axis=(1, 2))
```
",2015-04-24 13:12:50,,np.average vs np.ma.average for axis tuple,"['01 - Enhancement', 'component: numpy.ma']"
5782,open,jaimefrio,"When calling `np.lexsort` on multidimensional arrays with a negative `axis` argument, the `axis` gets changed to the one with the smallest stride of the first `keys` array:

```
>>> a = np.random.rand(3, 2)
>>> b = np.random.rand(2, 3).T
>>> np.lexsort([a, b], axis=0)
array([[1, 0],
       [2, 1],
       [0, 2]])
>>> np.lexsort([a, b], axis=1)
array([[1, 0],
       [0, 1],
       [0, 1]])
# All calls with axis < 0 return the same as axis=1
>>> np.lexsort([a, b], axis=-2)
array([[1, 0],
       [0, 1],
       [0, 1]])
>>> np.lexsort([a, b], axis=-1)
array([[1, 0],
       [0, 1],
       [0, 1]])
>>> np.lexsort([a, b], axis=-100)
array([[1, 0],
       [0, 1],
       [0, 1]])
```

Using `keys=[b, a]` results in all calls with negative `axis` returning the same as with `axis=0`, for this is the smallest strided axis of `b`:

```
>>> np.lexsort([b, a], axis=-100)
array([[2, 0],
       [1, 1],
       [0, 2]])
>>> np.lexsort([b, a], axis=0)
array([[2, 0],
       [1, 1],
       [0, 2]])
```

This seems to be due to the initialization of iterators over each of the `keys` arrays before bounds-checking the axis parameter, [here](https://github.com/numpy/numpy/blob/f1f9e14e5d63aee6c8309ad425e3928192c0873b/numpy/core/src/multiarray/item_selection.c#L1441), which for negative `axis` results in resetting `axis` to the minimum-strided one, according to the [docs](https://docs.scipy.org/doc/numpy-1.9.0/reference/c-api.array.html#c.PyArray_IterAllButAxis).
",2015-04-22 13:45:45,,Wrong lexsort with negative axis,['unlabeled']
5780,open,pitrou,"On Linux:

```
>>> np.round(-0.5)
-0.0
```

On Windows:

```
>>> np.round(-0.5)
0.0
```

Note that Python's round() gets it right:

```
>>> round(-0.5, 0)
-0.0
```
",2015-04-22 13:14:48,,np.round() always returns positive zero on Windows,['unlabeled']
5776,open,estansifer,"Minimal example code:

```
In [1]: import numpy as np
In [2]: import numpy.ma as ma
In [3]: import numpy.version
In [4]: numpy.version.full_version
Out[4]: '1.9.2'
In [5]: x = np.zeros((2,), dtype = [('a', int), ('b', int)])
In [6]: y = ma.array(x)
In [7]: ma.allequal(y, y)
---------------------------------------------------------------------------
MaskError                                 Traceback (most recent call last)
<ipython-input-7-5ad9932e8937> in <module>()
----> 1 ma.allequal(y, y)

/usr/lib64/python3.4/site-packages/numpy/ma/core.py in allequal(a, b, fill_value)
   6924         y = getdata(b)
   6925         d = umath.equal(x, y)
-> 6926         dm = array(d, mask=m, copy=False)
   6927         return dm.filled(True).all(None)
   6928     else:

/usr/lib64/python3.4/site-packages/numpy/ma/core.py in array(data, dtype, copy, order, mask, fill_value, keep_mask, hard_mask, shrink, subok, ndmin)
   5881     return MaskedArray(data, mask=mask, dtype=dtype, copy=copy, subok=subok,
   5882                        keep_mask=keep_mask, hard_mask=hard_mask,
-> 5883                        fill_value=fill_value, ndmin=ndmin, shrink=shrink)
   5884 array.__doc__ = masked_array.__doc__
   5885 

/usr/lib64/python3.4/site-packages/numpy/ma/core.py in __new__(cls, data, mask, dtype, copy, subok, ndmin, fill_value, keep_mask, hard_mask, shrink, **options)
   2722                     msg = ""Mask and data not compatible: data size is %i, "" + \
   2723                           ""mask size is %i.""
-> 2724                     raise MaskError(msg % (nd, nm))
   2725                 copy = True
   2726             # Set the mask to the new value

MaskError: Mask and data not compatible: data size is 1, mask size is 2.
```

Python 3.4.3
NumPy 1.9.2

Thanks!
",2015-04-21 07:11:30,,numpy.ma.allequal raises MaskError on structured arrays,"['00 - Bug', 'component: numpy.ma']"
5739,open,abalkin,"I am opening a new issue to follow up on the discussion started at gh-5709.

> Why not add an out to the function also? (@charris)

In the past, it was considered safe to give ma methods additional arguments that were not present in ndarray prototypes.  It turned out that was not a good idea.  Consider what happened to the `numpy.dot`
function.   Not being a ufunc, it did not originally have an `out` argument, so when a matching method was impemented in ma, it was considered safe to have `ma.dot(a, b, strict=False)`, but when `out=None`
was later added to `numpy.dot` and people started using the third argument in `numpy.dot`, it became impossible to drop in `ma.dot` in expressions like `numpy.dot(a, b, out)`.

Now, no matter what we do to `ma.dot`, it is not possible to simultaneously have backward compatibility and drop-in substitution.

What we can probably do is to deprecate use of strict as a positional argument.  For example, we can change `ma.dot` signature to   `ma.dot(a, b, *args, out=None, strict=False)` and issue a deprecation warning whenever args is non-empty.  After a deprecation period, we can probably start supporting `ma.dot(a, b, out)`. 
",2015-04-02 01:57:47,,ENH:  Unify ma.dot and MaskedArray.dot interfaces,"['01 - Enhancement', 'component: numpy.ma']"
5738,open,abalkin,"Ideally, I would like to see .filled() and .compressed() return MaskedArray by default, but this is probably not possible for backward compatibility reasons.  (Although MaskedArray is derived from ndarray, there are way too many cases where masked arrays cannot be used _instead_ of ndarrays.)

The motivation for this feature is that when working with masked arrays, it is important that all operations are performed as masked, but if you have an expression like

```
x + 1/(1 + y.filled(0))
```

the division will be performed as ndarray division and infinities won't be masked.  This is a very common mistake and a work-around is rather verbose:

```
x + 1/(1 + ma.asarray(y.filled(0)))
```

With the proposed feature, one will be able to write

```
x + 1/(1 + y.filled(0, ma=True))
```
",2015-03-30 17:59:17,,ENH: Add ma=False option to filled and compressed methods of MaskedArray,"['01 - Enhancement', 'component: numpy.ma']"
5735,open,abalkin,"Compare the following two behaviors:

```
In [11]: type(np.float64(0).view(ma.MaskedArray))
Out[11]: numpy.float64

In [12]: type(np.array(0).view(ma.MaskedArray))
Out[12]: numpy.ma.core.MaskedArray
```

I believe `type(x.view(ma.MaskedArray)` should always be `ma.MaskedArray` regardless of what `x` is.
",2015-03-29 18:45:03,,The view method does not work with numpy scalars ,['unlabeled']
5721,open,jaimefrio,"I have taken a close look at the `flags` attribute of `ndarray`s, and have found several inconsistencies with the C API definitions, one of which is almost certainly a bug. To set things up for the discussion, here's a list of the attributes of `arr.flags`, the flags that they check for, and the equivalent C function/macro:
- `contiguous` and `c_contiguous`
  - checks for `NPY_ARRAY_C_CONTIGUOUS`
  - C equivalent: `PyArray_ISCONTIGUOUS` / `PyArray_IS_C_CONTIGUOUS`
- `fortran` and  `f_contiguous`
  - checks for `NPY_ARRAY_F_CONTIGUOUS`
  - C equivalent: `PyArray_IS_F_CONTIGUOUS`
- `updateifcopy`
  - checks for `NPY_ARRAY_UPDATEIFCOPY`
  - C equivalent: `---`
- `owndata`
  - checks for `NPY_ARRAY_OWNDATA`
  - C equivalent: `---`
- `aligned`
  - checks for `NPY_ARRAY_ALIGNED`
  - C equivalent: `PyArray_ISALIGNED`
- `writeable`
  - checks for `NPY_ARRAY_WRITEABLE`
  - C equivalent `PyArray_ISWRITEABLE`
- `behaved`
  - checks for `NPY_ARRAY_BEHAVED`
  - C equivalent: `PyArray_ISBEHAVED` (also checks dtype is NBO)
- `carray`
  - checks for `NPY_ARRAY_CARRAY`
  - C equivalent: `PyArray_ISCARRAY` (also checks dtype is NBO)
- `forc`
  - checks for `NPY_ARRAY_F_CONTIGUOUS || NPY_ARRAY_C_CONTIGUOUS`
  - C equivalent `PyArray_ISONESEGMENT`
- `fnc`
  - checks for `NPY_ARRAY_F_CONTIGUOUS && !NPY_ARRAY_C_CONTIGUOUS`
  - C equivalent: `PyArray_ISFORTRAN`
- `farray`
  - `(NPY_ARRAY_ALIGNED || NPY_ARRAY_WRITEABLE || NPY_ARRAY_F_CONTIGUOUS) && !NPY_ARRAY_C_CONTIGUOUS`
  - C equivalent: `---`

Based on the above, here's what I would like to change:
- The one that's obviously wrong is `farray`, which rather than checking for the array being aligned **and** writeable **and** Fortran contiguous, checks if it is aligned **or** writeable **or** Fortran contiguous. It also checks that it is not C contiguous, a check that the C macro `PyArray_ISFARRAY` does not do. **I would like to make it behave the same as `PyArray_ISFARRAY`.**
- `fortran` is inconsistent with the C behavior. I believe it should behave as `PyArray_ISFORTRAN`, and hence be the same as `fnc`, not the same as `f_contiguous`. **I would like to make that change, but understand we may be stuck with the current behavior on account of backwards compatibility.**
- The object returned by `arr.flags` holds a reference to `arr`, as well as a local copy of the array's flags at the moment of creation. All of the checks are now carried out on the local copy. This is wrong, as discussed [in this thread](http://mail.scipy.org/pipermail/numpy-discussion/2015-March/072563.html). **There are two things I would like to change here:**
  - Do not use the local copy if the array is available.
  - Use the C functions/macros directly on the array, rather than equivalent implementations. This would create some subtle changes in behavior, e.g. `behaved` and `carray` would start checking that the dtype of the array be in native byte order.
",2015-03-26 07:39:37,,Inconsistent behavior in the flags attribute of ndarray,['unlabeled']
5718,open,astrofrog,"At the moment arrays can't be initialized from dictionary values in Python 3:

```
In [1]: import numpy as np

In [2]: d = {'a':1,'b':2,'c':3}

In [3]: np.array(d.values())
Out[3]: array(dict_values([2, 1, 3]), dtype=object)
```

and one has to instead explicitly convert to a list first:

```
In [4]: np.array(list(d.values()))
Out[4]: array([2, 1, 3])
```

It might be worth adding a special case in the initializer for the array to automatically do this conversion, since `d.values()` is a valid iterator?
",2015-03-25 15:51:14,,Make it possible to initialize array from dictionary values in Python 3,['unlabeled']
5695,open,ewmoore,"The iterator is nice in some regards, simple things are simple, and simply iterating over all elements is straightforward.  However, how to do things like what `PyArray_IterAllButAxis` does but with multiple broadcasted arrays, etc is pretty opaque. (To me at least; maybe I'm just dense.)

My currently go at using nested iterators to try things out is [here](https://gist.github.com/ewmoore/e88ee9dd84c1d9d58892), FWIW.  I'm performing a reduction along a specified tuple of axes, computing both the sum and product stored in the same output array.  Its a bit of mess, but it does seem to work. Doing this kind of thing appears to take quite a lot of setup though.
",2015-03-19 13:29:55,,Add examples of more complex NpyIter usage to the documentation.,"['17 - Task', 'component: documentation']"
5691,open,gdementen,"On my system (Numpy 1.9.2 from conda, Windows 7 64b), there is almost a 3x difference between nanmin and nanmax while there is barely any difference between np.amin & np.amax. I do not see any reason for this to be the case. I've tracked the difference to np.fmin.reduce & np.fmax.reduce but going further is beyond my abilities...

``` python
import numpy as np

>>> a = np.arange(1e6)
>>> timeit np.nanmin(a)
1000 loops, best of 3: 850 µs per loop
>>> timeit np.nanmax(a)
100 loops, best of 3: 2.56 ms per loop
>>> timeit np.fmin.reduce(a)
1000 loops, best of 3: 831 µs per loop
>>> timeit np.fmax.reduce(a)
100 loops, best of 3: 2.6 ms per loop
>>> timeit np.amin(a)
1000 loops, best of 3: 515 µs per loop
>>> timeit np.amax(a)
1000 loops, best of 3: 515 µs per loop
```
",2015-03-18 15:38:45,,nanmax strangely slow (3x slower than nanmin),"['component: numpy.lib', '03 - Maintenance']"
5687,open,mieswicht,"Hi,

when working with complex impedance data, i encountered an issue and narrowed it down to:

> > > from numpy import *
> > > x=(616.47292227535877+53.814558958179042j)
> > > 
> > > tanh(x)

Warning (from warnings module):
  File ""C:\python\Test\Impedance class reduced.py"", line 1
    from numpy import *
RuntimeWarning: overflow encountered in tanh

Warning (from warnings module):
  File ""C:\python\Test\Impedance class reduced.py"", line 1
    from numpy import *
RuntimeWarning: invalid value encountered in tanh

When actually, it should return (1-0j), see http://tinyurl.com/nm4goq3
Then i tried

> > > sinh(x)/cosh(x)

Warning (from warnings module):
  File ""C:\python\Test\Impedance class reduced.py"", line 1
    from numpy import *
RuntimeWarning: overflow encountered in cdouble_scalars

Warning (from warnings module):
  File ""C:\python\Test\Impedance class reduced.py"", line 1
    from numpy import *
RuntimeWarning: invalid value encountered in cdouble_scalars
(nan+nan*j)

However, the solution was to use numpy.divide:

> > > divide(sinh(x),cosh(x))
> > > (1-0j)

This took me quite some time to figure out, so i guess it would be a good idea to include it in the next release (maybe even with with coth?).

Thanks a lot!
",2015-03-17 15:52:38,,BUG:  Blocklist MSVC ctanh? It misbehaves for large inputs.,"['00 - Bug', 'component: npy_math']"
5686,open,alimuldal,"This issue has cropped up a [couple](http://stackoverflow.com/q/29097917/1461210) of [times](http://stackoverflow.com/q/16020137/1461210) before on StackOverflow. `np.genfromtxt` uses a `numpy.lib._iotools.NameValidator` which automatically modifies field names by replacing spaces and stripping out certain non-alphanumeric characters etc.:

``` python
import numpy as np
from io import BytesIO

s = 'name,name with spaces,(x-1)!\n1,2,3\n4,5,6'
x = np.genfromtxt(BytesIO(s), delimiter=',', names=True)
print(repr(x))
# array([(1.0, 2.0, 3.0), (4.0, 5.0, 6.0)], 
#       dtype=[('name', '<f8'), ('name_with_spaces', '<f8'), ('x1', '<f8')])
```

However, these are all perfectly legal field names for structured arrays:

``` python
names = ['name', 'name with spaces', '(x-1)!']
types = ('f',) * 3
dtype = zip(names, types)

x2 = np.empty(2, dtype=dtype)
x2[:] = [(1.0, 2.0, 3.0), (4.0, 5.0, 6.0)]
print(repr(x2))
# array([(1.0, 2.0, 3.0), (4.0, 5.0, 6.0)], 
#       dtype=[('name', '<f4'), ('name with spaces', '<f4'), ('(x-1)!', '<f4')])
print(x2['(x-1)!'])
# [3. 6.]
```

This behavior is not very well-covered in the documentation for `np.genfromtxt` - at best it's indirectly alluded to in the descriptions for some of the keyword arguments (`deletechars`, `autostrip`, `replace_space` etc.). What is perhaps most confusing is the fact that `np.genfromtxt` will even mangle field names that you pass _directly_ via the `names=` kwarg.

The rationale behind mangling field names in this way is also not clear to me. It should either be the case that spaces and non-alphanumeric characters are disallowed in field names altogether (and my second example should raise an exception) , or `np.genfromtxt` should leave field names alone by default.
",2015-03-17 15:02:15,,Use of NameValidator in np.genfromtxt is inconsistent with the rules for naming structured array fields,"['00 - Bug', 'component: numpy.lib']"
5671,open,naught101,"like `np.timedelta64`s, `datetime.time`s are a relative measure of time (time since midnight). However, there is no easy method to convert from and array of `datetime.time`s to `np.timetelta`s. About the simplest way is:

```
t = datetime.time(4,30)
td = np.array(datetime.timedelta(hours=t.hour, minutes=t.minute), dtype=np.timedelta64)
```

But this doesn't work with arrays of datetimes. The other option is to `datetime.combine` the `datetime.time`s with an arbitrary `datetime.date`, and then subtract a `datetime.datetime` with the same date at midnight from them, to get a `datetime.timedelta`, and convert that. Convoluted as hell, and it also doesn't work with arrays.

It'd be nice to just ignore `datetime.time` altogether - to me it seems like a stupid datatype, when `datetime.timedelta` already exists, and has no practical difference. But there are cases when it can't be avoided. For example, pandas converts all date/time indexes into Timeindexes, which are based on datetimes. And when you do something like `df.groupby(df.index.time).mean()` (to get an average daily cycle), the resultant dataframe is indexed by `datetime.time` objects. This is a pretty common precedure, for me at least, and it'd be really good to be able to convert those times back in to something useful, like `timedelta64`s.
",2015-03-12 06:43:28,,Function for converting datetime.time to np.timedelta64,['component: numpy.datetime64']
5669,open,embray,"When calling `array.view(SomeArraySubclass)`, a new instance of `SomeArraySubclass` is created, and its `__array_finalize__` is called per the usual array subclassing rules.

However, when calling `array.view(new_dtype)`, where `new_dtype` is just a different dtype for which we would like to view the array, `__array_finalize__` is still called.  However, the argument passed to it is the original array with the original dtype.  There is no way to find out what we are changing the dtype to in the view casting.  This is because `PyArray_View` calls `PyArray_NewFromDescr` which handles all the `__array_finalize__` stuff where applicable, then afterwards just directly updates the `dtype` attribute.  It would be nice if instead the new dtype were elevated a bit in importance here so that it is possible to discover through `__array_finalize__`, though I'm not sure yet how best to handle that.

(Apologies of this has come up before, but I didn't see any issues in GH concerning this.)

I have an ndarray subclass wherein some of its initialization depends on its dtype, so viewing it to a different dtype results in incorrect initialization.  A workaround that seems _okay_ is to make `dtype` a property, like:

``` python
class MySubclass(ndarray):
    @property
    def dtype(self):
        return super(MySubclass, self).dtype

    @dtype.setter
    def dtype(self, dtype):
        # Do my __array_finalize__ stuff, using knowledge of my new dtype
        np.ndarray.dtype.__set__(self, dtype)
```

but this is still pretty hacky, and also won't suffice if, say, I don't want the dtype attribute to be overridable.  I guess another workaround I could try would be to just override the `view` method, but I still feel like that shouldn't be necessary.  I'd be happy to come up with a fix for this in Numpy if it seems like an acceptable change.
",2015-03-11 22:40:04,,Call __array_finalize__ when creating a view of a different dtype,['unlabeled']
5657,open,endolith,"Python:

```
In [117]: abs(-2147483646)
Out[117]: 2147483646

In [118]: abs(-2147483647)
Out[118]: 2147483647

In [119]: abs(-2147483648)
Out[119]: 2147483648L

In [120]: abs(-2147483649)
Out[120]: 2147483649L

In [121]: abs(-2147483650)
Out[121]: 2147483650L
```

NumPy:

```
In [93]: absolute(array([-2147483646]))
Out[93]: array([2147483646])

In [94]: absolute(array([-2147483647]))
Out[94]: array([2147483647])

In [95]: absolute(array([-2147483648]))
Out[95]: array([-2147483648])

In [96]: absolute(array([-2147483649]))
Out[96]: array([2147483649], dtype=int64)

In [97]: absolute(array([-2147483650]))
Out[97]: array([2147483650], dtype=int64)
```

[In C this is undefined](http://stackoverflow.com/q/11243014/125507) because abs() returns an int and there's no +2147483648, but NumPy should convert to int64?  and then `absolute(array(-2**63))` should convert to object array with longs instead of returning -9223372036854775808?

Related to https://github.com/numpy/numpy/issues/593?
",2015-03-10 00:58:48,,Silent overflow in absolute(),['unlabeled']
5652,open,ingmarschuster,"So my problem has been the following: I had to calculate the logsumexp of some numbers in logspace for accuracy reasons, but some of the elements of the array have negative sign. I implemented the following code, which you might want to integrate into the upstream logsumexp implementation of numpy.
The `sign` argument is a matrix assumed to be filled with +1 or -1 depending on the sign of the number before the log transformation. 

```
from numpy import exp, log, sqrt
from scipy.misc import logsumexp
import numpy as np
import scipy.stats as stats


def log_sign(a):
    a = np.array(a)
    sign_indicator = ((a < 0 ) * -2 + 1)
    return (log(np.abs(a)), sign_indicator)

def exp_sign(a, sign_indicator):
    return exp(a) * sign_indicator    

def logsumexp_sign(a, axis = None, b = None, sign = None):
    if sign is None:
        abs_res = logsumexp(a, axis = axis, b = b)
        sign_res = np.ones_like(abs_res)
        return (abs_res, sign_res)
    else:
        if not (np.abs(sign) == 1).all():
            raise ValueError(""sign arguments expected to contain only +1 or -1 elements"")

        m = np.copy(a)
        m[sign == -1] = -np.infty # don't look at negative numbers
        m = logsumexp(m, axis = axis, b = b)
        m[np.isnan(m)] = -np.infty # replace NaNs resulting from summing infty

        s = np.copy(a)
        s[sign == +1] = -np.infty # don't look at positive numbers
        s = logsumexp(s, axis = axis, b = b)
        s[np.isnan(s)] = -np.infty # replace NaNs resulting from summing infty


        sign_res =  np.ones(np.broadcast(m, s).shape) #default to positive sign
        abs_res  = -np.infty * sign_res #default to log(0)

        idx = np.where(m > s)
        abs_res[idx] = log(1 - exp(s[idx] - m[idx])) + m[idx]

        idx = np.where(m < s)
        sign_res[idx] = -1
        abs_res[idx] = log(1 - exp(m[idx] - s[idx])) + s[idx]
        return (abs_res, sign_res)
```

I'm not sure I passed on the `b` parameter correctly (I never made use of it before). Here is a test method:

```
def test_logsumexp_sign():    
    for ax in (0, 1):
        for a in [np.array([[1,1,-1], [-1,-1,1]]),
                  np.array([[1,1,1], [-1,-1,-1]])]:
            (la, sa) = log_sign(a)
            log_res = logsumexp_sign(la, axis = ax, sign = sa)
            res = exp_sign(*log_res)
            if not np.all(np.abs(res - np.sum(a, axis = ax)).mean() <= 0.1):
                raise RuntimeError(""problem when summing accros"", ax,
                                   ""for"", repr(a), ""getting"", log_res)
```
",2015-03-09 15:00:43,,logsumexp with sign indicator - enable calculation with negative signs,['unlabeled']
5624,open,ewmoore,"This would be nice to have for a variety of things.  Could probably be a simply loop, preferably in the C level. 
",2015-03-02 20:14:06,,wishlist: Add an axis argument (or two) to np.convolve/np.correlate,"['01 - Enhancement', '23 - Wish List']"
5617,open,seberg,"It might make sense to deprecate broadcast_arrays giving back a writeable view (see also gh-5371). This should be relatively easy by copying how the diagonal deprecation is done.

Just tagged for 1.10, but probably no need to do it for that. We have the new broadcast_to in it which is readonly though.
",2015-02-28 13:27:03,,Consider deprecating writeable broadcast_arrays,"['component: numpy.lib', '54 - Needs decision', '07 - Deprecation']"
5613,open,jakirkham,"The fill value type chosen for integers is always `int64` and always `999999` even when that doesn't make sense [numpy.ma.MaskedArray.fill_value](http://docs.scipy.org/doc/numpy/reference/maskedarray.baseclass.html#numpy.ma.MaskedArray.fill_value). For instance, take the type `uint8`. In this case, the fill value is way outside of bounds and ends up getting converted to `63`. If the behavior must be preserved, then for `uint8` the fill value should be `63` and it's dtype should be `uint8`. Alternatively, better fill values could be picked that work on a range of types (i.e. `0` for integers and `nan` for floats and complex) or something similar.
",2015-02-27 16:40:09,,Default fill value dtype does not match that of data for masked arrays,"['00 - Bug', 'component: numpy.ma']"
5543,open,brandon-rhodes,"In many wonderful cases an ndarray can be used in place of a Python float and Just Work.

But not in one case:

```
import numpy as np

n = 1.23
print('{0:.6} AU'.format(n))

n = np.array([1.23, 4.56])
print('{0:.6} AU'.format(n))
```

The output of the above code, at least under Python 3.4, is:

```
1.23 AU
Traceback (most recent call last):
  File ""tmp9.py"", line 7, in <module>
    print('{0:.6} AU'.format(n))
TypeError: non-empty format string passed to object.__format__
```

It would be a great convenience if the ndarray grew a `__format__()` method that understood the tiny mini-language of float formatting, and used the number of digits of precision specified there to make its own call to the standard NumPy vector array formatting. Users could control array appearance on the screen using a Python standard that many programmers already understand.
",2015-02-08 17:42:48,,ndarray should offer __format__ that can adjust precision,['01 - Enhancement']
5534,open,anntzer,"The docs for ndarray.view explicitly allows for views with dtypes of different sizes (although it warns that the result is layout-dependent), but does not specify how the size of the new array is calculated.  I _believe_ from quick experimentation that the number of dims is conserved, but haven't tried e.g. weirdly strided arrays so I don't know the answer either.
",2015-02-04 16:52:41,,Document size of ndarray.view when using dtypes of different sizes,['unlabeled']
5506,open,coolplay,"When I constructed an array named `list` in fortran, like code shown below:

``` fortran
subroutine blah(list, n)
    integer, intent(in) :: n
    real, intent(out) :: list(2*n,0:2**n)

    list = 1.0

end subroutine
```

I encountered the following error, which seems to treat exponentiation as a pointer. Any ideas how to get rid of it? Thanks!

```
/tmp/tmpHYL2Dx/src.linux-x86_64-2.7/cmmodule.c:216:37: error: invalid type argument of unary ‘*’ (have ‘int’)
   list_Dims[0]=2 * n,list_Dims[1]=2**n+1;
                                    ^
```
",2015-01-26 09:10:37,,ENH: f2py compilation error with array argument in Fortran subroutine,"['01 - Enhancement', 'component: numpy.f2py']"
5452,open,dmopalmer,"When a datetime64 is created from a string with more than 9 decimal places in the second, the result is corrupt.

```
>>> numpy.datetime64('2015-01-01T01:00:00.000000000-0700')
numpy.datetime64('2015-01-01T01:00:00.000000000-0700')
>>> numpy.datetime64('2015-01-01T01:00:00.0000000000-0700')
numpy.datetime64('1969-12-28T12:38:26.324364525568Z')
```

The first instance with 9 decimal places in the seconds is correct.  The second, with 10 decimal places is incorrect.

Versions: numpy 1.9.1; OSX 10.9.5; Python 2.7.9;
Python and numpy installed using macports.
Replicated on RHEL/Python 3.2.5/numpy 1.8.0
",2015-01-14 00:49:13,,datetime64 chokes on sub-nanosecond input string,"['00 - Bug', 'component: numpy.datetime64']"
5422,open,maniteja123,"The negative order norm for vectors are failing when some of the elements of the vector are zeros.

The documentation says that negative norm, strictly speaking is not _math_, but they are used 
in general purpose.[here](https://github.com/numpy/numpy/blob/v1.9.1rc1/numpy/linalg/linalg.py#L1953)

But some of the examples provided in the documentation fails at finite negative order :

```
>>> a = np.arange(9) - 4
>>> a
array([-4, -3, -2, -1,  0,  1,  2,  3,  4])
>>>la.norm(a, -1)
-4.6566128774142013e-010
>>> la.norm(a, -2)
nan
>>> la.norm(a, -3)
nan
```

The results on the dev version are:

```
 >>>la.norm(a,-1)
 /home/maniteja/FOSS/numpy/numpy/linalg/linalg.py:2118: RuntimeWarning: divide by zero    encountered in reciprocal
 absx **= ord
 0.0
 >>> la.norm(a,-2)
 /home/maniteja/FOSS/numpy/numpy/linalg/linalg.py:2118: RuntimeWarning: divide by zero   encountered in power
 absx **= ord
 0.0
 >>> la.norm(a,-3)
 0.0
```

The raising of 0 to negative power is causing an error,

```
0.0 ** -1
Traceback (most recent call last):
   File ""<stdin>"", line 1, in <module>
ZeroDivisionError: 0.0 cannot be raised to a negative power
```

But the raising of a `ndarray` with zero to negative power is causing these exceptions.

```
a = np.arange(9)
>>> a ** -1
 __main__:1: RuntimeWarning: divide by zero encountered in power
 __main__:1: RuntimeWarning: invalid value encountered in power
 array([-2147483648,                    1,                    0,
                          0,                    0,                    0,
                          0,                    0,                    0])
```

Suppose the initial array was not consisting of zeros, it was working fine, as in following example.

```
 >>> a = np.arange(9) + 1
 >>> la.norm(a,-1)
 0.35348576237901524
 >>> la.norm(a,-2)
 0.80588373958852921
  >>> la.norm(a,-3)
 0.94194431453393834
```
",2015-01-04 11:11:30,,Vectors containing zero get incorrect norm with negative order,"['00 - Bug', 'component: numpy.linalg']"
5412,open,abalkin,"I encountered this error in my own extension module, but it can be easily demonstrated using the `_tesbuffer` module that is included with Python 3.4:

```
>>> import _testbuffer, numpy
>>> nd = _testbuffer.ndarray(list(range(6)), shape=[2,3],
...                          format='i', flags=_testbuffer.ND_PIL)
>>> numpy.array(nd)
array([[-1974954736,       32754, -1974954724],
       [-1974954724,       32754,           0]], dtype=int32)
```

It looks like numpy fails to implement proper pointer dereferencing in the presence of suboffsets.  In this case

```
>>> nd.suboffsets
(0, -1)
```
",2015-01-02 00:26:37,,ENH: NumPy could accept PIL-style buffers (if it copies the data!) ,['unlabeled']
5407,open,charris,"The ndarray.diagonal method currently returns a view, but it is not writeable. It is documented to return a writeable view in 1.10.
",2015-01-01 01:52:00,,Make a.diagonal() writeable.,"['17 - Task', '54 - Needs decision']"
5384,open,argriffing,"``` python
from __future__ import print_function, division

import numpy as np

print('numpy version', np.__version__)

a = np.ma.array([1.0, 2.0, np.inf], mask=[0, 0, 0])
b = np.ma.array([1.0, 2.0, 3.0], mask=[0, 0, 0])
q = a/b
print(repr(a))
print(repr(b))
print(repr(q))
```

```
numpy version 1.10.0.dev-243ab56
masked_array(data = [1.0 2.0 inf],
             mask = [False False False],
       fill_value = 1e+20)

masked_array(data = [1.0 2.0 3.0],
             mask = [False False False],
       fill_value = 1e+20)

masked_array(data = [1.0 1.0 --],
             mask = [False False  True],
       fill_value = 1e+20)
```
",2014-12-22 03:44:52,,numpy.ma wants to mask out my infs,"['00 - Bug', 'component: numpy.ma']"
5363,open,bnordgren,"Summing over the histogram should always give the number of non-masked records when no weighting is used. As shown below, the sum is reduced, but not by the correct amount. Calling the ""compressed()"" method and reshaping before running the histogram produces correct results.

The following uses numpy 1.8.1.

``` python
 x = ma.arange(300).reshape(100,3)

In [5]: print x[:,0].count()
100

In [6]: H,e = np.histogramdd(x)

In [7]: H.sum()
Out[7]: 100.0

In [8]: for i in range(10) : 
   ...:     x[i*i,:]=ma.masked
   ...:     

In [9]: print x[:,0].count()
90

In [10]: H,e = np.histogramdd(x)

In [11]: H.sum()
Out[11]: 98.0

x = x.compressed().reshape(90,3)

H,e = np.histogramdd(x)

H.sum()
Out[14]: 90.0
```
",2014-12-11 02:30:26,,Incorrect histogramdd behavior with masked arrays,"['00 - Bug', 'component: numpy.lib']"
5357,open,parkus,"When histogramming float32 data with weights, the counts in each bin become progressively more quantized (i.e. grouped, rounded, ...) with increasing bin number. Below is an example with random data. This might be related to #4823.

I'm not sure whether this is a bug or expected behavior, but if the latter I suggest that it at least be noted in the function docstring. 

```
from numpy.random import rand, seed
from numpy import histogram
from matplotlib.pyplot import figure, plot, show, ylim

#make fake single precision data
N = 10000
seed(0)
x = rand(N)*N
x = x.astype('f4')

#make some weights that are slightly scattered around unity
amplitude = 0.01
scatter = (rand(N) - 0.5)*amplitude
weights = 1.0 + scatter
weights = weights.astype('f4')

#if we histogram all the points at once, we see that the count in each bin
#becomes progressively more quantized with increasing bin number
h,bins = histogram(x, N, weights=weights)
figure()
plot(h, '.')
ylim(1.0 - amplitude/2.0, 1.0 + amplitude/2.0) #focus in on single-count bins
show()

#yet if we histogram the first fifth of the data and the last fifth separately
#one is not more quantized than the other
i0, i1 = N/5, 4*N/5
h0 = histogram(x[:i0], bins[:i0+1], weights=weights[:i0])[0]
h1 = histogram(x[i1:], bins[i1-1:], weights=weights[i1:])[0]
figure()
plot(h0, '.')
plot(h1 + amplitude, '.') #shift points up a bit to separate from h0
ylim(1.0 - amplitude/2.0, 1.0 + 3*amplitude/2.0)
show()

#also, if we convert the data and weight arrays back to double precision, the
#problem disappears
x = x.astype('f8')
weights = weights.astype('f8')
h,bins = histogram(x, N, weights=weights)
figure()
plot(h, '.')
ylim(1.0 - amplitude/2.0, 1.0 + amplitude/2.0)
show()
```
",2014-12-08 18:34:36,,histogram progressive rounding error with float32 weights,['unlabeled']
5353,open,argriffing,"This possibility was raised by https://github.com/numpy/numpy/issues/5303#issuecomment-63904939 so I'm moving this topic to its own github issue.  Problems like https://github.com/scipy/scipy/issues/4239 among others could be softened by making this change although it could bring backwards compatibility issues.
",2014-12-07 01:29:40,,object array construction should require explicitly specifying dtype=object,['62 - Python API']
5350,open,abalkin,"The structure exposed by `__array_struct__` is currently defined as follows:

```
typedef struct {
    int two;              /*
                           * contains the integer 2 as a sanity
                           * check
                           */
    int nd;               /* number of dimensions */
    char typekind;        /*
                           * kind in array --- character code of
                           * typestr
                           */
    int itemsize;         /* size of each element */
    int flags;            /*
                           * how should be data interpreted. Valid
                           * flags are CONTIGUOUS (1), F_CONTIGUOUS (2),
                           * ALIGNED (0x100), NOTSWAPPED (0x200), and
                           * WRITEABLE (0x400).  ARR_HAS_DESCR (0x800)
                           * states that descr field is present in
                           * structure
                           */
    npy_intp *shape;       /*
                            * A length-nd array of shape
                            * information
                            */
    npy_intp *strides;    /* A length-nd array of stride information */
    void *data;           /* A pointer to the first element of the array */
    PyObject *descr;      /*
                           * A list of fields or NULL (ignored if flags
                           * does not have ARR_HAS_DESCR flag set)
                           */
} PyArrayInterface;
```

The descr field is currently only filled by record arrays, but without it, datetime64 (or timedelta64) array description is incomplete.  There is no other field from which the time unit can be deduced.

See also gh-4983.
",2014-12-06 00:57:57,,datetime64 arrays should fill in descr in __array_struct__,"['01 - Enhancement', 'component: numpy.datetime64']"
5337,open,senderle,"Both `cov()` and `corrcoef()` accept a `ddof` parameter that allows the caller to tune the bias term. Would it make sense for `polyfit()` to do the same, when returning the covariance matrix? 

I imagine this would be implemented with an optional `ddof` parameter that would imply `cov=True` if passed. Or for the most minimal possible change to the function signature, it could be implemented by checking `cov` for a numeric value; if `cov` is numeric, use the number passed instead of the current fixed default of 2.0. 

Inspired by this question at Stack Overflow: http://stackoverflow.com/q/27230285/577088
",2014-12-02 18:10:20,,Should polyfit() accept a ddof parameter?,"['01 - Enhancement', '15 - Discussion', 'component: numpy.polynomial']"
5295,open,larsmans,"I just found out that there are two iteration APIs in NumPy: the `PyArray_ITER` family and the `NpyIter` ones. The latter are apparently the preferred API nowadays, but they're not really advertised as such.

The [array API docs](http://docs.scipy.org/doc/numpy/reference/c-api.array.html) in `doc/source/reference/c-api.array.rst` link to the new API, but then list the old API. The link goes to [`doc/source/reference/c-api.iterator.rst`](http://docs.scipy.org/doc/numpy/reference/c-api.iterator.html), which calls the old API the ""existing"" API, as if the new one were merely proposed (and bothers the reader with some development details; see gh-5294). The documentation starts off with a conversion table, instead of encouraging new code to use it.

I'm not sure how to fix this. The first thing to do, I think, is move the conversion table down the page and start with an example of the new API. However, that leaves the problem that the old API is still in the main docs. We can...
- move the old API to the iterator API docs and put it at the bottom,
- merge the new API docs into the main array docs,
- or do both so we have a doc chapter about the old iterator API and the new API in the array docs.

Thoughts?
",2014-11-19 16:25:57,,NpyIter documentation needs love,['component: documentation']
5272,open,durack1,"Hi folks I've just been investigating some type conversion and came across what appears unusual behaviour to me - I would have expected these statements to more faithfully print the values at their respective precisions:

```
>>> import numpy
>>> numpy.version.version
'1.9.0'
>>> a = numpy.array([3991.86795711963],dtype='float64')
>>> print a
[ 3991.86795712]
>>> print numpy.float32(a)
[ 3991.86791992]
>>> print numpy.float64(a)
3991.86795712 ; # Why does this print statement look different to the above (float32) - no brackets
>>> a = numpy.array([3991.86795711963],dtype='float128')
>>> print numpy.float128(a)
[ 3991.868] ; # Why is this truncated when it should have double the precision of float64?
>>> a
array([ 3991.868], dtype=float128)
>>> numpy.float128(a)
array([ 3991.868], dtype=float128)
>>> print a
[ 3991.868]
>>> print a*1
[ 3991.868]
```

I haven't investigated playing around with  `numpy.set_printoptions(precision=2)`
",2014-11-11 07:02:49,,Data type precision problems?,['00 - Bug']
5261,open,divenex,"The documentation below for numpy.polyfit is incorrect/misleading regarding the definition of the optional input weights vector <b>w</b>

http://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html

In least-squares fitting one generally defines the weights vector in such a way that the fit minimizes the squared error (in Numpy notation)

`chi2 = np.sum(weights*(p(x) - y)**2)`

In common situation where the 1&sigma; errors ""sigma"" are known one has that the weights are the reciprocal of the variance

`weights = 1/sigma**2`

see e.g. http://en.wikipedia.org/wiki/Least_squares#Weighted_least_squares or http://www.itl.nist.gov/div898/handbook/pmd/section4/pmd432.htm

However the numpy.polyfit documentation defines the weight as ""weights to apply to the y-coordinates"". This definition is not correct. The weights apply to (=multiply) the fit residuals, not only to the y-coordinates.

More importantly, looking at the math in the Numpy (v1.9.1) code, the resulting definition of squared residuals adopted by polyfit is the following, with the optional input weights vector <b>w</b> inside the parenthesis, contrary to standard practice

`chi2 = np.sum((w*(p(x) - y))**2)`

in such a way that the relation between <b>w</b> and the 1&sigma; errors is

`w = 1/sigma`

which is different from what everybody will expect.

The confusion in the documentation likely arises from the fact that the Numpy code solves the linear problem below in the last-squares sense, where the <b>w</b> vector does multiply the y-coordinate

`(vander*w[:, np.newaxis]).dot(x) == y*w`

And solving the above array expression in the least-squares sense is equivalent to minimizing the expression below with <b>w</b> inside the parenthesis

`np.sum((w*(vander.dot(x) - y))**2)`

A non-optimal solution, to maintain compatibility, would be to change the documentation and clearly define the weight <b>w</b> by including it in the equation for the ""squared error"" <i>E</i> given in the Notes. One should also make clear that the adopted definition differs from standard practice by giving the relation between weights and error w=1/&sigma;

Even better would be to define a new optional keyword <b>weights</b>, which follows standard practice and satisfies  <b>weights</b> = 1/sigma**2. In this case, in the code one should simply calculate w=np.sqrt(weight) of the input weights and the rest of the code applies unmodified.
",2014-11-05 19:05:46,,Wrong definition of weights in numpy.polyfit,['unlabeled']
5236,open,jjmortensen,"``` python
>>> a = np.zeros((5, 0))
>>> b = a.reshape((-1, 0))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ValueError: total size of new array must be unchanged
```

I would expect this to set the shape to (5, 0).
",2014-10-26 18:38:57,,Reshape of size=0 array fails,"['01 - Enhancement', '15 - Discussion']"
5221,open,ischwabacher,"It looks like `arange` is misinterpreting `stop` as `len` when the requested dtype is `datetime64`:

``` python
In [1]: import numpy as np

In [2]: np.arange(-2, 4, dtype=t)
   ...:     for t in ['f8', 'i8', 'm8[ns]', 'm8[D]', 'm8[Y]', 'M8[ns]', 'M8[D]', 'M8[Y]']]
Out[2]:
[array([-2., -1.,  0.,  1.,  2.,  3.]),
 array([-2, -1,  0,  1,  2,  3]),
 array([-2, -1,  0,  1,  2,  3], dtype='timedelta64[ns]'),
 array([-2, -1,  0,  1,  2,  3], dtype='timedelta64[D]'),
 array([-2, -1,  0,  1,  2,  3], dtype='timedelta64[Y]'),
 array(['1969-12-31T23:59:59.999999998Z', '1969-12-31T23:59:59.999999999Z',
       '1969-12-31T18:00:00.000000000-0600',
       '1969-12-31T18:00:00.000000001-0600'], dtype='datetime64[ns]'),
 array(['1969-12-30', '1969-12-31', '1970-01-01', '1970-01-02'], dtype='datetime64[D]'),
 array(['1968', '1969', '1970', '1971'], dtype='datetime64[Y]')]

In [3]: map(len, _)
Out[3]: [6, 6, 6, 6, 6, 4, 4, 4]
```
",2014-10-22 21:06:02,,BUG: arange(dtype='M8[*]') has wrong number of elements,"['54 - Needs decision', 'component: numpy.datetime64']"
5193,open,jlippi,"Hello, 
 I have found that when I make the following call using apply_along_axis and return an int rather than a float, the field with the incorrect type is cast to the right type and put into the array properly, but the value of the subsequent field is set to zero:

```
def f0(m,l,val):
    return np.apply_along_axis(f1,0,m,m,l,val)

def f1(v,m,l,val):
    return np.apply_along_axis(f2,0,m,v,l,val)

def f2(v1,v2,l,val):
    l[0] += 1
    if l[0] <= 2:
        return 1.
    if l[0] == 3:
        return val
    if l[0] == 4:
        return .34151
```

```
print f0(np.array(np.ones((1,2))),list([0]),0) <-- int
print f0(np.array(np.ones((1,2))),list([0]),0.) <-- float
print f0(np.array(np.ones((1,2))),list([0]),1) <-- int
print f0(np.array(np.ones((1,2))),list([0]),1.) <-- float
print type(f0(np.array(np.ones((1,2))),list([0]),1)[0][1])
print numpy.__version__

output:

[[ 1.  0.] <-- correctly zero
 [ 1.  0.]] <-- value dropped
[[ 1.       0.     ] <-- correctly zero
 [ 1.       0.34151]] <-- value correct
[[ 1.  1.] <-- correctly 1
 [ 1.  0.]] <-- value dropped
[[ 1.       1.     ] <-- correctly 1
 [ 1.       0.34151]] <-- value correct
<type 'numpy.float64'> <-- type is float64, this was correctly cast
1.8.2 <-- numpy version
```

discovered with @asna1005
",2014-10-16 02:52:00,,numpy apply_along_axis drops values after casting incorrectly,['unlabeled']
5150,open,njsmith,"Problem: For floats, our default on divide-by-zero is to print a warning and return `nan`. This is a pretty good default. But we've also carried this over to integers, where there is no `nan`. Instead we pick a random value (zero, apparently) and return that:

```
In [21]: np.array(0) // np.array(0)
/usr/bin/ipython3:1: RuntimeWarning: divide by zero encountered in floor_divide
  #!/usr/bin/env python3
Out[21]: 0
```

This violates the rule that we shouldn't just make up random nonsense when the user requests an impossible operation. With a `nan` return, it's obvious that something has gone wrong; even if you don't handle the error immediately, your overall computation won't silently go wrong. In the integer case, right now, unless you are specifically watching the console and notice the warning, there's no way to know that anything has gone wrong at all, and probably people are getting incorrect answers right now without realizing it.

My feeling is that we should (after a deprecation period) make integer divide-by-zero a hard error.

Affected ufuncs: `divide` (python 2 only), `floor_divide`, `remainder`

(`remainder` is particularly terrible -- `np.remainder(5, 0)` returns 0 as well, so we don't even satisfy the invariant that `num = (num // denom) * denom + (num % denom)`.)

Relevant mailing list thread:
http://mail.scipy.org/pipermail/numpy-discussion/2014-October/071306.html

Not sure if the mailing list discussion has converged yet or not -- there is [one objection from Robert Kern](http://mail.scipy.org/pipermail/numpy-discussion/2014-October/071330.html)
which [I think is unconvincing](http://mail.scipy.org/pipermail/numpy-discussion/2014-October/071331.html), but that's just me; I could be missing something.
",2014-10-04 02:42:48,,"integer / and % should probably raise divide-by-zero errors unconditionally, instead of just flagging divide-by-zero errstate",['unlabeled']
5142,open,anntzer,"If `a_min > a_max`, the results can get a bit strange:

```
>>> np.clip(0, 1, 0)
1
>>> np.clip(1, 1, 0)
0
```

Now this can arguably be blamed on faulty inputs, and a precise reading of the docs (it returns ""An array with the elements of `a`, but where values < `a_min` are replaced with `a_min`, and those > `a_max` with `a_max`."") explains that behavior (once we realize that the two tests have to be made one after the other), but throwing a `ValueError` (possibly silenceable/activatable by a keyword switch) could be helpful.
",2014-10-01 10:07:14,,Unintuitive behavior of np.clip on inconsistent amin and amax,['unlabeled']
5114,open,rstoneback,"Using NumPy's nanmin on a pandas Series worked as expected, now however the Series is returned with each element set to the minimum, rather than just getting the minimum. Not sure if this is a Numpy of Pandas issue.

np.nanmin(pds.Series([1,2,3,4]))
Out[14]: 
0 1
1 1
2 1
3 1
dtype: int64

np.min(pds.Series([1,2,3,4]))
Out[15]: 1
",2014-09-24 20:03:22,,nanmin doesn't appear to work with Pandas Series,['00 - Bug']
5108,open,scharlois,"Method numpy.ma.power computes the result taking into account the masked values:

Example:

```
a = np.ma.array([1.8446744e+19, 1, 2, 3], 
                           mask=[True, False, False, False], 
                           fill_value=1.8446744e+19, dtype=np.float32)
np.ma.power(a, 2)
/usr/lib/python2.7/dist-packages/numpy/ma/core.py:6012: RuntimeWarning: overflow encountered in power 
result = np.where(m, fa, umath.power(fa, fb)).view(basetype)
```
",2014-09-24 08:38:13,,ma.power should not take masked values into account,"['00 - Bug', 'component: numpy.ma']"
5098,open,ewmoore,"https://github.com/numpy/numpy/blob/master/numpy/f2py/src/fortranobject.c#L777

In fortranobject.c, function check_and_fix_dimensions calls fprintf on error. This dumps a message to stderr which should probably be included in the error message passed up as a python exception instead.  
",2014-09-22 18:05:41,,F2py wrapped routines shouldn't dump messages to stderr,['component: numpy.f2py']
5069,open,jelmd,"When running nosetests on numpy 1.9.0 numpy/core/tests/test_blasdot.py test_dot_2args(44) fails.

Inserted the following line into numpy/core/blasdot/_dotblas.c DOUBLE_dot(102):

``` C
fprintf(stderr, ""2: chunksz=%d incx=%d incy=%d\n"", chunk, na, nb); fflush(stderr);
```

reveals for the failed test:

```
2: chunksz=80 incx=1 incy=1
```

Huh?

``` python
> /tmp/_root/usr/lib/python2.7/site-packages/numpy/testing/utils.py(665)assert_array_compare()
-> raise AssertionError(msg)
(Pdb) p msg
'\nNot equal to tolerance rtol=1, atol=0\n\n(mismatch 100.0%)\n x: array([[ 3.,  2.],\n       [ 7.,  4.]])\n y: array([[ 1.,  2.],\n       [ 4.,  6.]])'
(Pdb) 
```

Just to make sure, ddot works correctly, I inserted the following line right before the ddot call:

``` C
if (chunk == 80) chunk = 2;
```

This time the test succeeds, but suprisingly when the test gets invoked a 2nd time, it fails with:

```
2: chunksz=78 incx=1 incy=1
```

So it seems, that numpy doesn't determine the number of elements in the vector correctly. I guess, this has something to do with the `def test_vecself` in the test class, but I'm not a python expert, so ...

Env is: Python 2.7.6 on Solaris 11.2
",2014-09-13 19:52:17,,numpy/core/tests/test_blasdot.py:test_dot_2args fails - caching error?,['unlabeled']
5041,open,mamikonyan,"This is a patch I posted to #4903, but it must have fallen through the cracks. It brings the code inline with the documentation in that it always returns the NaN argument (first, if both).

```
diff --git a/numpy/core/src/umath/funcs.inc.src b/numpy/core/src/umath/funcs.inc.src
index 3aad44c..0102a4f 100644
--- a/numpy/core/src/umath/funcs.inc.src
+++ b/numpy/core/src/umath/funcs.inc.src
@@ -65,8 +65,13 @@ npy_Object@Kind@(PyObject *i1, PyObject *i2)
     PyObject *result;
     int cmp;

-    cmp = PyObject_RichCompareBool(i1, i2, @OP@);
-    if (cmp < 0) {
+    if (PyFloat_Check(i1) && Py_IS_NAN(PyFloat_AS_DOUBLE(i1))) {
+        cmp = 1;
+    }
+    else if (PyFloat_Check(i2) && Py_IS_NAN(PyFloat_AS_DOUBLE(i2))) {
+        cmp = 0;
+    }
+    else if ((cmp = PyObject_RichCompareBool(i1, i2, @OP@)) < 0) {
         return NULL;
     }
     if (cmp == 1) {
```
",2014-09-03 21:06:01,,return NaN in npy_ObjectMax() and npy_ObjectMin() if it's an argument,"['00 - Bug', 'component: numpy.ufunc']"
5030,open,charris,"Some timings:

```
In [8]: a = np.float64(123)

In [9]: b = np.array(4.)

In [10]: timeit a*b
10000000 loops, best of 3: 110 ns per loop

In [11]: timeit b*a
1000000 loops, best of 3: 985 ns per loop

In [12]: timeit a*a
10000000 loops, best of 3: 66.6 ns per loop

In [13]: timeit b*b
1000000 loops, best of 3: 349 ns per loop

In [14]: timeit a + b
10000000 loops, best of 3: 109 ns per loop

In [15]: timeit b + a
1000000 loops, best of 3: 1.01 µs per loop
```

Maybe a fastpath it needed.
",2014-09-02 14:43:17,,array_scalar * numpy_scalar is slow.,"['00 - Bug', 'component: numpy._core']"
4983,open,abalkin,"```
In [4] m = memoryview(np.array(['2001-01-01'], dtype='M8[D]'))
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-4-033a8bae908a> in <module>()
----> 1 m = memoryview(np.array(['2001-01-01'], dtype='M8[D]'))

ValueError: cannot include dtype 'M' in a buffer
```
",2014-08-22 19:52:01,,datetime64 arrays don't support buffer protocol,['component: numpy.datetime64']
4979,open,robertwb,"This concerns PEP3118 buffer formats.

```python
def buffer_type(dt):
     return memoryview(np.empty((), dt)).format
```

Consider
```python
>>> buffer_type([('a', [('s', np.int16), ('t', np.int8)]), ('x', np.int8)])
""T{T{h:s:b:t:}:a:b:x:}""
```
which is missing the ""="" none alignment spec (although internally numpy packs this into 4 bytes). 

Adding another field (or otherwise making the entire struct odd in size)

```python
>>> buffer_type([('a', [('s', np.int16), ('t', np.int8)]), ('x', np.int8), ('y', np.int8)])
""T{T{=h:s:b:t:}:a:b:x:b:y:}""
```
gives a correct result",2014-08-21 17:13:17,,NumPy's Py_buffer format string doesn't correctly handle sub-struct padding when the entire struct is a multiple of 4.,['unlabeled']
4966,open,skiguy0123,"This applies when the number of output points is not specified
for example

```
>>>> numpy.fft.irfft2(np.random.random((10,1)))
```

fails
rfft outputs n // 2 + 1 points, and irfft reverse this with (n-1)*2. This clearly fails when n is 1
See fftpack.py line 416
",2014-08-14 20:06:03,,irfft2 fails when the last dimension has a dimension of size 1,['component: numpy.fft']
4965,open,PierreAndreNoel,"I think that the following new feature would make `numpy.einsum` even more powerful/useful/awesome than it already is. Moreover, the change should not interfere with existing code, it would preserve the ""minimalistic"" spirit of `numpy.einsum`, and the new functionality would integrate in a seamless/intuitive manner for the users.

In short, the new feature would allow for repeated subscripts to appear in the ""output"" part of the `subscripts` parameter (i.e., on the right-hand side of `->`). The corresponding dimensions in the resulting `ndarray` would only be filled along their diagonal, leaving the off diagonal entries to the default value for this `dtype` (typically zero). Note that the current behavior is to raise an exception when repeated output subscripts are being used.

This is simplest to describe with an example involving the dual behavior of `numpy.diag`.

``` python
# Extracting the diagonal of a 2-D array.
A = arange(16).reshape(4,4)
print(diag(A)) # Output: [ 0 5 10 15 ]
print(einsum('ii->i', A)) # Same as previous line (current behavior).

# Constructing a diagonal 2-D array.
v = arange(4)
print(diag(v)) # Output: [[0 0 0 0] [0 1 0 0] [0 0 2 0] [0 0 0 3]]
print(einsum('i->ii', v)) # New behavior would be same as previous line.
# The current behavior of the previous line is to raise an exception.
```

By opposition to `numpy.diag`, the approach generalizes to higher dimensions: `einsum('iii->i', A)` extracts the diagonal of a 3-D array, and `einsum('i->iii', v)` would build a diagonal 3-D array.

The proposed behavior really starts to shine in more intricate cases.

``` python
# Dummy values, these should be probabilities to make sense below.
P_w_ab = arange(24).reshape(3,2,4)
P_y_wxab = arange(144).reshape(3,3,2,2,4)

# With the proposed behavior, the following two lines should be equivalent.
P_xyz_ab = einsum('wab,xa,ywxab,zy->xyzab', P_w_ab, eye(2), P_y_wxab, eye(3))
also_P_xyz_ab = einsum('wab,ywaab->ayyab', P_w_ab, P_y_wxab)
```

If this is not convincing enough, replace `eye(2)` by `eye(P_w_ab.shape[1])` and replace `eye(3)` by `eye(P_y_wxab.shape[0])`, then imagine more dimensions and repeated indices... The new notation would allow for crisper codes and reduce the opportunities for dumb mistakes.

For those who wonder, the above computation amounts to $P(X=x,Y=y,Z=z|A=a,B=b) = \sum_w P(W=w|A=a,B=b) P(X=x|A=a) P(Y=y|W=w,X=x,A=a,B=b) P(Z=z|Y=y)$ with $P(X=x|A=a)=\delta_{xa}$ and $P(Z=z|Y=y)=\delta_{zy}$ (using LaTeX notation, and $\delta_{ij}$ is [Kronecker's delta](http://en.wikipedia.org/wiki/Kronecker_delta)).
",2014-08-14 15:34:36,,numpy.einsum new feature request: repeated output subscripts as diagonal,"['23 - Wish List', 'component: numpy.einsum']"
4959,open,gerritholl,"When doing a mathematical operation on a masked array, and some elements are invalid for the particular mathematical operation, numpy should not issue a warning if all invalid elements are masked.

This cannot be handled by `numpy.seterr`: I want numpy to warn or raise when performing an invalid operation on any non-masked value, but to be silent when performing an invalid operation on a masked value.

```
In [1]: A = arange(-2, 5)/2

In [2]: Am = numpy.ma.masked_less_equal(A, 0)

In [3]: numpy.log10(Am)
/export/data/home/gholl/venv/gerrit/bin/ipython3:1: RuntimeWarning: divide by zero encountered in log10
  #!/export/data/home/gholl/venv/gerrit/bin/python3.4
/export/data/home/gholl/venv/gerrit/bin/ipython3:1: RuntimeWarning: invalid value encountered in log10
  #!/export/data/home/gholl/venv/gerrit/bin/python3.4
Out[3]: 
masked_array(data = [-- -- -- -0.3010299956639812 0.0 0.17609125905568124 0.3010299956639812],
             mask = [ True  True  True False False False False],
       fill_value = 1e+20)
```
",2014-08-12 19:22:33,,Do not warn when invalid values are masked in a MaskedArray,"['00 - Bug', 'component: numpy.ma']"
4952,open,pitrou,"Numpy ufuncs have a `__name__`, but don't have a `__qualname__` as defined in PEP 3155. Similarly, they don't seem to define a `__module__`.
(note that for numpy's builtin ufuncs, the `__qualname__` should simply be the same as the `__name__`, e.g. `np.sin.__qualname__ == 'sin'`)
",2014-08-06 14:22:09,,numpy ufuncs don't have __qualname__,['component: numpy._core']
4945,open,argriffing,"https://github.com/numpy/numpy/pull/4943#discussion_r15767060
",2014-08-04 17:48:56,,Should numpy matrix methods use 'super' instead of hardcoding ndarray?,['component: numpy.matrixlib']
4934,open,charris,"Just so, a task.
",2014-08-01 00:05:30,,"Audit Fixmes in numpy/lib, numpy/lib/tests","['17 - Task', 'component: numpy.lib']"
4915,open,seberg,"Comparing a datetime array with None (using np.equal, but at some point also `==`), will give that `NaT` is equal to `None`. Doing the same with the datetime scalar does currently always evaluate to False (hardcoded, but array == None used to be just False).
",2014-07-29 09:05:53,,Datetime scalar None comparison,"['54 - Needs decision', 'component: numpy.datetime64']"
4895,open,yha,"When numpy is set to raise exception on floating point errors, taking the mean of masked arrays sometimes raises spurious errors, even when no values are masked.
For example:

```
import numpy as np
from numpy import ma

np.seterr('raise')

a = ma.masked_all( (1,1) )

# Underflow reported for values <=~ 0.46
a[0,0] = 0.4


# Raises FloatingPointError: underflow encountered in multiply
ma.mean(traces,0)
# Also raises FloatingPointError
np.mean(traces,0)

# These work fine
ma.mean(traces.data,0)
np.mean(traces.data,0)
```
",2014-07-21 16:04:00,,Masked array mean reports spurious floating point error (underflow),"['00 - Bug', 'component: numpy.ma']"
4867,open,endolith,"For example:

```
print(poly1d([1, 0], variable='y'))

1 y

print(poly1d([1, 1, 1]))
   2
1 x + 1 x + 1

print(polyval(poly1d([1, 1, 1]), poly1d([1, 0], variable='y')))
   2
1 x + 1 x + 1

```

I would expect the result of polyval to produce the same output as

```
print(poly1d([1, 1, 1], variable='y'))
   2
1 y + 1 y + 1
```

since x is being substituted by y
",2014-07-15 03:30:26,,BUG: polyval does not substitute variable names,"['00 - Bug', 'component: numpy.lib']"
4829,open,jason-s,"GCD for polynomials is an important operation for computing sums of polynomial ratios A(x)/B(x) + C(x)/D(x). The naive approach to computing GCD(B(x),D(x)) is numerically ill-conditioned:

```
import numpy as np

def polygcd(a,b):
    '''return monic GCD of polynomials a and b'''
    pa = a
    pb = b
    M = lambda x: x/x[0]
    # find gcd of a and b
    while len(pb) > 1 or pb[0] != 0:
        # Danger Will Robinson! requires numerical equality
        q,r = np.polydiv(pa,M(pb))
        pa = pb
        pb = r
    return M(pa)

def polylcm(a,b):
    '''return (Ka,Kb,c) such that c = LCM(a,b) = Ka*a = Kb*b'''        
    gcd = polygcd(a,b)
    Ka,_ = np.polydiv(b,gcd)
    Kb,_ = np.polydiv(a,gcd)
    return (Ka,Kb,np.polymul(Ka,a))
```

I figured there was an easy workaround that would be more robust, but apparently there isn't. There is some promising literature on the subject (see below), but I'm woefully underskilled for porting these algorithms to numpy, much less understanding how they work:
- http://math.univ-lille1.fr/~bbecker/ano/pub/1997/ano369.pdf
- http://www.mathcs.emory.edu/~boito/newfifthspecial.pdf
- http://www.mathcs.emory.edu/~boito/thesis.pdf
- http://www.mathcs.emory.edu/~boito/fast_gcd_wls.m [matlab code]
- https://who.rocq.inria.fr/Jan.Elias/pdf/je_masterthesis.pdf
- http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=875462
- http://www.mmrc.iss.ac.cn/pub/mm21.pdf/zhi1.pdf
- http://arxiv.org/pdf/1207.0630v2.pdf
- http://www.neiu.edu/~zzeng/
- http://www.neiu.edu/~zzeng/uvgcd.pdf
- http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.27.4881
",2014-06-27 00:12:57,,ENH: add numerically robust polygcd function for numerical calculation of polynomial greatest common divisor,['unlabeled']
4817,open,frozflame,"Is there any function similar to `numpy.mat` to create an `numpy.ndarray`? I mean something equivolent to 

```
np.array(np.mat('.2 .7 .1; .3 .5 .2; .1 .1 .9'))
```

which is a little bit verbose. And 

```
np.array([[.2, .7, .1], [.3, .5, .2], [.1, .1, .9]])
```

sounds daunting for beginners. This might seem trivial (as `mkarry = lambda s:np.array`), but when I do some demonstrations to people who have never used Python,  it matters. 

Is there such a function in `numpy`? If no, is it sensible to add one in `numpy`?
",2014-06-21 16:07:01,,Array creation in `numpy.mat` style,['01 - Enhancement']
4767,open,tatarinova,"Hello,

I would like to know if it is possible to calculate a percentile when an input array is a masked array.

Natalia
",2014-06-02 11:06:18,,percentile for masked array,"['01 - Enhancement', 'component: numpy.ma']"
4715,open,ekta1007,"Quick Question : What is the ""tol"" set for linalg for matrix inversion in linalg.py ?
Usecase : Given a non-invertible(non full rank) matrix, reduce the matrix  to its independent features.

Towards the same, I am building on the code here 
http://stackoverflow.com/questions/13312498/how-to-find-degenerate-rows-columns-in-a-covariance-matrix
and , post that feeding the ""reduced matrix"" which is then converted back to a pandas dataframe to fit a Logit model. Here's the stack trace -:
""""""
I checked that X is a full rank matrix prior to running the code
y is the outcome var & x is the regressor var
""""""
Traceback (most recent call last):
  File ""/home/ekta/Python_scripts/DYB logistic regression/iab_list.py"", line 275, in <module>
    test_scores = smf.Logit(y,X,missing='drop').fit()
  File ""/usr/local/lib/python2.7/site-packages/statsmodels-0.5.0-py2.7-linux-i686.egg/statsmodels/discrete/discrete_model.py"", line 1186, in fit
    disp=disp, callback=callback, *_kwargs)
  File ""/usr/local/lib/python2.7/site-packages/statsmodels-0.5.0-py2.7-linux-i686.egg/statsmodels/discrete/discrete_model.py"", line 164, in fit
    disp=disp, callback=callback, *_kwargs)
  File ""/usr/local/lib/python2.7/site-packages/statsmodels-0.5.0-py2.7-linux-i686.egg/statsmodels/base/model.py"", line 357, in fit
    hess=hess)
  File ""/usr/local/lib/python2.7/site-packages/statsmodels-0.5.0-py2.7-linux-i686.egg/statsmodels/base/model.py"", line 405, in _fit_mle_newton
    newparams = oldparams - np.dot(np.linalg.inv(H),
  File ""/usr/local/lib/python2.7/site-packages/numpy/linalg/linalg.py"", line 445, in inv
    return wrap(solve(a, identity(a.shape[0], dtype=a.dtype)))
  File ""/usr/local/lib/python2.7/site-packages/numpy/linalg/linalg.py"", line 328, in solve
    raise LinAlgError, 'Singular matrix'
LinAlgError: Singular matrix
",2014-05-15 11:54:27,,Tolerance threshold limit in linalg.py ,['unlabeled']
4660,open,hamogu,"It seems that in python 2.7 I can do `a = {np.ma.masked: 'AAA'}` while in python3 this code throws `TypeError: unhashable type: 'MaskedConstant'` (numpy 1.8. in both cases).
",2014-05-03 16:41:26,,Should np.ma.masked be hashable?,"['component: numpy.ma', '54 - Needs decision']"
4581,open,ewmoore,"It would be really nice if numpy had a function that could set the DAZ (denormals are zero) and the FTZ (flush to zero) flags. Setting these flags causes a zero to be stored or loaded instead of a denormal number. This can dramatically effect the performance, for instance, in a simulation of mine, using float32 instead of float64 causes the run time to be more than twice as long. Setting these flags using a tiny custom python extension cuts the time for float32 to 75% of the time used for float64.   It would be nice to be able to make this trade off without requiring it at all times as setting these flags using compiler options does. 
",2014-04-03 17:44:56,,Allowing setting denormal handling,['unlabeled']
4564,open,mhvk,"In astropy, we have defined an array subclass `Quantity` that carries units. One method that does not always keep subclass information is `.flat`:

```
import astropy.units as u
q = u.Quantity([1., 2., 3.], 'm')
q.flat[:2]
# <Quantity [ 1., 2.] m>  --> slices are fine
q.flat[0], q[0]
# (1.0, <Quantity 1.0 m>)  --> this is not; returned a float
for _q in q.flat: print(_q)
#1.0
#2.0
#3.0  --> not good either
```

Right now, we are working around this by overriding `.flat` and using our own iterator, similar to what is done in `MaskedArray` with `MaskedIterator`. In principle, though, it would seem that the iterator should not just return python float, but perhaps pass through some hook that can be overridden (possibly `__array_finalize__`).
",2014-03-28 13:46:09,,BUG .flat on ndarray subclasses does not return subclass items,['unlabeled']
4560,open,abalkin,"> ma.asarray has an order keyword which is not used, nor available in masked_array.  Also, ma.asanyarray doesn't have an order keyword, but is documented as having one.

See @charris comment at gh-4045.
",2014-03-27 13:55:31,,DOC: Improve ma.asarray documentation,['component: numpy.ma']
4554,open,xnx,"`x, y = np.random.rand(100), np.random.rand(100, 25)`
`fit = np.polynomial.polynomial.polyfit(x, y, 2)`
works fine, but
`fit = np.polynomial.Polynomial.fit(x, y, 2)`
fails with `ValueError: Coefficient array is not 1-d` from `polyutils.py`.
",2014-03-26 13:43:45,,Polynomial.fit() fails for multi-dimensional coefficient arrays,['Proposal']
4440,open,seberg,"When trying gh-4434, I ran into a force-cast difference between timedelta force casting and timedelta item assignment. There is also a discrepancy for `float64` vs. python floats:

```
In [12]: np.array(3.).astype('timedelta64[D]')
Out[12]: array(datetime.timedelta(3), dtype='timedelta64[D]')

In [13]: np.array(3., dtype='timedelta64[D]')
ValueError: Could not convert object to NumPy timedelta
```

and:

```
In [17]: np.empty(1, dtype='timedelta64[D]')[[0]] = np.float64(3.)

In [18]: np.empty(1, dtype='timedelta64[D]')[0] = np.float64(3.)
ValueError: Could not convert object to NumPy timedelta
```

I could easily hack around this by adding an ellipsis, but this all seems a bit weird...
",2014-03-05 10:39:41,,Timedelta does not allow assignment from float,"['01 - Enhancement', 'component: numpy.datetime64']"
4425,open,cpelley,"Two bugs seem to be present (when considering masked arrays):
- The mask of the resulting array does not update with the following `np.add(a, b, out=a)`
- The behaviour of the ufunc and the object operator don't agree.  Possibly this is as intended?? however, this difference of behaviour is not documented as far as I can tell (`a + b` does not give the same values as `np.add(a, b)` - one uses the mask to ignore, the other doesn't ignore masked values).

Not limited to `numpy.add`, those tested and observed as being effected include: `add`, `multiply`, `divide`, `multiply`.

``` Python
import numpy as np

a = np.ma.array([1, 2, 3, 4], mask=[True, False, False, True])
b = np.ma.array([1, 2, 3, 4], mask=[False, True, False, True])

# ufunc not in-place
c = np.add(a, b)

# object operator
d = a + b 

# ufunc in_place
np.add(a, b, out=a)

print 'not in-place:\n{}\n{}\n'.format(repr(c), c.data)
print 'object operator:\n{}\n{}\n'.format(repr(d), d.data)
print 'in-place:\n{}\n{}\n'.format(repr(a), a.data)
```

```
not in-place:
masked_array(data = [-- -- 6 --],
             mask = [ True  True False  True],
       fill_value = 999999)

[2 4 6 8]

object operator:
masked_array(data = [-- -- 6 --],
             mask = [ True  True False  True],
       fill_value = 999999)

[1 2 6 4]

in-place:
masked_array(data = [-- 4 6 --],
             mask = [ True False False  True],
       fill_value = 999999)

[2 4 6 8]
```
",2014-03-03 13:46:29,,BUG: ufunc in-place results in an incorrect mask set,"['00 - Bug', 'component: numpy.ma']"
4422,open,jnothman,"A masked array containing `nan` currently sorts such that `nan`s appear at the end of the array, violating `endwith=True`:

``` python
>>> m = np.ma.masked_array([np.nan, -1], mask=[0, 1])
>>> m
masked_array(data = [nan --],
             mask = [False  True],
       fill_value = 1e+20)
>>> m.sort()
>>> m
masked_array(data = [-- nan],
             mask = [ True False],
       fill_value = 1e+20)
```

Apart from its own contract, this breaks consistency within the current implementation of `np.ma.median` (which should probably not rely on `sort` anyway), returning:

``` python
>>> np.ma.median(np.ma.masked_array([np.nan, 5, -1], mask=[0, 0, 1]))
5.0
>>> np.ma.median(np.ma.masked_array([np.nan, 5], mask=[0, 0]))
nan
```

(not that I'm sure what it means to take a median when there are `nan`s present)

Perhaps `minimum_fill_value` needs an alternative for the `sort` case because of the convention that `nan` comes last.
",2014-03-02 21:30:50,,"ma.sort(..., endwith=True) broken with nans","['00 - Bug', 'component: numpy.ma']"
4398,open,FrancescAlted,"It is quite frequent for some applications (numexpr, but others too) to hit the NPY_MAXARGS limit.  You can find a report about this problem here:

https://github.com/PyTables/PyTables/issues/286

Making this number larger (say 256) would alleviate the issue a lot.

PR #226 tries to tackle the problem, but probably just increasing the value would be enough.  There has been a recent discussion in the numpy mailing list too:

http://mail.scipy.org/pipermail/numpy-discussion/2014-February/069266.html
",2014-02-28 16:16:48,,Increase NPY_MAXARGS to more than 32,"['01 - Enhancement', '63 - C API']"
4384,open,dsm054,"This is related to #4328, I think:

```
>>> np.__version__
'1.9.0.dev-3a2f048'
>>> a = np.array([1,2,3], dtype=np.int64)
>>> np.repeat(a, a)
Traceback (most recent call last):
  File ""<ipython-input-23-739b853dec18>"", line 1, in <module>
    np.repeat(a, a)
  File ""/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py"", line 391, in repeat
    return repeat(repeats, axis)
TypeError: Cannot cast array data from dtype('int64') to dtype('int32') according to the rule 'safe'

>>> np.repeat(a, a.astype('int32'))
array([1, 2, 2, 3, 3, 3], dtype=int64)
```

with configuration

```
python: 2.7.3.final.0
python-bits: 32
OS: Linux
OS-release: 3.2.0-58-generic-pae
machine: i686
processor: i686
byteorder: little
LC_ALL: None
LANG: en_CA.UTF-8
numpy: 1.9.0.dev-3a2f048
```
",2014-02-27 16:32:44,,int32/64 cast issue,['unlabeled']
4356,open,abalkin,"Since ma.MaskedArray inherits from ndarray, all ndarray methods are inherited, but with exception of ufuncs, such inheritance is ill-advised and may lead to silent data corruption:

```
>>> x = ma.masked_all(5)
>>> x.item(0)
-1.2882297539194267e-231
```

The list of such methods includes:
1. choose
2. dumps
3. dump
4. item
5. partition
6. argpartition
7. fill
8. searchsorted
9. dot

I would classify this issue as a bug, but a fix can be as simple as making these methods raise NotImplementedError. 
",2014-02-24 01:31:51,,BUG: Many ndarray methods are not specialized for ma.MaskedArray ,"['00 - Bug', 'component: numpy.ma']"
4355,open,abalkin,"The following constructors are missing from ma:
1. full
2. full_like
3. ones_like
4. zeros_like
",2014-02-24 01:01:51,,ENH: Equivalents of newer array constructors are not implemented in ma,"['01 - Enhancement', 'component: numpy.ma']"
4332,open,abalkin,"``` python
>>> (numpy.ma.array(5.5, mask=True) > 0).dtype
dtype('float64')
```

Moreover, the result has float dtype even if the masked scalar is an int:

``` python
>>> (numpy.ma.array(5, dtype='i', mask=True) < 0).dtype
dtype('float64')
```

The problem is not present in `oldnumeric`:

``` python
>>> (numpy.oldnumeric.ma.array(5.5, mask=True) < 0).dtype
dtype('bool')
```

```
>>> numpy.__version__
'1.8.0'
```
",2014-02-20 15:10:36,,BUG: Masked scalar comparison returns float,"['00 - Bug', 'component: numpy.ma']"
4320,open,seberg,"Usual assignment and now also advanced indexing assignments allow trailing ones in the values array such as:

```
np.arange(10)[:] = np.ones((1, 1, 1, 10))
```

However boolean indexing does not have any logic in place to ignore these.

Also for object assignments view based assignments (and now also advanced indexing assignments through a temporary step) have some extra logic to allow sequences:

```
In [6]: a = np.zeros(3, dtype=object)

In [7]: a[:] = [(1, 2), (2, 3), (3, 4)]

In [8]: a
Out[8]: array([(1, 2), (2, 3), (3, 4)], dtype=object)

In [9]: a[np.array([True, True, True])] = [(1, 2), (2, 3), (3, 4)]
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-9-361179bc09da> in <module>()
----> 1 a[np.array([True, True, True])] = [(1, 2), (2, 3), (3, 4)]

TypeError: NumPy boolean array indexing assignment requires a 0 or 1-dimensional input, input has 2 dimensions

In [10]: a[[0, 1, 2]] = [(1, 2), (2, 3), (3, 4)]
```

---

EDIT: I will note that I think times have shifted in that there may be a will now to slowly disallow the first cases as well.  The searchable word should be ""reverse broadcasting"".",2014-02-18 16:33:25,,small boolean assignment incosistencies,"['00 - Bug', 'component: numpy._core']"
4317,open,inducer,"This just happened to me with numpy 1.8.0 (from Debian):

``` python
>>> import numpy as np
>>> x = np.dtype(np.float32)
>>> from pickle import dumps, loads
>>> y = loads(dumps(x))
>>> x.isbuiltin
1
>>> y.isbuiltin
0
```
",2014-02-18 06:01:17,,Pickling/unpickling a dtype resets isbuiltin flag,"['00 - Bug', 'component: numpy._core']"
4308,open,argriffing,"copypasting a comment from @empeeu  https://github.com/numpy/numpy/issues/4301#issuecomment-35223028
# 

This actually seems to be a problem with partition.

```
>>> np.median(a, axis=0)
matrix([[ 0.,  1.,  2.]])
>>> np.median(a, axis=1)
matrix([[ 1.]])
>>> np.partition(a, 1, axis=1)
matrix([[0, 1, 2]])
>>> np.partition(a, 1, axis=None)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py"", line 619, in partition
    a.partition(kth, axis=axis, kind=kind, order=order)
ValueError: kth(=1) out of bounds (1)
>>> np.partition(np.array(a), kth, axis=None)
array([0, 1, 2])
```

median just so happens to use partition as part of its calculation.
",2014-02-17 04:47:16,,problem with np.partition of np.matrix,"['00 - Bug', 'component: numpy.lib', 'component: numpy.matrixlib']"
4301,open,argriffing,"```
>>> import numpy as np                                                          
>>> a = np.matrix([[0, 1, 2]])                                                  
>>> np.mean(a)                                                                  
1.0
>>> np.median(a)                                                                
Traceback (most recent call last):
[...]
/numpy/core/fromnumeric.py"", line 619, in partition
    a.partition(kth, axis=axis, kind=kind, order=order)
ValueError: kth(=1) out of bounds (1)
```
",2014-02-16 18:20:58,,median of np.matrix is broken,"['00 - Bug', 'component: numpy.lib', 'component: numpy.matrixlib', '54 - Needs decision']"
4280,open,argriffing,"```
$ python
Python 2.7.5+ (default, Sep 19 2013, 13:48:49) 
[GCC 4.8.1] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy as np
>>> np.__version__
'1.7.1'
>>> np.linalg.matrix_rank([[1e3, 0], [0, 1]])
2
>>> np.linalg.matrix_rank([[1e300, 0], [0, 1]])
1
>>> np.linalg.matrix_rank([[1e3000, 0], [0, 1]])
0
```

The first and second values look reasonable but the third one looks wrong.  I think this is caused by feeding lapack a matrix with an `inf` and then returning 0 because lapack doesn't understand `inf`.  Or maybe this is not considered a numpy bug and the solution is just that I should know better than to try computing `np.linalg.matrix_rank` of a matrix with an `inf`.
",2014-02-11 23:31:04,,maybe a problem with matrix_rank,"['00 - Bug', 'component: numpy.linalg']"
4260,open,juliantaylor,"scipy has the logsumexp function which avoids unnecessary calls to `log` and thus probably saves a couple ulp of rounding errors.
The underflow avoidance should already be present in numpys current version.

I don't see a reason why we couldn't do this directly in logaddexp.reduce. The only possible issue is that the reduce loop is blocked to a few thousand elements by nditer so the maximum in the kernel is no global maximum. But I don't think that makes a big difference in numeric terms as only the largest terms even contribute to the sum.
But I would appreciate input of an expert in this regard.

A prototype implementation is available here:
https://github.com/juliantaylor/numpy/tree/logaddexp
",2014-02-04 18:23:17,,improve accuracy of logaddexp.reduce,"['01 - Enhancement', 'component: numpy._core']"
4224,open,andreas-h,"Is there a technical reason why `searchsorted` only works with 1d arrays? In a current problem, I have an array `v.shape = (61, 160, 320)` and an array `a.shape = (1, 160, 320)`. It would be great if I could do `np.searchsorted(a, v, axis=0)` to yield a `(1, 160, 320)` array of the searched indices.
",2014-01-23 15:13:07,,searchsorted should get a `axis` kwarg,"['54 - Needs decision', 'Proposal']"
4217,open,jgehrcke,"I think `digitize` should change its behavior with respect to binning a value that matches the rightmost bin's right edge. Such a value should go _in_ the rightmost bin and not be classified as ""beyond the bounds"". This would make the output of `histogram` compatible to `digitize` (currently it's not).
## Example (short explanation)

``` python
>>> from numpy import histogram
>>> from numpy import digitize
>>> data = [1, 2, 3, 4, 5, 6, 7]
>>> histvals, binedges = histogram(data, 3)
>>> binedges
array([ 1.,  3.,  5.,  7.])
>>> histvals
array([2, 2, 3])

# Up to here, a histogram has been created characterized by 
# four bin edges (three bins). The rightmost bin contains 
# three data values, as its right edge is considered to belong
# to the bin.

# Now assign the data values to given bins 
# (represented by `binedges`) via `digitize`
>>> digitize(data, binedges)
array([1, 1, 2, 2, 3, 3, 4])

# The first two data values have been assigned to bin 1,
# the next two data values were assigned to bin 2,
# the next two data values were assigned to bin 3,
# the remaining data value has been declared as ""out of bounds"".
# This is where `histogram` and `digitize` behave differently by
# default.

# Reproduce `histogram` binning by manually shifting the 
# rightmost bin edge by an epsilon value:
>>> binedges[-1] += 10**-6
>>> digitize(data, binedges)
array([1, 1, 2, 2, 3, 3, 3])
```
## Longish explanation

I am assuming that `digitize` generally is considered to do the inverse operation of `histogram`, i.e. whereas `histogram` creates bins (the edges of the bins) and assigns values to bins, `digitize` (quote from `digitize` docs) _""Returns the indices of the bins to which each value in input array belongs.""_ (given the data values and the bin edges). This ""reverse indexing"" behavior has also been referred to in this issue: https://github.com/numpy/numpy/issues/990. It seems like other numerical frameworks provide this functionality, too.

In the `histogram` specs (http://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html), the meaning of the bin edges is clarified. By default, bins contain the left edge and do _not_ contain the right edge, **except** for the rightmost bin, whose right edge belongs to the bin (_""The last bin, however, is [3, 4], which includes 4.""_)

In contrast, the `digitize` specs state that _""Each index i returned is such that bins[i-1] <= x < bins[i]""_. There is no comment on the rightmost boundary, so this general statement also applies to it.

In conclusion, whereas `histogram` always creates the rightmost bin edge in a way that it corresponds to the maximum data value, `digitize` does not count this data value as part of the histogram represented by the bin edges array. 

Restoring compatibility between both functions requires manually incrementing the rightmost bin edge by an epsilon value.

I think it comes down to the question of what people expect `digitize` to do exactly. I would also be fine with always manually correcting one boundary, but am pretty sure that that has not been the original idea behind `digitize`.
",2014-01-20 16:35:18,,digitize is not the inverse of histogram (right limit treated differently),"['01 - Enhancement', 'component: numpy.lib']"
4198,open,ml31415,"I'm referring to this [discussion](http://stackoverflow.com/questions/20932361/resizing-numpy-memmap-arrays) on stackoverflow. I'm trying to resize memmap arrays just as it works with ordinary arrays.

``` python
a = np.memmap('bla.bin', mode='w+', dtype=int, shape=10)
>>> memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
a.resize(20, refcheck=False)
>>> ValueError: cannot resize this array: it does not own its data
```

I'm not sure about implications and side effects, but for me it looks like, if `mmap` supports resizing, `np.memmap` also should. I understand, that it might be hard to guarantee private access for a memmap. But if access can be excluded by the programmer, it should at least be possible to override any check by hand. And if resizing really shouldn't be possible, at least the [documentation](http://docs.scipy.org/doc/numpy/reference/generated/numpy.memmap.resize.html) could be clear about this point.
",2014-01-14 13:02:56,,Resizing memmap arrays,['unlabeled']
4126,open,elgehelge,"Uncertain whether this is the desired behaviour or a bug. In my opinion the programmer should not have to consider whether the length of the arrays are too long.
## Code:

``` python
import np
ones = np.ones(32768, dtype=np.int16)
np.dot(ones, ones)
```
## Returns:

-32768

Integer overflow also happens when using other bit sizes of int.

It especially becomes a problem when using `csr_matrix` (and probably many more modules) that automatically sets the `dtype` to `np.int8` when dealing with a matrix of zeros and ones.
",2013-12-16 17:20:22,,Integer overflow using numpy.dot,"['component: numpy._core', 'Proposal']"
4065,open,ChrisBeaumont,"These should all be zero, right?

```
In [6]: x = np.ma.array([1.]); print np.log(x, out=x) 
[--]

In [7]: x = np.ma.array([1.]); print np.log(x)
[0.0]

In [8]: x = np.array([1.]); print np.log(x)
[ 0.]

In [9]: x = np.array([1.]); print np.log(x, out=x)

In [10]: print np.__version__
1.8.0
```
",2013-11-19 21:05:45,,np.log(1) gives the wrong answer when using the out argument and masked arrays,"['00 - Bug', 'component: numpy.ma']"
4043,open,abalkin,"For performance and consistency with `np.asarray`, `ma.asarray` should pass through objects that are already of the right type:

```
>>> x = np.array([])
>>> m = np.ma.array([])
>>> x is np.asarray(x)
True
>>> m is np.ma.asarray(m)
False
```

Same applies to `ma.asanyarray`.
",2013-11-12 16:15:09,,ma.asarray() should pass through objects of the right type,"['01 - Enhancement', 'component: numpy.ma']"
4013,open,chatcannon,"Compiling this Fortran module

```
module bar
contains
real function foo(x)
  real, intent(in), optional :: x

  if (present(x)) then
    foo = x
  else
    foo = 1
  end if
end function foo

end module bar
```

with `f2py -m foobar -c foobar.f90` and trying to use it gives the following results

```
In [1]: import foobar

In [2]: foobar.bar.foo(20)
Out[2]: 20.0

In [3]: foobar.bar.foo()
Out[3]: 0.0
```

I'm using numpy 1.8.0
",2013-11-05 12:00:42,,f2py passes 0 to omitted optional arguments instead of omitting them,"['00 - Bug', 'component: numpy.f2py']"
3994,open,Nodd,"See the following code (run on Ubuntu 64bits):

```
In [1]: import numpy as np

In [2]: b = np.random.rand(500, 500) + 1j * np.random.rand(500, 500)

In [3]: %timeit np.sqrt(b.real**2 + b.imag**2)
100 loops, best of 3: 4.15 ms per loop

In [4]: %timeit np.abs(b)
100 loops, best of 3: 6.06 ms per loop
```

`abs()` is slow compared to the manual formula. Is there any drawback in using `np.sqrt(b.real**2 + b.imag**2)` (like possible overflow ?)

Also, it could be useful to add an `abs2()` function which would return the value of `abs()**2` but should be really faster for complex:

```
In [5]: %timeit b.real**2 + b.imag**2
1000 loops, best of 3: 1.4 ms per loop
```

It is commonly used to compute the energy of a signal or to sort complex numbers by norm value, for example.
",2013-10-29 16:19:02,,"abs() is slow for complex, add abs2()","['component: numpy._core', 'Proposal']"
3836,open,mdickinson,"It looks as though the `datetime64` dtype breaks the Python rule that `x == y` should imply `hash(x) == hash(y)`.  This broke a Pandas application that was grouping on dates, and then doing a dictionary lookup to find the lines of a `DataFrame` associated to a particular date.

```
Python 3.3.2 (default, May 21 2013, 11:48:51) 
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy
>>> numpy.__version__
'1.7.1'
>>> import datetime
>>> x = numpy.datetime64(datetime.datetime.now())
>>> y = numpy.datetime64(x, 'ns')
>>> x == y  # gives the expected 'True'
True
>>> hash(x) == hash(y)  # expected True
False
```
",2013-09-30 08:26:45,,datetime64 breaks equality and hash invariant.,"['00 - Bug', 'component: numpy.datetime64']"
3817,open,BenFrantzDale,"I often find myself writing

```
x.reshape(x.shape[0], -1)
```

and occasionally:

```
x.reshape(prod(x.shape[:2]), x.shape[2], -1)
```

It seems like a common sort of thing that should have some syntactic sugar added. I'm thinking something like the newaxis=None definition: magic values to let you do this:

```
x.reshape(keepaxis, -1)
```

and

```
x.reshape(mergeaxis, mergeaxis, keepaxis, -1)
```

respectively.

I could see using

```
keepaxis = slice(None)
```

and

```
mergeaxis = None
```

or

```
mergeaxis = Ellipsis
```

the advantage of which is that you could give reshape a `__getitem__` method and with some more logic do

```
x.reshape[:,...]
```

and

```
x.reshape[...,...,:]
```

Even without indexing notation, a little logic in reshape would make it easy to keep an existing axis while flattening those around it. Thoughts?
",2013-09-28 02:37:12,,reshape() should have a way to keep a dimension unchanged.,"['component: numpy._core', '54 - Needs decision', 'Proposal']"
3763,open,felixlen,"It may sound silly but it may be of interest to be able to compute the norm of [] so that one has not to catch this case before (would make my code more readable).

At the moment numpy.linalg.norm(numpy.array([]),ord=):
- works with ord = 0,1,2,... giving the result 0. Although here is a incosistency returning np.int in the case of ord = 0 and np.float in the others.
- works not not with ord = numpy.inf since this computes abs(x).max() without checking if x is nonempty.

It would be very nice, if in the [] case also for ord = inf one would return 0, which would be consistent with matlab behaviour.
",2013-09-19 07:23:40,,linalg.norm with ord=numpy.inf does not work for empty array,"['00 - Bug', 'component: numpy.linalg']"
3762,open,agartland,"I first raised this issue on stackoverflow (see link on the bottom)

Seems like the new masked_array should inherit the fill_value from the two masked_arrays being summed?

Can someone explain to me this behavior of a numpy masked_array? It seems to change the fill_value after applying the sum operation, which is confusing if you intend to use the filled result.

```
data=ones((5,5))
m=zeros((5,5),dtype=bool)

""""""Mask out row 3""""""
m[3,:]=True
arr=ma.masked_array(data,mask=m,fill_value=nan)

print arr
print 'Fill value:', arr.fill_value
print arr.filled()

farr=arr.sum(axis=1)
print farr
print 'Fill value:', farr.fill_value
print farr.filled()

""""""I was expecting this""""""
print nansum(arr.filled(),axis=1)
```

Prints output:

```
[[1.0 1.0 1.0 1.0 1.0]
 [1.0 1.0 1.0 1.0 1.0]
 [1.0 1.0 1.0 1.0 1.0]
 [-- -- -- -- --]
 [1.0 1.0 1.0 1.0 1.0]]
Fill value: nan
[[  1.   1.   1.   1.   1.]
 [  1.   1.   1.   1.   1.]
 [  1.   1.   1.   1.   1.]
 [ nan  nan  nan  nan  nan]
 [  1.   1.   1.   1.   1.]]
[5.0 5.0 5.0 -- 5.0]
Fill value: 1e+20
[  5.00000000e+00   5.00000000e+00   5.00000000e+00   1.00000000e+20
   5.00000000e+00]
[  5.   5.   5.  nan   5.]
```

http://stackoverflow.com/questions/18879272/why-does-sum-operation-on-numpy-masked-array-change-fill-value-to-1e20
",2013-09-19 00:13:20,,Operation on masked_array changes fill_value,"['component: numpy.ma', '54 - Needs decision', 'Proposal']"
3758,open,khinsen,"Some array methods fail when passed a immutable view of an array, although they should not try to write to it. Tested with 1.6.1 and 1.7.1.

A short demonstration:

``` python
import numpy as np

class ImmutableNDArray(np.ndarray):

    def __new__(cls, *args, **kwargs):
        return np.ndarray.__new__(cls, *args, **kwargs)

    def __array_finalize__(self, obj):
        self.setflags(write=False)

indices = np.array([2, 3], np.uint16).view(ImmutableNDArray)

data = np.array([1, 1, 5, 5, 4, 3])


# These two raise the exception
#     ValueError: assignment destination is read-only
print data.take(indices)
print data[:2].repeat(indices)


# These two works fine
print data.take(np.array(indices))
print data[:2].repeat(np.array(indices))
```
",2013-09-17 15:44:08,,"Exception ""assignment destination is read-only"" when reading from a read-only array","['00 - Bug', 'component: numpy._core']"
3751,open,chatcannon,"I've just spent ages tracking down a bug that was (at least partially) caused by forgetting to list a module variable in the .pyf file - I set the attribute in Python, but when Fortran tried to access it there was a segfault (the variable was an allocatable array).

It would be helpful if there was an option to make the generated module override `__setattr__` and refuse to set attributes that aren't connected to the Fortran module.
",2013-09-15 10:21:12,,Wish: option for f2py modules not to allow setting attributes that weren't defined from Fortran,"['01 - Enhancement', 'component: numpy.f2py']"
3722,open,charris,"A number of single precision functions look to have come in with the gufunc work. It would be good to support them in all of the current routines.
",2013-09-12 00:00:31,,"Support single precision in linalg, don't coerce to double","['17 - Task', 'component: numpy.linalg']"
3659,open,toivoh,"The `array_size` macro defined in `numpy.i` ([here](https://github.com/numpy/numpy/blob/938f586d6f61654dec8e9e3f14084e98d86f60e9/doc/swig/numpy.i#L43) and [here](https://github.com/numpy/numpy/blob/938f586d6f61654dec8e9e3f14084e98d86f60e9/doc/swig/numpy.i#L57)) conflicts with the `boost/range` headers from the [boost](http://www.boost.org/) library, since they use `array_size` as an identifier. There is also a number of similarly named macros in the same file, which may presumably cause similar name clashes in the future.

I don't know if it would be possible to reimplement these as functions; otherwise I hope that they can be renamed to something less generic in order to avoid name clashes.
",2013-08-29 12:55:06,,SWIG: numpy.i Macro array_size conflicts with boost/range,"['component: Other', 'Proposal']"
3585,open,pv,"Copied from scipy issues:

_Original ticket http://projects.scipy.org/scipy/ticket/1677 on 2012-06-14 by trac user iandavis, assigned to unknown._

As far as I can tell, numpy.memmap() does not currently support anonymous mode (MAP_ANONYMOUS for C's mmap), where there is no backing file.  However, I've found MAP_ANON useful for creating sparse arrays larger than available memory, where most array entries are zero.  It's fast and efficient, and doesn't use any disk space (or cause any disk IO) unless you outgrow RAM.  For certain applications, it has major advantages over scipy.sparse -- for instance, it can handle any shape and number of dimensions efficiently, while scipy.sparse only does 2-D matrices.

Anyway, I've adapted numpy.memmap() into the sparse_zeros() class below.  I don't understand what all the support code was for originally, so it's possible there are bugs / places to improve.

I'd propose this be added to either scipy.sparse, or directly to numpy itself.  I didn't see an obvious way to add this to numpy.memmap() because it expects there to be a file name, but that would be a third possibility.

```
import numpy as np
class sparse_zeros(np.ndarray):
    """"""
    Copied from numpy.core.memmap (v1.5.1)
    Provides a zeros()-like array backed by an anonymous mmap().
    Only pages with non-zero values require memory storage.
    If enough pages are written to, however, you'll still get swapping.
    """"""
    __array_priority__ = -100.0
    def __new__(subtype, shape, dtype=np.uint8, order='C'):
        # Import here to minimize 'import numpy' overhead
        import mmap
        descr = np.dtype(dtype)
        if not isinstance(shape, tuple): shape = (shape,)
        bytes = descr.itemsize
        for k in shape:
            bytes *= k
        acc = mmap.ACCESS_COPY
        mm = mmap.mmap(-1, bytes, access=acc)
        self = np.ndarray.__new__(subtype, shape, dtype=descr, buffer=mm, order=order)
        self._mmap = mm
        return self
    def __array_finalize__(self, obj):
        if hasattr(obj, '_mmap'):
            self._mmap = obj._mmap
        else:
            self._mmap = None
    def flush(self): pass
    def sync(self):
        """"""This method is deprecated, use `flush`.""""""
        warnings.warn(""Use ``flush``."", DeprecationWarning)
        self.flush()
    def _close(self):
        """"""Close the memmap file.  Only do this when deleting the object.""""""
        if self.base is self._mmap:
            # The python mmap probably causes flush on close, but
            # we put this here for safety
            self._mmap.flush()
            self._mmap.close()
            self._mmap = None
    def close(self):
        """"""Close the memmap file. Does nothing.""""""
        warnings.warn(""``close`` is deprecated on memmap arrays.  Use del"", DeprecationWarning)
    def __del__(self):
        # We first check if we are the owner of the mmap, rather than
        # a view, so deleting a view does not call _close
        # on the parent mmap
        if self._mmap is self.base:
            try:
                # First run tell() to see whether file is open
                self._mmap.tell()
            except ValueError:
                pass
            else:
                self._close()
```
",2013-08-07 12:54:12,,Support anonymous mmap in numpy.memmap(),"['01 - Enhancement', 'component: numpy._core', 'Patch']"
3562,open,jsolbrig,"I have tried to put the problem in the first paragraph.  The rest shows a basic example of the problem.

I am attempting to compile a module that contains a `USE` statement pointing to another, more general, module.  I would prefer to keep the used module separate so that it can be used in several places as a set of general settings.  When I complie the two modules using f2py, everything works correctly from the fortran side, but fromt he python side `USE` appears to be ignored.  If I allow f2py to generate a signature file, the file contains a `USE` statement as is appropriate, but if I complete the compilation and import from the resulting library, the parameters from the used module are not available in the module that contains the use statement.  Below are two modules illustrating the situation:

```
MODULE test
    INTEGER, PARAMETER :: a = 1
END MODULE test

MODULE test2
    USE test
    INTEGER, PARAMETER :: b = 2
END MODULE test2
```

In order to show the intermediate step I ran `f2py -h test.pyf test.f90 test2.f90`.  The following signature file is generated; note that the ""test2"" module contains `use test`:

```
!    -*- f90 -*-
! Note: the context of this file is case sensitive.

python module test ! in
    interface  ! in :test
        module test ! in :test:test.f90
            integer, parameter,optional :: a=1
        end module test
        module test2 ! in :test:test2.f90
            use test
            integer, parameter,optional :: b=2
        end module test2
    end interface
end python module test

! This file was auto-generated with f2py (version:2).
! See http://cens.ioc.ee/projects/f2py2e/
```

If I now compile with `f2py --fcompiler=gfortran -c test.pyf test.f90 test2.f90` I obtain test.so (same as running `f2py --fcompiler=gfortran -m test -c test.f90 test2.f90` without creating the signature file first).  Importing from this library in python exposes test.test.a and test.test2.b, but does not expose test.test2.a as can be seen here:

```
In [1]: import test

In [2]: print test.test.a
1

In [3]: print test.test2.b
2

In [4]: print test.test2.a
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/users/solbrig/svn_checkouts/inversion/satmet/branches/solbrig/rootpath/data/users
/GeoIPS/src/test/<ipython-input-4-bffcf464e408> in <module>() 
----> 1 print test.test2.a

AttributeError: a
```

Just to illustrate that `b` is defined properly in test2 from the perspective of fortran, the following code uses test2 and prings both `a` and `b`:

```
SUBROUTINE run_test()
    USE test2
    IMPLICIT NONE
    print *, ""a = "", a
    print *, ""b = "", b
END SUBROUTINE run_test
```

After compiling with `f2py -m run_test -c test.f90 test2.f90 run_test.f90` and obtaining run_test.so, run_test can be imported in python and works as expected:

```
In [1]: import run_test

In [2]: run_test.run_test()
 a =            1
 b =            2
```

Please let me know if there is any other information that I can provide or if there is a solution for this issue.
",2013-08-01 01:27:04,,f2py: Unable to expose parameters from “used” modules,"['00 - Bug', 'component: numpy.f2py']"
3555,open,piannucci,"The companion matrix is already upper Hessenberg, so it should be possible to use a specialized routine to find its eigenvalues, rather than the general non-symmetric eigenvalue routine.  In particular,

lib/polynomial.py#L227 should call (d,z)hseqr rather than eigvals, which calls (d,z)geev.
",2013-07-26 06:55:52,,Polynomial roots computation calls unnecessarily general LAPACK routine,"['component: numpy.linalg', 'component: numpy.polynomial', 'Proposal']"
3543,open,jjhelmus,"When performing an operation with a masked array and a numpy scalar the order of the operands will change the results.

Example

``` Python
import numpy as np
print np.__version__
data = np.ma.masked_greater(np.arange(10), 7.5)
two = np.int64(2)
print ""Multiplication""
print (data * two).data
print (two * data).data
print ""Addition""
print (data + two).data
print (two + data).data
```

Output

```
1.8.0.dev-90ececa
Multiplication
[ 0  2  4  6  8 10 12 14  8  9]
[ 0  2  4  6  8 10 12 14 16 18]
Addition
[2 3 4 5 6 7 8 9 8 9]
[ 2  3  4  5  6  7  8  9 10 11]
```

I assume this occurs because numpy.int64.**mult** and the like do not take into account masked values.  This differs from ndarray objects where the order of the operands does not matter.
",2013-07-22 15:49:16,,masked arrays do not operate commutatively with numpy scalars,"['00 - Bug', 'component: numpy.ma']"
3540,open,mattip,"round() does not consistently return an ndarray subtype
test results are similar on windows and linux64

C:\Program Files (x86)\console2>c:\Python27\python
Python 2.7.3 (default, Apr 10 2012, 23:31:26) [MSC v.1500 32 bit (Intel)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

```
>>> import numpy as np
>>> np.version.version
```

'1.7.0'
    >>> class C(np.ndarray):
    ...     pass
    ...
    >>> a=np.arange(4).view(C)
    >>> a
C([0, 1, 2, 3])
    >>> a.round(decimals=0)
C([0, 1, 2, 3])
    >>> a.round(decimals=-1)
array([0, 0, 0, 0])
    >>> a.round(decimals=1)
C([0, 1, 2, 3])
    >>> a=np.arange(4,dtype=float).view(C)
    >>> a.round(decimals=1)
array([ 0.,  1.,  2.,  3.])**
    >>> a.round(decimals=0)
C([ 0.,  1.,  2.,  3.])
    >>> a.round(decimals=-1)
array([ 0.,  0.,  0.,  0.])**
",2013-07-20 17:57:22,,subtypes and round(),"['00 - Bug', 'component: numpy._core']"
3503,open,inducer,"This issue is a continuation of the disussion with @seberg in #3501, along with #3502:

Lots of places (such as [1](https://github.com/numpy/numpy/blob/master/numpy/core/src/multiarray/multiarraymodule.c#L950), [2](https://github.com/numpy/numpy/blob/master/numpy/core/src/multiarray/scalartypes.c.src#L255)) use `PyArray_Type->tp_as_number->nb_multiply` to call the `multiply` ufunc (etc.). Because of `__array_priority__`, the semantics of `nb_multiply` are no longer the same as the ufunc. Specifically, `nb_multiply` is required to to return `NotImplemented` in cases of priority inversion, whereas the ufunc has a job to do and just needs to do it, without asking questions about priorities.

---

EDIT (seberg): This is still the case and probably cause of a few bugs (I am very sure reading an issue recently).  It shouldn't be hard to fix, we have the `nb_ops` or mapping that is used for the array object:  That needs to used throughout for `scalartypes.c.src` and `scalarmath.c.src` would do so indirectly then, although directly might solve additional unnecessary dances.",2013-07-05 01:17:16,,array_multiply != the multiply ufunc,"['15 - Discussion', 'component: numpy._core', 'Project']"
3481,open,jonathanrocher,"In my opinion the following code should work following Python's regular operation on arrays instead of raising exceptions:

```
import numpy as np
a = np.array([""a"", ""b"", ""c""])
print a+a
TypeError: unsupported operand type(s) for +: 'numpy.ndarray' and 'numpy.ndarray'

print 2*a
TypeError: unsupported operand type(s) for *: 'int' and 'numpy.ndarray'
```
",2013-06-28 15:27:58,,Addition and multiplication by a number on string numpy arrays raise exceptions,"['01 - Enhancement', 'component: numpy._core', 'Proposal']"
3479,open,giogadi,"The following code causes a division by zero exception:

``` python
x = np.array([[None, 1, 2], [None, 4, 5], [None, 7, 8]])

mx = np.ma.masked_array(x, np.equal(x, None))

mx.mean(axis=0)
```

Strangely, if `x` doesn't have any `None`s but `mx` is created with the same elements masked, the `mean()` call doesn't throw an exception.

This issue is important to address when analyzing data in which some measurements are invalid (`None` in this case).
",2013-06-28 00:19:49,,np.ma should complain that it can't compute a .mean on a object array ,"['00 - Bug', 'component: numpy.ma']"
3437,open,JuliusDegesys,"I get inconsistent behavior when a user-defined class implements `__len__`, `__getitem__`, and binary operators.

``` python
import numpy as np
import sys
print(sys.version)
print(""Numpy Version: {}"".format(np.__version__))

class Table(object):
  def __init__(self, data):
    self.data = data

  def __str__(self):
    return str(self.data)

  def __len__(self):
    return self.data.shape[1]

  def __getitem__(self, value_idx):
    new_data = self.data[:, value_idx]
    new_data.shape = (-1, 1)
    return Table(new_data)

  def __add__(self, rhs):
    return Table(self.data + rhs)

  def __radd__(self, lhs):
    return Table(lhs + self.data)

data = np.array([[0.0, 1.0], [1.0, 2.0], [2.0, 3.0]])

table = Table(data)

print(""{:=^80}"".format("" Table + np.float64(2) ""))
print(table + np.float64(2))

print(""{:=^80}"".format("" np.float64(2) + Table ""))
print(np.float64(2) + table)

print(""{:=^80}"".format("" float(2) + Table ""))
print(float(2) + table)
```

The output of this code is the following:

```
$ ./test.py 
2.7.1 (r271:86832, Jan  4 2012, 17:35:19) 
[GCC 4.1.2 20080704 (Red Hat 4.1.2-51)]
Numpy Version: 1.7.1
============================ Table + np.float64(2) =============================
[[ 2.  3.]
 [ 3.  4.]
 [ 4.  5.]]
============================ np.float64(2) + Table =============================
[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[<__main__.Table object at 0x294ffd0>]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]






























 [[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[<__main__.Table object at 0x29d5050>]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]
=============================== float(2) + Table ===============================
[[ 2.  3.]
 [ 3.  4.]
 [ 4.  5.]]

```

In this example, I would expect to be able to make the following assertions:

``` python
assert float(2) + table == table + float(2)
assert np.float32(2) + table == float(2) + table
assert np.float64(2) + table == float(2) + table
```
",2013-06-15 00:12:19,,"numpy scalar types behave differently from python types with __len__, __getitem__, and custom operators",['unlabeled']
3424,open,juliantaylor,"current git head 75cdf3d82e
test_kind.TestKind.test_all fails on sparc64 (debian unstable, linux 2.6.32-5, big endian, 16 byte long double, no unaligned loads)

```
======================================================================
FAIL: test_kind.TestKind.test_all
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/python2.7/dist-packages/nose/case.py"", line 197, in runTest
    self.test(*self.arg)
  File ""/home/jtaylor/local/lib/python2.7/site-packages/numpy/f2py/tests/test_kind.py"", line 32, in test_all
    'selectedrealkind(%s): expected %r but got %r' %  (i, selected_real_kind(i), selectedrealkind(i)))
  File ""/home/jtaylor/local/lib/python2.7/site-packages/numpy/testing/utils.py"", line 42, in assert_
    raise AssertionError(msg)
AssertionError: selectedrealkind(16): expected 10 but got 16

----------------------------------------------------------------------
```
",2013-06-11 22:05:24,,test_kind.TestKind.test_all fails on sparc,"['00 - Bug', 'component: numpy.f2py']"
3423,open,jmozmoz,"I would like to use a class method as callback function/subroutine from Fortran when using Fortran code with the help of f2py. Therefore, it is necessary to somehow store the `self` ""pointer"" in Fortran, because it is necessary as first argument when calling a class method. How can this be accomplished? (If I use static class methods, so I do not need `self` everything works)

Here is an example which compiles but fails during runtime:

classCallback.py:

``` python
import classCallback as cb

class Test:
    def __init__(self):
        cb.callback.set_callback(self, self.callback)

    def callback(self, callbackID):
        print ""callback: "", callbackID

x = Test()
```

classCallback.f90:

``` fortran
MODULE callback
    INTERFACE
      SUBROUTINE callback_prototype( selfOut, callbackID )
        PROCEDURE( ), POINTER, INTENT(OUT) :: selfOut
        CHARACTER(*), INTENT(OUT) :: callbackID
      END SUBROUTINE callback_prototype
    END INTERFACE

    ! Callback procedure pointer
    PROCEDURE( ), POINTER :: f_ptr => NULL()
    PROCEDURE( ), POINTER :: self => NULL()

    contains
        subroutine set_callback(selfIn, func)
            IMPLICIT NONE
            EXTERNAL :: selfIn
!f2py call selfIn()
            EXTERNAL :: func
!f2py call func()

            self => selfIn
            f_ptr => func
            call test()
        end subroutine

        subroutine test()
          call f_ptr(self, ""callback1"")
        end subroutine
end module
```

The f2py command (using cygwin):

```
f2py -c --fcompiler=gnu95 --f90flags=""-O3 -march=native -ffast-math -funroll-loops"" -m classCallback classCallback.f90
```

This is the runtime error:

```
$ python classCallback.py
Call-back argument must be function|instance|instance.__call__|f2py-function but got instance.
Traceback (most recent call last):
  File ""classCallback.py"", line 10, in <module>
    x = Test()
  File ""classCallback.py"", line 5, in __init__
    cb.callback.set_callback(self, self.callback)
classCallback.error: failed in processing argument list for call-back selfin.
```

Obviously self is not a function/subroutine pointer but this was the only way to get it compiled or pass the Python type check. E.g. if I use `INTEGER(4)` for the ""pointer"" `self` it also fails:

```
$ python classCallback.py
Traceback (most recent call last):
  File ""classCallback.py"", line 10, in <module>
    x = Test()
  File ""classCallback.py"", line 5, in __init__
    cb.callback.set_callback(self, self.callback)
classCallback.error: classCallback.callback.set_callback() 1st argument (selfin) can't be converted to int
```
",2013-06-11 11:58:50,,ENH: Add class methods as callback function/subroutines from fortran in f2py,"['00 - Bug', 'component: numpy.f2py']"
3295,open,astrofrog,"For string arrays of type `S1` or `S2`, the default fill value in Numpy masked arrays (`N/A`) is not appropriate:

```
In [1]: from numpy import ma

In [2]: x = ma.array(['a', 'b', 'c'], dtype='S1', mask=[1,0,0])

In [3]: x.filled()
Out[3]: 
array(['N', 'b', 'c'], 
      dtype='|S1')

In [4]: x.get_fill_value()
Out[4]: 'N/A'
```

this causes issues for codes that assume the fill value is present in the filled array and indicated masked values. It's not clear to me what the fill value _should_ be, but I'd just thought I'd raise the question.

cc @taldcroft
",2013-04-30 20:50:34,,numpy.ma default fill value for strings not appropriate for S1 and S2,"['00 - Bug', 'component: numpy.ma', '54 - Needs decision', 'Proposal']"
3215,open,pavle,"This used to be possible in numpy 1.6 but isn't in 1.7, a bug or a feature?

dts = [datetime.datetime(2013,4,1) + i*datetime.timedelta(days=1) for i in range(10)]
npDts = np.array(dts)
## npDts.astype(datetime64)

TypeError                                 Traceback (most recent call last)
<ipython-input-6-c682ed3993b0> in <module>()
      1 dts = [datetime.datetime(2013,4,1) + i*datetime.timedelta(days=1) for i in range(10)]
      2 npDts = np.array(dts)
----> 3 npDts.astype(datetime64)

TypeError: Cannot cast datetime.datetime object from metadata [us] to [D] according to the rule 'same_kind'
",2013-04-09 19:33:53,,datetime.datetime to np.datetime64 conversion in astype,"['00 - Bug', 'component: numpy.datetime64']"
3207,open,hohlraum,"Trying to use fromfile() on a file opened within a zip archive with zipfile.ZipFile() always yields an error. 

```
import numpy as np
import zipfile
z=zipfile.ZipFile('archive.zip')
f=z.open('file.txt')
np.fromfile(f, count=1)

IOError: first argument must be an open file
```

This is with numpy version 1.6.1.
",2013-04-08 13:49:21,,numpy.fromfile cannot read from zipfile ,"['00 - Bug', 'component: numpy._core']"
3184,open,charris,"OK, the lead is a bit extreme. However, in python3 both genfromtxt and savetxt, and auxiliary functions expect bytes strings, generators returning bytes strings, and files opened in binary mode. That seems a bit extreme for functions advertised to deal with text and text files. Among other things, the file encoding can't be specified. I think what happened here is that turning everything into bytes looked like the easy way to deal with unicode, but it may have ended up papering over problems.
",2013-03-31 20:20:20,,np.genfromtxt hates text files,"['00 - Bug', 'component: numpy.lib']"
3143,open,FreddieWitherden,"Currently if one does:

``` python
f = open('test.npy', 'rb')
x = np.load(f, mmap_mode='r')
```

an exception is raised stating that:

`ValueError: Filename must be a string.  Memmap cannot use existing file handles.`

However, numpy.memmap is more than happy to work with files.  This restriction should be lifted if at all possible as it would be nice to be able to seek to a point in an open file and have numpy memmap this.
",2013-03-13 17:16:09,,Memmap cannot use existing file handles.,"['component: numpy._core', 'Proposal']"
3016,open,seberg,"The `__contains__` method is written to be used for a single array element. However for example in list of list, `__contains__` does a check more equivalent to subarrays. `in` must return a single boolean.

After [some discussion on the list](https://mail.python.org/pipermail/numpy-discussion/2013-February/065562.html) ([nabble](http://numpy-discussion.10968.n7.nabble.com/What-should-np-ndarray-contains-do-td32964.html)), there are three main possibilities:
1. The first item must be an element. That means that an array `a` for `a in b` will normally be a simple error. (As nathaniel mentioned on the list).
2. Do a list of list like comparison. I.e. `in` operates on the first dimension.
3. Do some kind of subarray matching (there are many different versions of this allowing different things)

Point 2. seems wrong, since arrays are not list of lists. Point 3. has some merit, it can go as far as allowing things similar to strings `'a' in 'cat'`, however there are some problems with the details. Point 1. is the simplest and safest solution. One problem with 2. is that for object arrays it can be not quite clear how to interpret for example a tuple/list.

At this time (there was not much discussion yet though), it seems that the best solution is to just raise an error (i.e. solution 1.). Finding subarrays is better suited for a dedicated function.
",2013-02-25 12:29:48,,Contains method is not consistent for subarrays,"['00 - Bug', 'Priority: high', 'component: numpy._core']"
2998,open,stevengj,"As recently [discussed on the mailing list](http://mail.scipy.org/pipermail/numpy-discussion/2013-February/065516.html), the NumPy API's array data-structure information is unnecessarily hard to access from non-C languages because the accessor functions `PyArray_NDIM` etcetera are defined either as macros or as inline functions, neither of which are exported for dynamic linking.

Please consider exporting (non-inline) versions of these functions (adding them to `PyArray_API` for dynamic linking).  e.g. you could call them `PyArray_ndim` etcetera.

(This is very similar to how, in the Python API, important macros like `Py_XDECREF` have equivalent exported functions `Py_DecRef` [""for runtime dynamic embedding""](http://docs.python.org/dev/c-api/refcounting.html).)
",2013-02-18 01:41:59,,need exported (PyArray_API) functions for Array API data-structure access,['Proposal']
2997,open,stevengj,"As recently [discussed on the mailing list](http://mail.scipy.org/pipermail/numpy-discussion/2013-February/065516.html), NumPy's API is unnecessarily tricky to link from a non-C language because the `numpy.core.multiarray._ARRAY_API` lookup table of API pointers (`PyArray_API` in C) has a meaning that can only be determined by parsing the `__multiarray_api.h` header file.

A simple improvement would be to export an `_ARRAY_API_NAMES` variable (or similar) as well in Python, a simple list of strings corresponding to the symbols (`PyArray_GetNDArrayCVersion` etc.) pointed to by the `ARRAY_API`/`PyArray_API` array.   This should hopefully be a straightforward modification to `generate_numpy_api.py` and related files.  Or even just export the variables in the `numpy/core/code_generators/numpy_api.py` file.
",2013-02-18 01:33:10,,export multiarray_api to numpy.core.multiarray,"['01 - Enhancement', '23 - Wish List', 'Proposal']"
2981,open,rgommers,"From http://projects.scipy.org/scipy/ticket/1187, comment of Pearu:

f2py supports user-defined functions that have multiple return values. These return values are expected to be returned as a single tuple object. So, the issue here is not a result of f2py bug but due to this f2py feature that is unintentionally triggered by the given example.

As a possible fix, we could have the f2py code to treat the returned tuples as ordinary sequences when the expected number of returned values is exactly 1. On the other hand, in the case of f2py misuse (returning 2 values when 1 is expected, for instance) the values will be silently converted to an array and may result even a more complicated issues to be analyzed.

I think the best fix is to document this f2py feature in the corresponding codes, that is, never return tuple as a single return value, and perhaps improve the error message giving a hint why the crash might have been occurred.
",2013-02-12 21:21:22,,f2py issue when returning a single tuple,"['00 - Bug', 'component: numpy.f2py']"
2814,open,tjanez,"Simple example with one missing value:

``` python
import numpy as np
import numpy.ma as ma
X = np.array([[2., 3.],  [None, 1.]])
X_m = ma.masked_array(X, mask=np.equal(X, None))
X_m.mean(axis=0)
```

It gives the following error and traceback:

``` python
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib64/python2.7/site-packages/numpy/ma/core.py"", line 4678, in mean
    result = dsum * 1. / cnt
  File ""/usr/lib64/python2.7/site-packages/numpy/ma/core.py"", line 3650, in __div__
    return divide(self, other)
  File ""/usr/lib64/python2.7/site-packages/numpy/ma/core.py"", line 1073, in __call__
    m = ~umath.isfinite(result)
TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule 'safe'
```

If I understand the code in [numpy/ma/core.py](https://github.com/numpy/numpy/blob/maintenance/1.6.x/numpy/ma/core.py) correctly, the problem is that the variable `result` from [line 1069](https://github.com/numpy/numpy/blob/maintenance/1.6.x/numpy/ma/core.py#L1069) has `dtype=object`, and subsequently the call to `umath.isfinite()`
fails because it doesn't support this type - [line 1073](https://github.com/numpy/numpy/blob/maintenance/1.6.x/numpy/ma/core.py#L1073).

I'm using Fedora 17 (x86_64) with:
numpy-1.6.2-1.fc17.x86_64
python-2.7.3-7.2.fc17.x86_64
",2012-12-13 15:08:12,,numpy.ma.mean over an axis doesn't work,"['00 - Bug', 'component: numpy.ma']"
2802,open,erg,"It would be nice if numpy raised a warning when people assigned into temporary arrays, since it's rather easy to end up with these accidentally and not realize what's going on, so people keep barking their shins on this. The original report below is one example; another would be

```
a.ravel()[:] = [...]
```

which will usually modify `a`... but sometimes, depending on `a`'s memory layout, it may be a no-op instead. Quite sneaky.

The way to do this would be to write a function like `PyArray_DefinitelyTemporary(arr)` that returned true iff _all_ entries in `arr`'s `.base` chain fulfilled the properties:
- is an ndarray
- with reference count exactly 1

Then in the Python entry points for numpy assignments -- `np.copyto`, `__setitem__`, ufunc `out=` parameters, etc. -- have some code like

```
if (PyArray_DefinitelyTemporary(in_arr)) {
  warn(""assignment to temporary value has no effect!"");
}
```

(NB: I say _Python_ entry points above, because any numpy C API entry points can legitimately take borrowed references to non-temporary arrays that will have reference count 1. So we need to be careful that these checks are done before we hit code that could be reached by C API users.)

---

Original report:

I know that I should be using slice notation instead of `np.copyto`, but I also feel like case #1 should throw an error.

```
The case that works:

import numpy as np
emp = np.empty(3)
# emp is array([  1.72723371e-077,   1.72723371e-077,   2.19630579e-314])
np.copyto(emp, np.arange(3))
# emp is array([ 0.,  1.,  2.])  # great, just like I expect


Case #1:

import numpy as np
emp = np.empty(3)
# emp is array([  1.72723371e-077,   1.72723371e-077,   2.19630579e-314])
np.copyto(emp+1, np.arange(3))  # want an error here, acts like no-op or worse
# emp is array([  1.72723371e-077,   1.72723371e-077,   2.19630579e-314])
```
",2012-12-10 00:05:24,,"Raise a warning when assigning to a temporary (fix described, patch needed)",['unlabeled']
2776,open,ghost,"@juliantaylor raised this in a pandas issue. 

The example from the ticket:

``` python
import numpy as np
import random
random.sample(np.array([1,2,3]),1)

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/user1/py33/lib/python3.3/random.py"", line 298, in sample
    raise TypeError(""Population must be a sequence or set.  For dicts, use list(d)."")
TypeError: Population must be a sequence or set.  For dicts, use list(d).
```

This occurs on 3.3 with 1.7.0rc1.dev-3a52aa0, and on 3.2 with 1.6.2.
2.7 is unaffected of course.

The relavent code from cpython/Lib/random.py:297

``` python
from collections.abc import Set as _Set, Sequence as _Sequence
def sample(self, population, k):
# ...
    if not isinstance(population, _Sequence):
        raise TypeError(""Population must be a sequence or set.  For dicts, use list(d)."")
```

I couldn't grep another location in the stdlib with a similar test, but lib2to3
did show an assumed equivalence:

```
lib2to3/fixes/fix_operator.py
5:operator.isSequenceType(obj)   -> isinstance(obj, collections.Sequence)
```

in 2.7

``` python
In [6]: operator.isSequenceType(np.array([1]))
Out[6]: True
```

but in 3.3/3.2

``` python
>>> isinstance(np.array([1]), collections.Sequence)
False
```
",2012-12-01 01:00:49,,ndarray should derive from collections.abc.Sequence?,['unlabeled']
2732,open,tyarkoni,"The behavior of cov and corrcoef seems somewhat counterintuitive. As far as I can tell, if the user passes an array_like y in (in addition to x), x and y are simply concatenated before computing the covariance or correlation matrix. It's not clear what the purpose of this is, since the user could presumably have just concatenated the two arrays before calling cov(). Moreover, it seems confusing given the behavior of standard correlation routines in R and Matlab, where the expected result if you pass in two 2D arrays x and y  is an M x N matrix (where M and N are the number of columns in x and y, respectively). Situations where one needs to correlate two matrices column-wise (or a matrix and a vector) are quite common; perhaps it would make sense to follow the same convention in numpy?
",2012-11-13 06:46:47,,counterintuitive argument handling in cov and corrcoef,"['01 - Enhancement', 'component: numpy.lib']"
2682,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/2090 on 2012-03-22 by @pv, assigned to unknown._

Consider this on Python 3:

```
import gzip
import numpy as np
f = gzip.open('foo.gz', 'wb')
f.write(b""1 2 3"")
f.close()
f = gzip.open('foo.gz', 'rb')
print(np.fromfile(f, sep=' '))
# -> [-1.]
```

On Python 2 this fails with an IOError.

The problem here is that `fromfile` seems to read the compressed stream. This is probably a bug in Python 3 --- `PyObject_AsFileDescriptor` when called on a `GzipFile` object does not fail (although there is no OS level file handle corresponding to the uncompressed stream!). As a workaround, `npy_PyFile_Dup` should probably check that the object passed in inherits from a suitable subclass in the io object hierarchy.
",2012-10-19 22:35:46,,`npy_PyFile_Dup` should check file object subclass (Trac #2090),"['00 - Bug', 'component: numpy._core']"
2667,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/2075 on 2012-03-07 by trac user lxop, assigned to unknown._

When I try to write to the transpose of an array, I get

`AttributeError: attribute 'T' of 'numpy.ndarray' objects is not writable`

which seems fine, but the write _has_ actually taken place.
In particular, this occurs with in-place operations (+= etc).

Test code:

```
a = np.array ([1,2,3]).reshape ((1,3))
b = np.array ([4,5,6]).reshape ((3,1))
a.T += b
```

The result is the above error is raised and the operation actually goes ahead anyway.  I would expect only one or the other.
",2012-10-19 22:35:14,,Writing to a transpose results in strange error (Trac #2075),"['00 - Bug', 'component: numpy._core']"
2657,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/2065 on 2012-02-26 by @wesm, assigned to unknown._

3000x slowdown observed:

```
In [25]: arr = np.random.randn(350000, 5)

In [26]: timeit arr.take(np.arange(5), axis=0)
100000 loops, best of 3: 2.86 us per loop

In [27]: arr = np.random.randn(350000, 5).copy('F')

In [28]: timeit arr.take(np.arange(5), axis=0)
100 loops, best of 3: 9.03 ms per loop
```
",2012-10-19 22:34:48,,Poor ndarray.take performance on Fortran order arrays (Trac #2065),"['01 - Enhancement', 'component: numpy._core', 'component: numpy.dtype']"
2629,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/2037 on 2012-02-03 by trac user risotto, assigned to unknown._

I've got installed on one machine (debian linux 6) numpy in 32bit, with virtual environment, and on the other machine is numpy in 64bit installed via virtual environment.

There's an issue, if in your file is an float value, but want to read it as integer. The example will be more apparent.

On 32bit:

```
>>> import sys
>>> sys.maxint
2147483647
>>> from StringIO import StringIO
>>> import numpy as np
>>> s = StringIO(""333 12.5 22"")
>>> data = np.genfromtxt(s, dtype=[('x', float), ('y', int), ('z', int)])
>>> data
array((333.0, 12, 22),
      dtype=[('x', '<f8'), ('y', '<i4'), ('z', '<i4')])
>>> s = StringIO(""333 12.5 22"")
>>> data = np.genfromtxt(s, dtype=[('x', float), ('y', long), ('z', int)])
>>> data
array((333.0, -1L, 22),
      dtype=[('x', '<f8'), ('y', '<i8'), ('z', '<i4')])
```

On 64bit:

```
>>> import sys
>>> sys.maxint
9223372036854775807
>>> from StringIO import StringIO
>>> import numpy as np
>>> s = StringIO(""333 12.5 22"")
>>> data = np.genfromtxt(s)
>>> data
array([ 333. ,   12.5,   22. ])
>>> s = StringIO(""333 12.5 22"")
>>> data = np.genfromtxt(s, dtype=[('x', float), ('y', int), ('z', int)])
>>> data
array((333.0, -1, 22),
      dtype=[('x', '<f8'), ('y', '<i8'), ('z', '<i8')])
>>> s = StringIO(""333 12.5 22"")
>>> data = np.genfromtxt(s, dtype=[('x', float), ('y', long), ('z', int)])
>>> data
array((333.0, 12L, 22),
      dtype=[('x', '<f8'), ('y', '<i8'), ('z', '<i8')])
```
",2012-10-19 22:33:46,,genfromtxt defaults to `loose=True` which can be confusing for floats parsed as integers (Trac #2037),"['00 - Bug', 'component: Other']"
2563,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/1970 on 2011-10-28 by trac user AlanFrankel, assigned to unknown._

Currently, the display for an array looks like this:

[[   0.      0.64    0.64 ...,   61.    278.      1.  ]
 [   0.      0.      0.   ...,    6.     21.      1.  ]
 [   0.      0.      0.   ...,    9.     23.      1.  ]
 ...,
 [   0.      0.      0.   ...,    5.     11.      0.  ]
 [   0.      0.      0.   ...,   11.     79.      0.  ]
 [   0.      0.      0.65 ...,    5.     40.      0.  ]]

It would be helpful if instead of the ellipses, we had the number of missing items:

[[   0.      0.64    0.64 {12x}   61.    278.      1.  ]
 [   0.      0.      0.   {12x}    6.     21.      1.  ]
 [   0.      0.      0.   {12x}    9.     23.      1.  ]
 {205x}
 [   0.      0.      0.   {12x}    5.     11.      0.  ]
 [   0.      0.      0.   {12x}   11.     79.      0.  ]
 [   0.      0.      0.65 {12x}    5.     40.      0.  ]]

Here, you can tell at a glance that the array is 211 rows by 18 columns. Otherwise, you need a separate command to list the array size.

It could be debated whether {12x} or {..12..} or <..12..> or something else would be best for symbolizing the omitted 12 items, but any of those choices would be fine for me.

I personally would always prefer this style of display to the one with ellipses. However, even if you made this a nondefault option, that would make me happy.
",2012-10-19 22:31:47,,have array list number of omitted items rather than ellipsis (Trac #1970),"['01 - Enhancement', 'component: Other']"
2547,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/1954 on 2011-09-26 by trac user azrael, assigned to @pearu._

In order to impliment exception handling for Fortran errors into a f2py wrapped function, I followed this advice:

[http://mail.scipy.org/pipermail/numpy-discussion/2009-January/039672.html]

I wanted this to work without editing the .pyf file, and got that by adding the following to the Fortran code:

```
!f2py callstatement (*f2py_func)( ... ); if (ierr == 1) PyErr_SetString(PyExc_ValueError, ""msg"")
!f2py callprotoargument ...
```

(The ... is just a replacement for actual code)

When I tried to compile this with

```
f2py -c -m m src.f
```

I got several warnings/errors like this

```
warning: implicit declaration of function ‘pyerr_setstring’
```

Obviously the case-lowering acted on this line. As far as I understand the User Guide this should not happen, since I didn't call f2py with '-h' or '--lower'. Nevertheless I added the '--no-lower', but still got the same warnings/errors.

Then I tried whether the added line will show up in the .pyf correctly, which it did, when I used '--no-lower' like this:

```
f2py --no-lower -m m -h m.pyf src.f
```

After generating the .pyf I can compile the module - **without** further editing in the .pyf!

```
f2py -c m.pyf src.f
```

---
1. Is this intended behaviour? Should it make a difference whether I generate the .pyf first and then compile or use the one-step way?
2. Is there a way to circumvent using ""callprotoargument""? I'd much rather have f2py do it's magic to guess the first line in callstatement and the callprotoargument, and then just add my piece of extra code. Can this be done? Something like a 'addtocallstatement' directive comes to mind ...
",2012-10-19 22:31:21,,f2py: --no-lower does not work in one-step compilation (Trac #1954),"['00 - Bug', 'component: numpy.f2py']"
2543,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/1950 on 2011-09-09 by trac user stereotype441, assigned to unknown._

This bug was discovered during the development of the Piglit OpenGL test suite (see https://bugs.freedesktop.org/show_bug.cgi?id=40697)

Numpy seems to define multiple classes called ""numpy.int32"", which are considered distinct by Python's isinstance() function.  This makes it unreliable to use isinstance() to figure out the type of a numpy numeric value.  For example, this is the behavior I see on Windows using using numpy 1.6.1:

```
Python 2.7.2 (default, Jun 12 2011, 15:08:59) [MSC v.1500 32 bit (Intel)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy
>>> numpy.version.version
'1.6.1'
>>> x = numpy.int32(5)
>>> y = numpy.abs(x)
>>> type(x)
<type 'numpy.int32'>
>>> type(y)
<type 'numpy.int32'>
>>> isinstance(x, numpy.int32)
True
>>> isinstance(y, numpy.int32)
False
>>> type(x) == type(y)
False
```

I would have expected the last two ""False"" outputs to be ""True"".  And when I run the same experiment on Fedora Linux (which is currently shipping numpy 1.5.1), that is indeed what I see.  Since I'm not running the same version of numpy on Linux and Windows, I'm not sure whether this problem is linked to numpy version or to platform.

I have seen similar problems with uint32.

You can see further evidence that there are multiple redundant numeric type classes by typing ""help(numpy)"" from the command prompt.  When I do this on Windows, I see duplicate copies of the following numeric classes: complex128, complex192, complex64, float16, float32, float64, float96, int16, int32, int64, int8, datetime64, timedelta64, uint16, uint32, uint64, and uint8.  Surprisingly, I see similar duplication even on Fedora Linux, even though it doesn't exhibit this bug.

This arose in our project because we were trying to use isinstance() to tell the type of a value that might be one of several numpy numeric types, or might by a Python built-in type.  We are planning to work around the problem by using isinstance(value, numpy.numeric) to detect whether the value is a numpy numeric type, and then using value.dtype to figure out which numeric type it is.  But it seems like this extra work shouldn't be necessary--isinstance(value, numpy.int32) ought to be sufficient to determine whether a value is a numpy 32-bit int.
",2012-10-19 22:31:10,,Redundant numeric type classes lead to unreliable behavior of isinstance() (Trac #1950),"['01 - Enhancement', 'component: numpy._core']"
2520,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/1927 on 2011-08-08 by @chrisjordansquire, assigned to unknown._

I have a record array with the following elements
```
np.array([(datetime.date(1991, 11,3), np.datetime64('NaT')),
(np.datetime64('NaT'),datetime.date(2001, 3, 10))],
dtype=[('date1', '<M8[D]'), ('date2', '<M8[D]')])
```
np.array_repr of the above array gives
```
array([(datetime.date(1991, 11, 3), None),
       (None, datetime.date(2001, 3, 10))], 
      dtype=[('date1', '<M8[D]'), ('date2', '<M8[D]')])
```
So the np.datetime64('NaT') are printed as None. This certainly isn't correct for a repr. 

Additionally, for a masked record array the repr drops the 'datetime.date' if there is a missing entry in the row. For example, if you let z be the array given above, and then do
```
w = np.ma.array(z)
w.mask[0] = (False, True)
```
Then np.array_repr(w) will yield

```
MaskedArray([(1991-11-03, --), (None, datetime.date(2001, 3, 10))], 
      dtype=[('date1', '<M8[D]'), ('date2', '<M8[D]')])
```

This is wrong because it gives 1991-11-03 instead of datetime.date(1991, 11, 3) in the [0,0] entry. 
",2012-10-19 22:30:23,,datetime masked array and record array repr bugs (Trac #1927),"['00 - Bug', 'component: numpy.ma', 'component: numpy.datetime64']"
2494,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/1901 on 2011-07-11 by @mwiebe, assigned to unknown._

As part of the missing value functionality, I have implemented a masked data copying and casting mechanism. It has all the hooks in place to be fast, but I have only implemented a slow wrapper around the unmasked routines. One example where it currently performs poorly is filling based on a mask:

```
In [1]: import numpy as np
In [2]: a = np.zeros((100,100,100))
In [3]: m = np.random.rand(100,100,100) > 0.5

In [4]: timeit np.copyto(a, 1, where=m)
100 loops, best of 3: 9.22 ms per loop

In [5]: timeit np.putmask(a, m, 1)
100 loops, best of 3: 6.02 ms per loop
```

To do this optimization,

1) Learn how dtype_transfer.c works, by reading its code and the documentation in comments. In particular, the function PyArray_GetDTypeTransferFunction is the main thing to understand. Its arguments are documented in private/lowlevel_strided_loops.h.

2) Learn how lowlevel_stided_loops.c.src works, just as for 1). In particular, understanding how zero strides, contiguous strides, and alignment affect the nature of the inner loop functions.

3) Create specialized inner loops for various masked transfer functions. Just specializing aligned data is probably ok. The important cases are likely:
- contiguous src, dst, and mask
- zero stride src, contiguous dst and mask
- zero stride src, general strided dst and mask
  This should be done for both straight data copies and cast operations.

4) Edit PyArray_GetMaskedDTypeTransferFunction to return these specialized masked loops where appropriate, analogously to how PyArray_GetDTypeTransferFunction does it.

5) Demonstrate that it's working with some before/after benchmarks of the different cases.
",2012-10-19 22:29:23,,"Speed up np.copyto with 'where=', and any routines using nditer in its masked mode (Trac #1901)",['component: documentation']
2486,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/1893 on 2011-07-04 by trac user smif1984, assigned to unknown._

The exception which is currently, (v1.6), raised when two non 
broadcastable arrays are summed is a ValueError exception. I propose to create a specific exception class, e.g. BroadcastError, to be more specific and give better control in exception catching?
",2012-10-19 22:29:00,,Broadcasting shape mismatch exception (Trac #1893),"['01 - Enhancement', 'component: numpy._core', 'Project']"
2481,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/1888 on 2011-06-30 by @brickZA, assigned to unknown._

I think it would be feasible to support multi-dimensional saving/loading quite simply by using multiple line-breaks for 'higher-dimensional' linebreaks.

```
d3D = numpy.array([[[111, 112], [121, 122]], 
                   [[211, 212], [221, 222]]])
```

could be written to text quite unambigiously as 

```
111 112
121 122
            <--- two line breaks indicate break in the 3rd dimension
211 212
221 222
```

Similarly, 3 line breaks could indicate a break in the 4th dimension.

Another option would be to have the option to write out text in nested-list format. I.e., simply write

[[[111, 112], [121, 122]], [[211, 212], [221, 222]]]

to the file.
",2012-10-19 22:28:48,,Multi-dimensionial array support for savetxt / loadtxt (Trac #1888),"['01 - Enhancement', 'component: numpy.lib']"
2457,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/1864 on 2011-06-10 by @bsouthey, assigned to unknown._

When using a float step but a integer dtype, np.arange appears to full the array with a constant value (first value in the range) rather than an integer array of different integers or giving an error. The following code illustrates the problem:

```
>>> import numpy as np
>>> np.__version__
'2.0.0.dev-a1e7be3'
>>> np.arange(0.,5.,0.5)
array([ 0. ,  0.5,  1. ,  1.5,  2. ,  2.5,  3. ,  3.5,  4. ,  4.5])
>>> np.arange(0.,5.,0.5, dtype=int)
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
>>> np.asarray(np.arange(0.,5.,0.5), dtype=int)
array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4])
```
",2012-10-19 22:27:50,,arange using float for step and integer dtype results in an array with all values being the same (Trac #1864),"['00 - Bug', 'component: numpy._core']"
2453,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/1860 on 2011-06-07 by trac user wieland, assigned to unknown._

I (and M. Wiebe) suggest a new function that generalizes the broadcasting that is so nicely implemented into 'einsum'.

Consider the following example:

```
>>> A = np.arange(25).reshape(5,5)
>>> B = np.arange(5)
>>> np.einsum('ij,j', A, B)
array([ 30,  80, 130, 180, 230])
```

Here, einsum takes the product of every element A_{ij}, multiplies with B_{j} and then sums over j leaving i fixed. So, two binary operations are at the heart of einsum, np.add and np.multiply. In this logic we could rewrite 'einsum' using a more general function 'broadcast_op',

```
einsum('ij,j', A, B) = broadcast_op('ij,j', A, B, oper=[np.add, np.multiply])
```

With this notation we can consider any kind of binary operation to replace np.add and np.multiply. As an example, consider np.add instead of np.multiply,

```
>>> broadcast_op('ij,j', A, B, oper=[np.add, np.add])
array([ 20, 45, 70, 95, 120])
```

Equivalent but more cumbersome to write (especially in higher dimensions (!)) is

```
>>> sum(A + B.reshape((1,5)),axis=1)
array([ 20, 45, 70, 95, 120])
```

Note that you are spared from any reshapes. One could also image to use 'np.power' instead of 'np.multiply' such that B is the vector of powers and we take all elements of A_{ij} to the power of B_j. In some sense, you can see this broadcasting as a generalization of 'reduce' to higher dimensions.
",2012-10-19 22:27:44,,"einsum with arbitrary operations: chaining ufuncs using einsums `'ij,j'` notation (Trac #1860)","['01 - Enhancement', 'component: numpy._core']"
2402,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/1809 on 2011-04-27 by trac user tfmoraes, assigned to unknown._

**Note**: see post below this for underlying problem.

Running the following code https://gist.github.com/929168 in Windows (I've tested only in XP) a AttributeError exception happens, here a more detailed traceback https://gist.github.com/944144 .

That code creates a new python process via multiprocessing to run a function, the parameter to this function is a memmap array created by the main process. This code works with the last numpy stable version in Linux and Mac OS X, but in Windows. Numpy version 1.4.1 runs correctly that code in Windows, Linux and Mac OS X.
",2012-10-19 22:25:34,,Instance attributes of ndarray subtypes get lost during pickling (Trac #1809),"['00 - Bug', 'component: numpy._core']"
2388,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/1795 on 2011-04-06 by @mwiebe, assigned to unknown._

These macros are supposed to help portably print different types, but they're currently not consistent. The points are
- PyString_FromFormat and PyOS_snprintf support different sets of formatting codes (the first is intended to be portable and consistent, the second calls the OS vsnprintf routine, according to the 2.7 documentation).
- Within the NumPy code base, NPY_INTP_FMT is used with PyString_Format, while the rest of them are used with PyOS_snprintf.
- In 2.6, PyString_Format doesn't support a long long formatting character, so on 64-bit windows, there is no mechanism to format the npy_intp type.
- The code also casts to long and uses %ld in some places. This would produce the wrong value when using extremely large arrays.
",2012-10-19 22:25:01,,Sort out the NPY_*_FMT printf-formatting macros (Trac #1795),"['00 - Bug', 'component: Other']"
2376,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/1783 on 2011-03-25 by @mwiebe, assigned to unknown._

For #2271, support was added for the format() function to numpy scalars. This support downcasts long doubles to doubles, so cannot print long doubles at their full precision. This should be fixed with custom formatting code in that case.
",2012-10-19 22:24:36,,Support full long double precision in __format__ (Trac #1783),"['00 - Bug', 'component: numpy._core']"
2348,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/1753 on 2011-03-03 by trac user bryancole, assigned to unknown._

If I create a memmap object:

```
>>> a = numpy.memmap(""foo"", mode=""w+"", shape=10)
```

then later I create a new array thus:

```
>>> b = numpy.empty_like(a)
```

then b is also a memmap object. However, even though the type of b is 'memmap', it has no _mmap attribute (as you'd expect, since empty_like cannot know what file object to use).

The broken memmap object b causes failures elsewhere in **array_finalise** as it tries to copy attributes from b (filename, offset, mode) which do not exist.

In summary there are TWO bugs here:

 1) empty_like (and friends) should not return objects of type memmap

 2) the memmap.**array_finalise** method is checking for the existence of the _mmap attribute. In non-memmap objects, this may exist but be set to None, therefore this method needs to check that both _mmap exists as an attribute and is not None.
",2012-10-19 22:23:25,,empty_like not passing across attributes of memmap objects (Trac #1753),"['00 - Bug', 'component: numpy._core']"
2339,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/1744 on 2011-02-17 by @fengy-research, assigned to unknown._

test case:

ar = zeros(dtype=[('pos', ('f4', 3)), ('vel', ('f4', 3))], shape=10)

savetxt(""ar"", ar)

TypeError: float argument required, not numpy.ndarray.
",2012-10-19 22:23:01,,savetxt doesn't handle recarrays with substructures. (Trac #1744),"['00 - Bug', 'component: numpy.lib']"
2310,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/1714 on 2011-01-15 by trac user bubla, assigned to unknown._

Hello,
I have made a patch that extends the `correlate` function so it can compute normalized cross-correlation now
See [http://en.wikipedia.org/wiki/Cross-correlation#Normalized_cross-correlation the Wikipedia article].

I have added documentation and simple doctest too, of course.
The patch is against the latest master Git
",2012-10-19 22:21:50,,normalized cross-correlation (Trac #1714),"['01 - Enhancement', 'component: numpy._core', 'Patch']"
2269,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/1673 on 2010-11-13 by trac user tom3118, assigned to unknown._

The ""numpy for matlab users"" suggests using
`nonzero(A)[0][0]`
to find the index of the first nonzero element of array A.

The problem with this is that A might be a million elements long and the first element might be zero.

This is an extremely common operation.  An efficient, built-in method for this would be very useful.  It also would easy people's transition from Matlab in which `find` is so common.
",2012-10-19 22:20:11,,first nonzero element (Trac #1673),"['01 - Enhancement', 'component: Other']"
2253,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/1657 on 2010-11-01 by trac user bevanj, assigned to @pierregm._

It seems that the use of np.ma.masked_invalid results in arrays printing precision of 12 regardless of the default setting or any subsequent setting.  Generating masked arrays does not seem to have this behaviour.

```python
In [1]: eg = np.random.random(1)
In [2]: eg_mask = np.ma.masked_array(eg)
In [3]: eg_mask_invalid = np.ma.masked_invalid(eg)
In [4]: eg
Out[4]: array([ 0.78592569])
In [5]: eg_mask
Out[5]:
masked_array(data = [ 0.78592569],
            mask = False,
      fill_value = 1e+20)

In [6]: eg_mask_invalid
Out[6]:
masked_array(data = [0.785925693305],
            mask = [False],
      fill_value = 1e+20)

In [7]: np.set_printoptions(precision=3)

In [8]: eg
Out[8]: array([ 0.786])
In [9]: eg_mask
Out[9]:
masked_array(data = [ 0.786],
            mask = False,
      fill_value = 1e+20)
In [10]: eg_mask_invalid
Out[10]:
masked_array(data = [0.785925693305],
            mask = [False],
      fill_value = 1e+20)
```",2012-10-19 22:19:32,,np.ma.masked_invalid and precision (Trac #1657),"['00 - Bug', 'component: numpy.ma']"
2230,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/1634 on 2010-10-11 by trac user foobaron, assigned to unknown._

aArray = numpy.fromfile(signalfile, numpy.dtype('u1'), number_of_bytes)

signalfile = <StringIO.StringIO instance at 0xa0008ec>1
-->
IOError: first argument must be an open file

used python version: 2.6.5
",2012-10-19 22:18:43,,numpy.fromfile does not accept StringIO object (Trac #1634),"['01 - Enhancement', 'component: numpy._core']"
2211,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/1615 on 2010-09-15 by trac user alefnula, assigned to unknown._

genfromtxt cannot handle the csv files that use quoting. For example:

""This is my text, that has a comma inside"",""Other value"",""3""
""Another text, with coma"",""More text, with comma"",5

This is a csv text where the delimiter is "","", but the values also contain "","".

Here is the pach that enables the user to specify the quoter (the quoting character). The default behaviour is the same as the old behaviour of genfromtxt function, but if the quoter is set, quoting is taken into account.

Patch is in the attachment.
",2012-10-19 22:17:46,,genfromtxt reading quoted csv files enhancement (Trac #1615),"['01 - Enhancement', 'component: numpy.lib', 'Patch']"
2060,open,thouis,"_Original ticket http://projects.scipy.org/numpy/ticket/1463 on 2010-04-24 by @pv, assigned to @pv._

Eigenvalue computations via `eig` should perhaps allow the user to optionally disable matrix balancing. The LAPACK expert driver DGEEVX supports such additional options, so adding the interface should be possible.

Balancing occasionally may cause some problems, such as in #2051, and having an option to turn it off could be useful in some special cases...
",2012-10-19 21:03:32,,ENH: Allow disabling balancing in `linalg.eig` (Trac #1463),"['01 - Enhancement', 'component: numpy.linalg']"
1991,open,thouis,"_Original ticket http://projects.scipy.org/numpy/ticket/1393 on 2010-02-09 by @lebigot, assigned to unknown._

An assignment such as 

```
my_array[0] = 5.4
```

does not produce a warning when my_array is an array of integers.

This is different from what

```
my_array[0] = 5.4j
```

does when assigning to an array of floats: a warning is issued, in this case (which I think is very much appropriate, as ""explicit is better than implicit"").

It would be better if both behaviors were consistent with each other (issue a warning or not, for both complex in float array, and float in integer array).  And I second issuing a warning, to be consistent with ""import this"". :)
",2012-10-19 21:00:30,,assignment of float to int array causes loss of decimal part (Trac #1393),"['01 - Enhancement', 'component: numpy._core']"
1914,open,thouis,"_Original ticket http://projects.scipy.org/numpy/ticket/1316 on 2009-12-07 by @mdboom, assigned to unknown._

This is a sensible operation.  It would be nice to make it work if possible.

```
In [25]: x.max()
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)

/wonkabar/data1/builds/betadrizzle/<ipython console> in <module>()

TypeError: cannot perform reduce with flexible type

In [26]: x.min()
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)

/wonkabar/data1/builds/betadrizzle/<ipython console> in <module>()

TypeError: cannot perform reduce with flexible type

In [27]:
```
",2012-10-19 20:57:10,,.max() and .min() don't work on arrays of type 'S' and 'U' (Trac #1316),"['00 - Bug', 'component: Other', 'Project']"
1905,open,thouis,"_Original ticket http://projects.scipy.org/numpy/ticket/1307 on 2009-11-28 by @thouis, assigned to unknown._

> > > from numpy import *
> > > **version**
> > > '1.3.0.dev5934'
> > > q = matrix(arange(9).reshape((3,3)))
> > > take(q, matrix([[0]]), axis=0)
> > > matrix([[0, 1, 2]])
> > > take(q, matrix([[0]]), axis=1)
> > > matrix([[0, 3, 6]])

Expected behavior for the last expression:

> > > take(q, [0], axis=1)
> > > matrix([[0],
> > >        [3],
> > >        [6]])
",2012-10-19 20:56:47,,take with axis=1 from matrix with matrix indices gives row instead of column (Trac #1307),"['00 - Bug', 'component: numpy._core']"
1903,open,thouis,"_Original ticket http://projects.scipy.org/numpy/ticket/1305 on 2009-11-26 by trac user tgirod, assigned to @pierregm._

when creating a view on a masked array with mask=False, changing the mask in the view won't propagate to the mask in the original array.

However, when the mask is already an array before the view is created, altering the mask in the view alters the mask in the original array.

this difference of behaviour can lead to troubles

```python

import numpy as np
import numpy.ma as ma

x = ma.zeros(25)
xv = x.reshape((5,5))

print x.mask

> False

print xv.mask

> False

xv.mask = np.identity(5)

print x.mask

> False

print xv.mask

> [[ True False False False False]
>  [False  True False False False]
>  [False False  True False False]
>  [False False False  True False]
>  [False False False False  True]]

```
",2012-10-19 20:56:44,,masked arrays and views (Trac #1305),"['00 - Bug', 'component: numpy.ma']"
1879,open,thouis,"_Original ticket http://projects.scipy.org/numpy/ticket/1281 on 2009-11-03 by @dwf, assigned to unknown._

I don't really know why this is happening (I've seen this 2028 value before, in an fwrite failure, on this same Ubuntu 6.06 version) but suffice it to say this error should probably be caught and an error message reflecting the true nature of the situation issued.

```
In [2]: x = num.load(""hActs.npy"")
1451583488 items requested but only 2028 read
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)

/ais/gobi1/gdahl/speech/babble/filtDatadebug/<ipython console> in <module>()

/nobackup/murray/bin/pylab-02/x86_64_GenuineIntel_6.06/Python-2.5.4/lib/python2.5/site-packages/numpy/lib/io.pyc in load(file, mmap_mode)
    193             return format.open_memmap(file, mode=mmap_mode)
    194         else:
--> 195             return format.read_array(fid)
    196     else:  # Try a pickle
    197         try:

/nobackup/murray/bin/pylab-02/x86_64_GenuineIntel_6.06/Python-2.5.4/lib/python2.5/site-packages/numpy/lib/format.pyc in read_array(fp)
    373 
    374         if fortran_order:
--> 375             array.shape = shape[::-1]
    376             array = array.transpose()
    377         else:

ValueError: total size of new array must be unchanged
```

As you can see the computation continues until it tries to assign array.shape. Ideally, the fact that fread() failed to return the correct number of items should raise an exception immediately (or as soon as possible from the Python code, if not from the C extension code).
",2012-10-19 20:55:50,,fread failure causes unintelligible error message (Trac #1281),"['00 - Bug', 'component: numpy.lib']"
1862,open,thouis,"_Original ticket http://projects.scipy.org/numpy/ticket/1264 on 2009-10-16 by @dalloliogm, assigned to @charris._

numpy's fromrecords raises an ""IndexError: list out of range"" when applied to an empty list.

I think the expected behaviour should be to generate an empty array, as happens for the array() function.

```
>>> import numpy
>>> numpy.rec.fromrecords([])
Traceback

IndexError: list index out of range
```


---

Summary by @seberg:

  * Error could should improved
  * As Mark also wrote, raising an error is correct `np.rec.fromrecords([], dtype={})` already works (which is already a bit dubious, since the shape of `(0,)` is guessed, could error as well, but larger change).",2012-10-19 20:55:08,,numpy.rec.fromrecords on an empty list (Trac #1264),"['00 - Bug', 'component: Other', 'defunct — difficulty: Intermediate']"
1860,open,thouis,"_Original ticket http://projects.scipy.org/numpy/ticket/1262 on 2009-10-15 by @dalloliogm, assigned to unknown._

If I understand correctly, genfromtxt should be the standard function to read csv files now.

The problem is that by default it tries to read all the fields of the file as if they were floats (because dtype=float by default).

So every time a newbie tries to read a csv file containing a column with strings, he obtains an array with many misterious NaN fields, unless he understand he has to set dtype to None (and this is not documented anywhere except for some mailing list messages).

Yesterday I gave an introductory talk to pylab and everybody was complaining about this, it is really weird to have to set this parameter manually.

For example, you can see that all the R's read.delim/csv etc.. infer the data type from the files, without having to specify it explicitely.

So, please: change the default value to None.
",2012-10-19 20:55:02,,genfromtxt: dtype should be None by default (Trac #1262),"['00 - Bug', 'component: numpy.lib', '54 - Needs decision']"
1858,open,thouis,"_Original ticket http://projects.scipy.org/numpy/ticket/1260 on 2009-10-12 by trac user roger, assigned to unknown._

The `convolve` and `correlate` functions appear to be much slower than their MATLAB 2009a equivalents.

The MATLAB command `xcorr(randn(1e6,1))` takes about 0.35s to execute, while the Python equivalent `x = randn(1e6);correlate(x, x)` takes more than a minute (then killed). MATLAB is also much faster for arrays of 1e5 elements.

`fftconvolve` in `scipy.signal` is even slower.

Tested with Python 2.6.3 and numpy 1.30 (x86) under x64 Windows 7 and x64 Ubuntu on an i7.
",2012-10-19 20:54:53,,Use FFT in np.correlate/convolve? (Trac #1260),"['01 - Enhancement', 'component: numpy._core']"
1812,open,thouis,"_Original ticket http://projects.scipy.org/numpy/ticket/1214 on 2009-09-01 by @timmie, assigned to @pierregm._

In R_project:
# Note: values is my dataset

```
### masking zeros
values_mask=ifelse(values==0, NA, (values))

# http://stat.ethz.ch/R-manual/R-patched/library/stats/html/ecdf.html
plot(ecdf(values_mask), main=""Emprical cumulative distribution function"",
do.points=F, xlab='Data Values')
```

There masking the zeros has an effect.
With numpy not.
",2012-10-19 20:52:47,,np.hist: needs to be enabled for masked arrays (Trac #1214),"['01 - Enhancement', 'component: numpy.ma']"
1740,open,thouis,"_Original ticket http://projects.scipy.org/numpy/ticket/1142 on 2009-06-19 by @inducer, assigned to unknown._

numpy could really do a better job here by propagating the .real to the elements of the object array:

```
Python 2.5.4 (r254:67916, Feb 18 2009, 03:00:47) 
[GCC 4.3.3] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy as n
>>> n.__version__
'1.3.0'
>>> a = n.array(5+7j)
>>> b = n.array([a], dtype=object)
>>> b.real
array([(5+7j)], dtype=object)
>>> n.real(b)
array([(5+7j)], dtype=object)
```


---

Summary by @seberg:

Object arrays could call `.real` on all objects, but that would not return a view as typically promised by `arr.real`. It seems that moving forward here may require a new function that returns a copy?",2012-10-19 20:49:32,,.real doesn't work on object arrays (Trac #1142),"['00 - Bug', 'component: numpy._core', '54 - Needs decision']"
1737,open,thouis,"_Original ticket http://projects.scipy.org/numpy/ticket/1139 on 2009-06-17 by @huard, assigned to unknown._

If a tuple is passed to the formats argument to rec.fromarrays for example, the following error is raised: 

```
/usr/local/lib/python2.5/site-packages/numpy-1.4.0.dev7045-py2.5-linux-x86_64.egg/numpy/core/records.pyc in fromarrays(arrayList, dtype, shape, formats, names, titles, aligned, byteorder)
    506         _names = descr.names
    507     else:
--> 508         parsed = format_parser(formats, names, titles, aligned, byteorder)
    509         _names = parsed._names
    510         descr = parsed._descr

/usr/local/lib/python2.5/site-packages/numpy-1.4.0.dev7045-py2.5-linux-x86_64.egg/numpy/core/records.pyc in __init__(self, formats, names, titles, aligned, byteorder)
    110     """"""
    111     def __init__(self, formats, names, titles, aligned=False, byteorder=None):
--> 112         self._parseFormats(formats, aligned)
    113         self._setfieldnames(names, titles)
    114         self._createdescr(byteorder)

/usr/local/lib/python2.5/site-packages/numpy-1.4.0.dev7045-py2.5-linux-x86_64.egg/numpy/core/records.pyc in _parseFormats(self, formats, aligned)
    124                 formats.append('')
    125             formats = ','.join(formats)
--> 126         dtype = sb.dtype(formats, aligned)
    127         fields = dtype.fields
    128         if fields is None:

TypeError: data type not understood
WARNING: Failure executing file: <load.py>
```

If the tuple is converted into a list, then everything works fine. 
",2012-10-19 20:49:26,,record array: passing formats as tuple raises error (Trac #1139),"['00 - Bug', 'component: numpy._core']"
1725,open,thouis,"_Original ticket http://projects.scipy.org/numpy/ticket/1127 on 2009-06-05 by @FrancescAlted, assigned to unknown._

The dtype object seems to be shared after a copy of a structured array is done.  This is inconvenient when you change properties in one of arrays and these are propagated to the other.

The next snippet exposes the problem:

```
In [21]: dt = np.dtype('f4,f8')

In [22]: ra = np.empty(1, dt)

In [23]: ra.dtype
Out[23]: dtype([('f0', '<f4'), ('f1', '<f8')])

In [24]: rb = ra[:]

In [25]: ra.dtype.names = ('float', 'double')

In [26]: ra.dtype
Out[26]: dtype([('float', '<f4'), ('double', '<f8')])

In [27]: rb.dtype
Out[27]: dtype([('float', '<f4'), ('double', '<f8')])  # not the original!
```
",2012-10-19 20:48:54,,Make the dtype object immutable and not coerce other types when compared (hashable requirements) (Trac #1127),"['00 - Bug', 'component: numpy._core', 'component: numpy.dtype']"
1640,open,thouis,"_Original ticket http://projects.scipy.org/numpy/ticket/1042 on 2009-03-09 by @cournape, assigned to unknown._

For example:

```
#!python
import numpy as np
a = np.array([1, 2, 3])
np.fill(a, 1.3) # 1.3 converted to 1
```

Clip is another interesting case...
",2012-10-19 20:44:44,,"clip, fill and co behavior when the fill/clipping values cannot be casted meaningfully to the input (Trac #1042)","['00 - Bug', 'component: Other']"
1536,open,thouis,"_Original ticket http://projects.scipy.org/numpy/ticket/938 on 2008-10-18 by trac user smoerz, assigned to unknown._

choose is much slower in numpy than in numarray, and even more if an
output array is specified, as these tests show:

```
import timeit

setups = {
    'numarray': """"""
import numarray as N

n1, n2 = 4, 1000000
a1 = N.arange(n1*n2, type='Float64', shape=(n1,n2))
a2 = -N.arange(n1*n2, type='Float64', shape=(n1,n2))
a3 = -N.arange(n1*n2, type='Float64', shape=(n1,n2))
b1 = N.arange(n2, type='Float64')
b2 = -N.arange(n2, type='Float64')
b3 = -N.arange(n2, type='Float64')
c = N.remainder(N.arange(n2, type='Int32'),2)
"""""",
    'numpy': """"""
import numpy as N

n1, n2 = 4, 1000000
a1 = N.arange(n1*n2, dtype='Float64').reshape((n1,n2))
a2 = -N.arange(n1*n2, dtype='Float64').reshape((n1,n2))
a3 = -N.arange(n1*n2, dtype='Float64').reshape((n1,n2))
b1 = N.arange(n2, dtype='Float64')
b2 = -N.arange(n2, dtype='Float64')
b3 = -N.arange(n2, dtype='Float64')
c = N.remainder(N.arange(n2, dtype='Int32'),2)
""""""
    }

stmta = ""N.choose(c, (a1, a2))""
stmtao = ""N.choose(c, (a1, a2), a3)""
stmtb = ""N.choose(c, (b1, b2))""
stmtbo = ""N.choose(c, (b1, b2), b3)""


timeit.Timer(setup=setups['numarray'], stmt=stmta).repeat(3,100)
[3.3187780380249023, 3.2966721057891846, 3.3234250545501709]

timeit.Timer(setup=setups['numpy'], stmt=stmta).repeat(3,100)
[14.842453002929688, 14.833296060562134, 14.836632966995239]

timeit.Timer(setup=setups['numarray'], stmt=stmtao).repeat(3,100)
[3.1973719596862793, 3.2031948566436768, 3.2093629837036133]

timeit.Timer(setup=setups['numpy'], stmt=stmtao).repeat(3,100)
[17.546916007995605, 17.548220157623291, 17.536314010620117]

timeit.Timer(setup=setups['numarray'], stmt=stmtb).repeat(3,100)
[0.6694338321685791, 0.66939401626586914, 0.67307686805725098]

timeit.Timer(setup=setups['numpy'], stmt=stmtb).repeat(3,100)
[3.7615809440612793, 3.7627589702606201, 3.7547731399536133]

timeit.Timer(setup=setups['numarray'], stmt=stmtbo).repeat(3,100)
[0.67037606239318848, 0.67186903953552246, 0.66994881629943848]

timeit.Timer(setup=setups['numpy'], stmt=stmtbo).repeat(3,100)
[4.4750981330871582, 4.4650890827178955, 4.4679431915283203]
```
",2012-10-19 20:40:00,,choose() much slower than in numarray (Trac #938),"['00 - Bug', 'component: numpy._core']"
1530,open,thouis,"_Original ticket http://projects.scipy.org/numpy/ticket/932 on 2008-10-14 by trac user xamusk, assigned to unknown._

Convolve and other functions which depend on convolve-like operations, like acorr, xcorr (those from matplotlib), etc, could use a new method to operate only on one side of the ""full"" mode.

So, besides mode being ""full"", ""same"" and ""valid"", there could be a ""half"" mode (name only as a proposition) that could compute only one side of the function (lag>=0) so that it would return only len(x) elements instead of 2*len(x)-1.

This is particularly useful with the acorr function, for which a mode like that could compute the result much faster and represent only the wanted elements, much like Matlab's autocorr().

Actual behavior in matlab: autocorr(x)[[BR]]
What is currently needed to do in numpy/pylab:[[BR]]
a=acorr(x-x.mean(),normed=True)[1][len(x)-1:][[BR]]
close()[[BR]]
plot(a,marker='.')[[BR]]
# And that doesn't count doing fancy markings in the plot like the matlab ones.
",2012-10-19 20:39:44,,Add new convolve method for faster computation of even functions (Trac #932),"['01 - Enhancement', 'component: numpy._core']"
1076,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/478 on 2007-03-26 by @FrancescAlted, assigned to unknown._

Hi,

I find this to be incosistent:

```
In [134]:1. == numpy.array(1.)
Out[134]:True
# However...
In [135]:numpy.rec.array([[1.]], dtype=[('var1', 'f8')])
Out[135]:
recarray([(1.0,)],
      dtype=[('var1', '<f8')])
In [136]:numpy.rec.array([[numpy.array(1.)]], dtype=[('var1', 'f8')])
Out[136]:
recarray([[(1.0,)]],
      dtype=[('var1', '<f8')])
```

out[135] has a different level of nesting than out[136], but they should be equal.

In the same way:

```
In [138]:numpy.rec.array([[1.]], dtype=[('var1', 'f8', (2,))])
Out[138]:
recarray([(array([ 1.,  1.]),)],
      dtype=[('var1', '<f8', (2,))])
In [139]:numpy.rec.array([numpy.array([1.])], dtype=[('var1', 'f8', (2,))])
Out[139]:
recarray(([1.0, 1.0],),
      dtype=[('var1', '<f8', (2,))])
```

Why the difference?
",2012-10-19 18:53:35,,numpy.rec.array is inconsistent on objects (Trac #478),"['00 - Bug', 'component: numpy._core']"
835,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/237 on 2006-08-07 by trac user martin_wiechert, assigned to unknown._

to the very least <operator>.reduceat (a, I) should accept len (a) in I

usage example:

```
def weighted_histo (binborders, data):
    sd = sort (data)
    I = sd.searchsorted (binborders)
    # Now elements of I may range between 0 and len (sd).
    return add.reduceat (sd, I)
```
",2012-10-19 18:15:44,,reduceat should handle outlier indices gracefully (Trac #237),"['01 - Enhancement', 'component: numpy._core']"
834,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/236 on 2006-08-07 by trac user martin_wiechert, assigned to unknown._

<operator>.reduceat does not handle repeated indices correctly. When an index is repeated the neutral element of the operation should be returned. In the example below [0, 10], not [1, 10], is expected.

```
In [1]:import numpy

In [2]:numpy.version.version
Out[2]:'1.0b1'

In [3]:a = numpy.arange (5)

In [4]:numpy.add.reduceat (a, (1,1))
Out[4]:array([ 1, 10])
```
",2012-10-19 18:15:41,,reduceat cornercase (Trac #236),"['01 - Enhancement', '23 - Wish List', 'component: numpy._core']"
823,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/225 on 2006-08-03 by @stefanv, assigned to @teoliphant._

Under r2944:

```
In [22]: N.bincount(N.array([1],N.uint16))
Out[22]: array([0, 1])





In [23]: N.bincount(N.array([1],N.uint32))
---------------------------------------------------------------------------
exceptions.TypeError                                 Traceback (most recent call last)

/home/stefan/work/scipy/hough/<ipython console>

TypeError: array cannot be safely cast to required type
```
",2012-10-19 18:15:12,,bincount does not accept input of type > N.uint16 (Trac #225),"['00 - Bug', 'component: numpy._core', 'Patch']"
641,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/2188 on 2012-07-23 by @yarikoptic, assigned to unknown._

tested against current master (present in 1.6.2 as well):

If with python2.x series it works ok, without puking:

```
$> python2.7 -c 'import numpy as np; print repr(repr(np.unique(np.array([1,2, None, ""str""]))))' 
'array([None, 1, 2, str], dtype=object)'
```

NB I will report a bug on **repr** here separately if not yet filed

it fails with python3.x altogether:

```
$> python3.2 -c 'import numpy as np; print(repr(repr(np.unique(np.array([1,2,None, ""str""])))))'
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python3.2/dist-packages/numpy/lib/arraysetops.py"", line 194, in unique
    ar.sort()
TypeError: unorderable types: int() > NoneType()
```

whenever IMHO it must operate correctly -- semantic of unique() action should not imply ability to sort the elements
",2012-10-19 15:10:50,,python3: regression for unique on dtype=object arrays with varying items types (Trac #2188),"['00 - Bug', 'component: numpy._core']"
630,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/2176 on 2012-06-28 by @endolith, assigned to unknown._

There's no simple way to make a sequence based on step size that includes both endpoints.  For instance, to produce the sequence [1.0, 1.1, 1.2, 1.3, 1.4, 1.5], you'd have to do something clunky like:

```
linspace(1.0, 1.5, (1.5-1.0)/0.1+1)
```

or

```
arange(1.0, 1.5+0.01, 0.1)
```

it would be more convenient if you could just say

```
linspace(1.0, 1.5, step=0.1)
```

or maybe

```
arange(1.0, 1.5, 0.1, endpoint=True)
```

Of course, `step` could not be specified at the same time as `num`.
",2012-10-19 15:10:24,,Add step parameter to linspace (or endpoint parameter to arange) (Trac #2176),"['01 - Enhancement', 'component: numpy._core']"
626,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/2172 on 2012-06-19 by trac user lmeyn, assigned to unknown._

I'm trying to read several data files with column headers using numpy.genfromtxt. The issue is that some of the data files only have one row of data and squeeze operation  used in genfromtxt returns an object with no rows (shape = () ). I would suggest adding an argument, squeeze=True, and the replace the last three lines

```
if unpack:
    return output.squeeze().T
return output.squeeze()
```

with:

```
if squeeze:
    output = output.squeeze()
if unpack:
    return output.T
return output
```

This would keep the current behavior if the squeeze argument isn't specified, yet allow the squeeze operation to be avoided if desired.
",2012-10-19 15:10:18,,npyio.py: Make .squeeze() an option for genfromtxt() (Trac #2172),"['00 - Bug', 'component: numpy.lib']"
619,open,numpy-gitbot,"_Original ticket http://projects.scipy.org/numpy/ticket/2163 on 2012-06-15 by trac user thatistosay, assigned to unknown._

dotblas_matrixproduct() contains the comment ""This function doesn't handle dimensions greater than 2"" and calls PyArray_MatrixProduct2() for these cases. This means BLAS is never used for calls to dot() with arguments of ndim>2!! In more detail...

If I want to contract a pair of tensor indices for ndim=3 such that 

```
A = np.rand(d,D,D); B = np.rand(d,D,D)
AB[s,i,t,j] == sum(A[s,i,:], B[t,:,j])
```

then currently, although it can be done in a single line

```
res = sp.dot(A,B)
```

it can often be done much faster with explicit (python!) loops

```
res = np.zeros((d,d,D,D))
for s in xrange(d):
    for t in xrange(d):
        np.dot(A[s], B[t], out=res[s,t])
res = np.rollaxis(res, 2, 1)
```

..assuming dot() is using optimized BLAS for ndim=2, and the dimensions are large enough so that calling BLAS is worth it. 
In general, reproducing the behaviour of dot() for ndim>2 is just a matter of calling GEMM in loops as above and then calling rollaxis() once.

I therefore propose doing this within _blasdot.c as far as possible (to eliminate the use of python loops) so that ndim>2 dot(), and tensordot(), can benefit from BLAS.

Some comparisons of the two methods above (attached script):

```
dtype=complex128
AB[s,i,t,j] = sum(A[s,i,:], B[t,:,j])

A.shape = (16, 512, 512); B.shape = (16, 512, 512)
looping over 2D dot() vs. 3D dot(): 24% (about 4 times faster)

A.shape = (20, 64, 64); B.shape = (20, 64, 64)
looping over 2D dot() vs. 3D dot(): 35%

A.shape = (20, 48, 32); B.shape = (20, 32, 48)
looping over 2D dot() vs. 3D dot(): 45%

A.shape = (32, 32, 16); B.shape = (32, 16, 32)
looping over 2D dot() vs. 3D dot(): 82%

A.shape = (64, 10, 8); B.shape = (64, 8, 10)
looping over 2D dot() vs. 3D dot(): 158% (slow python loops..)
```

(this was on a 4-core i7 system using ATLAS under heavy load)
",2012-10-19 15:10:06,,BLAS matrix product (dot) never used for ndim > 2 (tensordot does not use BLAS) (Trac #2163),"['01 - Enhancement', 'component: numpy._core']"
389,open,RONNCC,"I have a file (exactly);

```
# Data from 'Naive Bayes Classifier example' by Eric Meisner November 22, 2003 
# http://www.inf.u-szeged.hu/~ormandi/teaching/mi2/02-naiveBayes-example.pdf 
Color Type Origin Stolen
Red Sports Domestic Yes
Red Sports Domestic No
Red Sports Domestic Yes
Yellow Sports Domestic No
Yellow Sports Imported Yes
Yellow SUV Imported No
Yellow SUV Imported Yes
Yellow SUV Domestic No
Red SUV Imported No
Red Sports Imported Yes
```

if i give it 

```
a = genfromtxt(filename,dtype=None,names=True,comments='#',skip_header=2)
print a
print a.dtype
```

it gives:

```
    [('Red', 'Sports', 'Domestic', 'Yes') ('Red', 'Sports', 'Domestic', 'No')
    ('Red', 'Sports', 'Domestic', 'Yes')
    ('Yellow', 'Sports', 'Domestic', 'No')
    ('Yellow', 'Sports', 'Imported', 'Yes')
    ('Yellow', 'SUV', 'Imported', 'No') ('Yellow', 'SUV', 'Imported', 'Yes')
    ('Yellow', 'SUV', 'Domestic', 'No') ('Red', 'SUV', 'Imported', 'No')
    ('Red', 'Sports', 'Imported', 'Yes')]
    [('Color', '|S6'), ('Type', '|S6'), ('Origin', '|S8'), ('Stolen', '|S3')]
```

if I only do: 

```
a = genfromtxt(filename,dtype=None,names=True,comments='#')
print a
print a.dtype
```

I get:

```
ValueError: Some errors were detected !
    Line #3 (got 4 columns instead of 1)
    Line #4 (got 4 columns instead of 1)
    Line #5 (got 4 columns instead of 1)
    Line #6 (got 4 columns instead of 1)
    Line #7 (got 4 columns instead of 1)
    Line #8 (got 4 columns instead of 1)
    Line #9 (got 4 columns instead of 1)
    Line #10 (got 4 columns instead of 1)
    Line #11 (got 4 columns instead of 1)
    Line #12 (got 4 columns instead of 1)
    Line #13 (got 4 columns instead of 1)
```

Per my understanding it should be automatically skipped the beginning as those are marked as comments.

Thanks.
",2012-08-18 21:22:46,,GenFromText should be able to read field names from first uncommented line,"['00 - Bug', 'component: numpy.lib']"
368,open,njsmith,"If you pass a 'long' integer greater than sys.maxint to np.sqrt, then it blows up with a confusing error message:

``` python
>>> np.__version__
'1.8.0.dev-1234d1c'
>>> np.sqrt(sys.maxint)
3037000499.9760499
>>> np.sqrt(sys.maxint + 1)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: sqrt
```

This was noticed because it breaks SciPy's kendalltau function: http://mail.scipy.org/pipermail/scipy-user/2012-July/032652.html

It was hard to diagnose because the error message is so misleading.

It looks like what's going on is that ndarray conversion on very large 'long' objects gives up and simply returns an object array. So... that seems reasonable, not sure what else we could do:

``` python
>>> np.asarray(sys.maxint + 1)
array(9223372036854775808L, dtype=object)
```

And then when called on an object array, np.sqrt does its weird ad-hoc fallback thing, and tries calling a .sqrt() method on each object. Which of course doesn't exist, since it's just something we made up. (But this is where the confusing error message comes from.) In fact, our handling of object arrays is pretty broken all around -- we can't even take the square root of float objects:

``` python
>>> np.sqrt(np.array(1.0, dtype=object))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: sqrt
```

The math module versions of sqrt and friends accept long integers:

``` python
>>> math.sqrt(sys.maxint + 1)
3037000499.97605
>>> math.cos(sys.maxint + 1)
0.011800076512800236
```

Mostly this just works by calling PyFloat_AsDouble, which goes via the **float** method if defined (as it is for longs). However, the math module does have special code for longs in some cases (log in 2.7, maybe more in future versions, who knows):

``` python
>>> math.sqrt(sys.maxint ** 100)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
OverflowError: long int too large to convert to float
>>> math.log(sys.maxint ** 100)
4366.827237527655
```

So in conclusion: np.sqrt and friends, when operating on object arrays, should fall back to the stdlib math functions. (This would be in addition to the current .sqrt method fallback. I guess the current fallback should probably be tried, since anyone defining our ad-hoc methods is presumably doing so specifically because they want numpy to respect that.)
",2012-07-29 10:25:12,,math functions fail confusingly on long integers (and object arrays generally),"['00 - Bug', 'component: numpy.ufunc', 'component: numpy.dtype']"
